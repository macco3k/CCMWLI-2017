Socrates' statement is an admonition to others that they should be more introspective, that they should look inside themselves to see what is good or bad, what kind of values one has and whether one meets them. 

But this is all in the context of others judging -him-. This is simply a more indirect way of pointing out hypocrisy.

(forgive the scripture but...) NIV Matthew 7: 


  1Do not judge, or you too will be judged. 2For in the same way you judge others, you will be judged, and with the measure you use, it will be measured to you.
  
  3“Why do you look at the speck of sawdust in your brother’s eye and pay no attention to the plank in your own eye?


People are judging Socrates and he is just turning the tables back on to them.

The context of the hypocrisy is that people (in all cultures) are already examining others to such a degree that they judge them left and right without bothering to do so to themselves.

As to more directly address the question, if you are not examining and judging others you won't be able to learn from their mistakes or successes. To deny examining others wold be somewhat unfeeling and distant from reality. The trick is not to be hypocritical or arrogant (which are both lesser failures than outright saying and doing bad things). It is the extent and pettiness of some judgments which earn the admonition that they should look into their own hearts too.
Two separate things:


The difficulty with the example of switches is simply that the mapping between the real world example (operation of switches) and the logical symbols is faulty. There is a a more complicated relationship in the switch configuration that is not captured by the logical statements.
as to an intuition for 'material implication' or the logical 'if-then' P -> Q, consider the truth-valuation and how upset you might be given the values of P and Q.


So suppose I claim P->Q. How upset will you be, how trustworthy do you think I am if:
(suppose P is "it rains tonight", and Q is "I'll go to the movies with you")


P is true and Q is true: You won't be upset at all, it's what you'd expect. If P occurs you'd expect Q to occur
P is true and Q is false: You'd be very upset. If P happens, you expect Q to happen, and when it doesn't, you should think I lied (it's raining and I didn't go to the movies with you means I'm an untrustworthy liar).
P is false and Q is true: hm...weird, so we went to the movies without it raining. I didn't say what I would do if it did not rain, so going to the movies is just fine, I haven't lied about it.
P is false and Q is false: also weird, but same reasoning, I didn't make any claims about what would happen if it did not rain, so not going, though not great, doesn't make me out to be a liar.


The truth value explanation of material implication may lead to those 'paradoxes' like 'ex falso quodlibet', and it is somewhat stipulative (and, by that, contrary to natural expectations of 'if-then') but the above intuition, when applied to independent statements, makes consistent sense.
This is a very, very good question, and a discussion that is a critical cornerstone in formulating and ethic which governs social interactions among men.  The conclusions that we reach regarding social ethic will apply from the smallest interaction between to neighbors to the largest policy decisions implemented by governments.

We all can agree that "violence" is generally undesirable and unpleasant.  I believe it is critical to distinguish between the INITIATION of violence, and the use of defensive or retaliatory force.  It is, in my view, always unjustified to INITIATE violence / force against another person, specifically, against another person's life, liberty, or property.  It is, however, appropriate to engage in a proportional use of defensive or retaliatory force.

I highly recommend a viewing of the following short flash animation created by economics professor Ken Schooland:

http://www.isil.org/resources/philosophy-of-liberty-english.swf http://www.isil.org/resources/philosophy-of-liberty-english.swf
There isn't anything wrong with justifying a claim that Peano Arithmetic is either inconsistent or incomplete by reference to Gödel's Incompleteness Theorems; the claim is a direct application of the first Incompleteness Theorem.

Gödel showed that any formal system of adequate expressive power for doing formal arithmetic was either inconsistent or incomplete in the sense that there would be sentences in the language of that system which that system could neither prove nor refute. 

Tarski's theorem showed that no consistent formal language could define its own truth predicate. A a special case of that, no consistent formal system of arithmetic can include a predicate which applies to all and only the true sentences of arithmetic in the language of that formal system (and, on the intended interpretation of that language).

Smullyan's point, as I understand it, is that: 

1) With the result that arithmetical truth cannot be defined in a system of arithmetic, Tarski "almost" got Gödel's results. 

2) Much of the (somewhat sloppy) philosophical discussion about Gödel's theorems takes them to be showing that there are unprovable arithmetical truths. However, Gödel's proofs of his theorems were expressed purely syntactically in that they made no reference to truth but only to formal provability within a purely syntactically defined formal system. Indeed since Tarski gave the first satisfactory definition of truth for formal systems in 1936, whereas Gödel's results were published in 1931, there was no formally tractable notion of truth available to Gödel. Essentially, Gödel found a clever way in which to interpret arithmetical sentences as being about formal proofs in arithmetic and constructed a sentence that "said" of itself that it was not provable within the system of arithmetic. If the system is consistent that sentence will be true. However, Gödel's conclusion was not that there were unprovable truths; rather it was that every system adequate for arithmetic is either inconsistent or would have arithmetical sentences the system did not decide. This in no way invokes a concept of truth. 

3) The point of Gödel's theorems is that arithmetic is not mechanistically decidable. Tarski's point is that no consistent system can define a truth predicate for the language in which it is articulated. Of the two, Tarski's seems to have richer philosophical import. (For instance, no formal model of English could define "true in English" while Gödel's results do not seem to apply to English at all.)

I am unaware of any example "proof" where appeal is made to Gödel that really ought be made to Tarski in the way your questions asks. (But, it is a big world.) 
Now, I am not really familiar with the matter, although I had already encountered similar discussions. I would say that certain interpretations of religion suggest something similar to what you pointed out, and I have read several books containing a similar messages. Whether an entire society based on such concept exists, I would say no. Not because of the concept itself, but simply because finding one society in which every single member has the exact same view of the world is seemingly impossible, it would basically mean a group of clones. Even within the most strict assemblies, members still have maintain certain degree of divergence on their views, whether or not they are aware of it, this is a different story.
Yes it's true, the http://en.wikipedia.org/wiki/Ludwig_Wittgenstein#Completion_of_the_Tractatus Wikipedia article on Wittgenstein refers to Bartley, that refer to his years in the Austrian army, being captured and personal tragic circumstances. His brother comitting suicide, his close friend (and arguably his lover) dying in a plane crash. Ray Monk describes in his 'Ludwig Wittgenstein: The Duty of Genius' as an ethical path, going from growing up in Vienna amongst the richest of the rich, to an ascetic life-style (wearing old army trousers).

Yes, there is a connection, as you point out.  In the Tractatus, Wittgenstein writes:


  3.332 No proposition can say anything about itself, because the
  propositional sign cannot be contained in itself (that is the whole
  "theory of types").


Gödel, as you know, proceeded to do precisely that.
Wittgenstein's argument against type theory is one of many factors that changed Logical Positivism.  Russell's "barber paradox" was another.  If the history of Logical Positivism interests you, I'd recommend a delightful graphic novel called http://rads.stackoverflow.com/amzn/click/0747597200 Logicomix which covers the territory nicely.
Not really. 

Death is not the opposite of existence. The opposite of existence of is non-existence, and the opposite of death is life. Existence does not necessarily entail life, but life necessarily entails existence. So when philosophers argue for the existence of God, they're not arguing that he is "alive", and thus the notion of death is not applicable.
It depends on how precisely you define "conditions favorable for it to happen"; do you mean the "ideal" conditions or just generally "acceptable" conditions? In "ideal" conditions, for example (warm temperature, moist environment, bacterial presence, etc), mold will grow on bread. In "acceptable" conditions, it is obviously not as certain; at the very least, it will not grow as fast or large as mold in an ideal state in the same amount of time. It seems like you are leaning towards the former, in which case it appears you are asking about the validity of http://en.wikipedia.org/wiki/Determinism causal determinism; i.e., if you were to add up all the factors which cause an event, it's the logically valid to say that the event will happen? In a deterministic system—yes, absolutely. 

Given that there is some debate, however, whether our own universe it deterministic or not, you might not be able to say with great surety depending on who you talk to. But generally, for non-quantum level events, I think most people would say yes (theoretically). In practice, of course, it is essentially impossible to set up a system as you describe in your example. I think a better way of conceptualizing it would be to think about it this way: 


  If you were to flip a coin, and it lands—say—heads. Then you were go
  back in time to the moment just before you flipped that coin, and you
  left every variable in the universe precisely the same as it was
  when you flipped it the first time. Would the coin land heads again?
  Yes! Theoretically, it would land heads every single time no matter
  how many times you went back in time pre-flip, because all the factors
  stayed the same.


Thus, if all the conditions that are favorable to an event are in place, the event will occur, and it will not be able to occur in any other way beyond what is defined by the factors involved in it's execution.
That could be an http://en.wikipedia.org/wiki/Appeal_to_consequences Appeal to Consequences, or a more general http://en.wikipedia.org/wiki/Appeal_to_emotion Appeal to Emotion, depending on the situation.
Well, it is difficult to get a good handle on Socrates's philosophy without relying on Plato; however, all sources (including Xenophon) agree that Socrates was associated with Critias, one of the Thirty Tyrants-- in other words, we can safely assume that Socrates's view of Athenian democracy was not terribly positive.

Similarly, Aristotle was (up until a year or two before his death, at least) tutor to Alexander, and was thus associated with Macedon, which had subjugated Athens. So, he, too, was no friend to Athenian democracy.

In short: I'd argue that (in broad strokes, at least), Socrates, Plato and Aristotle shared a fundamental partiality toward oligarchy.  
I think that one can find a rational justification for the position, if one is willing to accept certain basic principles.

Let us start from the standpoint of Kant's second formulation of the Categorical Imperative (from the Groundwork of a Metaphysics of Morals) which reads: Act in such a way that you treat humanity, whether in your own person or in the person of any other, never merely as a means to an end, but always at the same time as an end.

Let us then take the position that all sentient beings, by virtue of being capable of suffering are entitled to the same moral protections as humans (but not necessarily the same moral obligations).

From the combination of these two positions, it follows that to eat an animal (or use an animal product in any direct form) consists of treating said animal as a means to an end, and not as an end in itself.

Now, obviously, both of the premises of the above syllogism are open to debate, and I'm not particularly motivated to try to defend either of these premises from the variety of criticism they may be open to; but I don't think there is anything about these positions that puts them beyond the pale of reason.
There is a legal notion, enshrined in law, called "theft of services" that covers this.  The canonical example is sneaking into a movie theater; in this case, there is no physical item stolen, but rather, the provider of a service is deprived of the income he would normally receive for the enjoyment of said service.  In the pre-internet days, this notion was most commonly applied in cases of unauthorized (and unpaid) connection to cable television service. 

In addition, music and literature are protected under law by copyright, which is (as the name indicates) a temporary monopoly on the right to copy an artifact granted to the creator.  By downloading music one is making an authorized copy, and thus violating copyright as well.


  How can we come to understand this issue in a way that is both beneficial for the consumer and productive for the producer?


I think this is a false parallelism; the (non-paying) consumer has no particular rights in this regard, and there's no particular reason we should be concerned about their benefits.  Let's turn the question around: what possible moral justification is there for taking that which is not given?  If the composer/performer of a song states "I do not want you to download my music without paying for it", is there any moral imperative that would permit one to override their desire in this regard?
Perseus digital library has quite a few Greek texts and provides access to a few Attic Greek Lexicons. 
http://www.perseus.tufts.edu/hopper/searchresults?q=qumoeidhs&target=greek http://www.perseus.tufts.edu/hopper/searchresults?q=qumoeidhs&target=greek

This entry is from the Liddle-Scott-Jones on Perseus:

θυ_μο-ειδής , ές,

A. high-spirited, τὸ θ. Hp.Aër.12; opp. ἄθυμος, Pl. R.456a; opp. ὀργίλος, ib.411c.
2. passionate, hot-tempered, opp. πραΰς, ib.375c.
b. of horses, mettled, X.Mem.4.1.3; opp. εὐπειθέστατος, Id.Smp.2.10: Comp., opp. βλακωδέστερος, Id.Eq.9.1.
3. Philos., τὸ θ. spirit, passion, opp. τὸ λογιστικόν, τὸ ἐπιθυμητικόν, Pl. R.440e, al., cf. D.L.3.67. Adv. “-δῶς” Hdn.4.3.3.

There should be an acute accent over the "eta" as shown above. 

The sentence cited from the Republic is this: "καὶ γυμναστικὴ δ᾽ ἄρα οὔ, οὐδὲ πολεμική, ἡ δὲ ἀπόλεμος καὶ οὐ φιλογυμναστική; οἶμαι ἔγωγε.τί δέ; φιλόσοφός τε καὶ μισόσοφος; καὶ θυμοειδής, ἡ δ᾽ ἄθυμός ἐστι;
καὶ ταῦτα." So here "θυμοειδής" (spirited) is contrasted with "ἄθυμός" (spiritless). I suggest you look into Plato's tripartite soul (especially in the Republic): he divides the soul into the appetitive, the rational, and the spirited. In the Republic, Adeimantus perhaps is an example of someone ruled by the spirited part of his soul (as opposed to say, Glaucon, who seems more concerned with the "relishes", and thus, might be governed more by the appetitive part).    
The simplest way that I've found to to think about property is to consider it a 'right to exclude' others from the use of a valued thing (land, material goods, designs etc.). So when you say that individual or a group has a legitimate property right claim over, say, a piece of land, what that means is just that he/she can legitimately exclude or include others from the used of that land at their discretion. 

Whether everyone should have that kind of right as basic, over things they have in their possession is naturally something that's disputed (Poudhon the anarchist thinker for instance, believed 'property is theft'). and even with people who believe in the fundmental importance of instituting property rights believe in different ways of legitimising claims to property. 

Locke has a famous idea about how to determine whether something is legitimately owned by an entity capable of property right [note:corporations, governments, individuals etc.  people also differ on what kind of entity should be allowed property rights]. Locke thinks that anyone can legitimately claim over unclaimed property iff he/she leave "enough and as good" for others, so original claims are settled that way (think of the wild west when settlers started drifting there), and as long as there's an uncoerced contractual agreement between parties during the exchange of property, then that right remains legitimate in whoever's hands. 

There are many reasons to agree or disagree with specific property types (e.g. intellectual property), and the criterion for legitimate claims to properties in general. There's by no means a consensus on how to think about property rights, beyond that definition at the start. But it is definitely an idea that is at the core of modern societies and governments -- first thing thing a country wishing to modernise does, is always to establish the rule of law, which includes, in large parts, protection of people's right to exclude others from assets they have a legitimate claims to. 

Hope this helps
There are several points of confusion here.

First, your examples are someone more complicated than they need to be because of the temporal aspect of the problem (immortality).  We need to make a distinction between global statements that are falsifiable in practice (such as "All crows are  black" which equals "No crows are  white" which can be falsified by examining all crows, and which remains true even if a white crow is hatched tomorrow) and those which are not (such as "All men are mortal" which equals "No man is immortal", which cannot be tested as the test of immortality would require waiting until after the end of time).

If we set aside this problematic, we now come to an question of what we mean by truth. Some philosophical traditions make a distinction between observed truths and inferential truths; other argue that they have the same truth status.  Pragmatism, on the other hand, argues that the "cash value" of truth is tied to its utility, so a statement can be considered to be "true" even if it cannot be rigorously and falsifiably verified.

All of this is orthogonal to your next problematic, which is known in the philosophy world as the "http://en.wikipedia.org/wiki/Is%E2%80%93ought_problem is-ought distinction", and was most famously raised by Hume.  There's no need to drag normative statements into the discussion, as they have no bearing on truth and falsifiability.

Finally, you write:


  This classic tautology [A = A] doesn't seem falsifiable since it can never be shown to be false.


That may or may not be the case, depending on how we are reading "A".  We certainly can determine whether or not some given (sensible) entity is identical to itself, if we have stipulated the identity conditions for that entity.  If we are dealing with A as an abstraction, we are beyond the scope of sensibility and are relying on pure reason.

In other words: there are differing notions of epistemology and truth in play.  Falsifiability applies to empirical matters, but is not appropriate to mathematics or pure logic, where the canons of logic are applied to non-sensible abstractions.  The process of determination of whether or not a given statement is "true" is going to vary based on the type of statement and the epistemological framework chosen.
I guess you mean the Social Contract described by Locke/Hobbes?

As soon as you somehow agreed with the reasons Hobbes/Locke offer for entering the contract you made your commitment. (this includes the pure acceptance of not being threaten by anybody around you)


  Why should anybody willingly renounce their autocracy?


-> (translation from German, so please forgive if I missed the original text)
John Locke: About the Government


  The answer is obvious as he does have all his rights in his natural
  state but he cannot  enjoy them as he has to be afraid of his
  security and is continuously threaten by the other  around him. He
  also can never be sure of his posessions.


Thomas Hobbes: Leviathan


  The people, who naturally love freedom and power entered that
  self-restrained called  government to secure their existens and a
  peaceful life, in other words to flee from the normal state of
  chronicle warfare (the natural state of human kind)

I am not clear on what you mean by "support" of a given law, but I can break it down into at least two cases:


The first case is where I take supporting to mean doing so by contributing to its creation in the law-making process, in which case one is morally responsible for the ethical standing of the law.
In the second case, if by supporting you mean doing so by enforcing a law, then matters are a bit more complicated. It definitely does not necessarily imply that the person who inforces a law  "believes" the law at all. The predominant school of thought in legal scholarship today is http://en.wikipedia.org/wiki/Legal_positivism Legal Positivism which holds that there is an important distinction between morality and law. In particular, the rule of law does not imply the rule of good law. In this view it is considered a virtue of the rule of law to uphold it regardless of its moral standing. Judges who interpret morally bad laws or police who enforce these laws are not morally responsible for upholding them insofar as they are fulfilling their legal duties. It is useful to contrast this school of thought with the classical http://en.wikipedia.org/wiki/Natural_law Natural Law view which holds that law is a universal that is grounded in nature. The interpretation of what this means varies greatly but it can be taken to mean that some "natural" precept can be used as a standard to measure the validity of law. If this standard is taken to be a moral standard then "bad" laws are also invalid laws and one has no duty to enforce "bad" laws.

The difference between the Law of Non-Contradiction and the Law of the Excluded Middle is subtle; fortunately, it's also irrelevant to most purposes.

The distinction becomes most evident if we contrast classical logic to the Indian http://en.wikipedia.org/wiki/Catu%E1%B9%A3ko%E1%B9%ADi Catuṣkoṭi, where four positions are available:


P 
Not P
Both P and Not P
Neither P Nor Not P


These can be conveniently recast as


P is true
P is false
P is both true and false
P is neither true nor false


For Aristotle (and classical logic), the bottom two options are forbidden-- "Both P and Not P" because of the Law of Non-Contradiction (there exists no P such that P is both true and false), and "Neither P Nor Not P" because of the Law of the Excluded Middle (there exists no P such that P is neither true nor false, but some third state.)

So, they are not equivalent-- but are only relevant if you are looking to exclude deviant logics.  If you are already playing by the rules of classical logic, the effect of each is the same (P is either true or false).
For most people who have lived, this question is on-the-face nonsensical.  Most cultures up until now have had some sort of value for http://faculty.virginia.edu/haidtlab/mft/index.php purity or sanctity.  The very idea of extracting an organ from one person and placing it in another prompts a feeling of disgust and revulsion.  Many cultures impose a strong taboo on even touching a dead body.  Western culture has largely rejected the ancient value of purity for a variety of practical reasons—not the least of which is to allow Western medicine to proceed.

I won't argue here that such a value ought to outweigh the practical considerations of allowing trade in human organs, but I will say that the value of sanctity neither is nor can be defended via reason.  Valuing it is just part of being human.



There is also a very real social justice consideration: the people most likely to need to sell organs are also the most likely to need to remain healthy.  The simplest way to get at the concern is to ask yourself how much it would take to sell, for instance, one of your kidneys.  Would $10,000 be enough to give up a vital organ that you may want to hold onto for your later years?  What about $1 million?  If you are very poor, the first number would be tempting because it represents a large percentage of their yearly income.  But I don't think the second number would interest me at all.

It's the poor of the world (and it's quite likely you, the reader, are quite rich relative to the entire world's population) who have the hardest time remaining healthy.  And health is a much bigger factor in the earning power of the poor.

Again, this consideration might not be a problem in light of the benefits of a used organ market, but you can't just dismiss them either.
http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3atext:1999.01.0168%3abook=1%3asection=333e In context, these aren't an example of a linear argument, but four separate a illustrations of the idea that one who is good at doing one part of an occupation well is also able to other parts well.  In fact Socrates introduces the idea with this phrase:


  Then, my friend, justice cannot be a thing of much worth if it is useful only for things out of use and useless. (http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3atext:1999.01.0168%3abook=1%3asection=333e 333e)


It seems Plato (via Socrates) did mean these examples to be persuasive, but they are a part of a larger reductio ad absurdum that ends:


  "So justice, according to you and Homer and Simonides, seems to be a kind of stealing, with the qualification that it is for the benefit of friends and the harm of enemies. Isn't that what you meant?" 
  
  "No, by Zeus," he replied. "I no longer know what I did mean. Yet this I still believe, that justice benefits friends and harms enemies." (http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3atext:1999.01.0168%3abook=1%3asection=334b 334b)




I'm not sure what translation you are using, but the question you marked at (3) doesn't seem properly translated.  Paul Shorey translated the question as: 


  Is it not also true that he who best knows how to guard against disease is also most cunning to communicate it and escape detection? (http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3atext:1999.01.0168%3abook=1%3asection=333e 333e)


The idea seems to be that one who knows how a disease is spread will have the knowledge of how to avoid spreading it as well as the knowledge of how to secretly infect people.  I don't think there is any question but that this is http://en.wikipedia.org/wiki/Eugeniusz_%C5%81azowski true.
I think that captures the sense of Plato's argument, and also points to the problem with it.

In most of the examples, the "opposites" are actually just different headings on a unilinear continuum; for example, "larger" and "smaller" are just the two directions you can go on the continuum of "size", and "slower" and "quicker" are just two directions you can go on the continuum of "speed", etc.

Now, this gets more complicated when we look at the state of being asleep and awake.  We can posit a continuum of "wakefulness", and suggest that one can be (at various times) more asleep or more awake, but this seems a bit outside of ordinary usage, where we often think of "asleep" and "awake" as two completely distinct states.

When we apply this to "alive" and "dead", the notion breaks down completely.  Here, there doesn't seem to be a continuum at all, rather just two differing states.

What this means, in the end, is that it doesn't seem appropriate for Plato to generalize from the fast/slow, small/large, worse/better oppositions to the alive/dead opposition, and the cyclical argument appears specious.
Hobbes would probably not tell you to fight back and gain honor, because your first quotation indicates that he looks unfavorably upon what he calls "Vain Glory." 

Given that quotation, he would probably say that if you actually try to retaliate against the bully, you would have that very "false esteeme" of your own strength; any honor you gain would be false, and only meaningful to yourself. The bully has been dominating you, so you either fight back and fail, or you succeed but gain nothing but vain glory. It's a lose-lose situation, because inevitably either your body or your mind will experience a loss (vain glory, to Hobbes, is negative for the mind. It leads you to see yourself as above others, while he says that all are equal in their rights). 

However, if you were to seek peace, he would likely praise you for being reasonable. In your second quotation, he explicitly says "that peace is to be sought after where it may be found." He only says that that you should prepare for war if peace can not be found. Also, note that he says that even if peace can't be found, you should prepare for, but not declare war. From the quotations, it seems that Hobbes treats war as a last resort. Therefore, the victim of the bully would do best, from Hobbes' perspective, to escape from the situation. 
This is known as "http://en.wikipedia.org/wiki/Process_of_elimination the process of elimination."
Possible worlds have been used as a way to understand modalities like necessity and possibility. For instance something is necessary if and only if it is true in all possible worlds. This is a way of trying to understand what it means to say that X is necessary.

More basically, you can use possible worlds to cash out the idea of an inference's being valid: the inference from X to Y is valid if and only if Y is true in every possible world where X is true.

Possible worlds have also been used to give a semantics for counterfactual conditionals, though this is more controversial. How do you evaluate conditionals like: 


"If I were to drop this glass, it would smash"
"If I were to drop this glass, it would turn into a porcupine"


Now, as a matter of fact, I don't drop the glass. Thus the antecedent of both conditionals is false, so if they were material conditionals, both would be true conditionals. But intuitively, the first is true, the second false. So we need a better understanding of conditionals like this. David Lewis offered an account of what makes the first true and the second false in terms of possible worlds. The idea is that in the closest possible world where I drop the glass the glass smashes, and it does not turn into a porcupine. Thus the first conditional is true, the second is not. Obviously, the devil is in the detail and much ink has been spilled trying to get a handle on closeness of possible worlds. But the intuitive idea should be clear.

I don't think possible worlds fix a flaw or bug in logic: they are a neat conceptual tool to make understanding things easier.
I believe the fallacy in your example is a specific form of a http://en.wikipedia.org/wiki/Non_sequitur_%28logic%29 non sequitur, defined as:


  An argument in which its conclusion does not follow from its premises.


Now, as pointed out by stoicfury, a non sequitur is merely an umbrella term encompassing many logical fallacies. The term you may be looking for is http://en.wikipedia.org/wiki/Fallacy_of_relevance Ignoratio Elenchi, which is specifically:


  The informal fallacy of presenting an argument that may in itself be valid, but does not address the issue in question.


Which is exactly what your example is: a long set of premises about the native people, all quite believable but irrelevant to the question of what is or is not "primitive" (which is itself only an implied argument - there is no explicit logical argument in the image). The argument in the list of "without" terms is that the native people do not have many of the problems of the industrialized world; this is true, but it only allows one to reach a conclusion along the lines of:


  The native people do not have many of the problems we experience in the industrialized and modern world.


But, given the most relevant definition of http://dictionary.reference.com/browse/primitive primitive as:


  Unaffected or little affected by civilizing influences; uncivilized; savage: primitive passions.


The premises do not allow one to argue that the natives are not primitive. The list of premises is entirely irrelevant to its conclusion, and so the argument contains the fallacy Ignoratio Elenchi. The only way the argument could avoid being fallacious is:


If the definition of primitive could be the possession of all the attributes the image disowns (stress, bombs, etc.);
If the natives truly do not have these attributes.


However, as you point out, 1 is not the case; those attributes are (by the general definition) irrelevant to what is or is not primitive. Thus, the argument does indeed contain a fallacy, an Ignoratio Elenchi.
The Hobbesian Concept of Fear

According to Hobbes, fear is the ultimate motif. Fear, according to Hobbes, suffuses and shapes human life.

Now, I will present to you some quotes: 


  The hobbesian man is drive by pride but above all, by fear of other men.


also,


  He lives in constant fear of his life because he is bloated with unnatural passions such as pride, which cause men to invade one another.


Hopefully, we can now simply agree on Hobbes' concept of fear. Now let's link it to love.

The Hobbesian Concept of Love

I will present to you several quotes from Thomas Hobbes himself (can be found in his Leviathan):


  What quality soever maketh a man beloved or feared of many, or the reputation of such quality, is power; because it is a means to have the assistance and service of many.


To Hobbes, it does not matter if one is feared or loved, as long as he attains power.


  To show any sign of love or fear of another is honour; for both to love and to fear is to value. To contemn, or less to love or fear than he expects, is to dishonour; for it is undervaluing.


Again, Hobbes does not make a distinction between love and fear, just that in either case they attain values for a person.

Finally: 


  And therefore to be honoured, loved, or feared of many is honourable, as arguments of power. To be honoured of few or none, dishonourable.


Therefore, Hobbes doesn't really care. He doesn't make a distinction between the two. According to him, all that matters is attaining honour and if that means attracting the love or even fear of others, then so be it.

This, therefore, is how love is related to fear. As a means of attaining honour, power and values. However, based on my own interpretation, I don't believe that Hobbes believes that the love expressed by the people is genuine, rather merely a means of expressing their agreement.

EDIT: The above quotations are from Part I, Chapter X of Hobbes's Leviathan, which can be found http://ebooks.adelaide.edu.au/h/hobbes/thomas/h68l/chapter10.html here.
The short answer (tl;dr)

The relevant historical context to answer your question is the long quest of logic to provide foundations for all mathematics. Zermelo's axiomatic set theory displaced contenders like type theory and won the race early on, because logicians in this tradition developed metalogical tools (model theory, proof theory) to investigate axiom systems. Zermelo's contribution came around 1908 and, through the work of Fraenkel, Skolem and others in the mid 20s (before Gödel's completeness result), it quickly became the standard set theory we know today, Zermelo–Fraenkel set theory + Choice (ZFC)

That ZFC became a pure first-order theory is due to http://en.wikipedia.org/wiki/David_Hilbert David Hilbert's early work on a subsystem of logic which he called restricted functional calculus (effectively today's first-order logic) and http://en.wikipedia.org/wiki/Thoralf_Skolem Thoralf Skolem, who in 1923 gave the original first-order axiomatization of Zermelo set theory. Axiomatic set theory effectively became a dominant first-order theory in the mid 30s and is first-order up to this day. The majority of set theorists like the properties of first-order logic (completeness, compactness, etc.) a lot. The fact that first-order set theory deviates from mathematical practice is actually seen as a feature, not as a bug.



The long answer

It seems trivial today, but in order to adopt first-order logic (FOL), one has to be able to isolate it from second-order logic (SOL) or higher-order logics. And this possibility was itself an important conquest. Up to Principia Mathematica, several versions of second-order logic (including http://en.wikipedia.org/wiki/Infinitary_logic infinitary logic) were commonly employed by logicians without much care. FOL and SOL were not really distinguished (distinguishable?) until the relative merits and vices of each one were investigated. Or, to state it the other way around: It was the foundationalist quest which lead to study not only the expressibility, but also the properties of various fragments and it was through this venue that FOL and SOL were discerned.

1. Two traditions in logic

It seems to me that it was Russell's discovery of the http://en.wikipedia.org/wiki/Russell%27s_paradox famous paradox in 1901 in Cantor's naive set theory (discovered independently by Zermelo, who communicated it to Hilbert) that started it all. Since the paradox also appeared in Frege's formalized version of naive set theory, logicians started to devise various ways of avoiding the problem and built new set theoretic approaches. The two most important proposals which fixed these (and other) paradoxes were Russell's type theoretic set theory and Zermelo's set theory. 

These two solutions are commonly regarded as expressions of two different "strains" within logic, the Peano-Frege-Whitehead-Russell tradition and the (Peirce)-Schröder-Hilbert-Zermelo tradition. 

2. Metalogical research

The important point is that these two traditions scored unevenly in the above mentioned task of investigating logical fragments and their properties. Of the two, the latter was in Lakatosian terms the progressive research program because it was interested from the start in http://en.wikipedia.org/wiki/Metalogic metalogical questions, while the former was not. 

To understand why this is the case, it helps to remember that the first tradition is commonly identified with http://en.wikipedia.org/wiki/Logicism logicism, a conception which defines the raison d'être of logic as the task of giving foundation for all of mathematics. For most logicists this implied that it was impossible to "stand outside" of logic and thereby to study it as a system (in the way that one might, for example, study the real numbers). This had severe consequences: Russell and Whitehead 


lacked any conception of a metalanguage
explicitly denied the possibility of independence proofs for their axioms
believed it impossible to prove that substitution is generally applicable in type theory
insisted that the Principle of mathematical induction cannot be used to prove theorems about their system of logic


It is hard nowadays to understand what doing logic means without a language-metalanguage distinction and a syntax-semantics distinction!

3. The emergence of FOL

It is not a coincidence, I think, that it was in this metatheoretical setting, and in the Schröder-Hilbert-Zermelo tradition, that logic was put on the operating table to be dissected, so to speak. It was a direct consequence of the need to investigate metalogically the properties (soundness, completeness, compactness, consistency, categoricity etc.) of various logical fragments: In 1918 Bernays gave the first rigorous proof of completeness of such a subsystem of logic, i.e. propositional logic. Another fragment turned out to be what we now call FOL. Specifically, it was first developed in 1917 under the name restricted functional calculus by Hilbert as a subsystem of his functional calculus (effectively a http://en.wikipedia.org/wiki/Type_theory#The_1908_.22ramified.22_theory_of_types ramified type theory), but was only published in the classic textbook coauthored with Ackermann Grundzüge der theoretischen Logik in 1928, where it was still treated as a subsystem.

In fact, the case was far from settled. While the http://en.wikipedia.org/wiki/Zermelo_set_theory original Zermelo set theory can be interpreted as being a FO theory (with the separation axiom replaced by an axiom scheme with an axiom for each FO formula), according to Zermelo's own conception it was a second-order theory (with the separation axiom as a single axiom). Zermelo remained a strong proponent of a second-order set theory. Indeed, most logicians did use different fragments of logic for different task and they did not shy to employ second-order theories or anomalous FO theories (i.e. including infinitary operations).

The fact that set theory is today a FO theory is probably due to http://en.wikipedia.org/wiki/Thoralf_Skolem Thoralf Skolem. In 1923 Skolem presented the original FO axiomatization of Zermelo set theory. Now, there is a standard view that sees a Skolem-Gödel "axis" as urging to adopt a FO set theory and being responsible for the ultimate establishment of FO set theory. It is, however, not clear at all that this was the case, i.e. that Skolem (and Gödel) were pushing an FO object language in set theory. While Skolem was critical of second-order set theory as a foundation of mathematics, he proved his (downward) theorem to hold in FOL. The result - in a countable model it is true that there is a uncountable set - he called a philosophical (albeit not formal) paradox in order to argue that also FOL could not serve as a foundation of mathematics:


  I believed that it was so clear that axiomatization in terms of sets was not a satisfactory ultimate foundation of mathematics that mathematicians would, for the most part, not be very much concerned with it. But in recent times I have seen to my surprise that so many mathematicians think that these axioms of set theory provide the ideal foundation for mathematics; therefore it seemed to me that the time had come for a critique. (Skolem 1922)


(There is even the suspicion that Skolem's axiomatization was FO by chance…! There is some good evidence that a lot of the finest minds of their times - like Fraenkel and von Neumann  - had trouble developing a real understanding of the difference between FOL and SOL in the mid 20s!). 

And Gödel, although he argued for a FO *meta*language, used a variant of type theory in his famous paper in 1931!

To be sure, it is undeniable that both Skolem and Gödel contributed important theorems to help establish FOL, but they did not actually argue for it. The truth seems that there is no simple, success story with heroes to be told here. A more correct account would probably involve multiple causal factors.

The OP's statement that


  the completeness of first-order logic was only proved by Kurt Gödel at a time when first-order logic had already displaced second-order logic


is however incorrect. FO axiomatizations of set theory became only dominant starting in the mid 30s. There is an hypothesis to the effect that this timeline should be correlated with Tarski's important contribution to model theory (truth, logical consequence). On this view, FOL became standard not (only) because of its intrinsic qualities, but because it was shown to have a particularly nice model theory.

4. Why (not) first-order set theory?


  Nevertheless, I still wonder why […] why the inability of first-order logic to characterize infinite structure is not considered a problem.


Well, a pragmatic answer is that it is not considered a problem because of FOL's inability  ;) 

As it is not possible to characterize (i.e. axiomatize categorically) infinite structures in FOL, as you say, set theorists simply work with the http://en.wikipedia.org/wiki/Interpretation_%28logic%29#Intended_interpretations intended model and they care about http://en.wikipedia.org/wiki/Non-standard_model non-standard models only when they're needed. That's as good as it gets, I worry.

The more general dispute is about pondering the merits and vices of FOL and SOL. At first glance it seems that


FOL 

 + complete, compact, nice model theory 
 - deviating from mathematical practice

SOL

 + adherent to mathematical practice
 - completeness does not hold



Since nobody would dispute the merits of FOL, it comes down to the question in which way adherence to mathematical practice is really a good thing and how the loss of the merits of FOL is evaluated. From my experience with logicians


the supporters of FOL would deem a language without FOL's merits as too much of a loss. In addition, the don't see the deviation from mathematical practice as a vice, but as a feature. This might be a remaining of the finitary tradition in logic: Logic is required to be more strict than mathematics (and its practice) in order to serve as a foundation for it.
the supporters of SOL wouldn't deem the loss of completeness etc. as fatal. They see set theory not so much as a foundation of mathematics. Instead, set theory should be more a description of mathematics, i.e. the more adherent to mathematical practice, the better (=more precise) the description becomes. 
some see a middle-way between the two by adopting another FO set theory like Morse–Kelley set theory. MK, which allows http://en.wikipedia.org/wiki/Proper_class proper classes along withs sets, is syntactically almost identical to second-order ZFC, but differs quite in its semantics.


Pick your choice :)



Sources and further readings: 


Some knowledge in the history of logic 
Gregory H. Moore: http://www.mcps.umn.edu/philosophy/11_4moore.pdf The Emergence of First-Order Logic (paper)
Matti Eklund: http://www.hf.uio.no/ifikk/forskning/publikasjoner/tidsskrifter/njpl/vol1no2/howlogic.pdf On How Logic Became First-Order (paper)
W. D. Hart: http://books.google.at/books?id=7IK3M3Fd4WYC The Evolution of Logic (book)
Geraldine Brady: http://books.google.at/books?id=ahoH-tLm2S0C From Peirce to Skolem: A Neglected Chapter in the History of Logic (book)
Ethan Jerzak: http://www.math.uchicago.edu/~may/VIGRE/VIGRE2009/REUPapers/Jerzak.pdf Second-Order Logic, Or: How I Learned To Stop Worrying And Love The Incompleteness Theorems (paper)
Stewart Shapiro http://books.google.at/?id=btx10Q8GQ7oC Foundations Without Foundationalism: A Case for Second-Order Logic (book)


  Lacans notion of the mirror stage seems to rely directly on the ability of the child to see


Actually, it doesn't.  The case of an infant seeing itself in a mirror is used as an exemplary case of self-recognition; Lacan posits that all children pass through this stage of self-recognition, as reflected (primarily) in the mother as primary caregiver.  Clearly, for blind infants, this would occur in some other register within what Lacan calls the perception-consciousness system.

Similarly, Freud (to the best of my knowledge) does not explicitly describe the dreams of blind people in The Interpretation of Dreams; this omission does not imply that blind people are lacking the unconscious drive for wish-fulfillment, etc.
First, let's get the terminology straight.  What you are talking about does not appear to be "selflessness" at all, but "self-sacrifice", or "altruism."

Now, with that in mind, let's refine the question.  You appear to be asking "Is it possible for someone to act in a manner that is not motivated, directly or indirectly, by self-interest?"

If this is the question, we immediately run into two difficulties:

1) We need to have a clear idea of what we mean by "motivation" in this sense; unfortunately, this is an extremely difficult problem, as most people recognize the possibility of unconscious motivations-- this means that we have no reliable manner of ascertaining precisely what one's motivations were for any particular act.

2) We need a good definition of "self-interest."  This is a much more difficult problem than it appears, and a critical analysis of forms the first part of Derek Parfit's classic http://rads.stackoverflow.com/amzn/click/019824908X Reasons and Persons.  I'd recommend this book as a good starting point, if questions like the one you posed interest you.

Finally: if we set aside all of the above, and still try to plow through to an answer, I suppose the answer would have to be "Why not?"  Is it possible?  I don't see any reason it should be impossible to believe that at least once in the history of humanity, a single human has taken a single action which offered no foreseeable benefit to the actor.  But what does that really tell us?

  You can always take away you opinion and everything is opinion. Why, all of a sudden, is the perception of this smell not an opinion that can be taken away [...]


You've made a hasty generalization; everything is not opinion. More specifically, perception is not opinion-- I cannot eat a lemon and decide that it tastes sweet, nor look at it and decide that it is purple, and I can't smell a foul odor and decide that it smells pleasant.  
"Action", in this case, is used to mean activity which is "intentional under some description", to use Davidson's phrase.  The SEP has a nice http://plato.stanford.edu/entries/action/ article that elaborates the key factors.

Contrarily, activity is simply motion.  In the Southey quote, http://en.wikipedia.org/wiki/Sydenham%27s_chorea St. Vitus's Dance refers to a neurological disorder which results in bizarre and uncontrolled twitching; Southey is saying that Coleridge's mind is running around frantically without accomplishing anything intentional or meaningful.
The name of the idea is "weighted voting."

There are a large number of proposed ways to weight the votes: contribution-based allocation, education-based allocation, occupation-based allocation, etc.

What these variants all have in common is that they are running counter to the "one person-one vote" notion, which has been historically expanding through the notion of universal suffrage.
At the end of the second section of http://rads.stackoverflow.com/amzn/click/0486421309 On Liberty, Mill gives a summary of the four reasons why there should be free speech:


  First, if any opinion is compelled to silence, that opinion may, for aught we can certainly know, be true. To deny this is to assume our own infallibility.
    Secondly, though the silenced opinion be an error, it may, and very commonly does, contain a portion of truth; and since the general or prevailing opinion on any subject is rarely or never the whole truth, it is only by the collision of adverse opinions that the remainder of the truth has any chance of being supplied.
    Thirdly, even if the received opinion be not only true, but the whole truth; unless it is suffered to be, and actually is, vigorously and earnestly contested, it will, by most of those who receive it, be held in the manner of a prejudice, with little comprehension or feeling of its rational grounds. And not only this, but, fourthly, the meaning of the doctrine itself will be in danger of being lost, or enfeebled, and deprived of its vital effect on the character and conduct: the dogma becoming a mere formal profession, inefficacious for good, but cumbering the ground, and preventing the growth of any real and heartfelt conviction, from reason or personal experience.    


I think you really need to understand what "utilitarianism" means to Mill. The caricature of utilitarianism as "the greatest good for the greatest number" is good as far as pithy sayings go, but it leaves a lot of complexity out. You might consider Mill's famous claim:


  it is better to be a human being dissatisfied than a pig satisfied; better to be Socrates dissatisfied than a fool satisfied


He's obviously not interested in "happiness maximization" in a straightforward sense; Mill believed in a "hierarchy of pleasures" - i.e. certain types of happiness are better than others. So to Mill, the argument that free speech might decrease certain types of happiness would not be a conclusive argument against it - we might value the happiness which comes from exercising our rationality above that which comes from other things, so the tradeoff is worth it.

(This is, as you say in your comment, closer to what some deontologists believe than what some consequentialists believe.)
i would agree that it is an extension of the socio/cultural context within which Plato existed, but i don't think it was done to endow his work with a glamour, or to approach the subject matter he addresses through some kind of artistic enterprise. It would be interesting to do a http://en.wikipedia.org/wiki/The_medium_is_the_message McLuhan style analysis of how the dialogic form of expression impacts its reception, but I think a better way to look at it is to consider the way the Greek's viewed truth not so much as a corrospondence between propositions and states of affairs, but as something to be produced through rigourous adherence to the various principals (temperance, etc) thought to be conducive to its realisation. Foucault has written many things relevant to this question, for http://foucault.info/documents/parrhesia/foucault.DT4.praticeParrhesia.en.html eg., but most of all his analysis in The Order of Things of the Greek conception of truth would be useful for investigating the significance of the form which Plato's work takes
One way to approach this is by Aristotle's metaphysics. According to Aristotle, everything there is can be classified in one of the following ten categories: (1) substance; (2) quantity; (3) quality; (4) relatives; (5) somewhere; (6) sometime; (7) being in a position; (8) having; (9) acting; and (10) being acted upon (1b25-2a4).

According to his definitions, a substance is something that cannot "be said of" nor "be in" another thing. And substances come in degrees: primary substances are individuals, like this particular man or that particular horse. Next on the scale, we find species (e.g. "man", in the sense of human, encompassing the whole class of human beings) and genera (e.g. "animal"). Substance has a primacy over the other nine categories, since all of them exist "in a" substance. A quantity, for example, refers to how many items of a certain substance we are talking about. Similarly for all the other categories, known collectively as accidents.And, among substances, the primacy is reserved to the primary substances. Everything has to ultimately rely upon the existence of individuals. We can only talk about man – as a species – if there are particular men in the world, so we can apply the name "man" to them and say things like "Socrates is a man".

In particular, the category "quality" is important to your question. Although Aristotle does not give us a precise definition of what quality is, we can grasp the concept with little difficulty. When we say that a certain horse is beautiful, it is clear that we are ascribing a quality to the horse. Consequently, we can also beautify a horse, that is, we can increase that quality in the horse.

So, what about "beauty"? Is it also a substance? For linguistic reasons, we are sometimes led to think so. After all, "beauty" is a noun and therefore can be modified by an adjective, like "beautiful". Likewise, it can be the object of a verb, like "beautify". Semantically, of course, the phrases "beautiful beauty" or "beautify the beauty" are problematic (the very heart of your question, after all). But syntactically, they are ok. As in the famous example "Colorless green ideas sleep furiously", which is a perfectly grammatical sentence, but full of self-contradictions. But the fact that "beauty" is a noun doesn't change the fact that it still is quality. Moreover, there are no individual beauties, that exist by themselves, detached from substances and to which we can predicate things.

A bit more formally: "beautify X" means "increase the quality beauty in X", where X is a substance. As we have seen, beauty can only be a quality, not a substance. So we cannot substitute beauty for X, in this case. In other words, "beautify the beauty" makes no sense at all.

I remember having read about experiments concerning the communication of trees in a popular science magazine (when I was still a teenager). IIRC, the experiment went something like exposing one tree to a poisonous substance, and monitoring whether the nearby trees show (chemical) activity indicating that they were informed about the incident. It turned out that the nearby trees really get informed. Then the experiment went on trying to cut the communication channel. It was possible to establish that the communication channel was chemical (but I no longer remember whether it was above or below ground), even so the exact chemical substances couldn't be identified.
I think it is quite probable that trees posses mental qualities like identity and intension. It may be important to keep in mind that although what we can see from a tree are its trunk, branches, and leaves, the most important part of a tree might be its roots. So a tree doesn't care too much about loosing a branch, but is quite sensitive to damage to its roots.
There is no question that intentional actions influencing and reacting to its environment are evolutionary advantageous for trees. However, as many questions regarding trees are well suited to be investigated by the "normal scientific method", we don't really need to speculate metaphysically about them. Perhaps philosophy could identify interesting metaphysical questions regarding trees, but a big part of the answers should come from normal scientific investigations.

I'm not certain quite how to respond to this question. It strikes me that the way it is asked tries to force an answer out of Leibniz's thought which it is not equipped to give.

First, it is important to ask what the status of space and time are in Leibniz's own words. A helpful text in this regard is Leibniz's correspondence with Samuel Clarke, which may be found http://www.earlymoderntexts.com/pdfs/leibniz1715.pdf here (PDF). Here in Leibniz's third paper we find the passage (§4, p. 9 of the linked document):


  For my part, I have said several times that I hold space to be
  something merely relative, as time is, taking space to be an order of
  coexistences, as time is an order of successions. For space
  indicates... an order of things existing at the same time, considered
  just as existing together, without bringing in any details about what
  they are like. When we see a number of things together, one becomes
  aware of this order among them.


The instance here on space and time being "an order of things" is repeated throughout the correspondence. The meaning of this phrase itself and it's implications might be debatable, but what it is meant to oppose is quite clear. Leibniz is here concerned with objecting to two views (1) that space is a thing (this he does in §5 of the third paper and again in §10 of the fourth paper albeit for difference reasons) or (2) that space is a property of a thing (the objection here is more clear-cut: which thing is it a property or an attribute of? -- see §8 and §9 of the fourth paper).

Leibniz gives us a further hint here when he writes (fourth paper, §41):


  [S]pace doesn’t depend on this or that particular spatial lay-out of
  bodies, but it is the order that makes it possible for bodies to be
  situated, and by which they have a lay-out among themselves when they
  exist together, just as time is that order with respect to their
  successive position.


So, for Leibniz, space and time are orders among bodies, which I suppose means their positioning vis-à-vis other bodies. In this sense, space and time are constituted by a multiplicity of bodies (an infinity, as we find out in §61-69 of the Monadology). If this sense is what your are asking, then it is true, but only in the trivial sense that the very notion of space and time for Leibniz are such that they are comprised of nothing other than the multiplicity of bodies.

However, it still seems to me wrong to suppose that, for Leibniz, there is a "space" or "time" out there (whatever that would mean) containing bodies. Rather, it bears paying attention to something Leibniz says about the relation between bodies and monads, viz.:


  And as this body expresses all the universe through the
  interconnection of all matter in the plenum, the soul also represents
  the whole universe in representing this body, which belongs to it in a
  particular way.
  
  (http://www.marxists.org/reference/subject/philosophy/works/ge/leibniz.htm Monadology, §62)


"Space" and "time" therefore might best be seen as orders internal to the monad corresponding to how the monad represents it's body among the rest of the bodies. And such an order, for Leibniz, only exists in the mind of God.

All that said (and I'm not sure it's entirely clear), where Leibniz more radically diverges from the atomists is in his notion of le plein -- translated in the version I've linked above as plenum -- i.e. the notion that there is no void. For the early atomists, the universe consisted in atoms and void between those atoms. For Leibniz, it is quite clear that there is no void. The universe is composed entirely of monads. 

Again, as with space and time this would be trivially true for Leibniz as a universe "containing" void or that was "larger" than all monads would be undifferentiable from a universe consisting of all monads -- Leibniz here takes the "nothingness" of the void quite literally. What is operating here is what called "the law of identity of indiscernibles", a statement of which can be found in §9 of the Monadology:


  Each monad, indeed, must be different from every other monad. For there are never in nature two beings which are exactly alike, and in which it is not possible to find a difference either internal or based on an intrinsic property.


Since a universe consisting of monads and void has "nothing" distinguishing it from a universe consisting of only monads the two are one and the same, and hence, there cannot be any void at all.
The coast of England cannot be a Platonic Form- at best it can be the flawed image of a Form that exists within space and time. 

One can ask if there is Form that the coast of England reflects? And in this case is that Form the Form of Fractal? Would this even be a Form? 

Seeing as traditional examples of Forms involve specific shapes ie- that of a Horse, a Chair, or a certain geometric shape (circle, square, cube, etc) I would argue that "Fractal" would not in and of itself be a Form as many different fractals have many different shapes. Note that there is no Form = Geometric Plane Figure. That level of abstractness appears to absent from the concept of Forms if read in the sense in which they were written.

No. The coast of England is a representation of the Form "Coast" as well it should be. A Coast being the Form that can be seen as separating Forms Land and Sea.
Let's skip straight to the end.


  then it is trivial that logic is circular.


Correct.  Logic is circular.  

Note that due to http://en.wikipedia.org/wiki/Agrippa%27s_Trilemma Agrippa's Trilemma, there are only three things logic could possibly be founded upon: unsupported axioms we take on faith, circular reasoning, or an infinite regress. Or, of course, a combination of the three.

Lewis Carroll famously http://en.wikipedia.org/wiki/What_the_Tortoise_Said_to_Achilles demonstrated that Modus Ponens is based upon an infinite regress.

There's no way to prove Modus Ponens, except by one of the three horns of Agrippa's Trilemma.  

So it goes.
Racism is one of the forms of distinction between 'us' and 'them'. Such distinction to 'us-friends' and 'them-enemy' were very often in relations between tribes and societies. Just open the Bible for the examples - the other peoples in Holy Land are described as the ones that must be conquered and eradicated. In many languages their tribe name means 'people'.

Memetics, such as Vera Birkenbihl argument, that distinctions based on external appearance are  similar to the distinction between friend and foe based on smell by rats. It enabled humans to join in large groups, but the rivalization for the resources have stayed (resources are always limited), it simply moved to the higher level (hostility between the whole communities, not the single groups).

However, such primitive distinctions, based only on physical appearance were sistematically replaced by distinctions based on ideology, customs, religion etc. Such distinctions have enabled the groups to join in even larger communities, and also enabled assimilation of the most valuable members of other groups.

The nazi is the return to very primitive and ancient distinction mechanisms. They are the great disadvantage in comparison to ideology-based distinctions. Just simple example - how many great scientists have left Germany because they were Judes - and what Hitler could achieve if he wasn't antisemite (he wasn't be Hitler then, however). It's a great example that the regime practicing racism have the big disadvantage which makes its fall very probable. 

Theoretically, if all regimes will become racist, they would survive, as long as they would be able to destroy any regime that would like to use advantages of refusing racism. However, I don't think they would be able to perform such policy for more than one century. 
Are you using the terms in a Russellian sense, or in light of the later literature on the subject?

In Russell's formulation, there is a difference between (on the one hand) things you know first-hand, through direct experience, like the color of your shoes, and (on the other hand) things you know inferentially, or through other indirect means, like the fact that George Washington was the first President of the US.

However: this easy division gets muddied fairly quickly when we look at the process of perception more carefully, and attempt to analyze the epistemological problems entailed.

The http://plato.stanford.edu/entries/knowledge-acquaindescrip/ SEP article on the subject does a nice job of laying out the terrain.
"Rights" are a rather problematic concept.

Let us first consider whether humans have any rights, and if so, how they get them and what it means to have them.

One way to proceed is to identify some characteristic of humans--possessing a rational will, let's say--and then try to deduce from that what behaviors are acceptable.  This leads to efforts like Kant's Categorical Imperative, which rephrased slightly can be taken as a statement of human rights.  Leaving aside all the problems in this approach (e.g. humans are rather irrational, goals aren't adequately treated, the naive idea of will is problematic, etc.), you can then ask to what degree various animals share those qualities that make humans deserving of rights.  And then you end up--unless you're going on genetic composition alone and setting a very high threshold indeed on similarity, or, like Kant, simply not examine animals very carefully and instead follow the prejudices of the time--deciding that animals should also have rights.  Schopenhauer argued such fairly prominently, and perhaps the most prominent current philosopher who advocates for animal rights for these reasons is http://en.wikipedia.org/wiki/Peter_Singer Peter Singer.  (N.B. this is my characterization of Singer; I am not entirely confident he would agree, as I am not aware of him characterizing his position exactly as I have done.)

Alternatively, one can view rights as not intrinsic properties but as those most sacred commitments that societies make to their members.  Thus it is not anything intrinsic about the individuals that renders them deserving of rights, but rather that it is a promise by society.  Societies may wish to make such promises for a variety of reasons: perhaps the individuals feel sad when they see certain atrocities happen, or perhaps the society is strengthened when its members feel confident that they do not have to worry about certain horrors being visited upon them.  In any case, this view of rights leaves one with much less insight into whether animals also ought to have rights.  Since very few other species (perhaps none) can understand a statement of rights, there would be no impact on confidence, and personal unease can vary dramatically from person to person.  As a practical matter, this seems to be the way that rights are actually decided, however, and "arguments" generally involve invoking sympathy by showing pictures of particularly cute animals in distress, or by stating that the distress is minimal or not worth worrying about; or they can be a little more general and involve higher-level concepts like cultivating compassion and avoiding inflicting pain.  I am not sufficiently impressed by the quality of any of these arguments to recommend any source, though I will note that Singer does not entirely avoid this type of reasoning either given his choice of rights-endowing features.
Thanks for the answers guys. the image of meaning i'm looking at is the perspective which is based on thinking about a possible identity of the sign and signified elements of speech. in a way it's similar to the kind of vulgar notion of sex being 'masterbation with a partner', in that the internalised identity of the other is constructed and contained in subjectivity, the physical presence of the other is just the virtual of the act. Blanchot says something very similar in elaborately describing an intimate event with an other and ending by saying of the other 'but what a shame that you will not be present' ..   literally it would mean that calling to mind an object would be bringing its actuality to presence, and the object's physical presence from the point of view of a person is only what it is inside the order of sense or meaning . From this perspective a thing can be multiple things, the morning star and the evening star, even p and not p .. i only persevere with this question because thinking about it kind of makes me crazy, if you can see holes in how or what's described please say so, or have advice about relevant philosophers, any info is appreciated :)  
I can think of at least two representational systems that would serve your purpose:


http://philosophy.lander.edu/logic/syll_venn.html Boolean Logic represented in Venn Diagrams.  This certainly adequate for representing the type of example in your question.  However, representation becomes more complicated when you deal with more complex sets of propositions
http://en.m.wikipedia.org/wiki/Existential_graph C. S. Peirce's existential graphs.  They are not that well known, but it sounds like they are exactly what you are looking for.

Why we should ? There is no reason actually. Out of millions of your father's sperms only one survived to become you, all others are where they statistically supposed to be - dead, dissolved, recycled, dissipated. Any life is a direct violation of natural state of the world, which is chaos. Intelligent life and intelligent civilization is in fact horrible, scary anomaly.

Yet, we are the survivors. We are the anomaly. We defy statistics by our own existence. Our life was not deserved or earned by us in any way. We, living humans, have just been extremely lucky. And so - even if we are about to live just a few decades, why not use this life for something fun ?
If rocks did this all the time, in apparent violation of basic laws of motion, then yes, the rocks are buying pies.  Otherwise, no, this is just an amusing example that doesn't demonstrate anything except the danger of extrapolating complex properties (like agency) from isolated incidents.

The point of agency is that you gain predictive power by considering the entity in question an agent with internal motivations which it uses to shape its actions.  Whether there is something fundamental about agency or whether it's just an emergent property of sufficiently complex physical systems is perhaps debatable, but the rock example doesn't meet the minimum preconditions.

  As I understand it, nothing that can be changed, or broken down into smaller parts is inherently real.


That's not inaccurate, but it is a dangerous way to put it, if we don't qualify the terms.

"Inherently real" here has a specific meaning--it means "existing from its own side" (as the Tibetans put it), or, in Western terminology, a substance.  As Spinoza put it, "By 'Substance', I understand that which exists in itself, and is conceived by itself, i.e., that which does not need the conception of any other thing in order to be conceived."

So, a substance inherently exists in that its existence is not dependent upon any other thing.

So, the cup on my desk does not inherently exists, as its existence was dependent upon a factory making it, etc.


  If my worldly perceptions of emptiness are not inherently real, but created out of some interaction between emptiness and my own mind, how does that explain that so many perceptions are shared?


First off, let's be careful not to reify "emptiness"; emptiness is itself empty.  So, my worldly perception of the cup is not the effect of the interaction between my mind and "emptiness" per se, but between my mind (which is empty) and the cup, (which is empty).  

No one is saying the that cup isn't there, or that I am imagining it, or anything like that.  Solipsism doesn't raise its head.  The point is that the cup is not existing in and of itself, partaking of an eternal Platonic essence of cupness, without dependence upon any other object.

There are a number of good English-language books on Madhyamaka which are aimed at readers with a background in Western philosophy; I'd particularly recommend Jan Westerhoff's "Nagarjuna's Madhyamaka", Jay Garfield's "The Fundamental Wisdom of the Middle Way" (which is a translation with commentary of the MMK), and Mark Siderits's "Buddhism As Philosophy" (which is an overview of Buddhist philosophy tout court, but has a chapter on Madhyamaka and emptiness.)
To quote the http://plato.stanford.edu/entries/contractarianism/ SEP:


  Social contract theorists from the history of political thought include Hobbes, Locke, Kant, and Rousseau. The most important contemporary political social contract theorist is John Rawls, who effectively resurrected social contract theory in the second half of the 20th century, along with David Gauthier, who is primarily a moral contractarian.


These thinkers were all born in Europe or North America within the last 500 years, which it sounds like you consider "mercantile" economies. With the exception of Kant (and the possible exception of Gauthier, whose biography I cannot find), they were all born to white collar fathers.


Locke - Born in England, 1632. Son of a lawyer.
Rousseau - Born in Geneva (modern day Switzerland), 1712. Son of a watch maker.
Kant - Born in Germany, 1724. Son of a harness maker.
Rawls - Born in America, 1921. Son of a lawyer.
Gauthier - Born in Canada, 1932.


Confucius is sometimes considered to have promoted a form of social contract theory. He would be an exception, having been born into poverty in rural China.
If you're asking about the moral aspects of forcing them into society, there are different approaches you could look at:

In general, social contract theories assume that the act of joining society has to be 1) voluntarily 2) out of self-interest 3) based on strict reciprocity. As society makes demands on you, your rights and your behaviour, it seems plausible that you should have to agree to that in advance. We would then conclude that they do have the right to remain outside the society. You can find that thought expressed in the three major social contract theories, namely the Leviathan, Locke's Treatises and Rousseau's Social Contract. 

A different example is Kant, who claimed that there's a moral obligation to leave the state of nature and that one may use force to make others join a legal state. There must be a legal relation between all human beings who could interact with one another or have influence on others (which, today, is pretty much everyone). According to Kant, they would not have a right to remain the way they are then, and in fact, I think that's plausible: They demand property and to be left alone. These claims are dealt with legally, and as soon as they are legal persons, they are part of society. Of course the native tribe (e.g.) is a society in itself. The "problem" is that Kant claims that the same applies to communities/states. They, too, need a legal relation to deal with differing interests. You can find some details http://ivr-enc.info/index.php?title=Kant%27s_Legal_Philosophy here or in the first part of Metaphysics of Morals. On the relation between states, have a look at The Perpetual Peace, a very interesting short book.

This are just two examples how your question could be answered, I bet there are other approaches I didn't think of. I wish to add that, whether we agree with these approaches or not, I find them much more satisfying than an answer that is based purely on rational considerations.
Easy.

The Christmas Dilemma

The rule "you must destroy remaining present" is absurd, just report to the Christmas authorities that you've destroyed a cheap one, but don't do it. The rule "siblings may not share their presents" is wicked and must be disobeyed if you're to raise the children well.

The Extension Problem

Split the food. You can't know that they will die if you do. Maybe they'll stay alive just long enough for help to get there. You are expecting help, since you "know" that you can save one of the two. But there is just no way you can calculate how long a person can last, so split the food. 

And how come you're not starving yourself, at least not to the point you'd worry about sharing the food with the twins? You just found them when you were close enough to the nearest food supply. Now you've got to feed them both and do your damned best to get them there.
As a consequentialist, I'm sure you can construct a scenario where removing a memory is the right thing to do.  For example, if someone is going to go on a shooting spree because they caught their girlfriend with another man, removing the memory of the indiscretion is probably better at maximizing whatever value-function you have.

With other frameworks, it may or may not be permitted.  I doubt it would be easy to will the universal law, "A person X should be able to remove the memory of person Y if X believes it would be better for Y to not have that memory", so it probably fails the Categorical Imperative.  It also probably violates intuitive morality, as people seem to value self-determination pretty highly.

To get an idea of what a particular framework might think about it, I suggest you look at their arguments about lying (including white lies).  Similar reasoning would seem to apply in this case: in each case your actions replaces an accurate perception in someone else with an inaccurate one.  There are difficult issues regarding lying in dire situations; suggesting that this is like lying is not to say that there is an obvious answer to the morality of memory alteration.
If a formula A is consistent then its negation -A is not a theorem.  We can consider two cases for the consistent formula. The formula is a theorem, but then, assuming the logic is sound, we would get a contradiction (a theorem is valid in every model of the particular type) but it is not satisfied.  Thus if we have that the consistent formula cannot be satisfied in any convergent frame, this means that its negation is valid in that class of frames. However, the negation of the formula is not a theorem, if it were then A would not be consistent.

The key point of an incompleteness theorem is to show that there is a formulas  which is valid in a class but which is not a theorem. In the case of a consistent formula, the valid formula would be its negation.
Here's an excerpt of what I found on the http://plato.stanford.edu/entries/presocratics/ SEP on Presocratic Philosophy: 


  Calling this group “Presocratic philosophers” raises certain difficulties. The term was made current by Hermann Diels in the nineteenth century, and was meant to mark a contrast between Socrates who was interested in moral problems, and his predecessors, who were supposed to be primarily concerned with cosmological and physical speculation. “Presocratic,” if taken strictly as a chronological term, is not quite accurate, for the last of them were contemporaneous with Socrates and even Plato. Moreover, several of the early Greek thinkers explored questions about ethics and the best way to live a human life. The term may also suggest that these thinkers are somehow inferior to Socrates and Plato, of interest only as their predecessors, and its suggestion of archaism may imply that philosophy only becomes interesting when we arrive at the classical period of Plato and Aristotle. Some scholars now deliberately avoid the term, but if we take it to refer to the early Greek thinkers who were not influenced by the views of Socrates, whether his predecessors or contemporaries, there is probably no harm in using it.


The Presocratics were 6th and 5th century BCE Greek thinkers (starting with the Milesian school), while the hellenistic period went well over into the Common Era (the first 400 years, roughly, were the era of hellenistic Christianity) and contains Greek as well as Latin philosophers. To be precise, "hellenistic philosophy is the period of Western philosophy that was developed in the Hellenistic civilization following Aristotle and ending with the beginning of Neoplatonism" (Wikipedia), so the pre-socratic thinkers aren't part of the group.

  We know, that tomorrow Sun will rise from the East, even if from
  epistemological point of view this position is not justified neither
  via evidentialism nor via reliabilism. Does that mean that any
  conclusion got from inductive reasoning is strictly speaking not
  knowledge?


Answer: The skeptic needs a language

Suppose a Bizarre Brain, BB, that is, the computer of the "brain in the vat" skepticism, the brain of the Descartes' devil that deceives us, a permanently dreaming brain, or the solipsistic brain that create the "universe" for itself or for us. If the BB makes her mind the object of scientific study, it will find that it behaves with the same complexity as the universe described by a Common Sense Brain, CSB. Thus what CSB calls "the universe", the BB calls "one's own mind." Understood this way, the distinction between CSB and BB collapses and amounts to different ways of describing the same thing: a massively complex process that causes all the BB's experiences. Presumably having made the case that the BB scientist is actually a CSB scientist, the BB applies Occam's Razor, and suggests to the BB scientist to prefer the CSB's standard external “reality” over something like a BB's "reality". This is because the standard "reality" fits all the data available to the scientist, and on the skeptic's hypothesis is impossible to find differences, rendering superfluous the other more complicated possibilities.

Unobservables such as atomic particles, the force of gravity, and the quantum physics, are useful representation models. "The brain in the Vat" skepticism and the solipsism are interpretations or models too. Any language which provide a practical way of thinking and make sense about natural laws, a common mind-independent world, must provide a way of expressing common true inductive inferences about this world, or a description that relates representation to prediction. What varies are the models and interpretations, but if they are to be true, what they predict does not change. There is no more an objective basis for choosing one theory/representation over another than there is for preferring the Fahrenheit to the Celsius scale for temperature ascriptions. In an interpretation, it is the meaning that is significant and that remains invariant between different, but equally adequate, theories or representations.

If too many of speakers utterances are false, then the link between what speakers say and the world is severed; and the enterprise of interpretation each other halts. Too much error in statements about the world is not an option if speakers are going to interpret each other. Therefore meaning is objective in the sense that most of what speakers say about the world are truths about the world. This is an assumption an interpreter makes because the only path into the world speakers share are the events in the world that cause them to hold those sentences true. Meaning is essentially inter-subjective.

An interpreter’s two most important assumptions are the Principle of Logical Coherence and the Principle of Correspondence. Assuming that a speaker reasons are in accordance with logical laws is not an empirical hypothesis, because satisfying the norms of rationality is a condition on speaking a language and failing to find consistency means there is nothing to interpret. The assumption that someone is rational is a foundation and condition on which the project of interpreting rests as possible. The Principle of Correspondence applies to speakers’ observation sentences, the points of causal contact between the world shared by speakers and interpreters, and the utterances of speakers. That there is no distance between the speaker’s observation and which sentences the speaker puts forward as true, is the another foundation on which the project of interpreting rests, is a condition on speaking a common language.

It does not matter that one cannot experience another's subjective sensations.  One cannot feel another person’s pain, but only infer it from their behavior and their reports of it. Unless the talk of such subjective experience is learned through public experience the actual content is irrelevant; all we can discuss is what is available in our public language. The idea of a dictionary is constitute a reference of justification for a translation, but justification consists in appealing to something independent. Without justification therefore a dictionary that exists only in private language, is pointlessness, if it were to describe those inner experiences supposed to be inaccessible to others. The last speaker of a dying language would not be speaking a private language, since the language remains in principle learnable. Imagine someone is to associate some recurrent sensation with a symbol S in private language when the sensation occurs. If it is a private language, it is presupposed that S cannot be defined using other terms, such as "the feeling I get when the manometer rises"; for to do so would be to give S a place in our public language. Or consider the example of someone pointing to two nuts while saying "This is called two". How does it come about that the listener associates this with the number of items, rather than the type of nut, their color, or even a compass direction? To participate in an ostensible public definition presupposes an understanding of the process and context involved. If there is a truly particular language, if it can exist, then it must be in principle untranslatable.


  Hume challenges other philosophers to come up with a deductive reason
  for the inductive connection. If the justification of induction cannot
  be deductive, then it would beg the question. To Hume, induction
  itself, cannot explain the inductive connection. (Wikipedia)


But I ask, why do we need to show that induction is a necessary truth? We can not demonstrate a necessary truth but we can demonstrate that we have a valid reason to believe.

The principle of uniformity refers to the assumption that the same natural laws and processes that operate in the universe now have always operated in the universe in the past and apply everywhere in the universe.

If nature is uniform, crystal gazing may or may not work, but induction works. If nature is not uniform, then induction will fail, but so will any alternative method. Because if the alternative method did not fail, if it consistently yielded true predictions, and the success of that alternative would constitute a uniformity that could be exploited by the inductive method. Because we could inductively infer the future success of the crystal gazer from her past success. Hence, the inductive method will succeed if any alternative method could.

Why do we need to know whether the inductive method is necessarily true if the inductive method will succeed if any alternative method could? We can’t have a reason for believing that induction is necessarily true because we can’t know in advance whether nature is uniform. We can’t have a reason necessarily true, but we can justify the inductive method by saying that it's the best method for making predictions about the future/unobserved, because if nature is uniform, crystal gazing may or may not work, but induction works. If nature is not uniform all methods will fail.

References: Donald Davidson, Wittgenstein, Quine, David Deutch, Wesley C. Salmon
No logics ever really "define" truth, they use it. It is assumed that there is some pre-theoretic understanding of what "truth" is.

But you don't even need a notion of truth. You can get by with any designated values. In mathematical logic the truth values are typically "1" and "0". Now, these are generally taken to code truth and falsity but that is not required. All that is required is that you have a designated value so that you can define a notion of a valid inference as one that preserves designated values.

In many valued logics they will often have more than one designated value. Also, it is hard to see what the values in fuzzy logic would be. Are they "degrees" of truth? Does truth come in degrees?

A quote from Russell's Principles of Mathematics seems appropriate here:


  In addition to these [indefinable primitives of mathematics], mathematics uses a notion which is not a constituent of the propositions which it considers, namely the notion of truth.


I think that much the same can be said of logic, especially math logic. The study of truth is the domain of truth theory. See the http://plato.stanford.edu/entries/truth/ SEP article on Truth.

I really can't state with confidence that informal logic is the same, using rather than defining truth. But a quick scan of the http://plato.stanford.edu/entries/logic-informal/ SEP article on Informal Logic makes me think that what I've said probably holds of informal logic as well.
Ok, there seems to be some confusion here.

A premise is a step in an argument put forward toward establishing some conclusion.

A proposition is the meaning of a given sentence.

Since what you have given is a conditional sentence and not clearly an argument, I'd be inclined to say that what you have is a single proposition.

Now, depending on your view on propositions you might think that this conditional proposition has parts like the antecedent and the consequent. Likely these would also both be propositions.

EDIT:

So, to reflect your update, I'll update my answer. The argument that you give is, as you note, valid. It is an instance of modus tollens. The problem, however, is that it (plausibly) isn't a sound argument. The premise "If men evolved from apes then there wouldn't be any ape nowadays" seems very open to doubt. In fact, it seems to actually be false. Our best theory is that we evolved from apes. But there are, obviously, still apes.

I'm not sure what informal fallacy this argument contains, perhaps a hasty generalization or something? But the most damaging point is that I don't think the argument is sound since I think the conditional is false. 
If they are wealthy then they don't worry about money
they don't worry about money
Then they are wealthy.

This is "affirming the consequent" fallacy.
1) If P, then Q.
2) Q.
3) Therefore, P.
An argument of this form is invalid,  the conclusion can be false even when statements 1 and 2 are true. Since P was never asserted as the only sufficient condition for Q, other factors could account for Q. To put it differently, if P implies Q, the only inference that can be made is non-Q implies non-P.
Scruton analyses Kant's writings as providing different accounts of the relationship between what Kant characterises as the empirical and the transcendental worlds. Scruton's points are clearly expressed and his analysis is accurate. His characterisation of Kant's views is thus both helpful and correct.

Kant's own views are not consistent. That is because he has set himself a compelling riddle but one which is insoluble in its own terms. If our knowledge of the phenomenal world is conditioned by the framework within which we experience it and if the properties of that world are thus also conditioned by that framework, what can we say about the properties of any world not so conditioned?  Kant's noumenal world of "the thing in itself", independent of the conditions under which we experience it, is by definition "unknowable".  So, is the relationship between the noumenal and the phenomenal worlds part of both these worlds or not? Kant's conceptions of time and action have both empirical and transcendental dimensions, but these dimensions (to put the point metaphorically) are themselves incommensurable. 
Aristotle classified states according to two variables: who holds power? And: in whose interest is it exercised? There are three politically possible answers to the first question (one, some and all:the  kingship, aristocracy, and politeia), and two politically possible answers to the second (the holder of power, and everyone). Aristotle treats kingship and aristocracy as an ideal constitution run by morally and practically virtuous people and aiming at the development and exercise of virtue.

The ideal is the state in which the best, who are inevitably few in number, exercise power in the interests of all. However, since that ideal is hard to achieve, and even harder to sustain, Aristotle advocated a form of mixed government, or "politeia", in which all citizens "rule and are ruled by turn", and power is monopolized by no particular class. Aristotle was a vigorous critic of democracy.
When one says "the majority is always wrong", what does one mean by this? Do we mean that the majority is incapable of making perfect decisions (supposing for the moment that it is possible for an external agent to objectively measure the excellence of decisions)? That with universal sufferage, a large portion of people will make their decision based on very badly imperfect information? Or some other criterion? And if they make decisions which are somewhat bad, how do you propose to put into power individuals who, on average, make better decisions?

The problem with autocracy, and the modes of gaining power in societies tending to autocracy, is that the only thing that it consistently selects for is aptitude in gaining and retaining power. The only cases where "gaining and retaining power" is beneficial for "a nation" — that is to say (albeit acknowledging my democratic and humanistic bias when I say this) the people living in that country — are when the general well-being and contentedness of the subjects play a significant role in the ability of the dictator to continue to govern. That is to say, in a dictatorship which is weak, and which must accomodate the masses, or fall to a  coup d'état. A democratic form of government is the logical (and also the historical) progression of governments being forced to accomodate the wishes of the citizenry.

You could, of course, remark that the main selection pressure for career politicians in a democracy is also their ability to obtain and retain power. The question is then: what is necessary for them to do so? Different democratic systems have many failings, but the worst of them — suceptibility to lobbyists or commercial interests, being prone to personal conflicts of interest, and so forth — not only apply to officers in a dictatorhsip as well, but are the sort of problem that we would categorize as corruption, that is to say, a failing to attain the intended standards of the system.

A benign, honest, and strong dictatorship is conceivable of course: otherwise you wouldn't have thought to ask the question. However, concievability is not the same as probability, or reliability. Historically, we seem to find the conditions for a strong dictatorship to be benign rather... rare. Furthermore, "benign" does not entail "well-informed", any more than "popular" does: a ruler who is well-meaning, or even well-informed in some matters, can be badly informed in other matters.

For certain ethical and moral priorities, such as those espoused during the enlightenment in Europe — freedom of movement and settlement, freedom of speech, and so forth — a democratic form of government is the best general scheme that we have found to enact those priorities. This is not to say that these forms of government are perfect for achieving important objectives (such as, perhaps, ensuring the integrity of the environment), but we have no experience with any other form of government which is more reliable for meeting those objectives; and (to make a rather bland understatement) dictatorships have an extremely spotty record for achieving them.
The question is based on a common misunderstanding of Derrida's work-- one he addresses repeatedly.

For example, in "Toward an Ethics of Discussion", he writes:


  "[L]et it be said in passing how surprised I have often been, how amused or discouraged, depending on my humor, by the user or abuse of the following argument: Since the deconstructionist (which is to say, isn't it, the skeptical-relativist-nihilist!) is supposed to not believe in truth, stability, or the unity of meaning, in intention or "meaning-to-say," how can he demand of us now that we read him with pertinence, precision, rigor? How can he demand that his own text be interpreted correctly? How can he accuse anyone else of having misunderstood, simplified, deformed it, etc.? In other words, how can he discuss, and discuss the reading of what he writes? The answer is simple enough: This definition of the deconstructionist is false (that's right: false, not true) and feeble; it supposes a bad (that's right: bad, not good) and feeble reading of numerous texts, first of all mine, which therefore must finally be read or reread. Then perhaps it wil be understood that the value of truth (and all those values associated with it) is never contested or destroyed in my writings, but only reinscribed in more powerful, larger, more stratified contexts. And that within interpretive contexts (that is, within relations of force that are always differential-- for example, socio-political-institutional--but even beyond those determinations) that are relatively stable, sometimes apparently almost unshakeable, it should be possible to invoke rules of competence, criteria of discussion and of consensus, good faith, lucidity, rigor, criticism, and pedagogy."


Or, in this extract from an interview:


  Q: It might be argued that deconstruction inevitably leads to
  pluralist interpretation and ultimately to the view that any
  interpretation is as good as any other.  Do you believe this and how
  do you select some interpretations as being better than others?
  
  JD: I am not a pluralist and I would never say that every
  interpretation is equal but I do not select.  The interpretations
  select themselves.  I am a Nietzschean in that sense.  You know that
  Nietzsche insisted on the fact that the principle of differentiation
  was in itself selective.  The eternal return of the same was not
  repetition, it was a selection of more powerful forces.  So I would
  not say that some interpretations are truer than others.  I would say
  that some are more powerful than others.  The hierarchy is between
  forces and not between true and false.  There are interpretations
  which account for more meaning and this is the criterion.
  
  Q: You would reject, then, the view that meaning is any response
  whatever to a sign?  That meaning is determined by the person who
  reads the sign?
  
  JD: Yes, of course.  Meaning is determined by a system of forces which
  is not personal.  It does not depend on the subjective identity but on
  the field of different forces, the conflict of forces, which produce
  interpretations.
  
  Q: You would, therefore, reject the theory of authorial intention as
  determinate of meaning?
  
  JD: Yes.  I would not say that there is no interest in referring to
  the intentional purpose.  There are authors, there are
  intentionalities, there are conscious purposes.  We must analyse them,
  take them seriously.  But the effects of what we caul author's
  intentions are dependent on something which is not the individual
  intention, which is not intentional.
  
  Q:There is a pragmatic aspect to this question of intentionality.  It
  has been suggested that it is only in the field of literary theory
  that reader-based theories of interpretation are taken seriously, that
  all other fields of discourse accept author-based intention.
  Reader-based theories of interpretation tend, therefore, according to
  this view to partition off literary speculation from the rest of
  experience and thus to trivialise literary speculation.  What are your
  views on this?
  
  JD: I do not accept this opposition between reader-based and
  author-based meaning.  It comes from a misunderstanding of
  deconstruction, one which sees deconstruction as free interpretation
  based only on the fantasies of the reader.  No one is free to read as
  he or she wants.  The reader does not interpret freely, taking into
  account only his own reading, excluding the author, the historical
  period in which the text appeared and so on.
  
  Q: So you would not consider yourself an anti-historicist?
  
  JD: Not at all.  I think that one cannot read without trying to
  reconstruct the historical context but history is not the last word,
  the final key, of reading.  Without being anti-historicist, I am
  suspicious of the traditional concepts of history, the Hegelian and
  Marxist concepts.

I would say Judaism, being that it is the base of Catholicism and Christianity in general and  at the same time the original or first form of the denominations, while the Hebrew community and Jewish practices are being relayed to be current during the time Catholicism had been integrated. I believe he is saying both religions suffered forced extensions(alternates) of the original forms of the ideologies, while the original is still very much in current practice. Superimposed having a forceful, assimilation type implication, the people of the culture where left without choice and both civilizations and religion were made to deal with a very similar issue. And prefigure meaning coming before hand, implies that this happened in North America(Aztec) before the introduction of Catholicism in Europe. The title preface "says in passing" and the situation being prefigured, means(only on a personal level) that it was almost as a prediction of events, one leading or making way for the next course of events.
I think I found http://en.wikipedia.org/wiki/Categorical_imperative#Inquiring_murderer the exact answer for you:


  One of the first major challenges to Kant's reasoning came from the
  French philosopher Benjamin Constant, who asserted that since truth
  telling must be universal, according to Kant's theories, one must (if
  asked) tell a known murderer the location of his prey. This challenge
  occurred while Kant was still alive, and his response was the essay On
  a Supposed Right to Tell Lies from Benevolent Motives (sometimes
  translated On a Supposed Right to Lie because of Philanthropic
  Concerns). In this reply, Kant agreed with Constant's inference, that
  from Kant's premises one must infer a moral duty not to lie to a
  murderer. Kant denied that such an inference indicates any weakness in
  his premises: not lying to the murderer is required because moral
  actions do not derive their worth from the expected consequences. He
  claimed that because lying to the murderer would treat him as a mere
  means to another end, the lie denies the rationality of another
  person, and therefore denies the possibility of there being free
  rational action at all. This lie results in a contradiction in
  conceivably and therefore the lie is in conflict with duty. It is
  possible to conceive of solutions to the problem according to Kantian
  ethics: one could, for instance, tell the murderer truthfully that
  they need to protect the person and thus cannot reveal their location.
  Because such an action might save the potential victim without
  treating the murderer merely as a means to that end, it seems a more
  likely candidate as a maxim of action.


  ... even if it is a crime?


Are crimes immoral per definition? I don't think so. A law is made to represent morality, but often fails and is rather a thin shadow of justice itself. Anyway, the question still stands.

Is it moral to make someone sick or kill someone in order to reproduce myself?


No. Kant, for example, says in his second formulation of the http://en.wikipedia.org/wiki/Categorical_imperative Categorial Imperative, that you should


  Act in such a way that you treat humanity, whether in your own person or in the person of any other, never merely as a means to an end, but always at the same time as an end.
  — Immanuel Kant, Groundwork of the Metaphysic of Morals


When you make someone sick or kill someone, you treat him as a means to an end (you use him), and that's the only way you treat him (i.e. not also as an end). You give in no way something back to the human you hurt as a 'thank you', and therefore Kant would consider such behaviour immoral.
Yes. At the moment, I can think of one philosopher that you could use to argue for this statement (whether he'd think it himself as well, is debatable): Locke. John Locke writes that God has made mankind, and has given mankind several things (like the earth). To respect this act of creation, we have to reproduce ourselves. If you wouldn't know if the one you hurt will get sick or die, it would be arguable to hurt him, as the number of people after that would be greater than or equal to the number before that. 


Let's assume it is okay to hurt someone in order to reproduce yourself. Then...

Is this still moral when it's a crime?

As I stated above, crimes aren't per definition immoral. 


Yes. According to Kant: sapere aude, i.e. dare to think! Kant found it very important not to rely on authority (the law) but to reason and do what your reason says you should.
No. When Sokrates was captured and held in prison, his friend Criton came to see him. Criton was a wealthy man and offered Sokrates to get people to get him out of prison. He wouldn't be able to walk free in Athens, but he could stay alive. Sokrates didn't accept the offer. Even if laws aren't good or moral, you cannot ignore them - for the sake of order. According to Plato, it's very important to have the people in a country do as the government says - the 'normal' people can't understand the reasons of the philosopher king, if everyone would do as he pleases there would be chaos. 

Not really. At least for two reasons (but I'm sure there are more). First, Hobbes' "war of all against all" is a specific view of the "state of nature": there is no regulation, no sovereign, no law. Everyone is equal in the sense that everyone is free to take what they can, notwithstanding moral or any other regulative principles. Might is right. Perfect competition, on the other hand, is based on a certain vision of order - market economy. Moreover, perfect competition means there are no monopolies, which is not exactly in accord with "war of all against all" where the strongest is free to take all.

Second, Hobbes' state of nature brings no positive outcome for the community as a whole (well, there is not even a community). Perfect competition, in contrast, supposedly results in a desirable situation where consumers benefit from lower prices and better quality. To put is somewhat differently, in Hobbes' state of nature scarce resources are distributed inefficiently - they just go to the strongest, whereas under the conditions of perfect competition scarce resources go, supposedly, to those who can make the most of them at the lowest price.

All in all, however, the two concepts come from two very different places and I doubt they can be compared meaningfully.
Losing a limb really hurts. That's a crude but not unreasonable criterion for parts of our body we consider to be our body (skin inwards, basically) and parts we don't (hair, nails, dead skin). We get sense impressions that build up our knowledge of the world from organs with sensory receptors; we don't get any of that from hair.

While the 'importance' answer above explains why we care, that's strictly a different issue - if I had dreadlocks that had taken me years to grow and style I might plausibly be as upset about their loss as I would about losing a less important extremity, but I don't think I'd feel like I'd lost part of my body.
I can only give a brief overview for the marxist defnition of Imperialism, as I don't know enough about the term as used in classical political theory. http://en.wikipedia.org/wiki/Imperialism The wikipedia entry may be better suited for that.

The classical marxist analysis of Imperialism (though I don't know if this was Lenin, or Luxemburg, or Marx himself) states that the developement of capitalism within one nation state sooner or later reaches a limit - capitalist systems want to grow and will always need more resources, workers or people buying the stuff.
At this point the state tries to secure overseas ressources, markets or workforce for it's capitalists. This can happen by subjugating and colonizing another part of the world or by forcing other countries to lower tariffs. This is usually called Imperialism.  

Note: Imperialism is often simply used as a deragoratory term for policies one does not like, or a state or certain policies are called 'Imperialist' in the same way, without bothering with the question of wether  the loose definition given above fits. Sometimes Imperialism is simply used as a synonym for the USA and it's allies.
The idea is that, in set theory, one is developing a formal language to talk about sets. In particular, one is quantifying over sets in set theory. Second-order logic, when it introduces predicate variables, seems to be doing something very similar, since the interpretation of these predicates is meant to be a set. You can't quantify over sets of sets in second-order logic, but as you ascend into higher and higher orders, you come closer and closer to set theory. Quine's objection, then, is that second-order "logic" isn't a logic at all: it's a restricted theory of sets.

If you want to learn more about these things, you really ought to read Shapiro's book Foundations without Foundationalism, which is a fantastic introductory source for both the technical and philosophical material regarding second-order logic. I would also suggest looking at Quine's thoughts on these things in Philosophy of Logic. Boolos also has some interesting articles defending (monadic) second-order logic philosophically as a kind of plural logic (i.e. a logic with plural terms).

With regard to using second-order logic in mathematics, you might want to look into the field of reverse mathematics which deals a lot with second-order PA (I don't know much about it myself). The issue of finding proofs that use second-order logic is a bit tricky, as I understand it, since when you do want to use second-order logic, you can often just get away with using first-order set theory.
This is a longer comment on commando's and your conversation: 

Kant's theory has nothing to do with solipsism. It's impossible that there's nothing else out there but our mind because we need to be affected before there is any thought. Nonetheless, as we human beings have no intellectual intuition, we can only perceive things, and what we perceive aren't the things as they are, but only how they are for us. There is no more "for us" when we die, there is no one left to perceive, and ergo the whole world (as it is for us) vanishes. 

Of everything else we know nothing. Of course we can think of a time when there was no "us" to perceive things. That doesn't change what people can and cannot know, namely things for us and things in themselves.
Your question seems to have few parts, so I'll go through it one piece at a time:


  Is it fair to say that Plato's answer to Juvenals question 'who guards the guardians' - is that they themselves do?


Yes. In The Republic, the guardians are the ultimate authority, and they rely on their own virtue to maintain the state of the whole Republic. This is ultimately why the "Decay of State" occurs, wherein the perfect Aristocracy/Monarchy decays to a Timocracy, which decays to a Democracy, which ultimately collapses into a Tyranny. Though the guardians can keep the populace in order, it's impossible to make them incorruptible themselves, and especially as new guardians come along Plato points out that there is increasing room for error. Ultimately something has to go wrong somewhere in the guardian's self-regulation. There's a whole page of complete mathematical gibberish (which I will cite once I have access to my book) that Plato uses to "argue" for the inevitability of decay, which is summarized by the paraphrase "even the aristocracy will erode with time."


  Is it fair to say that in Platos Republic he answers this by relying on virtue, that is the guardians refrain from selfishness (they guard themselves from themselves) by relying on the noble lie? That is they are taught by the educators of the city to not act in their own interests, but the interests of the whole.


Basically. The Noble Lie, which says that all people are of the same earth but have just been born with different constitutions (Iron, Silver, Gold), was invented by the first guardians. From then on it was taught as truth to everybody, including the new guardians, and because of this myth they feel like the whole city is their kin, and give up their selfishness to protect their family. Unfortunately for the city, such myths are always vulnerable to someone's disbelief, and when that happens the whole system will collapse.


  Could one answer this by setting up two opposing sets of guardians - each watches the other. But then what stops them from acting in concert? If they do we're back to one set of guardians, except of course they've allowed themselves to have been corrupted because they're acting in concert - so the situation is worse than the original context.


As you notice, it looks like there will always be a way for things to fall apart; and that's why Plato says they always will fall apart, eventually. There may be different implementations of The Republic's model, but they will never been incorruptible. One note: you suggest two "opposing" sets of guardians, but I think it helps to see Plato's guardians as autonomous agents, each on their own. Thus the single group of guardians already is many different "opposing" people keeping themselves and each other in check. 
This is unpacked a bit further on in Thoburn's introduction:


  In Deleuze and Guattari's monist thought, then, 'life' has no primary forms or identities but is a perpetual process of configuration and variation, where politics is an art of composition, an art that affirms the variation and creation of life: "'molecular' or 'minor' processes, against striation and identity", 'major' or 'molar' processes... The ramifications of this generalization of politics across the plane of life are great, and this maneuver plays a not insignificant part in the positive reception and use of Deleuze and Guattari's works in recent years, where a frequent theme is an explication of this politicized life in a 'politics of becoming'.


In A Thousand Plateaus, Deleuze and Guattari explain:


  The race-tribe exists only at the level of an oppressed race, and in the name of the oppression it suffers: there is no race but inferior, minoritarian; there is no dominant race; a race is defined not by its purity, but rather by the impurity conferred upon it by a system of domination. Bastard and mixed-blood are the true names of race. (379)


Immediately following this, they invoke the Rimbaud we discussed above about self-identification with racial aleatoriness and inferiority:


  It’s obvious to me I’ve always belonged to an inferior race. I don’t understand rebellion. My race never rose up except to pillage: like wolves round a beast they haven’t killed. (Rimbaud, A Season in Hell)


Just a page later, they explain a bit further about the nomad:


  The nomad has a territory; he follows customary paths; he goes from one point to another; he is not ignorant of points (water points, dwelling points, assembly points, etc.). But the question is what in nomad life is a principle and what is only a consequence. To begin with, although the points determine paths, they are strictly subordinated to the paths they determine, the reverse happens with the sedentary. The water point is reached only in order to be left behind; every point is a relay and exists only as a relay. A path is always between two points, but the in-between has taken on all the consistency and enjoys both an autonomy and a direction of its own. The life of the nomad is the intermezzo. (380)


http://rads.stackoverflow.com/amzn/click/0748669582 Deleuze and Race looks like it explores some of the anti-racist elements of D+G's philosophy pretty thoroughly -- might be worth a look.
Essentially, yes, though the details are still being worked out.

For example, da Costa and Ronde give a paraconsistent axiomatisation in `The Paraconsistent Logic of Quantum Superpositions':

http://arxiv.org/abs/1306.3121 http://arxiv.org/abs/1306.3121

Some additional discussion can be found here:

http://arxiv.org/abs/1404.5186 http://arxiv.org/abs/1404.5186

As an aside, I do think we are biased towards intuitionism. But that might not be such a bad thing; full blown dialethism is a bitter pill to swallow! But that's another story, the relationship between intuitionism, dialethism, and paraconsistent logic has a vast and ever expanding literature.
Just a few points:


  If man is born from the universe, we are a product of the universe.
  This much is certain.


don't be so certain! I would recommend looking at the work of philosopher Immanuel Kant, as well as those in the 'neo-Kantian' tradition for some compelling reasons why this is not a certainty. As an entry point I would consider the possibility that the entity designated by the term 'universe' may be what it is to you because of the perceptual apparatus which your being brings to the act of its perception.


  How can man fabricate the abstract and fictional despite being born of
  the Universe which is concrete and non-fictional?


David Hume I think provides a great explanation when he observes in The Principles of Human Nature that the principles of association governing the relations of ideas to ideas are simply not the same as the laws governing physical phenomena. 


  For example let's say I would find a night sky more beautiful if I
  could see the lights of the next closest galaxy. This is impossible
  due to physical limitations (atmosphere interference, human eyesight
  limitations, etc), but any person can comprehend the idea and paint an
  image of it within their own minds.


Given the fact we can demonstrate the existence of other galaxies empirically, I fail to see how this relates to the question of the possibility of fictions? 


  Is it strange that the mind, which is born out the universe, can
  conceptualize physically impossible things? 


Indeed it is strange. But is it not also evolutionarily advantageous to be able to envision things being other than the way they are?  


  Why are we able to think
  of things that cannot be factual despite being born out of the purely
  factual?


This is a good example of what is often referred to as 'https://en.wikipedia.org/wiki/Map%E2%80%93territory_relation mistaking the map for the territory'. Your being born refers to a physical phenomena, and was not the product of two abstract facts copulating. It is a fact that you were born, but in order to be recognised as a fact, it first has had to have happened. 
What you are looking for is the https://en.wikipedia.org/wiki/Sorites_paradox Sorites paradox. 


  A typical formulation involves a heap of sand, from which grains are individually removed. Under the assumption that removing a single grain does not turn a heap into a non-heap, the paradox is to consider what happens when the process is repeated enough times: is a single remaining grain still a heap? (Or are even no grains at all a heap?) If not, when did it change from a heap to a non-heap?


Much has been written about this. Wikipedia offers a good and accessible summary of proposed resolutions. In case you are interested in a more academic and thorough approach, the http://plato.stanford.edu/entries/sorites-paradox/ SEP-article on the Sorites-paradox is what you're looking for.
When the empirical apperception functions, it allows you to conceptually recognize this or that particular object, as a unified object. You sense a manifold of stuff, but are able to combine what you sense into a object (say a unique, specific cup of coffee). Kant argues that this presupposes the transcendental apperception, that in itself can have no objects, but conditions and apply rules, that allow you to recognize an object as a an object.

This entry from http://hume.ucdavis.edu/mattey/phi175/apperception.html G. J. Mattey's Kant Lexicon seems to make a clear differentiation. I highlight parts of it:


  The notion of apperception is introduced in the Transcendental
  Deduction of the categories of the understanding and plays a central
  role therein.
  
  (...)
  
  Generically, apperception for Kant is self-consciousness. It may be either empirical or pure. In the first-edition (A) Deduction
  (A107), Kant equates empirical self-consciousness with inner
  perception or inner sense, whose object is succession of states of
  ourselves in time. “This consciousness of oneself is merely empirical
  and always mutable; it can give us no constant or enduring self in the
  flow of inner appearances.”
  
  By contrast, pure apperception does not have anything experienced as
  its object. Moreover, unlike empirical intuition it is
  productive. In the second-edition (B) Deduction, Kant claims that
  it “produces the presentation I think” (B132). It is also “immutable” (in contrast to empirical apperception as described at
  A107). It is “numerically identical” (A132) or “one and the same in
  all consciousness” (B132). A further feature is that “it cannot be
  accompanied by any further presentation” (B132). Because of these
  features, pure apperception is also called “original” apperception.
  
  One crucial feature of pure apperception is its unity or thoroughgoing
  identity. The unity of pure apperception is described as
  “transcendental” because it is an a priori condition for the
  presentation of objects. The second key feature of pure apperception
  is its necessity. The unity of apperception can be either analytic or
  synthetic.


According to http://plato.stanford.edu/entries/kant-mind/ this very detailed entry in the  Stanford Encyclopedia of Philosophy, the crucial passages related apperception is "blindingly difficult". In the Stanford article it is also disputed, that we can equate apperception with self-consciousness.
It's difficult to answer this question without more context. That personal crisis brings us to question our selves and try to understand our place in life is perhaps a question of practical philosophy. But one needs to know where to begin.

Martha Nussbaum, is an American moral philosopher. In her book The Fragility of Goodness, she explores, what she calls luck (and others call mis/fortune or fate) on what it means to be an ethical being and to live a good or flourishing (eudaimonia) life. In her opening section she comments on a few lines of Pindar, a Greek lyric and praise poet:


  But human excellence grows like a vine tree
  
  fed by the green dew, raised up
  
  among wise men and just, to the liquid sky


She herself writes:


  The excellence of the good person, he writes, is like a young plant: something growing in the world, slender, fragile, in constant need of food from without. A vine tree must be of good stock if it is to grow well. And even if it has good heritage, it needs fostering weather (gentle dew and rain, the absence of sudden frosts and harsh winds), as well as the care of concerned and intelligent keepers, for its continued health and full perfection. So, the poet suggests, do we. We need to be born with adequate capacities, to live in fostering natural and social circumstances, to stay clear of abrupt catastrophe, to develop confirming associations with other human beings.  


The poems next lines are:


  We have all kinds of needs for those we love:
  
  most of all in hardships
  
  but, joy too, 
  
  strains, to track down eyes it can trust


She continues:


  Our openness to fortune, and our sense of value, here again, both render us dependent on what is outside of us;...because we encounter hardships and can come to need something that only another can provide; our sense of value, because even when we don't need the help of friends and loved ones, love and friendship matter to us for their own sake. Even the poets joy is incomplete without the tenuous luck of seeing it confirmed by eyes, on whose understanding, good will, and truthfulness he can rely on.   


Her book is on the vision of life by the tragic poets and dramatists of ancient Greece. Plato, although (apparently) a promising tragic poet turned away from this vision in an attempt to find a way of living life that would become impervious to the viccissitudes of fortune. His way is the way of contemplation. She considers this a radical solution - almost denying the body. She shows how Aristotle tries to harmonise what is best in the poets with can be salvaged from Plato. 

Bearing in mind what Nussbaum has said, let us follow the thread of thought of Octavio Paz, a Mexican writer who published in 1950, an essay - The Labyrinth of Solitude. He won the Nobel prize for literature in 1990, and this essay is worth quoting from at some length as it touches on some of your concerns:


  Solitude - the feeling and knowledge that one is alone, alienated from the world and oneself - is not an exclusively Mexican characteristic. All men, at some moment in their lives, feel themselves to be alone. To live is to be separated from what we were to approach what we are going to be in the mysterious future. Solitude is the profoundest fact of the human condition...Man is nostalgia and a search for communion. Therefore, when he is aware of himself he is aware of his lack of another, that is, of his solitude.


Although, Octavio does not say so, he is referring to the symbolic myth of Eden in Genesis where man is cast out of the green world when he is aware of himself. This is also reflected in the Greek myth of Prometheus who having brought the fire of intellect, the divine spark of consciousness is punished by the gods. 


  All our forces strive to abolish our solitude . Hence the feeling that we are alone has a double significance: on the one-hand it is self-awareness, and on the other it is a longing to escape from ourselves. Solitude - the very condition of our lives - appears to us as a test and purgation, at the conclusion of which our anguish and instability will vanish. At the exit of the labyrinth of solitude we will find reunion and plenitude and harmony with the world.


The labyrinth is of course the inescapable lair of the mythical minotaur and was killed by the Greek hero Theseus. He is helped by the monsters half-sister Ariadne who guides him out of the labyrinth via a thread she had tied to him. This is the reverse of the North European folk-tale Sleeping Beauty where the prince wakes the princess. Of course both are tales of sexual awakening, of love and of socialisation to the adult world.

Jung said, to lose Ariadnes thread, is to lose the thread to life. Borges, the Argentinian writer, in The House of Asterion, expands the myth by expanding the labyrinth to take on the proportions of the entire universe, Asterion (the Minoan name for the Minotaur) himself has forgotten he has created the labyrinth - he wanders it, lost - and looking forward to death, his redeemer; which does come as Theseus. He remarks to Ariadne later, before he abandons her in Naxos for her sister, 'that the Minotaur hardly defended himself'. Borges is writing here about the death of the Christian God and skillfuly inverts Christian narrative of Man being redeemed by God, of God being redeemed by Man. 


  In our world, love is an almost inaccessible experience. Everything is against it: morals, classes, laws, races and the very lovers themselves...Modern eroticism, is almost always rhetorical, a complacent literary exercise. It is not a revelation of man; it is simply one more document describing a society that encourages crime and condemns love. 


If Paz was writing now, it would be a complacent virtual or visual exercise, rather than a literary one. 


  Society pretends to be an organic whole that lives by and for itself. But while it conceives of itself as an indivisible unit, it is inwardly divided by a dualism which perhaps originated when man ceased to be an animal, when he invented his self, his conscience and his ethics. Society is an organism that suffers the strange neccessity of justifying its ends and appetites...too often they even deny mans profoundest instincts...when this last occurs, society lives through a period of crisis: it either explodes or stagnates. Its components cease to be human beings and are converted into mere souless instruments.


At the dawn of the American age, that is just post the Civil War, Walt Whitman sang out the organic wholeness of America in his long muscular lines, which two centuries later at Ferlinghettis City Lights bookshop shaped into one long Howl by Ginsberg in 1955. This was predated by the visit of the Spanish Poet, Lorca to New York where he wrote a seminal sequence of poems including this:


  Dawn arrives and no one receives it in his mouth
  
  because morning and hope are impossible there:
  
  sometimes the furious swarming coins
  
  penetrate like drills and devour abandoned children.


To continue with Octavia Paz:


  Industrial societies, regardless of their differing 'ideologies', politics and economics strive to change qualitative human - that is, human - differences into quantitative uniformity. The methods of mass production are applied to morality, art and emotions. Contradictions and exceptions are eliminated, and this results in the closing off our access to the profoundest experiences life can offer, that of discovering reality as a oneness in which opposites agree.  


Mercantile values replacing personal ones, is the diagnosis offered by Arthur Miller in Death of a Salesman in 1949. Francis Bacon in Britain was painting solitary figures cubically arranged in positions of estrangement.


  The child must face an irreducible reality, and at first he responds to its stimuli with tears or silence. The cord that united him with life has been broken, and he tries to restore it by means of play & affection. This is the beginning of a dialogue...through magic the child creates a world in his own image and thus resolves his own solitude. Self-awareness begins when we doubt the magical efficacy of our instruments.
  
  Adolescence is a break with the world of childhood and a pause on the threshold of the adult world. Solitude is a distinctive characteristic of of adolesence. Narcissus, the solitary, is the very image of the adolescent. It is during this period that we become aware of our own singularity for the first time. But the dialectic of the emotions intervenes once more: since adolescence is extreme self-consciousness it can only be transcended by self-forgetfulness, by self-surrender. 
  
  The literature of modern nations  are filled with adolescents, with solitaries in search of communion: the ring, the sword, the vision. Adolescence ends with entry to the world of facts. 


It is a world of facts, because it is a shared world, and thus objective one.


  Solitude is not a characteristic of maturity. When a man struggles with other men or with things, he forgets himself in his work, in creation or in the construction of objects, ideas and institutions. His personal consciousness unites with those of others: time takes on meaning and purpose and thus becomes history. 
  
  During vital and productive epochs, therefore a mature man suffering from the illness of solitude is always an anomaly. This type of solitary figure is very frequent today, and indicates the gravity of our ills. In an epoch of group work, group songs, group pleasures man is more alone than ever. Modern man never surrenders himself to what he is doing. A part of him - the profoundest part - always remains detached and alert. Man spies on himself. Work, the only modern god, is no longer creative. It is endless, infinite work, corresponding to the inconclusive life of modern society. And the solitude it engenders - the random solitude of hotels, offices and shops and movie theaters - is not a test that strengthens the soul - a necessary purgatory. It is utter damnation, mirroring a world without exit.


Since you posted this under Existentialism, its worth thinking about this in the context of Camus - The myth of Sisyphus, and Sartres drama of the soul - No Exit
Short answer:

No, anything "defined" is man-made, thus artificial.

Long answer:

The only natural laws are the laws of nature, like thermodynamics or gravitation. These laws are not defined, but described. They are not invented, but discovered. We could say that everything on Earth has the right and the obligation of being pulled (down) by Earth's gravity force, but we cannot get much further from that. These laws don't specify how things should be but how they necessarily are.

The rights about "the pursuit of happiness" and such are just agreements humans do because just by making this contract we live in a slightly better world, and we like it.

These rights are artificial, man-made contracts, and we should not deny our responsibility by any means in the definition of these rights and in the consequences that these definitions (or the lack thereof) may have. Denying that responsibility could be considered an act of http://en.wikipedia.org/wiki/Bad_faith_%28existentialism%29 bad faith.

Bonus: rights defined by a god would only be as natural as that god (from 0 to 100%). Besides, AFAIK gods are more interested in defining obligations than rights.
Of course not all fallacies have the same weight of importance. A logical fallacy is an error of argument that renders the argument invalid (e.g. assuming the consequent) and depends only on the logical form of the argument, but a fallacy of relevance (argument by authority) depends on judgment of that authority. 

Within logical fallacies, the severity of the error depends on its primacy in the argument (how close to tthe root in the proof tree it is) or fixability (it is sometimes easier to fix "A->B,B then A", than it is to fix something with an accidental negation dropped in.

And within fallacies of relevance, an argument by authority or numbers is less serious than an argument by force (ad baculum).

Are you trying to quantify relative strengths objectively, give numeric strengths to errors? That might be possible with logical fallacies, but fallacies of relevance really depend a lot on qualitative real world assessment (one person's fallacy by authority is another's expert witness), so I expect that will be very difficult, like trying to quantify esthetic judgments. It can be attempted, but might be difficult as nailing down jello.

  But are we really able to say that all lives are worth the same amount?


Sure we can, without judging. As each life has the same future potential, despite its past. So its reasonable to choose for 10 times more potential (to give the train direction) then to let it go and kill 10 people instead of 1.

Let me explain a bit more.
Do you know what life is about? Do you know of the general purpose of all living/non-living entities? If the purpose would be to discover, learn, grown and increase the quality of ones being in purpose of improving the overall system in which they are defined: Then wouldn't it be more likely for 10 people to improve the overall system, in which they are defined, more, than 1 person?

Compare it with cells in your body of equal function/levels. (assuming that humans, physical bodies on planet earth are of equal 'levels'). Would you choose to let only one cell become damaged instead of 10 if you had the choice? Sure you would choose for the least amount of damage, as its the best out come for your physical system, your body. You do not judge your cells here, you conclude. So by this logic you do not judge, you conclude. 

But now you question is misphrased: "Is it right to judge if the life of one person is worth more than ten others?"

So your question could become: "Is it right to conclude that the life of 10 persons is worth more than the life of one person".

I would say, yes.

  In a situation of limited resources, that doesn't seem to hold up; if I were given my state's entire treasury, for instance, it would undoubtedly be good for me, but disastrous for the state as a whole.


I think you are misunderstanding, or mis-applying, the rule.

If being given the state's entire treasury is good for you, it would likewise be good for someone else if that person were given the entire treasury.


  in a situation of limited resources, does the improvement of one member's well-being tend to improve or deteriorate the well-being of its group?


That depends upon how you define "the well-being of the group".

Suppose we have 10 people who each have one loaf of bread, and compare them to 10 people, where 9 have nothing and 1 has 10 loaves of bread.  

What is your metric for measuring the well-being of the group?
Your friend is in danger of justifying that it is acceptable to drive while under the influence. Here is why:


Your friend likes engaging in behavior which is sometimes considered unacceptable.
For situations where this behavior is considered unacceptable, your friend could:
a) maintain his [unacceptable] behavior, or 
b) modulate his behavior correctly, or 
c) avoid them, or 
d) never behave unacceptably so as to avoid the possibility of [e.g.] damage.
Your friend has chosen 2.a) and blames others when such situations arise.


You can see how this would be utterly unacceptable if the 'unacceptable behavior' were drunk driving instead of being a potty mouth. Our societal conventions on that matter are clear: ignorance is not an excuse. So, your friend wants to use ignorance/inability as an excuse for his behavior in a situation with lower stakes. Is this valid? Is it valid because the stakes aren't as high, and nobody 'really got hurt'? You and he will have to decide what rules you choose to live by. My own $0.02 is that people are generally terrible at estimating how much harm their behaviors can inflict, but many people think that their estimation algorithms are just dandy—except when others hurt them, of course.



A shorter version of my answer is that you are to blame to the extent that your friend needs to be treated like a child. We often do say that parents are to some extent responsible for their children's actions.
I believe what you are describing isn't a fallacy so much as a http://en.wikipedia.org/wiki/Confirmation_bias cognitive bias.  In this case, the closest cognitive bias that fits your description is http://en.wikipedia.org/wiki/Motivated_reasoning motivated reasoning where "when people form and cling to false beliefs despite overwhelming evidence, the phenomenon is labeled 'motivated reasoning'. In other words, 'rather than search rationally for information that either confirms or disconfirms a particular belief, people actually seek out information that confirms what they already believe.'"

The term logical fallacy isn't always defined the same way, but it generally refers to errors in the reasoning process itself, but doesn't describe the cognitive process by which people commonly acquire false beliefs.  Fallacies are often erroneous by their similarity to valid modes of reasoning, such as in the case of hasty generalization where it is difficult to define exactly how much data you need in order to make a valid inference, or in other cases the fallacy is simply deceptive in its own right, in the case of the fallacy of composition.  The syllogism you describe doesn't sound to me like a "syllogism" in it's own right because what you describe as an inference really looks like a description of the utterer's cognition, which you may or may not be correct about.
Mathematics isn't art.  Only art is primarily focused on emotional expression or evocation.  All other things that might evoke strong emotions and feelings as a secondary objective or byproduct (an algorithm, a hand made musical instrument, a mathematical solution) aren't art.  Even though we might call them "works of art" that's a superlative and technically incorrect, they're something else first and emotionally expressive second.

A meal can be art if the focus is on emotional response.  A painting can be not-art if it's more purposefully messaged (propaganda or advertising).  The boundaries are of course vague as so much art could be considered religious propaganda (no disparagement intended here).  And of course http://en.wikipedia.org/wiki/Readymades_of_Marcel_Duchamp a urinal can be art.  But mathematics is almost always purposed into the not-art category.  I suppose it could be repurposed as art (like the urinal), or it's possible that mathematics could be performed as art, but this would be the rare case, or maybe an art form that has yet to really be developed.

Obviously I'm putting a fairly rigid definition on art, but if not constrained this way anything can be art.  The anything-can-be-art viewpoint might be useful in some ways, but then what would artists do?
Yes, in part. Socrates is teasing Ion, but also with a purpose. He is drawing Ion out. Ion is flattered by the endorsement of his interpretive talents as derived from the gods, and thinks that this is an endorsement of his artistic or interpretive expertise. Part of why Ion appreciates this view of his talents lies in its connection with the Greek understanding of art as something channeled from the gods through an artist. If his talents are a gift from the gods, they are genuinely artistic. Artistic inspiration originates with the muse, passes through the artist (and maybe though an actor or rhapsode like Ion), and then reaches  audience. Plato even uses the idea of magnetic stones to try to help us think about how that force works. It's strongest with the muse, but then passes to the artist, who is then endowed with magnetic attraction, but slightly weakened as it gets further from the muse.

But with respect to the flattery: as Ion agrees with the idea that his talents are skills given by the gods, in the sense that he has become a good channel for the their artistic force, you rightly note that he thereby gives up the idea they are a matter of his having acquired knowledge. By encouraging Ion in this direction, and helping him along by stoking his considerable vanity, Socrates creates an opening to attack Ion's expertise. He argues that Ion does not have any knowledge of the things he rhapsodizes about. Even though Ion interprets Homer's depiction of a battle, he does not have any expertise about actual war, and the Athenians should not ask him to be their general. So, Socrates argues, he doesn't know anything about his subject-matter. He doesn't have expertise about it. So, if he doesn't know what he's talking about, all he is doing when he presents Homer, Socrates argues, is whipping up feeling. But the ability to stir up feelings is not expertise in anything worthwhile, in Plato's estimation, because having strong feelings provoked in a non-rational way is a bad thing to have happen to us. Specifically, Ion's skill lies leading his audience away from clear, rational, and  philosophical thought about subject matter like battle. And that is, for Plato, a very dangerous thing to do, maybe even the most dangerous thing to do.

So, in summary, by going along with Socrates' flattery, Ion puts himself in the position of many of Socrates' interlocutors, which is to say vulnerable to Socratic attack on his claims to know and have expertise about important questions.
I dont think that they are synonyms.

Akrasia (ακρασία = insobriety ) is the explanation of not being able to do the morally good although their knowledge, ie what moral knowledge has no per se effect enforcement, employs a central problem in philosophy since antiquity. For Socrates, if anyone from people not sin on purpose, the problem is the lack of actual knowledge (not willingly  bad). Plato, anticipating that the Socratic solution was impossible as such, constructed a metaphysical edifice ("theory of ideas") to explain the insobriety describing simultaneously the required moral knowledge of a particular type of people (State). Aristotle finally takes another route explanation accepting the crucial role of emotions and human passions in the psychology of human action.

Procrastination ( κωλυσιεργία = κωλύω + έργο) is just postponement. The energy of procrastination, the constant interference or reliance barriers, real or artificial, in a process in order for it to be delayed or not completed.

I can only see a connection if you suppose that procrastination is the cause of akrasia but its not the only factor to consider.
Peter Singer is a preference utilitarian as he expounds in his famous book, Practical Ethics. The author of the first document you cited also defines Mr. Singer as a preference utilitarian:


  Preference utilitarian Peter Singer argues that, most likely, each person is more qualified than anyone else to know what is in their own interests as long as they are competent (thus, parents probably know more than their infant child about what is in their child’s interests). 




The author is trying to argue that there is a fundamental division between act and rule utilitarians, and that these utilitarians are then further divided into flavours, such as preference and hedonistic:


  [...] a hedonistic act utilitarian thinks that an action is morally right if its effects help to maximize the sum of every being’s happiness. 
  
  And a hedonistic, rule utilitarian thinks that an action is morally right if it conforms to a set of rules that, when generally followed, help to maximize the sum of every being’s happiness. 
  
  Finally, a preference act utilitarian thinks that an action is morally right if, in the long run, it satisfies more informed preferences than it frustrates (with deference to the importance of those preferences).

You're looking for the http://en.wikipedia.org/wiki/Ship_of_Theseus ship of Theseus:


  The ship of Theseus, also known as Theseus's paradox, is a paradox that raises the question of whether an object which has had all its components replaced remains fundamentally the same object. The paradox is most notably recorded by Plutarch in Life of Theseus from the late 1st century. Plutarch asked whether a ship which was restored by replacing each and every one of its wooden parts, remained the same ship.


The Wikipedia page lists some variants, as well.
It depends on what you mean by "objective" and "existence."  Personally, I'd be very well-disposed to acknowledge that Santa has at the least a functional existence --after all, some entity delivers gifts to all those children, even it it isn't a physical human being dressed in red to who lives at the North Pole.  So, you might say that Santa exists, we're just significantly wrong about the nature and the details of his existence.

With that said, you might want to keep in mind that I'm also inclined to grant "existence", under my definition, to any number of conceptual and collective entities who other people might be inclined to argue against.
I take "a brown horse and a dark ox are three" in an ontological sense. If you have a brown horse and a dark ox you have three things:


Dark; hence light. Without light there would be no need for a word for dark.
A horse; hence Animals; hence living things;
The color brown; hence colors; hence a conscious observer who can see light and give names to particular frequencies like brown.


If you grant me a brown horse and a dark ox you grant me three of the most important things in the universe: Electromagnetic radiation; life; and consciousness.
I think I disagree with @David Schwartz. I'd read Aristotle's claim differently. I don't think he's offering a definition of ``good'' here per se (and despite what he says).*  It's more that he's giving three different examples of the kind of thing that it is proper to attribute the property of goodness to. 

In other words, I think he's saying there are three kinds of good things: 


First, those things which we "ought to choose for their own sake". Let's call these intrinsic goods.
Second, things which are chosen "for the sake of something else." Let's call these instrumental goods.
Finally, there if there is something that is the ultimate object of desire then that thing should be called good too.  


An example of the first kind of thing is happiness.** Aristotle says in Nicomachean Ethics Book I that happiness is just desirable for its own sake. Now Aristotle knows of course that some people want bad things, so the mere fact that somebody wants something that does make that thing intrinsically good. But he does think that virtuous people who have been brought up properly, who have lots of experience of the world, and who have thought carefully about life will be able to know which things are worth wanting and which aren't. The intrinsically valuable things are the ones that such people would want, and since we want to be like those people, those are the ones we should want too.  

An example of an instrumental good would be exercise. I don't like exercise itself, I find it painful. However, exercise is a means toward an end (good health) that I do want, therefore there is a sense in which exercise is good, even though it is not the thing I want per se. It's the thing that gets me the thing that is intrinsically good, so it exercise is instrumentally good as a means toward that end.

The final thing about the ultimate object of desire refers back to a little puzzle Aristotle takes up at the beginning of Nicomachean Ethics I. Is there just one thing that is intrinsically good that all other goods are instrumental goods for attaining, or are there many diverse goods? This is a famous interpretative problem in Aristotle, but I think Aristotle thinks that happiness is the one good or goal of all human activity, and other things that look like intrinsic goods are just part of "happiness" in some way. Obviously, if happiness is that for the sake of which all human activities are done, then happiness is good. Now the reference to other kinds of things, sensitive creatures (animals) and so forth is simply to indicate that for Aristotle humans are just one specific kind of animal with one specific kind of nature and one specific kind of end. Happiness is a very human thing and there might be other ultimate goals of the activities of other kinds of creatures. I don't know what more to say about this alternative though, b/c it looks very obscure to me what these might be and I'm not aware of anywhere else that Aristotle discusses it, although I don't know his biological works very well.

*The reason I say this is that Aristotle has a quite precise technical notion of what it is to give a definition of something that involves being able to express that thing's essence in terms of which kinds or types it belongs to. "Man is a rational animal" is an example of a definition in this technical sense. Words like "good," "being," and "one" are what Aristotle calls ``transcendental'' properties, because they aren't kinds or types,  rather they are properties that things of every kind can have. 

**"Happiness" for Aristotle is not just an emotional feeling or psychological state. It refers to a kind of holistic evaluation of one's life. "Blessedness"or "flourishing" might be a better translation.
Acording your definition, Power of fear.

Because to make sense, you must ask 1) Power of fear vs 2) Power of fear of power.
And then its clear that if you are affect by 2) you are affected by 1) but if you are affected by 1) it does not have to be 2). So total effect is trivialy greater.
The answer is yes, no and maybe - the answer being dependent on whether one takes an empirical or philosophical perspective.

Empirically laws do change. Although its commonly taken that Newon discovered gravity, in fact everyone does, its so obvious that it geneally doesn't usually warrant the name of a law. Aristotle had a dual nature of gravity where celestial objects moved in circles and terrestial objects moved in striaght lines towards the centre of the earth. Newtons discovery can then be put in perspective as seeing that these two separate phenomena where aspects of one force. Hence Universal gravity. But there was a lacuna  in his theory - how was force transmitted at a distance? Einstein fixed this by showing gravity was curvature of spacetime. He discovered this by pondering on the fixed speed of light which went against Galilean mechanics. This fixed speed of light is seen as one of the fundamental fixed constants of physics. But some physicists have speculated that the speed is not constant always, close to the big bang they have considered how & why it might change - this being a proposal to explain some cosmological features whose standard explanation is inflation.

Philosophically, that is metaphysically, that is after or before physics - one can consider whether the true laws of physics - the ones that physics aim at - are they fixed or not. This is a tricky question. First the laws may be fixed but our descriptions of them may vary. Secondly they may be fixed but we may never know this - since there is no reason per se that new laws come into existence at higher and higher energies. Perhaps there is a law whose effect is only apparent at energies close to the big bang. 

For this reason physicists generally talk about laws upto a certain energy. 

Further, taking a Kantian perspective, or correlationist in some contemporary discourse, the laws of nature as understood or intuited by us are within only the phenomenal realm; in the noumenal realm, it exists as the phenomenal world supervenes on it, but it itself is indescribable, having no attributes we can ever hope to have purchase on. If one then considers nature in its proper sense to be both this world of phenomena and the world of noumena then one can say that the larger part of nature is forever out of reach. 

Or one can take a Spinozan perspective. If God exists as the solely neccessary self-subsistent sunstance, and the world is his creation, then supposing nothing can come from nohing, the world itself is a part of God, for this Spinoza was denounced as a Pantheist; further he said that God had an infinite number of modes, with only two being cognisant by us - extension and thought. So again when world is considered as a whole, the larger part of the world, that is God, is beyond us. 

An Islamic perspective take all laws to be fixed by God except for human beings who are endowed with free will to make ethical choices. So all laws of nature at bottom until revoked by God are at bottom fixed. One could suppose that God could unfix these laws, but this goes against the spirit of what is meant by Nature here. The world of natural phenomena in the world.
No, Hegel did not consider private property a natural right in the sense in which Locke and others did. The reason is that for Hegel these things are not self-evident truths that appear to our perceptive faculties.

For Hegel, rights occur in a social framework as a judgment of the society that this attaches to a person. To put it another way, rights are for Hegel mediated social objects. At the same time, as a I interpret Hegel (and  there are those such as Rorty who differ), these rights are not merely social -- they are actually there in the world. But by world, he means the way that we as rational animals perceive what is out there (Hegel does not believe in immediate sense perception).

To put it another way, think about what a right is. It is either the license for one rational animal to do something without the interference of another rational animal or the requirement that a rational animal cooperate with the task that another rational animal (or group of rational animals) sets forth. This is abstract because it cannot make itself real. As an idea until implemented by rational beings, it does not mean anything without their activity. Moreover, it does not occur merely in nature -- its actualization occurs only in reason.

To give an example to make this make better sense, if you have a right to life, then that imposes on others a duty not to kill you. But It actually does not prevent murder. It merely makes it so that when that right is violated, we judge what has been done to be wrong. Returning to your case of private property, private property is the right to have something for one's own use without others being able to impose on you what you can do with or impede you.  But they can impede you unless they actualize your right by granting it to you. 

For Hegel, this will have to be just one moment in Absolute Spirit, because the arrangement is already flawed insofar as the maintenance of something individual depends on something social.  The solution will be to move to a concept of property that includes society's "rights" to the property and the individual's "rights" to the property into an integrated concept of social right.
https://philosophy.stackexchange.com/questions/9725/is-equality-an-virtue-in-itself-or-a-derivative-of-envy Is "equality" an virtue in itself or a derivative of envy?


  What is the underlying motivation of the desire for equality?


Envy is most likely part of the answer. Helmut Schoeck argued that envy is not often discussed, but has been observed/studied by thinkers in almost every society [https://en.wikipedia.org/wiki/Helmut_Schoeck%5d https://en.wikipedia.org/wiki/Helmut_Schoeck].

In addition to your "burn the excess" example, there are other thought experiments which illustrate envy. For example, Rawls, in A Theory of Justice, tried to argue that the difference principle (inequality is permitted in society iff it improves the lot of the worst-off) is not motivated by envy. He argued that the principle arises from an unbiased construction of self-interested agents in an original position. Nozick, in Anarchy, State, and Utopia, presented an alternative theory of just distribution which, he argued, should also appeal to these agents, provided that they were not motivated by envy.
When one person is unethical and everyone else is ethical, that person gains an advantage.  But when everyone is unethical, everyone suffers.  It's the classic "Prisoner's Dilemma".  

From an evolutionary point of view (if you accept the concept of group fitness) an ethical population as a whole could outcompete an unethical population.

I don't have a citation handy, but I believe there have been studies showing that the actual distributions of unethical versus ethical behaviors in areas such as sexual fidelity do roughly correspond to theoretically optimum distributions.
The truth values of classical propositional logic form a http://en.wikipedia.org/wiki/Boolean_algebra Boolean algebra. The only subdirectly irreducible Boolean algebra has cardinality 2 (True & False). Hence the equational theory of classical propositional logic is completely determined by the two element Boolean algebra.

The truth values of intuitionistic propositional logic form a http://en.wikipedia.org/wiki/Heyting_algebra Heyting algebra. Surprisingly, the class of subdirectly irreducible Heyting algebras is not really smaller than the class of Heyting algebras itself:


  Every Heyting algebra with exactly one http://en.wikipedia.org/wiki/Atom_%28order_theory%29 coatom is http://en.wikipedia.org/wiki/Subdirectly_irreducible subdirectly irreducible, whence every Heyting algebra can be made an http://en.wikipedia.org/wiki/Subdirect_irreducible SI by adjoining a new top. It follows that even among the finite Heyting algebras there exist infinitely many that are subdirectly irreducible, no two of which have the same equational theory. Hence no finite set of finite Heyting algebras can supply all the counterexamples to non-laws of Heyting algebra. This is in sharp contrast to Boolean algebras, whose only SI is the two-element one, which on its own therefore suffices for all counterexamples to non-laws of Boolean algebra, the basis for the simple http://en.wikipedia.org/wiki/Truth_table truth table decision method. Nevertheless it is decidable whether an equation holds of all Heyting algebras.


This quote from Wikipedia makes it pretty obvious that studying the subdirectly irreducible Heyting algebras won't help much for the understanding of intuitionistic logic.



Every http://en.wikipedia.org/wiki/Field_of_sets field of sets is a Boolean algebra, and every Boolean algebra can http://en.wikipedia.org/wiki/Stone%27s_representation_theorem_for_Boolean_algebras represented as a field of sets. As a consequence, classical propositional logic can also be regarded as the logic of subsets. In David Ellerman's paper about http://www.ellerman.org/the-logic-of-partitions/ the dual of the logic of subsets, I read that (it is supposedly well known that) intuitionistic logic can also be interpreted as the logic of open subsets. There is also a later simplified (32 pages instead of 64) http://www.ellerman.org/introduction-to-partition-logic/ introduction to partition logic, which I haven't read yet.
Every Heyting algebra can be represented as the open elements of an http://en.wikipedia.org/wiki/Interior_algebra#Heyting_algebras interior algebra. Every interior algebra can be represented as a http://en.wikipedia.org/wiki/Field_of_sets#Topological_fields_of_sets topological field of sets with its interior and closure operators corresponding to those of the topological space. Another aspect is that every "n-ary operator between open sets" composed of pointwise operators and interior operators can be defined by an intuitionistic formula. But note that in the same way as a field of sets is not necessarily isomorphic to the set of all subsets of a set, a topological field of sets is not necessarily isomorphic to the set of all open subsets of a topological space.
"Symbiosis (from Ancient Greek σύν "together" and βίωσις "living") is close and often long-term interaction between two or more different biological species."

http://en.wikipedia.org/wiki/Symbiosis http://en.wikipedia.org/wiki/Symbiosis

Assuming these married people belong to the same species, their pair-bond is not an example of symbiosis, by definition. In order for your idea to work, you would need to change the accepted definition of symbiosis.

You and your gut bacteria are in symbiosis. It's true that they don't cook you breakfast; but they do help you digest it.
The most widely-discussed theories of right-action in recent anglophone philosophy, Kantianism and Utilitarianism, each have ways of allowing that actions that would otherwise be wrong can be right under certain circumstances. Both understand such an action as right when it's done for the right reason (and no longer wrong).

Kantians have long discussed the “murderer at the door” scenario, where a murderer asks you where someone you know is, because the murderer wants to kill him/her. Kant brought up this example in Groundwork for the Metaphysics of Morals. Recent Kantians have argued that while it is usually wrong to lie, it is morally right to lie in this case. Philosopher Christine Korsgaard has a great discussion of this in her essay “The Right to Lie: Kant on Dealing with Evil."

Utilitarians, similarly, would argue that it's possible for actions that, performed in isolation, would be harmful to be good, if they produce the most overall happiness. The second chapter of John Stuart Mill's Utilitarianism discusses this point.

This forum is not the best place to find literary/fictional references, but one fun example I think of is Jack London's The Assassination Bureau, Ltd. which is about a group of professors who decide that it's okay to murder people who are harmful to society, and argue with each other about that.
I would prefer “hasty generalization” as the fallacy, because it generalizes to the full set from just two cases. If it were a slippery slope, it would usually at least be implied by the speaker that the second case follows from or is an effect of the first, such that the final result is a consequence of that initial move from the first to the second case. (E.g. “If you outlaw guns then you have to outlaw swords, and then ...  you'll have to outlaw butter knives.”) Instead, this is just a case of “Here are 2 cases, so clearly every case!”
The short answer is no. Physicalism is the thesis that everything is exhaustively describable and explainable in the language of fundamental physics, or in "higher-order" language, such as the language of chemistry, biology, etc. that is theoretically reducible to the language of physics. Whether physicalism is true or false is a difficult philosophical question. 

Here's one thing to say though: if physicalism is true, it's probably just a contingent truth, not a necessary one. Here's why. A sentence is a contingent truth if and only if there is at least one accessible possible world in which that sentence is false. The relevant sense of possibility here is nomological possibility. So, consider all those world that have exactly the same physical laws as the actual world. 

Now, suppose in one of those possible worlds there is a lonely ghost made up out of some goo that physics just cannot ever explain. We know that the ghost can't interact with any of the physical stuff--this would entail a violation of the laws of physics, but the mere fact that it couldn't interact doesn't mean it can't exist. If there is such a ghost in that world, physicalism is false. This means, that even if physicalism were true in the actual world (i.e. even if there aren't any ghosts in our world) then it is merely a contingent truth. 

All this argument relies on is the idea that if one conceive of such a ghost in a world nomologically identical to ours, then it is possible that such a ghost exists.  This conceivability-possibility principle is controversial, but it isn't clear what better guide to possibility there is. 
In this case, the optimal choice would be using modal logic (see http://plato.stanford.edu/entries/logic-modal/ this SEP article to get started). Machinery of modal logic was designed to deal with situations similiar to yours, i.e. talking about a thing and its counterparts in all possible worlds. This way, there is this https://philosophy.stackexchange.com/questions/2484/was-socrates-a-fictional-character-invented-by-plato historical figure named Socrates, which is a part of our (i.e. actual) world, and also there are other fictional Socrateses, who inhabit other possible fictional worlds.
Suicide is usually an expression of a lack of motivation.

So it is not an expression of fear or courage. 

So you are neither courageous nor cowardly in attempting suicide. You are just lost and don't know any other way out other than the ones you tried already.

A good expression could be: "Death by mental or emotional starvation by constant repetition of the same acts."

A person that dies of hunger also is not a coward or courageous. He is just lacking nutrition. Or in other words, he has no choice but to die. So it really is not a choice, but a lack of options to choose from.

If you are referring to the actual step of harming yourself. Well life is just slow death anyway. So we all are committing suicide everyday until we succeed. The actual suicidal person just found a way to get there quicker. He lost the will or the reason to live he lost the illusion that kept him going. 

Could he find a new illusion. Yes sure. But love is pretty strong. 

If you boil it down to logic it is just like begging your enemy to kill you finally vs. leaving you to die slowly and in agony. It just makes rational sense. You could say he is a coward for not wanting agony or you could say he is courageous for wanting to die vs begging for mercy. Either way. The individuals perspective says, there is no way out, let's end this instead of continuing to suffer without purpose.
The criterion for truth-functionality is something like the following: 


  Definition. (truth-functionality) A sentential operator ☆ of arity k is truth-functional if and only if the truth-value of ☆(φ1...φk) is a function of (i.e. depends entirely on) the truth-values of φ1...φk.


Example 1. The usual sentential operator ¬ (of arity 1) is truth-functional according to (Definition) iff the truth-value of ¬φ is a function of the truth-value of φ. Is that the case? The usual semantics of ¬ is the following: ¬φ is true if φ is false, and ¬φ is false if φ is true. As you can see, to calculate the value of ¬φ, it is necessary and sufficient to know what the truth-value of φ is.

Example 2. The usual sentential operator ∧ (of arity 2) is truth-functional according to (Definition) iff the truth-value of (φ1 ∧ φ2) is a function of the truth-values of φ1 and φ2. Recall the semantics of ∧: (φ1 ∧ φ2) is true just in case φ1 and φ2 are both true, and false otherwise. Again, since no other piece of information is needed to settle the truth-value of the conjunction, we know that ∧ is truth-functional.



The first step toward applying the (Definition) to your examples is to identify the operators (for some L):


  Translations.
    (1) φ → ψ
    (2) Analytic(φ, ψ)
    (3) Counterfactual(¬φ, ψ)
    (4) Contradiction(φ)
    (5) (⊥ → φ)


Assuming that these capture the meaning of your (1–5), the second task is to define their meanings:


In (1) we have a binary operator →, whose semantics is: (φ → ψ) is false if φ is true and ψ is false, otherwise it's true. As this standard semantics makes clear, nothing more than the truth-values of φ and ψ are needed to settle the truth-value of (φ → ψ). That means that (φ formally implies ψ in L) is a truth-functional operator.
In (2) we have a binary operator Analytic, whose semantics depends on one's particular view of what it means for a sentence to be analytic. There is no standard semantics to which we can appeal to settle the question. But, following Carnap, we can define a sentence φ to be analytic just in case φ is logically implied by a set of meaning postulates Π, which can be thought of as a giant conjunction of sentences that capture the meanings of terms relevant to us. 

For example, all bachelors (B) are unmarried men (U ∧ M), so Π will include a sentence like this: ∀x(Bx → (Ux ∧ Mx)). We can then ask: is it analytically true that if someone is a bachelor, he is a man, i.e., is Analytic(Bx, Mx) true? With our Carnapian semantics for Analytic, the problem reduces to checking whether: [(Π ∧ Bx) → Mx] is logically true. Without going into the precise semantics of 'logically true', to check that, we consider a model that satisfies Π and suppose that an arbitrary object is a bachelor. The sentence will be false if that object is not a man, otherwise it will be true. But since the model satisfies Π, which forces all bachelors to be unmarried men, the sentence will be evaluated to true.

The moral of this long excursion is this: analyticity relies on the meanings of the terms occurring in the formula, so simply knowing the truth-values of the component sentential symbols doesn't allow us to conclude whether the compound formula is true or false. So Analytic is not truth-functional.
In (3), like in (2), we have a binary operator that depends on information other than the truth-values of φ and ψ. I should say that, (3) is ambiguous, because you didn't specify whether the modal auxiliary is a 'might' or a 'would'; if it's a 'would', we have a universal operator, if 'might, an existential one. Whichever may be the case, to evaluate the truth-value of (3), we need to know the similarity ordering between the possible worlds. The truth-values of φ and ψ aren't enough to determine the truth-value of the compound expression. So Counterfactual is not truth-functional.
In (4) we have a unary operator standardly defined as follows: Contradiction(φ) is true just in case Tautology(¬φ) is true. And Tautology(φ) is true iff all truth-assignments to the sentential symbols occurring in φ make φ true. Since to determine whether a formula is a tautology it's necessary and sufficient to know the truth-values of the component sentences, Contradiction is truth-functional.
In (5) we have a disguised identity operator because (⊥ → φ) is equivalent to (⊤ ∨ φ), which is equivalent to φ. So the operator in (5) takes a sentence φ and returns true if φ is true, and false if φ is false. We have there a trivial case: the truth-value of the compound formula (φ) is not only dependent on the truth-value of the component formula φ, but is the truth-value of that component.


Needless to say, my explanations (1–5) depend crucially on the (Translations), the (Definition), and what I consider to be the standard semantics of the usual connectives. To figure out the answer to your homework, you need to make sure that the (Translations) make sense and the (Definition) captures the meaning of truth-functionality as defined by your teacher. If not clear about anything, leave a comment.
Take the logical fallacy that seems to come up the most: Appeal to Authority. People will make decisions all the time because someone with lots of experience or an important sounding title told them to do so.

I think rather than saying "No Fair! That's the appeal to authority fallacy," you guide the person to the meat of why it's not a compelling argument by asking questions:


"I'm trying to understand better, why do things work that way?"
"Can you provide an example in the past where such a thing worked?"
"Why did you come to that conclusion?"


Rather than putting some one on the defensive, it allows the other person a chance to make a sound argument.
I'm going to have to disagree with Chris. 

Any analogy is going to rely - http://en.wikipedia.org/wiki/Analogy to one extent or another - on the background knowledge of the audience it's being presented to. 

In this particular analogy, it relies on the audience knowing what guns are, that there is already legislation in place controlling them, that some people violate this legislation, and that they are proposing more legislation to restrict access to them.

It also relies on the audience knowing what cars are, that there are laws in place specifying who can drive and what is acceptable, and that there are those that violate these laws.

Since Chris did a good job of defining what the argument here actually is I am going to borrow that from him.

Gun control IS TO reducing deaths from gun violence AS making it harder for sober people to own cars IS TO reducing deaths from drunk driving.

However as we can see from the assumed background knowledge of our audience, they know that there are laws in place affecting the use of both cars and guns. Which means that we do have an analogous person on the gun side for our sober person, and heres why.

When we say sober person, we really mean law abiding person since the law specifically states that you must be sober to operate a car. So a law abiding gun owner is, (i.e. someone who does not commit gun violence) in this analogy, equal to a law abiding (sober) car driver. 

The next piece from Chris's response I want to tackle is this:


  This still has the issue however of the "drunk" stipulation on the right. Again, there's no obvious analog on the left, and again
  there's an illegitimate implication (that all gun deaths are results
  of the same type of flagrant misuse of guns as drunk driving is a
  flagrant misuse of a car)."


The fallacy here again goes back to background knowledge and the fact that the audience knows that gun control is an issue because of the violence for which guns are used.

Which means that it is specifically the "flagrant misuse of guns" that is at issue here. 

Since our person being discussed on both sides is a law abiding person, and the issue at hand is the breaking of laws ie: a person committing gun violence, or a person driving drunk, 
then accidents must of necessity be taken out of the equation since accidents happen with both cars and guns. 

With accidents dismissed from the equation and our background knowledge established what our argument is really trying to say is that:

Restricting law abiding citizens from owning guns to prevent gun violence is like restricting law abiding citizens from owning cars to prevent drunk driving.

Which is an effective statement of absurdity, clearly laying out the anti-gun control point of view. But also a statement that could - with enough research - be verified as true or false. Which to me says that it is a highly effective analogy.

(Interesting note http://www.cdc.gov/nchs/fastats/injury.htm almost as many people die in car accidents as are killed with guns in the US each year, while gun accidents are in most years http://wiki.answers.com/Q/How_many_accidental_gun_deaths_are_in_the_US_every_year?#slide=1 under 1000 deaths.)

DISCLAIMER: I neither endorse or denounce gun control, that is not what this question is about; it is about the effectiveness of a particular analogy and so my response digs into the reason I think the analogy is either good or bad. NOT wether gun control is good or bad. Also thanks Chris for giving me such a wonderful backdrop from which I could place my arguments against. +1.

All of the things you mention are big factors, and to that I would add:


Individuals working in the legal systems (especially prosecutors and police) can have personal incentives to "get" convictions because it helps their career.
(perhaps the biggest factor) people suck at being impartial. They trust their intuitions and emotions more than cold reasoning. The typical view is "I know that person is guilty, now I just have to prove it". Someone being acquitted due to lack of evidence is often seen as a failure of the legal system.

There are two notions to be distinguished here: http://en.wikipedia.org/wiki/Logical_consequence#Syntactic_consequence logical consequence (⊢) vs http://en.wikipedia.org/wiki/Material_conditional material implication (→), each of which, in classical logic, has the unusual property of http://en.wikipedia.org/wiki/Principle_of_explosion explosion, which we can summarize as:


  ⊢–Explosion. {S,¬S} ⊢ Q, for any Q.
  
  →–Explosion. Under assignment v(S) = ⊥: S → Q, for any Q.


1 The →–Explosion follows simply from the definition of → in terms of disjunction and negation:


  (φ → ψ) =df (¬φ ∨ ψ),


because whenever the truth-assignment v is such that v(φ) = ⊥, then φ ∨ ψ follows, for any ψ. Proof theoretically, whenever ¬φ is proved, you can ∨-introduce ¬φ ∨ ψ where ψ can be any sentence whatsoever. The reason why the classical consequence explodes is a little more interesting.

2 According to the usual http://en.wikipedia.org/wiki/Logical_consequence#Semantic_consequence Tarskian interpretation of logical consequence:


  Logical Consequence. Γ ⊢ φ is true iff it is impossible to make all ψ ∈ Γ true and φ false. 


Whenever you have a sentence S ∈ Γ s.t. ¬S is also in Γ, you have the sentence (S ∧ ¬S) ∈ Γ. Consider an arbitrary sentence Q; is Q a logical consequence of Γ, given that Γ contains such an inconsistent conjunction? Let's appeal to the definition above:


  {S, ¬S,...,Sn} ⊢ Q is true iff it is impossible to make all of S, ¬S,...,Sn true and Q false.


Since S and ¬S are inconsistent they cannot both be made true, therefore, whatever else may be contained in Γ and whatever Q may be, it follows that {S, ¬S,...,Sn} ⊢ Q. It is an immediate byproduct of the classical definition of consequence. 

There are, of course, many non-explosive logics, such as Belnap and Anderson's http://plato.stanford.edu/entries/logic-relevance/#Sem relevance logic where from a contradiction an arbitrary Q isn't allowed to follow because implication has to meet special 'relevance' requirements. Look at that last SEP article for the details on how exactly that works. Most relevantly, there are many http://plato.stanford.edu/entries/logic-paraconsistent/ paraconsistent logics specifically created to handle this so-called 'paradox' of material implication. Worth checking out.

Utilitarianism

From a http://en.wikipedia.org/wiki/Utilitarianism utilitarian (maximizing total benefit and reducing suffering or the negatives) point of view, you could argue that the one cent makes no difference to someone with $10000, while you benefit fairly significantly from all the one cents.

But, in fact, it makes little difference (not no difference) to that person. If someone were to steal one cent one million times from that person, they'd be broke. If it truly made no difference, they wouldn't end up broke.

Now it's unlikely that someone will have one cent stolen from them enough times to make a different to them, so the utilitarian argument isn't that particularly bad.

However, if you were to break it down into stealing one cent at a time, it basically comes down to who has more money (although presumably how much money affects happiness / suffering as well). So, if you were poorer than them, you could make a fairly strong utilitarian argument for stealing from them (Robin Hood comes to mind).
Deontology

http://en.wikipedia.org/wiki/Deontological_ethics Deontologically speaking (related to one's duties and others' rights), you could say they have a right to not have something stolen from them, and you have a duty not to steal.
Egoism

Considering http://en.wikipedia.org/wiki/Ethical_egoism egoism (maximizes good for the self), you can also consider it right simply because you yourself end up better off.

Then again, under some variation of this position, cold-blooded murder can also be justified.


There are also http://en.wikipedia.org/wiki/Normative_ethics a few other positions one can consider.
There's nothing wrong with your understanding of prior here. You've understood what he is saying correctly, though it appears you think he is wrong in claiming this.


  Further, the state is by nature clearly prior to the family and to the individual, since the whole is of necessity prior to the part


He's not talking in biological terms but in social terms. In biological terms, the family is made of individuals so that one nor the other are prior. Socially, that is in terms of relationships, all families grow up in the shade of the state, whether that be the nation or a city, the tribe or clan. 


  if the whole body be destroyed, there will be no foot or hand, except in an equivocal sense, as we might speak of a stone hand; for when destroyed the hand will be no better than that


Its quite clear that individuals cannot be nurtured to adult-hood if separated from the state - that is left in the wilderness. (This is also an observation captured in one of the folk-tales collected by the brothers Grimm - Babes in the Wood); and also from rare reports about feral children. Its also the reason why the Ancient Greeks considered exile to be considered a great punishment.


  The proof that the state is a creation of nature and prior to the individual is that the individual, when isolated, is not self-sufficing


Consider an adult, say a man, remove him from the bosom of the state - that is place him in solitary confinement - and he becomes like 'stone' - ie destroyed as a human being (often such confinement causes serious disturbance to the psyche - ie psychosis & sociopathy)
There's several ambiguities in your question that an answer needs to address. First, I am going to assume that your uses of "should" refer to moral rather than legal determinations. Otherwise, this isn't going to fit under philosophy. Second, I'm going to answer in terms of applied ethics rather than formal ethical theories. Third, I will following your question, use rights language.

Assuming citizens have rights, there are several different ways to conceive of rights. There's Isaiah Berlin's positive and negative rights distinction. On this account, negative rights where the government should not interfere with a course of action. Positive rights reflect those cases where the government or some other party is compelled to act for you. "Keep the government out of the bedroom" articulates a negative right --> to be able to sleep with whom you choose when you choose. "The government should make sure no one goes hungry" is a positive rights claim that the government has a duty to supply everyone with food. 

A second distinction some draw is between first-order and second-order rights. A first-order right is something you have a direct right to. A second-order right is something that you have in light of your first order rights. So if you have a first order right to live, then you may claim a second order right that the government give you food. If you have a first order right to speech, then you have a second order right to the means of speaking. (i.e., the government cannot prohibit you from making speeches in public areas if you have a right of free speech). Actually, some take this further and speak of 3rd order rights -- but that's irrelevant for our purposes.

We need one further distinction to complete the philosophical picture. There are absolute and mitigable rights. Absolute rights are those which are of "incomparable worth" (to use a construction from Kant) whereas mitigable rights are those of "price" -- meaning they are tradeable for other things of moral value.

Given these three axes, we can give some answers. 


For an absolute negative 1st order right, the government is always wrong to inhibit this right.
For any mitigable right (negative or positive / 1st or 2nd order), the government could for a sufficiently good reason abrogate its responsibility to its citizens.


After that it gets a little cloudy mostly because the philosophical pieces break down.

For an absolute positive 2nd order right, it seem possible that some conflict could prevent its execution without being immoral. imagine for instance, a right to paper and printing to build on a right to free expression. Conceivably if the government has no paper, it has lost its ability to perform this right and therewith failed in this right -- but not necessarily morally. (or to put it another way, is it possible for a right to be 2nd order and absolute? Also can there be absolute 2nd order rights? i.e. rights where the government's duty of non-interference is not immediate but is absolute? ) 

For an absolute positive 1st order right, it would be immoral for the government not to supply but it's difficult to understand what would qualify as such a right.
Natural rights and human rights originally come from different vocabularies. It's not fair to construe natural rights as "simply a less developed and more concise version of what we now consider human rights."

First, I want to start by pointing out an important but crucial ambiguity in the term "natural rights."  Viz., the problem is that "nature" can mean many things. Thus, one meaning of natural rights are those rights granted to an individual by Nature (though in naturalized versions not capitalized); another meaning is those rights granted to a person as a result of their nature. Thus, the US Declaration of Independence speaks of natural rights as those granted by the Creator in making humans the way they are. The second meaning of natural right could be stated in saying people have rights to food, community, and procreation, because these follow from human nature. The senses are somewhat integrated in that most rights granted by Nature (as God, Nature, nature) are generally presumed to accrue as consequences of the sort of creatures that are made. 

To make it simpler with an analogy, if God declares we have the right to free tacos, then this would be natural in the first but not the second sense, but if God makes us so that we need food, then we may have a natural right in the second sense to the provision of food [thought not necessarily tacos].

Human rights can differ from both of these senses or overlap. Here, the adjective identifies those things which accrue to humans (an objective genitive) rather than those things that follow from being human. Thus, any right that we think people deserve and/or we grant to people is a human right if we grant it to them because they are human. 

To give an example, the UN declares that all humans have the right to a country of citizenship. This can be understood in one of two ways. Either you could believe this follows from the nature of human beings and is thus a human right we should recognize or you believe that this is a right we have given humans by legal force and thus becomes a human right.

Again, we can return to our example. If the UN council passes a bill saying we deserve free tacos, then we have a human right to free tacos in those places where UN pronouncements mean something. The UN might argue that these rights follow logically from what we are in which case they are claiming a natural right-basis for this human right or rather that they are merely pronouncing an already existing natural right and codifying it.

We can now return to the sense in which you viewed human rights as progress over natural rights. This is definitely there in the literature. But this does not mean human rights are necessarily better. What it means is that our idea of rights has shifted. When rights talk first takes off in the natural rights form, it is seen as the perception of a pre-existing right inherent in nature (in the second sense) or a right God/Nature has woven into the fabric of existence (first and second senses together). Human rights talk can also include positive rights -- rights that exist only because we've decreed that they do. But with this comes the critiques.

Human rights can wind up being arbitrary precisely insofar as they are not necessarily grounded in nature. They also tend to be revocable or if rights with no basis are granted, deleterious to a society. Moreover, this type of rights talk requires a certain Western type of legal system. Thus, we see many critiques from Chinese philosophy (see Confucian Ethics: A Comparative Study of Self, Autonomy, and Community and the work of Theodore de Bary) and from Western communitarians (e.g. Alasdair MacIntyre) and from feminists sand others.
This isn't a philosophy question per se, but I find it interesting because it can be addressed from a cognitive perspective that targets reasoning, which is on-topic.

I don't see a paradox, strictly speaking, but I see three ways (and combinations thereof) in which the inconsistency that you mention can be dissolved:


The belief that I have an Impostor Syndrome doesn't imply that "I believe that I am worth more than what I believe", but that I should believe that I am worth more than what I believe. (No contradiction here.) Once I do believe that I am worth "more than what I believe", I am actually believing that I am worth more than what I once believed. (Again, no contradiction.)


However, this may not be a good representation of the cognitive processes. So:


There can be different http://en.wikipedia.org/wiki/Propositional_attitude propositional attitudes at work. So, you might accept the diagnose that you have an Impostor Syndrome, but still believing that you're worth less than you rationally should. (Insert other propositional attitudes in place of "accept" to address other psychological nuances.) 
The belief that I have an Impostor Syndrome isn't stable as long as I have an Impostor Syndrome. So I believe at a time t that I am worth more (when I am able to acknowledge my accomplishments) than I believe at another time t' (when I am unable to internalize my accomplishments). Through the course of therapy these mental attitudes might switch back and forth a lot, but (hopefully) will stabilize towards a rational acknowledgement of my accomplishments.

Assuming that there are no extant works of http://en.wikipedia.org/wiki/Thales Thales or http://www.iep.utm.edu/thales/ Thales of Miletus (c. 620 BCE – c. 546 BCE), numerous sources, starting from his contemporaries accredited him with sicentific discoveries (geometry) and a "naturalistic" approach, based on rejection of godly intervention in the explanation of natural phenomena.
I think your question turns out to be confused. Obviously, there are conceptions of justice that are incompatible with Kant's ethics. But I think the example you raise won't pass muster on this count.

If I'm understanding you correctly, you argue:

(1) Kant says we should treat people as ends and not mere ends
(2) Harsh penalties for drug traffickers further a policy and not people
Ergo, (C2) this is immoral since it is not treating people as ends


There are two main problems with this argument.

As a historical note, Kant's moral philosophy is not wholly captured by a single formulation of the categorical imperative in the Groundwork. Kant also wrote and published a volume called http://en.wikipedia.org/wiki/The_Metaphysics_of_Morals Metaphysics of Morals which subdivides into the "Metaphysics of Right" and "Metaphysics of Morals". You also need to consider what Kant thinks in Religion within the bounds of reason alone (which is a work primarily of moral philosophy).

The issue that happens which undermines the suggestion you are making is that Kant was opposed to all forms of what he would call drunkenness (as distinct from drinking). Kant has no problem with imposing harsh penalties on these sorts of cases. All of this highlights a problem with what you capture under the term policy.

viz., isn't the policy meant to further humanity, as in the good of people, on some level?

In other words, if we unpack the term policy as humanity:

(1) Kant says we should treat people as ends and not mere ends
(2') Harsh penalties for drug traffickers further humanity
(C1) this treats people as means
Ergo, (C2) this is immoral


But this too is invalid -- because Kant does not say we can never treat people as ends. Instead, it is that we can never treat people as mere ends. Thus, this fails to be problem for Kantian philosophy.
Can you define "natural necessity"? For Kant, this term has a specific meaning and you don't seem to be using it how he would. For Kant, the need to produce children is a natural necessity. Not sure how wanting to die could be a natural necessity? Other than that, he mostly opposes objective and natural necessity -- arguing that we need to subordinate the  our desires and physical needs to the demands of objective morality. 

For Kant, one clear case where you can will your own death is when you choose to die doing something heroic. But the trick there is that you are not in fact willing your own death as the core of your maxim -- you are willing something else and accepting the fact that you will your own death. If you consciously will to end your own rationality, this for Kant is per se immoral as it fails the universalization test and the treating humanity as an end test.

Kant more directly addresses your question in the Metaphysics of Morals when considering the duty not to kill yourself and mentioning casuistical questions about whether you can rationally chose to end your life when you know your rationality will be compromised shortly by a disease or other things. Kant leaves unanswered whether you can kill yourself when you know you will go crazy with rabies in the next day or so, but he seems sympathetic to that choice.

A key reason why is that Kant has a type of moral absolutism. That is to say, Kant does not grant exemptions because you got a bad lot in life. So even if you will suffer in agony for weeks on end, it is not at all clear that Kant would grant that you can pre-empt that by ending your life. He specifically distinguishes this from the cases of animals in pain on the grounds that we have a duty to respect our own rationality (this is also the reason Kant thinks lying is wrong in the Metaphysics of Morals)



Contemporary Kantians have tended to disagree with Kant. Christine Korsgaard and others generally try to make a similar move to the one you are suggesting. They try to include "quality of life" considerations but those are generally absent from Kant's own view.
http://en.wikipedia.org/wiki/Epictetus Epictetus, a Greek and a slave to a secretary to Nero is applying Stoic ethics to the problem and ethics of slavery. He says, essentially treat them kindly; and you will benefit too. I find this extract fascinating as the slight knowledge I have of Greek Philosophy suggests that the relations between master & servant weren't subject to ethics. Later in life, he was crippled, with one tradition stating this was done by his master, and another saying he was lame from childhood.
To run through this in detail:


  If you intend to improve, throw away such thoughts as these: 


He intends to offer advice


  if I neglect my affairs, I shall not have the means of living


The world is a large place, the ways of making a living are varied; one need not press on continually with ambition; rest is possible - indeed recommended; but also the world offers more to the sensability than that of affairs - politics  or trade.


  unless I chastise my slave, he will be bad. 


The slave is a some-one; who also has his ethical being; one need not assume from the outset that he must be bad; this does not mean that one should assume that he is wholly good; most men are imperfect; one should use observation & judgement. 


  For it is better to die of hunger and so be released from grief and fear than to live in abundance with perturbation; 


Rhetorical statement. He doesn't literally mean this. He is high-lighting the disabling nature of constant anxiety (perturbation). In Buddhism this is another form of dukkho - suffering.


  and it is better for your slave to be bad than for you to be unhappy.


One cannot watch him all of the time. If he is bad, then a little badness is liveable with. It is hard to make paragons of men. By releasing yourself from the task of turning your slaves into paragons of servitude - you yourself can attain a more equable frame of mind.


  Begin then from little things. Is the oil spilled? Is a little wine stolen? 


He's setting a scene - a little drama. Has the slave been careless whilst waiting on you at the table? Has wine gone missing from storage? It seems obvious we should blame those who are foreign to us - our slaves. We may allow
ourselves to feel a degree of righteous anger. 


  Say on the occasion, at such price is sold freedom from perturbation; at such price is sold tranquillity,


But if a little oil is spilled, or a little wine is stolen - do not start raging. Perhaps laugh: this slave is always a little clumsy. But she sings well. Or he plays with the children well. Through this your estate and yourself are tranquil.


  but nothing is got for nothing. And when you call your slave, consider that it is possible that he does not hear; and if he does hear, that he will do nothing which you wish. But matters are not so well with him, but altogether well with you, that it should be in his power for you to be not disturbed."


Recall the asymmetry of power here: you are the master & he is the slave. You have power over him; be mindful of him - if not charitable; the equivalent in Kantian terms is when you treat a man as an end (slave, servant or other hired help) then do not treat them as a mere end; but recall that they too, are men, with souls (psyche) as your own.

edit

Some additional context supporting the above interpretation follows the quoted text; for Epictetus goes on to write:


  'How then shall a man endure such a person as this slave?'
  
  Slave yourself, will you not bear with your own brother, who has Zeus as his progenitor, and is like a son from the same seeds and of the same descent from above? But if you have been put in any such higher place, will you immediately make yourself as a tyrant? Will you not remember who you are, and whom you rule?
  
  That they are kinsmen, that they are brethren by nature, that they are the offspring of Zeus. 

The http://en.wikipedia.org/wiki/Original_position original position is Rawls replacement of the (hypothetical) originary state of Mankind theorised in social contract theory - the state of nature, as by Hobbes; the principles that govern society are to be chosen behind a veil of ignorance; in the original position one does not know what talents, what gender what race or intelligence one will be endowed with; hence Rawls argue that fairness will govern society.

One supposes in this situation that eugenics would be ruled out; for after the veil is removed perhaps you find that one is in a category in which non-existence is prescribed, for you're not of the best genetic stock.

Now Rawls writes just prior to your extract:


  I have assumed so far that the distribution of natural assets is a fact of nature and that no attempt is made to change it, or even to take it into account.


Up until this point Rawls had analysed the institutions of society and not individuals; he will now go on to discuss Justice for inividuals but before doing so he commits himself to not discussing the 'natural assets' - that is the talents of a man - seeing them fixed; he won't discuss in depth the possibility of change; this of course is known as Eugenics, and Rawls will have known its implication in the Holocaust (as described by Hannah Arendt in her densely detailed book - Totalitarianism - the science of eugenics was widespread on both sides of the Atlantic); and this is the reason for his silence; his intellectual honesty pushes him to note however that:


  But to some extent this distribution is bound to be affected by the social system. A caste system, for example, tends to divide society into separate biological populations; while an open society encourages the widest genetic diversity


and also


  In addition, it is possible to adopt eugenic policies, more or less explicit. I shall not consider questions of eugenics, conﬁning myself throughout to the traditional concerns of social justice. 


He affirms here, that post-holocaust questions such as these were not even possible to speculate on.

So the answer to your question is no; Rawls refuses to develop a theory of eugenics; he purely admits its possibility.
Good would still be good if everyone was good.

The mere fact that we would be able to determine ourselves as good would imply the ability to determine bad , of which no one would be.

Throughout the ages the dictum 

Do unto others has you would have them do unto you

Would still apply. This seems to be inherent in most humans ( although that might be a discussion for another question ) and a principle based on these inherent truths would still exist ( unless it does not considering this being a hypothetical )

I think a certain ambiguity in your assumption is that the bases of good is that it is the antithesis of evil, and if that is the case , then evil is determined as the antithesis of good which creates circular reasoning.

This is an interesting question because what if we extend this to animals. If animals apply to the same good / evil paradigm as humans , then the mere action of getting food for a lot animals is evil. Could it not be that in the existence of the animal kingdom that their actions, being so deterministic, are by their very nature only good. Could we consider killer whales evil for hunting their own kind? Could they consider themselves evil? Is it the mere fact that we can consider at all, the primary reason that there is a separation between the two?

but getting back to your question

Does it mean that it's actually good that some people are evil and the statement "It's good to be bad" can be true?

This in itself is a contradiction of terms, relative only to the people unaffected by the evil perpetrated. Although the "The No Asshole Rule" is cute, the asshole can negatively effect member/s of the group ,where all the others are guided by the fact that they should not act like him, yet the effected member/s are still inflicted by the asshole, and so would not consider it good.

Would it be better if everyone was good? Well i think there would be less pain in this world, i think there would be more respect, i think it would be more human.
Only some sentences have truth values - those that do are called propositions. The given sentence isn't a proposition so one should not expect it to have truth-values. 

Propositions are a small part of meaningful sentences. For example 'Hello, how are you?', 'that was wicked!' are meaningful sentences, but they aren't propositions. The sentence 'the invisible worm that flies through the howling storm' is meaningful in context, which is a poem by Blake - http://en.wikipedia.org/wiki/The_Sick_Rose The Sick Rose - and here the 'Rose', is not just a Rose; but also England and/or perhaps Nature; to gather its full meaning one must engage in the the hermeneutics of intepretation taking account of Blakes ouevre and his influences. 

But to go back to your sentence - or rather Chomskys; as he suggested it as an example of a gramatically correct sentence but that it lacks meaning; this is correct in one sense, but not in another; the meaning of this specific sentence is the one that Chomsky gave it:'grammatically correct but lacks meaning'; this appears as a reflexive contradiction - but the contradiction is only apparent; Chomskys notion of meaning is a narrow one - the one alluded to earlier; it would be an interesting study to specify just how narrow.
My perspective is as a family medicine doctor in training. Yes, I do find it unethical for doctors to refuse patients who cannot pay on the basis of increasing profits.

I work in several underserved, safety-net clinics and hospitals that take all patients regardless of ability to pay.  This is my career choice because I want to contribute to the solution of our broken system.  

These clinics have social workers and financial advocates that help uninsured patients try to get insurance or financial support to cover costs and allow them to get consistent healthcare.  These clinics receive some funding from the government as well.  None of these practices become filthy-rich, regardless if they are private or state or university-based.  The practitioners are paid well, but not as much as in a private practice owned by the physician(s) who charge what they want and only take the insurances they want, rejecting anyone uninsured who can't pay up front.  

These income-driven physicians are pushing their self-pay (uninsured) patients towards the "safety-net clinics," causing these clinics to carry a high % of the burden of uninsured patients while receiving fewer insured patients to recover costs.  That imbalances the distribution of reimbursement to practices.

If there were limitless numbers of providers and clinics, it would be a minor issue.  

But there is very limited space in medical schools and residency training programs.  There is a primary care physician shortage in the tens of thousands and rising.  These profit-driven physicians are taking up spots in training programs that could instead be taken by people willing to accept a good (but not exorbitant) income while investing in balancing and improving the system, rather than just draining the profits of it.

So in summary, yes I do find it unethical.  But that is largely because it's in the setting of a broken, ineffective healthcare system (especially the insurance system) that drives costs excessively high and makes access to quality care a commodity, rather than a human right.
There are two senses of Unless.

It's possible that "unless" may indicate the strong sense of the word, in which case it takes a different form and you'd be correct, but this implication is contextual. This context can be implied by the sentence's content, or even from the way the sentence is constructed. For instance: 


  u1. Unless you study, you won't graduate.
  u2. You won't graduate unless you study.



The denial of u1's antecedent, "if you study, you will graduate" is certainly not implied. However, using u2's construction in english often (not always) indicates that the exact same word to be taken in a stronger sense. Rather than "if not", we read this stronger sense as "if and only if not". To illustrate, rewriting both examples to use the more familiar logical indicators results in the following clauses:


  u1*. If you do not study, then you will not graduate.
  u2*. You won't graduate if and only if you don't study.


Note that u1* only guarantees that if you graduate, you studied, allowing you to fail even if you study. However, u2* to use the correct form illustrates why you may feel the example you were reading "feels" wrong. If you're using the stronger sense of unless, then if you don't graduate, you must not have studied— which fits your intuitions.

Incidentally, P <—> ~Q will result in the same truth table as the exclusive disjunction, "xor":


+---+---+----------+--------------------+
| P | Q | P <—> ~Q | (P v Q) & ~(P & Q) |
+---+---+----------+--------------------+
| T | T |    F     |         F          |
+---+---+----------+--------------------+
| T | F |    T     |         T          |
+---+---+----------+--------------------+
| F | T |    T     |         T          |
+---+---+----------+--------------------+
| F | F |    F     |         F          |
+---+---+----------+--------------------+




For a full description of these two uses of "unless", see http://fitelson.org/109/hardegree_ch4.pdf sections 16 and 17 of this course reader. 
These could be referred to variously as common knowledge, common sense, assumptions, conventional wisdom and various other terms, depending on the correctness or otherwise that you are ascribing/insinuating of them. 

For instance, if you wish to imply that something without known proof is true, you may choose to state that "it is common knowledge that...", but to imply that you believe an assumption is false you may choose "it is conventional wisdom that ..."  . All such arguments encourage the listener to proceed without explicit proof, and should in such a case be carefully scrutinised.  
In the first part of your question, about recollection as a theory of knowledge, you've restated the theory of knowledge in Plato's dialog called The Meno. (This is also discussed more briefly at the end of The Republic.) Briefly put, every soul on the way to being born into the physical world has simply has forgotten everything while passing from its pure state into the physical world? But such souls still have access to all knowledge and merely need to recollect what was forgotten in order to regain the forgotten knowledge.

For Plato's proof of this theory in the Meno, the character of Socrates guides an uneducated slave boy through the process of deducing some geometric axioms. This is not all that different than the savant in the supplied example.

But Plato's example, as well as the savant example, fall into the category of being illustrations or likely stories rather than demonstrations of the truth of the theory. At best the two examples show correlation between two people and what things would look like if Plato were correct in his theory of knowledge. There needs to be additional evidence to demonstrate the truth of the theory. Depending on what means by "truth", different types of evidence might be called for. In the modern scientific sense of "truth", one would need many replicated trials with experimental controls in order to conclude that recollection of the Platonic forms is the only possible (or at least most likely) explanation. Philosophically speaking, the examples are mostly irrelevant so long as there are no counter-examples. The proof of a philosophical theory generally depends on it having no errors in logic or internal consistency and no available counter examples in the world. For example, the law of contradiction is presumed to be true because it is internally consistent and there are no ready counter examples of a object and its opposite being identical.
Acquinas has a http://www.newadvent.org/summa/3064.htm#article7 compelling position:


  The act of self-defense may have two effects, one is the saving of one's life, the other is the slaying of the aggressor. Therefore this act, since one's intention is to save one's own life, is not unlawful, seeing that it is natural to everything to keep itself in "being," as far as possible. And yet, though proceeding from a good intention, an act may be rendered unlawful, if it be out of proportion to the end. Wherefore if a man, in self-defense, uses more than necessary violence, it will be unlawful: whereas if he repel force with moderation his defense will be lawful, because according to the jurists [Cap. Significasti, De Homicid. volunt. vel casual.], "it is lawful to repel force by force, provided one does not exceed the limits of a blameless defense." Nor is it necessary for salvation that a man omit the act of moderate self-defense in order to avoid killing the other man, since one is bound to take more care of one's own life than of another's. But as it is unlawful to take a man's life, except for the public authority acting for the common good, as stated above (Article 3), it is not lawful for a man to intend killing a man in self-defense, except for such as have public authority, who while intending to kill a man in self-defense, refer this to the public good, as in the case of a soldier fighting against the foe, and in the minister of the judge struggling with robbers, although even these sin if they be moved by private animosity.


Then, it seems that there are situations where the death penalty is just, but once someone is captured those situations are rare. We shouldn't administer the death penalty for the sake of killing someone, instead we should first have a (just) goal for which the death penalty is the only option to achieve it. 

What things a group punishes with the death penalty will therefore depend on,  for example, how capable the authorities are at retaining prisoners.
Aristotle in the Nichomachean Ethics has a theory of the golden mean. The virtue that you have in mind is courage.

Thus one who has no fear is not courageous but reckless; thus one who is always afraid is a coward; but one who is in part afraid and in part courageous and both parts being of roughly equal magnitude is courageous.

Thus your daughter who has overcome her fear is courageous; and your daughter who shows no fear is reckless.

Of course in the siutuation you have described fearlessness doesn't show itself to be reckless; but a small modification makes it much more vivid and apparent; for imagine the cockatiel unbeknownst to you had the ability to secrete a fatal poison through its talons...one ses immediately that the fearless child is reckless.
There are some caveats to having desires, even if we can satisfy them and even if satisfying them is always joyful (I will go with these premises even though I don't believe, that it's always joyful to satisfy a desire).


Conflicting desires Assume you have more than one desire at a given time. Lets say you are hungry and you are tired. You have a meal and you have a bed available, but you cannot eat while you are asleep. So either you go to sleep hungry or you eat sleepy. Although you enjoy the meal the overall experience might be a bad one, because it eating gives you less pleasure than it pains you to fight against your fatigue.
Short-Term-Satisfaction Assume that satisfying a desire will reduce your happiness in the future. Lets say you have a desire to kill your wife, but you are a deeply moral man. Now you have two options: Either you give in to your desire and have a short moment of satisfaction but a lifetime of self-hatred and remorse. Or you don't kill her and your desire to kill her will remain unsatisfied indefinitely. Either way you are better off without the desire.
Dependency on satisfying objects Even if (as you stipulated) a given desire can always be satisfied, you will always know that you are dependent on the objects you need to satisfy it. Let's assume, you are an alcoholic, you do enjoy drinking and you have infinit supplies of alcohol. Still the dependency alone is something that can make you feel uncomfortable. You will have worries: "What if someone steels my supply? What if I make a long trip and have no access to it? What if it is empty one day?". All of that can never happen (as you stipulated), but it's the dependency alone that will trigger those sorrows. Some alcoholics stopped drinking just because of that, because they wanted to be free, to don't be dependent on the satisfying object, to not have the desire to drink.
The joy of satisfaction is (usually) smaller than the displeasure of having a desire If it was the other way around, all people would constantly try to be bitten by mosquitoes. Why? Because it is satisfying to scratch a mosquito-bite! People could constantly experience the joy of scratching: It's free, you can do it everywhere and the desire to scratch would never seize. Instead we see people going to great length to kill mosquitoes. Also people don't stand for prolonged durations just because it causes the desire to sit down, followed by the joy of finally sitting down.

We need first-order logic with equality.

We have :


  1) ∀x (Fx ∨ x=c)
  
  2) ¬Fb ∧ Gb
  
  3) ¬Fa → Ga


and we want to derive 3) from 1) and 2).

I think that the "trick" is to rewrite 1) as : 

a) ∀x (¬Fx → x=c)

b) ¬Fb --- from 2) by ∧-elim

c) ¬Fb → b=c --- from a) by ∀-elim

d) b=c --- from b) and c) by →-elim (modus ponens)

e) ¬Fa --- assumed

f) ¬Fa → a=c --- from a) by ∀-elim

g) a=c --- from e) and f) by →-elim (modus ponens)

h) a=b --- from d) and g) and laws of equality

i) Gb --- from 2) by ∧-elim

j) Ga --- from h) and i) and laws of equality


  k) ¬Fa → Ga --- form e) and j) by →-intro, "discharging" assumption e).




Note

The above "trick" can be avoided using Disjunctive Syllogism : form P and ¬P ∨ Q, infer Q.

  "Why is Sorge important in Heidegger? What makes it important for
  him?"


Care is the focal point of being because without taking care, being careful, nothing can be done.  Without taking care to organise thoughts one cannot think.  At the cellular level, if the cells of a plant aren't careful in their functioning the plant cannot grow properly.  It's a scalable concept, (my interpretation).


  "Dasein's facticity is such that its Being-in-the-world has always
  dispersed [zerstreut] or even split itself up into definite ways of
  Being-in.  The multiplicity of these is indicated by the following
  examples: having to do with something, producing something, attending
  to something and looking after it, making use of something, giving
  something up and letting it go, undertaking, accomplishing, evincing,
  interrogating, considering, discussing, determining.... All these kind
  ways of Being-in have concern ('Bersorgen') as their kind of Being."


(Heidegger, - ref: http://royby.com/philosophy/pages/dasein.html Dasein)
In the preface to his translation of the Gita, Swami Nikhilananda says "There are many who regard the story behind the Gita not as historical fact but as allegory. To them Arjuna represents the individual soul, and Sri Krishna the Supreme Soul dwelling in every heart. Arjuna's chariot is the body. The blind King Dhirtarashtra is the mind under the spell of ignorance, and his hundred sons are man's numerous evil tendencies. The battle, a perennial one, is ever going on between the power of good and the power of evil. The warrior who listens to the advice of the Lord speaking from within will triumph in this battle and attain the Highest Good." 

He further states in the introduction regarding the first couplet: "What he [Arjuna] saw chilled his bones. There on the battlefield were assembled his sons, nephews, elders, teachers, relatives, and intimate friends. To regain the kingdom of the Pandavas, he must wade through their blood. He was no coward, but the immensity of the situation confused his mind. He was caught on the two horns of a dilemma. One the one hand was the call of duty: the chastisement of the wicked, the vindication of truth, law, and order--in short, all that belonged to his kshatriya [king/warrior caste] honour. On the other was commiseration for his friends and relatives, whose destruction was unavoidable in the impending Armageddon. Was he to give the sign for the commencement of this carnage, or should he renounce the field, retire to the forest, and lead the peaceful life of a hermit?Unable to resolve the dilemma, he turned to the Divine Krishna and implored his counsel."

In your translation, Land of Dharma is a loose translation of the first verse. Most translations refer to the plain of Kurukshetra where the battle took place. I think your translator is trying to make the point that where the battle is taking place is where dharma will take place. dharma literally means that which holds together. The inmost constitution of a thing, the law of it's inner being, which hastens its growth and without which it ceases to exist. The dharma of a man is not imposed from the outside, but is acquired by him as a result of his actions in past lives. Thus, every man, in a special sense, has his own dharma, which determines his conduct, his righteousness, and his sense of right and wrong.        
¬(B ∧ C) = (¬ B ∨ ¬ C)  is http://en.wikipedia.org/wiki/De_Morgan's_laws DeMorgan's Law.

If you have access to that, then it's pretty easy.

1. A ∨ (B ∧ C)         Premise
2. (¬ B ∨ ¬ C) ∨ D     Premise
3. | ¬A                Assumption
4. | (B ∧ C)           DS 3,1
5. | ¬¬(B ∧ C)         DN 4
6. | ¬(¬ B ∨ ¬ C)      DeM 5
7. | D                 DS 2,6
8. ¬A -> D             Proof 3-6
9. ¬¬A v D             Mat. Imp 8
10. A v D              DN 9


Without DeMorgan's,

1. A ∨ (B ∧ C)         Premise
2. (¬ B ∨ ¬ C) ∨ D     Premise
3. | ¬A                Assumption
4. | (B ∧ C)           DS 3,1
5. | |  ¬ B ∨ ¬ C      Assumption
6. | |  B              CE 4
7. | |  ¬¬B            DN 6     
8. | |  ¬ C            DS 5,7     
9. | | C               CE 4
10.| ¬(¬ B ∨ ¬ C)      Proof by contradiction 5-9
11.| D                 DS 2,10
12. ¬A -> D            Proof 3-11
13. ¬¬A v D            Mat. Imp 12
14. A v D              DN 13


I'm not super-familiar with Fitch, https://www.ocf.berkeley.edu/~brianwc/courses/logic/rulesummary.html which apparently requires two proofs by assumption and a v to do vE. but DS and vE are not identical. But in any case, we can do without both:

1. A ∨ (B ∧ C)         Premise
2. (¬ B ∨ ¬ C) ∨ D     Premise
3. | ¬A                Assumption
4. | | (B ∧ C)         Assumption
5. | | (B ∧ C)         R
----
6. | | A               Assumption
7. | | A ∧ ¬A          ∧I 3,6  
8. | | [Contra. Intr.] 7
9. | | (B ∧ C)         Contra Elim 8
10.| (B ∧ C)           vE 1,9,5
11.| |  ¬ B ∨ ¬ C      Assumption
12.| | | ¬ B           Assumption
13.| | | B              CE 10
14.| | | B ∧ ¬ B       ∧ Introduction
15.| | | D       Contradiction Elimination 14 
------------------   
15.| | | ¬ C             Assumption
16.| | | C               CE 10
17.| | | C  ∧ ¬ C      ∧ Introduction
18.| | | D       Contradiction Elimination 17 
19.| | D            vE 11,15,18     
--------------------
20.| | D               Assumption
21.| | D               Repetition  
22.| D                 vE 2,19,21
23. ¬ A -> D            Proof 3-22
24. ¬¬ A v D             Material Implication 23
25. A v D                DN 24


And that's closer but looking at Fitch, you may not have access to material implication.  
I've voted to close, but the answer is obviously (a). First, in the second paragraph, it states "many now draw a distinction ..." implying the view of unmarried couples has changed whereas the view towards gay couples / homosexuality has not. The third paragraph confirms the relatively unchanged view of gay couples. The fourth paragraph notes how much the view of unmarried couples has changed: "But the ... is very different." It goes on to explain that 2:1 margin favor allowing unmarried couples to adopt. Fifth paragraph expands the details of that claim. Sixth paragraph reiterates that there is now a difference in the views (whereas they were both once condemned).
One crucial aspect of philosophy is to try and give exhausting analyses of concepts and notions. Often our intuitions play a big role in what a term means (this is very evident when it comes to debates in ethics). Finding out what notions mean and explaining our intuitions, is part of finding truth (or knowledge) about us and thus about the world. Of course you can still use JTB as a dictionary defintion but Gettier is suggesting that as a philosophical concept the JTB does not seem to be an exhausting account for our notion of knowledge. Asserting that this doesn't matter does not really concern only this case in particular, but it concerns the nature of philosophy. We would have to have a debate about what philosophy can and cannot achieve (a debate which is taking place, and has been for a long time) So I think the Gettier problem operates on two levels: (1) a methodological. as the others have already suggested, it seems to be very productive for not only philosophical discourse if the parties know what exactly is meant by a term, in order to avoid confusions that arise out of language use, but also (2) an explanatory level. JTB does not seem to be an completely accurate account of what our notion of knowledge is. whether we actually can give completely accurate accounts of notions and concepts is an even more tricky question.
Practical and completely non-philosophical refutation: If one of my children makes this argument to me, I'll slap them around the face and blame it on my parents. 

Practical refutation 2: Police officers, bosses, judges etc. don't accept that argument so it won't help you much. 

Practical refutation 3: If you make mistakes, you can blame these mistakes on your parents all you like, but that's not going to improve your situation one bit, so you better do something to improve your life instead of trying to shift the blame. (In practice, this argument often needs to be used when the parents actually might be at fault for the way they raised their child, but the child needs to be reminded that finding someone to blame doesn't actually improve their situation). 

Philosophical refutation: You may be right, but by claiming that your mistakes were already predetermined at your birth and you had no choice but making them, you give up the right to call yourself a human being. Worse for you, if being born is so awful that you blame your parents for it, that is a fixable problem. (Imagine instead of a child a robot who was created and programmed by a person. That person would have to accept blame for damage caused by the robot, and would probably decide to turn the robot off).  

Philosphical refutation two: If you claim that your parents are to blame for all the bad things you do, then surely they are the ones to praise for all the good things you do. So when you you get your next salary, you better send that straight to your parents because they deserve it. 

Philosophical refutation three: Being born is a necessary precondition for you to be able to misbehave, but it's not the cause of your bad behaviour. As evidence, there are millions of people who were also born but don't show the same bad behaviour. Even if it was the cause, we pass blame only for foreseeable consequences of actions. Your misbehaviour was not foreseeable when your parents decided to have a child. 

So there are two entirely different kinds of refutations: The last one just shows that the argument is wrong. Other refutations say that you may be right, but you haven't quite thought through what the consequences of being right are - and if you are right and think properly about the consequences, you'd be very quiet about this and not draw any attention to it. 
If you write, for example, the first one as a syllogism, you get something like:


P1: Top women chess players have a lower draw rate than top men chess players.
P2: If one group has a lower draw rate than another group, the first group is more aggressive.
C: Top women chess players are more aggressive than top men chess players.


The first thing I notice is that I had to change your conclusion of "women [chess] players are more aggressive [than men chess players]" to "Top women chess players are more aggressive than top men chess players". There was a http://en.wikipedia.org/wiki/Fallacy_of_composition fallacy of composition in your version.

With that change, it looks to me as if the argument is valid. But is it sound? Granting P1 is true (I have no idea whether it is or not), we may still be left with a http://en.wikipedia.org/wiki/Fallacy_of_the_single_cause Fallacy of the Single Cause in P2: there are reasons other than aggressiveness that could explain the outcome in P1, such as the difference in rankings.
The question in the title seems to be different from the elaboration below it.

In pragmatics, utterances are called "https://en.wikipedia.org/wiki/Speech_act speech acts". Specifically, sentences like I now pronounce you man and wife are an illocutionary speech act, and more specifically, are declarations. These become true with just their utterance.

The elaboration below the question's title seems to allude to the https://en.wikipedia.org/wiki/Liar_paradox liar paradox, which is beyond the scope of pragmatics. After all, the content of our speech need not be bound by rules of logic.
(A) : For the left-to-tight direction we have :

1.   P v Q
 _
|  2.   ¬Q --- assumed [1]
|  3.   P --- assumed [2] for v-Elim
|  4.   P --- from 3
|  5.   Q --- assumed [3] for v-Elim
|  6.   ⊥ --- from 2 and 5 by →-Elim (recall that : ¬Q is abbrev for Q → ⊥)
|  7.   P --- from 6 by ⊥-Elim
|  8.   P --- from 3-4 an 5-7 with 1 by v-Elim, discharging [2] and [3]
|  9.   ¬Q → P --- from 2 and 8 by →-Intro, discharging [1]
10.   (P v Q) → (¬Q → P)   --- from 1 and 9 by →-Intro




(B) : For the right-to-left direction we have :

1.   ¬Q → P
 _
|  2.   ¬(P v Q) --- assumed [1]
|  3.   Q --- assumed [2]
|  4.   P v Q --- from 2 by v-Intro
|  5.   ⊥ --- from 2 and 4 by →-Elim (recall that : ¬A is abbrev for A → ⊥)
|  6.   ¬Q --- from 3 and 5 by →-Intro discharging [2]
|  7.   P --- from 6 and 1 by →-Elim
|  8.   P v Q --- from 7 by v-Intro
|  9.   ⊥ --- from 2 and 8 by →-Elim 
|  10.  P v Q --- from 2 and 9 by RAA (or Double Negation) discharging [1]
11.   (¬Q → P) → (P v Q) --- from 1 and 10 by →-Intro

Check out http://en.wikipedia.org/wiki/Appeal_to_emotion Appeal to emotion. This seems to best describe the conversation, where the use of word friendship is to focus on emotion than facts.
From Kant.

Humans have an innate sense of their own limited interchangeability.


It is consonant with our genetic nature, to encourage genes similar to, but not among, our own, in order to have an adequate variety to breed together.
It is consonant with our dependence upon intellectual stability, in that it encourages us to help those that may not share our genes, but speak from similar experience and may preserve our "memes".
It is a direct survival trait, in that it vouches for our individual social value in cases of disaster and excessive consequence (which may impair our utility, but may leave genetic and memetic value).
It grows out of our social conscience, which may be partially inborn.


So one can consider it totally natural.  

It is in fact so natural to so many of us that it is reasonable to follow that impulse into a moral system, via the Golden Rule, or its mostly-debugged rewrite, which is Kant's Categorical Imperative.  What flies directly in the face of this notion of "There but for the grace of God, go I." is inherently wrong.

It provides an anodyne to runaway Utilitarian thinking.  We know, regardless of its utility, murder is wrong, as we would not accept being killed for utilitarian reasons, unless they were exceptionally strong.  Therefore one must allocate a much higher utility than would naturally balance a killing, in order to justify it.  I would let myself die to protect my husband, but not just because I am taking up space someone else wants.  So you may feel free to kill me in a war for survival of my people or my culture, but not to kill me just because you want my land, whatever the utility my land might provide you.

This does not do away with the reference to utility entirely.  But it makes it a secondary consideration under the principle of fairness, and it so outweighs measured utility that it shortcuts the computation completely in a lot of cases.

For instance: It provides U.S. society very little added utility to arrange its social institutions around the possibility of people being gay.  And there is a lot of utility to spend in making those arrangements.

Among other things, the tax code is shaped to encourage a certain type of marriage, which has a certain biological role that gay folks will not generally fulfill.  (It overtaxes couples with equally high incomes, to allow for non-working spouses and the experience gap temporary absence from the workforce causes.)  Making that subsidy less efficient wastes utility.

But we are going to do all of that re-arranging, eventually, just because you might have been me.  We do not need to do the math, because the math does not obviate that possibility.  Utility would have to really mount up to matter, and the differences are not great enough.  

Likewise, most of our criminal laws exist primarily because the victim "might have been me".  Libertarians are not idiots -- they can do math.  It is not, in general, worth paying for most laws in a strictly person-to-person economic computation.

But there is more data.  We do not want to deal with the fear that we might end up in the victim's place, so we establish authorities to keep us safe.  Dispelling that fear increases the efficiency of the whole system enough that it offsets the expenditures.  The return on investment is so great that most of economic history has been a succession of protection rackets.  Insurance has a huge profit margin.

Still, we would never have discovered that, if our thinking were primarily utilitarian by nature.  Each cave-man would think (rightly) that he was best off keeping his own kill and defending his own cave, or maybe stealing what he needed.  Instead, he looked at the things happening to others, saw that he was not so special, and agreed that it would be better if none of the worst ones were allowed.
There are several different interpretations of probability and not all of them are are problematic in that regard. Your approach to take the frequency of an outcome as the probability is but one of the proposed concepts.

Others take the probability as the result of a http://en.wikipedia.org/wiki/Propensity_probability property of a system. If you examine your die, you will notice, that it is perfectly symmetrical in regards of it's geometry, mass distribution, friction etc. So you know it is equally likely for the die to land on each side. If you assume Kolmogorov's axioms, you know that the sum of all probabilities must be one. Now if every side has the same chance to land top and there are six sides, you can simply divide 1 by 6 and you have your probability for every single side. In this interpretation the experiment does not even have to run once let alone several times. You know it, by knowing the die. The same case could be made for the weather: You know the properties of the sun, our atmosphere, heat absorption of land masses etc. It is like the die only way more complex. "The probability of rain tomorrow" is therefor just a result of the properties of the system.

Another approach interprets probability as an expression of our certainty that something will happen. It is no property of the die, http://en.wikipedia.org/wiki/Bayesian_probability it is a property of us - of the amount of knowledge we have about something. If you look at a coin you might assign the probability of it landing tails by 0.5. If you would know more about it, you might come up with a different probability. Assume you learn, that the coin will be flipped by a master-flipper who can flip a coin with such precision that it almost always lands the way he wants. Now you still will not know, what side it will land on: You don't know what the master-flipper aims for. So you will still be forced to go for 50:50. Now he tells you, he aims for tails. Do you believe him? How certain are you, he told you the truth? But now you can move away from 50:50. Say you are almost certain he said the truth and so you go for 20:80. It's still the same coin and the same guy, only the amount of data you have has changed. Probability in this model is just about how certain you are. Applied to the weather-problem: 25% says: "After thoroughly going through everything I know about the weather and the laws of thermodynamics and so on, I am almost certain it will not rain tomorrow. But better bring an umbrella - just in case!"
One possible objection is that you're claiming something doesn't exist merely because people have varying abilities for recognizing (or not) said candidate existant (which you seem to posit in premise 2).


A heap of sand is made up of grains.
So, a certain number of grains of sand comprises a heap.
However, how many grains are needed to make a heap depends on vague perceptual, semantic rules or guidelines which may not be followed by all people in all situations to their full degrees.
Because of this, different people disagree on the number of sand grains needed for a sand heap.
Therefore if a heap is vaguely defined, it must be the case that no heaps actually exist.


Does this really follow?  Seems to me that there's a pretty big leap in 5.  I take issue especially with "If proof is debatable, nothing can be proof." 
I am guessing you've only read Kant's Groundwork which is the whole of his moral (and legal) writing in coming to this conclusion. I want to suggest several points where you might be misreading Kant's moral philosophy to reach your conclusion.

First, you state "one should not refuse to help others who are in need of help" as being an outcome of the Categorical Imperative (hereafter, CI). I think this is quite probably a misreading. There are a lot of formulations of the CI, which can be roughly grouped into three kinds: (a) universalization formulas, (b) treat people as ends formulas, and (c) Kingdom of Ends / autonomy formulas. But none of  them state quite what you are saying. 

Instead, what you are referring to is a concept that occurs roughly (working from memory as my texts are about 70 miles away right now) is that we have perfect and imperfect duties. Perfect duties are those we are obligated to perform at all times, because they are pure obligations of duty, i.e., they are necessary outcomes of reason, like do not murder, do not lie. Imperfect duties are obligations we have insofar as we are finite rational beings who have needed help at some point in our own lives. Kant specifies two: (a) a duty of self-improvement and (b) a duty to help others. But what's problematic in your formulation is that you are making an absolute out of something that for Kant is contingent -- we are obligated to provide aid at some points -- not all points, because we are the sort of  finite rational beings who have at some points (not all points) needed help.

To put it another way, you're misunderstanding Kant's ethics when you speak of it as "generalizations." At least for Kant, these are deductions of reason applied to the idea of a rational creature with a free will. But the imperfect duties are kind of generalizations or universalizations that follow contingently from our limits.

Second, your analysis of when need kicks in is one that seems to project several contemporary values onto Kant. It's not clear that he has any regard for lifespan or such things. Moreover, "need" is not social worker need  but only imminent need for Kant. This in part follows from respecting the rationality of other creatures. In other words for Kant, what you are suggesting would be paternalistic in a problematic way, because, for the same reason we should not lie to other rational beings, we should not assume that a different rational being is not making rational choices. (N.b., This is a problem with Kant's theory -- that it doesn't handle cases of mental illness well).

Third, Kant's notion of freedom and of the sort of freedom required for morality is one where money or the lack thereof has no place. None of the elements in Kant's idea of morality require money -- except insofar as you should feed yourself (without being glutinous). Here, I think you're importing a more modern notion of freedom and working with a concept of freedom as the idea that you can do what you want. But for Kant the only material sort of freedom is that you can do what is right -- a freedom that others can only inhibit insofar as they lie to you or deceive you. 

For Kant, exploitation (using as a means rather than an end) primarily refers to human-to-human interactions and only indirectly and by extension that is not present in the original text to paying living wages or something like that. But it refers directly to not lying or deceiving that person. It's not clear that Kant's moral framework even has the room to calculate something like a living wage.

Fourth, when you refer to the fairness of the market, you are for Kant inverting the problem. If the market is unfair, it is only because people are lying in the market and false dealing with one another. And in such circumstances, the solution for Kant is not redistribution but acting like we already in the kingdom of ends (i.e., keep telling the truth and being fair in your own dealings). Redistributing for Kant would be immoral and reflect precisely the sort of non-CI mentality in dealing with others. We cannot assume that other rational beings are acting immorally or should have resources stripped against their will.

Fifth, Kant draws a distinction between Rechte (often translated "right") and Pflecthe (often translated "virtue"). This is primarily in the Metaphysics of Morals. The first half of the text treats Rechte and describes the sorts of things we can codify into laws and require of people. But for Kant, this will always be different than Pflechte which is what is right. The standard of the latter is higher but not legally enforceable, because Kantian morality has to do not only with actions but with the relationship between the self's will and the maxim of the action.

Private property is a key element in the organization of the state, because it determines who has a right to use things as they please. For Kant, the state's interference there is fundamentally immoral, because morality is about the relationship of will to action -- not property to person.

tl;dr

You're importing modern understandings of notions like 'fair' and misreading the duty to help others.
One possible loophole is if you divulged this information about Moonlight to your spouse (if you have one) or someone of similar trust. This would give you the ability to keep your promise (since spouses are often considered single entities when it comes to secrets) and also give you someone with which you can talk with about it in detail and trust that the secret is still kept.

In the least this option would give you opportunity to get things off your chest and lesson the burden of keeping a secret while still keeping it to what might be considered an acceptable degree. 

Otherwise, I would have to say that a promise is a promise and without proof that the other person is someone with which you cannot form a loyal bond with, you must remain true to your word.
This is the http://en.wikipedia.org/wiki/Nirvana_fallacy nirvana fallacy, when a solution is rejected because it is not perfect.  It's also related to the http://en.wikipedia.org/wiki/Sorites_paradox sorites paradox, which is the concept that adding individual grains of sand can never result in production of a heap.
These are some interesting questions.

I'll start with the third one:


  Can an abstract construct such as a species even have rights of itself, like a right to survive (as a species)?


Here you're importing a pretty strong assertion in the "abstract construct" language. There's a lot of different ways of parsing what a species is. On the one hand, it's a type.  Classically, it was a Form or Essence. We'll skip Plato here, but for Aristotle's idea of an essence. What happens is that we are recognizing something in giraffes when we look at them that makes the giraffes. To simplify that, it's not that we are making up "giraffe", Aristotle thinks it's an empirical observation.

Not everyone agrees, but the modern biological classifications building on Linneaus took species to be real kinds out there in the world. As we've refined that, we've realized he was wrong on some. And we've gotten some blurry realities where two species can intermate or even one's where they have to.

But bringing us back to ethics, I'm going to make a claim: only abstractions have rights at all, because rights are abstraction. Again, what matters is if they are mere abstractions. Abstracting is kind of what humans do and need to do in order to think.



But this question is centrally important to your other questions because it provides the scope of what we are answering. So now we can return to your middle question:


  How can we defend the concept of species in animal rights? For example: What exactly is the moral difference of killing 2 giraffes out of a total population of 1 million versus killing the last 2 giraffes on earth?


On a spectrum, if we take species to be merely abstract constructs with no substantive reality behind them, then it's going to be hard to explain why any sort of action towards things based on this arbitrary distinction would matter. On the other hand, if we take them to be abstract constructs with some basis in reality, we could begin to add diversity concerns. If we take them to be real kinds (like Aristotle), we might be able to strengthen that further.

At a minimum to make it matter it cannot be merely arbitrary (or rather if merely arbitrary species matter, then we're just writing pure fiction here and we could make our ethical theory center on dice and wood grain directions). Setting that aside, there are several different but related motives I can imagine using:


Valuation of a specific species [e.g., humans]
Valuation of a specific kind of species [i.e., ones that seem to demonstrate some rationality: e.g., humans, dolphins, monkeys, bears ?]
Valuation of diversity of species
Valuation of biodiversity with species seen as integral to that
Valuation of the planet and a belief biodiversity is important such that we need differing species.


Now the why of the valuation could be anything. It could be a utilitarian calculation. It could be Singer's pain minimization view. It could be religious. It could be spiritual but not religious. It could be Kantian. It could be intuitionalist. It could be purely economic.

Depending on the reasoning, it might be acceptable to kill of the last two hawks to save millions of https://www.google.co.jp/search?q=momongar&oq=momongar&aqs=chrome..69i57j0j69i60j0l3.531j0j4&sourceid=chrome&es_sm=91&ie=UTF-8#q=momonga momonga. Say for instance, if your preference is the cuteness of the animal or its relative intelligence. Alternately, you might consider some species to have prices and others to have worth but not price. (I for instance place the price of mold at about 200 yen).



This moves us to your first question:


  How can we defend the killing of individuals of a species A in order to protect the survival of species B (in the thought experiment above)?


It would depend on our reasons for valuing specific species. To give an example of where it might be permissible, consider Peter Singer's view that what we want to do is minimize pain and suffering and that animal pain and suffering are not that different from our own (i.e., they count). If that's the reason we should kill, then it seems we would just need to calculate the net suffering produced by keeping invasive species A around.

Again, for some reason to value animals, it won't matter. If for instance, the invasive species are humans and the existing species are dolphins and you think both are rational, then it's unacceptable to wipe one out to help the other.
It is never going to be a good idea to exterminate a species entirely.  I think it is addressed easily by utilitarian thinking on biodiversity, removing the last occupant of a given niche endangers the system as a whole.  And any given species might have been the last in its niche to survive.  It is a low-to-moderate risk of a very high cost, and just not worth it.

But I think this is reinforced more powerfully by a less intelligence-focused framing of Kant's duty not to kill.  Genetic niches produce goals in much the same way human autonomy produces goals.  Eradicating a species removes something that has a goal in the larger scheme of things that you probably do not comprehend.  An individual horse may be a means, but the entire species of horses should still be an end-in-itself.  (It is an interesting question where in the middle we pass over the gap.  But we don't need an answer here.)

On the other hand, if it is invasive, the species foreign and strong, so it is probably thriving elsewhere, and this is not really the issue.  (This is not natural selection, it is unnatural for the species to be here to begin with.)

Still, eradicating a colony of ants may be the ant-equivalent of a killing (in a way that just killing off large numbers of ants for convenience is not) because the colony represents their sole genetic source, a piece of their identity, and their genetic trust the same way your body does for you.

At the same time, in order to be invasive, a species also has to be threatening the local ecosystem.  Just being foreign does not make a species invasive, it has to endanger the species whose niches it is filling.  And it has to have a genetic advantage acquired from the change in environment that allows it to do so -- tasting bad to local predators, going unnoticed as potential prey, being larger and stronger because the old environment was more hostile, etc.

Killing something that is in the process of killing other things of comparable worth via inappropriate leverage is analogous to taking down an crazed man who is waving a gun at a crowd, especially if you just brought him there, not knowing he would go off.  (His craziness makes him as innocent as your ants are.  His gun represents their unfair genetic advantage.  Our gun represents our many unfair genetic advantages.  Humans or human land alterations are almost always the culprits that bring invasive species to their new environments.) 

This kind of killing in defense of others, even if your target is innocent because he is unaware of what he is doing, is defensible on many bases, from Kant to Mill: He might kill, and if he is disoriented enough, he would probably not consider himself guilty if he did so.  Also the aftermath would be a disaster on a few different fronts.  If you indirectly caused this confrontation, you cannot just walk away from it and let him remain a threat.

If such a killing is the only thing that is going to work, you may need to do it carefully, but it should happen.
Not at all.

First, God aside, we cannot attribute human characteristics to non-humans.  Do you think it would be really boring to be a paramecium, or a cow?  I see no reason one should.  And I don't think the reason is that we are somehow superior to them.  I don't find humans who crave excitement to be more intelligent, or even more interesting, than couch-potatoes with vivid imaginations.  The former constantly imagines the latter is bored.  Even from the standpoint we are God's image, we cannot attribute too many of our biological drives onto him.  An image is not a duplicate, it has attributes of its medium that the original lacks.

But in the end, there is a good reason for humans to get bored, we have limited lifespans and cumulative memories.  Filling up that memory with experiences that are not all related increases your likelihood of discovering things and moving us all forward.  That would not apply to God.

In particular, from a traditional Roman Catholic point of view God causes time, and would not be subject to it, by the likes of Augustine, who translates 'eternal' as 'extra-temporal' he wouldn't even experience it.  That would remove the possibility of boredom in a very complete way.

For certain other forms of Christianity, like C.S. Lewis' "Mere Christianity", at some point God had to be bored, but that can't be too common.  The need to be everything good is why it was necessary for God to be incarnated.  Otherwise, relative to humanity, he would have the "Mary" experience from "Mary's Room" (q.v. http://en.wikipedia.org/wiki/Knowledge_argument http://en.wikipedia.org/wiki/Knowledge_argument), but not the "real" experience.   And to be truly omniscient, he must fully relate to all forms of experience.  (The natural Gnostic extension is that this is why Satan is also necessary, and is also God.  It is quite mandatory, from a pretty common human point of view, that God understand completely what it is like to be evil, if he is going to judge evil humans fairly.)

So in some sense, God was just dying to get bored, it must have been new and exciting for him!
1. Your use of 'exact' reminds me of the following description of http://en.wikipedia.org/wiki/Structuralism structuralism in http://rads.stackoverflow.com/amzn/click/0520058380 Interpretive Social Science: A Second Look. The major critique in the essay from which the following excerpt comes is the idea that we can have 'exact' understandings like the 'exact sciences' of physics, chemistry, etc.


      From the interpretive point of view what is most striking about structuralism is not its difference from but its continuity with the older reductionism. That massive continuous theme is the priority and independence of logical structures and rules of inference from the contexts of ordinary understanding. As Lévi-Strauss puts it, one must avoid the "shop-grip's web of subjectivity" or the "swamps of experience" to arrive at structure and science. The ideal or "hope" of the intrinsic intelligibility of structures apart from "all sorts of extraneous elements" is the same animus that propelled the Vienna Circle. Ricoeur, in several of his essays, has drawn the clearest implications of this position. For him, the goals of structuralism can be accomplished, in fact already have been, but at a price the structuralists ignore. The conditions which make the enterprise possible—the establishment of operations and elements, and an algebra of their combinations—assure from the beginning and by definition that one is working on a body of material which is reconstituted, stopped, closed, and in a certain sense, dead.[19] The very success of structuralism leaves behind the "understanding of action, operations and process, all of which are constitutive of meaningful discourse. Structuralism seals its formalized language off from discourse, and therefore from the human world.[20] (12)


[19] See Paul Ricoeur, "Structure, Word, Event" in Conflict of Interpretations: Essays in Hermeneutics (Evanston: Northwestern University Press, 1974), 79.
[20] Ibid.

2. Charles Taylor's http://rads.stackoverflow.com/amzn/click/0674664779 Philosophical Arguments contains several chapters which bear on your question. You are treating language as if it is 100% descriptive, as if it merely talks about what is already there. Taylor emphasizes http://en.wikipedia.org/wiki/Johann_Gottfried_Herder Johann Gottfried Herder contribution to the philosophy of language: language exhibits a constitutive element. That is, language can create reality. This is actually a common experience: by attempting to explain a topic to someone who is not an expert, one can actually come to understand it better. To view language as constitutive, interpret such an experience this way: the use of language has given additional form to the idea, changing one's conception of the idea. From Taylor:


      Herder develops a quite different notion of expression. This is in the logic of a constitutive theory, as I have just described it. This tells us that language constitutes the semantic dimension, that is, possessing language enables us to relate to things in new ways, say as loci of features, and to have new emotions, goals, or relationships, as well as being responsive to issues of strong value. We might say: language transforms our world, using this last word in a clearly Heidegger-derived sense. We are talking not about the cosmos out there, which preceded us and is indifferent to us, but about the world of our involvements, including all the things they incorporate in their meaning for us. (107)


3. In contrast to structuralism, which allows for a rigorous, logical, quantitative analysis (akin to the 'exact sciences'), one can think of a more 'organic' way to understand a situation, where the result emerges from an https://philosophy.stackexchange.com/questions/14319/what-is-an-unarticulated-background unarticulated background which can always be further articulated (see 2.):


      In this approach understanding any action is analogous to textual interpretation. This means that the intelligibility of any action requires reference to its larger context, a cultural world. So, to take a powerfully developed example, when Clifford Geertz describes the Balinese cockfight, a text analogue, he progressively incorporates other essential Balinese symbols, institutions, and practices that are necessary to an understanding of the seemingly localized cockfight. The Balinese cultural and social world is not incorporated into the cockfight, but must be brought into the analysis in order to understand the event. This is the art of interpretation. The aim is not to uncover universals or laws but rather to explicate context and world.[24] (http://rads.stackoverflow.com/amzn/click/0520058380 Interpretive Social Science, 14)


[24] See Clifford Geertz, "Deep Play: Notes on the Balinese Cockfight," reprinted in this volume.

4. http://plato.stanford.edu/entries/wittgenstein-atomism/ Wittgenstein's Logical Atomism is also instructive. It is another way to look at the idea of each word having a single, definite meaning, which is sufficiently isolated from other meanings. This idea, which Wittgenstein endorsed at first, is fantastically false. Its falsity is intuitively obvious: when you look for the meaning of a word in a dictionary, and then you look for the meanings of the words used in that definition, and continue, you will ultimately 'loop back' onto previously used words. One isn't supposed to use a word to define itself, but language as a whole does exhibit this 'problem', necessarily. (Taylor talks about Wittgenstein and 'meaning atomism' in http://rads.stackoverflow.com/amzn/click/0674664779 Philosophical Arguments.)

One result of this is that as you attempt to define 'fear', you will visit other words. You will realize that you could actually understand them better. In the process, you will actually understand 'fear' better. So the very attempt to define 'fear' will result in you understanding the word better. Now, will the process stop? Will you reach that perfect, all-encompassing definition? Opinions are split on the matter, but I would argue a strong no. This is probably a purely philosophical opinion, but I think one can use some sort of induction (noting http://plato.stanford.edu/entries/induction-problem/ its problem) to point out that there is no foreseeable limit to how much there is to understand, and how complex ideas such as 'fear' and 'love' could grow to be.
Noone is arguing that everyone guilty of any crime should be killed. Arguing that death penalty is sometimes appropriate is arguing in favour of death penalty, while arguing that it never is is arguing against it.

Per my understanding, your solution implies that death penalty is sometimes appropriate, but that we should avoid it whenever possible. I suppose many defenders of death penalty would agree with you.

Regarding the "formal, objective" aspect of your question: I don't think ethical questions can take an objective form (such as a valued field on the space of possible situations), at least not in our current state of knowledge. 
First you'd need a "space of possible situations" (in what language? The language of physics? Lots of complications here) and even then according to some ethical theories it would still be difficult or impossible (for example, if value judgments are contextual or requires a "moral sense").
No. On what I take to be the most natural interpretation, two logically-equivalent ways of rendering the first statement in formal logic would be:


For any houses/each house in the area, it's not the case that food can be found in them
There does not exist a house such that food can be found in it.


Neither sentence entails that a house exists.

However, user “quis est ille” reminds us that, as with nearly all ordinary language sentences, we can imagine contexts in which this sentence could be taken to imply the existence of houses. If the sentence is asserted in a neighborhood of houses, then its meaning might be just about whether there is food in them, and rendered in formal logic as:


There are houses in this area and there is no food that can be found in any of them.


Yet, I think when there are houses around, we don't need to assert their existence, even if we acknowledge it. So while we might suppose that houses exist, it does not logically follow from this comment alone that houses exist, because we could equally well use the same comment, with the same meaning  (a) in a scenario where there are questions about whether there are houses (e.g. most have been bombed), and (b) in a scenario where there are no houses (e,g, when we're search in the desert for any source of food).
Utilitarianism is a brand of consequentialism --- that is, it judges actions by their effects.  Libertarianism (at least as you seem to be defining it) is a brand of deontology --- that is, it judges actions by something other than their effects.  They are therefore fundamentally at odds (as would be any other brand of consequentialism and any other brand of deontology), though of course there are modified versions of each that come closer to being compatible.
From the viewpoint of Jung, complexes are not necessarily abnormal, they are what makes for stereotyped behavior, and the vast majority of social behavior seems stereotyped, so we must all have a lot of them.

Complexes serve a purpose, in allowing you to behave without deep consideration of each individual action.  They become problematic only when they do not suit your current situation, or when they are so overdeveloped that they bar deeper consideration too completely.

So if jealousy is just an inferiority complex, what are the developmental advantages of inferiority complexes?  We all start as children.  And they encourage one to subject oneself to other's judgement, which is a useful thing for a child to do in a complex society that takes a long time to adapt to.

So, then why do they create resentment of those who presume to be superior?  Even though your parents are 'better than you' for most of your childhood, they will not always be.  If this resentment were not a natural part of an inferiority complex, the new generation would never come into its own by overpowering the status quo.

So, is retaining this behavior past adolescence adaptive?  It certainly creates rituals that enforce conformity when necessary.  To me this seems practical.  We allow people to 'parent' us in domains where we acknowledge their status.  The resentment may seem more problematic, but if its purpose is to shatter the ritual when it has run its course, then it serves as a foil against, for instance, military dictatorship, or technocracy.

So, to my mind, basic jealousy is perfectly logical.

The particular jealousy of future generations is more ambiguous.  Perhaps you could leverage it to encourage yourself to live longer, or to think futuristically in a way that might encourage invention or other kinds of imagination useful to the culture.  So I do not see it as a totally pointless complex.
It's not exactly that Aristotle works in triads, so much as he thinks that certain dispositions of character have known excesses and deficiencies.

We begin with the definition Aristotle supplies for virtue:


  Next we must consider what virtue is. Since things that are found in the soul are of three kinds- passions, faculties, states of character, virtue must be one of these. By passions I mean appetite, anger, fear, confidence, envy, joy, friendly feeling, hatred, longing, emulation, pity, and in general the feelings that are accompanied by pleasure or pain; by faculties the things in virtue of which we are said to be capable of feeling these, e.g. of becoming angry or being pained or feeling pity; by states of character the things in virtue of which we stand well or badly with reference to the passions, e.g. with reference to anger we stand badly if we feel it violently or too weakly, and well if we feel it moderately; and similarly with reference to the other passions. 
  
  Now neither the virtues nor the vices are passions, because we are not called good or bad on the ground of our passions, but are so called on the ground of our virtues and our vices, and because we are neither praised nor blamed for our passions (for the man who feels fear or anger is not praised, nor is the man who simply feels anger blamed, but the man who feels it in a certain way), but for our virtues and our vices we are praised or blamed. 
  
  For these reasons also they are not faculties; for we are neither called good nor bad, nor praised nor blamed, for the simple capacity of feeling the passions; again, we have the faculties by nature, but we are not made good or bad by nature; we have spoken of this before. **If, then, the virtues are neither passions nor faculties, all that remains is that they should be states of character. ** (Nicomachean Ethics --> EN II.5-6).


In other words, a virtue is how we respond to certain feelings. Aristotle defines courage thusly:


  With regard to feelings of fear and confidence courage is the mean; of the people who exceed, he who exceeds in fearlessness has no name (many of the states have no name), while the man who exceeds in confidence is rash, and he who exceeds in fear and falls short in confidence is a coward (EN 2.7)


Not every virtue has a triadic structure, because what's more important is that you react appropriately to the emotion and situation in question. (Even courage does not have a triadic structure [there's 4 named possibilities in the quote] -- note that some have no name -- and as you go through other virtues, some have no existing version of excess in one direction).
The unity of the Real and the Ideal is what Hegel called the Idea. The Idea means, as for Kant, an ideal, infinite concept. But unlike Kant, for Hegel the Idea does get actualized in Reality. Hence the unity of Real and Ideal.

This is what Hegel says in the Shorter Logic http://www.marxists.org/reference/archive/hegel/works/sl/slidea.htm §214:


  The Idea may be described in many ways. It may be called reason; (and this is the proper philosophical signification of reason); subject-object; the unity of the ideal and the real, of the finite and the infinite, of soul and body; the possibility which has its actuality in its own self; that of which the nature can be thought only as existent, etc. All these descriptions apply, because the Idea contains all the relations of understanding, but contains them in their infinite self-return and self-identity.

The answer is that there is no dividing line.

The definition of living vs. non-living is one which is very important to us, and yet nobody has a fully agreed upon line in the sand.  Even science, which has entire categorizations for living things, openly admits that all it has is a list of things living things tend to have; it lacks a checklist you can do to test whether something is alive or not.

This is actually a big deal on the edge case: death.  Because science doesn't have a solid definition of life, it also does not have a solid definition of death.  There are cases we all agree upon (a putrefying corpse is dead), but there are cases we aren't certain about (heartbeat but no brain function, or brain function with no heartbeat).
Do you define a lie as "a deliberate attempt to deceive" or as "a statement counter to the facts"?  We can presume your statement to the police was a deliberate attempt to deceive, even if it was technically not a statement counter to the facts. 

This type of situation is the reason behind the fact that people in a court of law are required to swear to tell the "truth, the whole truth and nothing but the truth." --the phrase the whole truth recognizes the existence of half truths intended to deceive.

As far as the seeming paradox:  In natural language, there are many ambiguities, and two exploited by this dilemma are the fuzziness of the definition of "lie" and the fuzziness of the definition of "beard."  In a formalized language, on the other hand, ambiguities are eliminated and statements are given exact truth values.  Since your statement was not in a formalized language, it is not unusual that it has an ambiguous truth value (see logician http://en.wikipedia.org/wiki/Alfred_Tarski Tarksi for more on the subject).
I see this argument as a version the https://yourlogicalfallacyis.com/strawman Straw Man Argument.

The accusation: Person A is a "racist," i.e. they hold (and potentially act upon) unfavorable views concerning people XXX ethnic background.

The rebuttal:  Person A claims to associate with at least one member of XXX ethnic background on a regular basis.

Their rebuttal reinterprets "racism" into being completely incapable of interacting with people of XXX ethnicity, which is an extreme exaggeration of the original accusation.  It is http://www.jstor.org/discover/10.2307/2748753?uid=3739928&uid=2&uid=4&uid=3739256&sid=21105656918513 well documented that general racial prejudices often do not translate to http://theracecardproject.com/youre-one-good-ones/ individuals http://open.salon.com/blog/jlw1/2014/09/21/youre_one_of_the_good_ones_my_son_black_men_abuse encountered http://racismschool.tumblr.com/post/44878736131/why-is-the-black-friend-bad regularly.  Of course, one might also be http://tvtropes.org/pmwiki/pmwiki.php/Main/SomeOfMyBestFriendsAreX conveniently expanding the definition of "friend."
If you have any doubt at all about whether you owe them money, you should alert them to that doubt. I think there are two reasons, one legal the other moral.

Amazon may be responsible for sorting out whether you owe them money, but it is a bad idea to put yourself in the position where you might get involved with the courts in any capacity. You know that you may owe them some money. In addition, you have admitted this in public. You may be in a very dangerous position, even if you are technically not guilty of anything for which you could be prosecuted or sued. The courts are not pleasant, they are not quick, they are expensive. The reason for this is that government officials have extremely limited accountability, so they do not have to make any effort at all to make your life easy. You should not want to get involved with them unless it is absolutely unavoidable.

The moral reason is that what you are doing is dishonest by your own admission. It is a bad idea to take an action of which you have a criticism because it is irrational to do so. 

I will address some of your comments on other posts. You said that:


  Ayn Rand says values /should/ be reasonable, but still even she does
  not say they are reasonable.


The Objectivist position is that it is rational to act according to correct moral standards. Some people may be irrational, but that does not make what they do right. You continue:


  But what we are valuing in this regard is reason. How can reason give
  us values when we value reason? Makes no sense. Simplest is to accept
  values exist in an unqualified fashion, then begin working on our
  values through reason. But still it is reason we value.


Neither values nor any other knowledge are derived from anything. Any purported derivation relies on premises and rules of inference that could be in error. Rather, knowledge is created by noticing a problem with current knowledge, proposing solutions to that problem, criticising the solutions until only one is left and it has no outstanding problems. See "Realism and the Aim of Science" by Karl Popper, Chapter I and "The Beginning of Infinity" by David Deutsch.

Another comment:


  I gave this an up vote because it answers my question, should I, and
  why. It is somewhat of a realist black and white perspective, but that
  is also a common way of thinking and fair to be represented. I am a
  relativist, which means I think values are contextual, but I also
  recognize we do have values, regardless how we came to have them.


Saying that values are contextual and that they are relative is not the same. If I kill somebody for eating a bag of crisps while he is sitting next to me that is wrong. If I kill somebody who is actively to murder me, that may not be wrong. So whether killing is right is contextual, as is knowledge in general, see "Understanding Objectivism" by Peikoff, Lecture 2. And morality is black and white, see:

http://freedomkeys.com/ar-moralgrayness.htm http://freedomkeys.com/ar-moralgrayness.htm.
A Wikipedia lookup definitively resolves this question.


  ... a Canadian study for 13 cities in the past 30 to 40 years found
  that the weather patterns predicted on Groundhog Day were only 37%
  accurate over that time period. According to the StormFax Weather
  Almanac and records kept since 1887, Punxsutawney Phil's weather
  predictions have been correct 39% of the time. The National
  Climatic Data Center has described the forecasts as "on average,
  inaccurate" and stated that "[t]he groundhog has shown no talent for
  predicting the arrival of spring, especially in recent years.


http://en.wikipedia.org/wiki/Groundhog_Day#Predictions_of_various_groundhogs_since_2008 http://en.wikipedia.org/wiki/Groundhog_Day#Predictions_of_various_groundhogs_since_2008
While I agree in general with the other answers, I think they miss something very important.  Ad hominem is a technical term, in a category, logical fallacy, that has a precise meaning: Something that is presented as (or appears as) a proof or disproof of an argument, but that does not legitimately address that argument.

In the examples you give, it is impossible to judge them as ad hominem without more context.  Is Person A advancing an actual argument (whether weak or strong), that Person B wants people to discard solely on the basis of who Person A is?  Or is Person A just making absurd personal accusations with no actual logical content (valid or not)?

Please note:  It would be perfectly reasonable for someone to say "Person A has a long, consistent history of making worthless claims, therefore it is likely that this claim is worthless (and/or therefore it is likely to be not worth the time to listen and try to understand).  It is, however, fallacious to say "Person A has a long consistent history of making worthless claims, therefore this current argument he is making is (or must be) invalid."  No possible accidents of personal history are legitimate disproofs of any strong or valid argument.
I am not aware of any argument directed against Nozick specifically, but I would point out that social contract theory, especially modern variants, disagree.  (You can read more http://www.iep.utm.edu/soc-cont/ here or http://plato.stanford.edu/entries/contractarianism-contemporary/ here.)

In brief, the idea is (usually) that receiving the benefits does imply consent to the benefits.  It is just a formalization and generalization of the kind of reciprocity that comes naturally with one-on-one interactions (e.g. if you need a pot and your neighbor loans you one, and then they need a lawnmower, you would be more likely to let them use yours; and, indeed, if you didn't, people would tend to view you as ungrateful or selfish).

To justify that this is not only natural but also good, there are a variety of approaches.  Most refer to game theory (directly or implicitly), wherein Prisoner's Dilemma situations cannot be negotiated successfully (i.e. for best overall outcome) without some sort of contract, or to the fact that we can do many things implicitly, so why not this?, or to the difficulty of maintaining a stable system given that there are children (where they are initially not able to consent), injured people (also unable to consent), and so on, for whom refusing help because they might post hoc decide they had not consented seems a more grievous ill than helping them yet holding them to have some degree of reciprocal obligation.

Anyway, the key is: (explicit) consent is not necessary to accrue some level of obligation, because if you make it necessary, society in practice will have higher levels of injustice, suffering, and cruelty.  (And saying that it is moral to have more of such things seems counter to what we want morality to be about.)

I don't believe Rawls (perhaps the most famous contemporary supporter of a flavor of social contract theory) ever directly addressed Nozick's criticisms of Rawls' view of the social contract, or Nozick's specific counterarguments to the principle of fairness.  I'm not familiar enough with Rawls' later writings to know if the indirect address contains a compact answer to your question.
First, your emotions around this question are something that I would advise you to seek help with - whether it is with a trusted friend or relative, or someone trained to help people deal with such a sense of despair. I can tell you from personal experience, it is not an academic matter, and can be a matter of life or death. So talk to someone about how you feel, irrespective of the philosophical ramifications.

Now to the question. Generally, people seek meaning for their lives from two types of sources: objective or personal. 

Objective purposes for life can be provided by the orthodox religions, some social movements, any number of collective constructs that adhere to a particular world view. In these cases, this is not a personal search of yours, but a matter of accepting a doctrine and committing to a life lived accordingly. Your part of it is deciding how you can fulfill this given purpose. Many wise and intelligent people have lived their lives according to a purpose provided to them by a school of thought, religion or other institution.

Those who are not convinced by such extrinsic influences, who have searched their own minds, explored the universe around them and found no extrinsic purpose that they believe is valid - these people have two options. 

On one hand, they can adopt a lack of purpose and a kind of hopelessness as their world view, and live accordingly. Some see this as a ticket to perfect freedom. How they use this freedom is entirely up to them.

On the other hand, they can decide that a purposeless life is not worth living, and there is no power in the universe besides themselves to provide it for them. In this case, they survey the landscape of their associations - to themselves, to their family and friends, to their profession, the groups, national or ethnic to which they belong - and decide to forge a purpose for themselves out of these relationships.

This is heavy going, a tremendous responsibility. However, I will simply name for you such luminaries as Albert Einstein, Bertrand Russell, probably Eleanor Roosevelt, and many others as merely famous examples.

As you discuss your feelings with others on this topic - which again, I urge you to do - you will discover less famous persons who have struggled with this responsibility. They will have suggestions on how to go about finding a purpose, from either objective sources, or from within yourself.

So in the short term, reading your question, you HAVE forged a purpose - of finding a purpose. You have set your feet on a journey traveled by many of your fellow humans. Step one completed. Congratulations.
The four causes are a direct consequence of Aristotle's explanation of motion as the reduction of one mode of being (potentiality) to another (actuality). Thus, your question amounts to how motion (change) is understood in modern physics.(http://moerwiki.us.to/misc/Physics%20papers%20and%20books/Philosophy%20&%20History/McLaughlin%20%28@%20St.%20Vianney%20Theological%20Seminary%29/ Thomas McLaughlin is an expert on Aristotelian vs. modern physics' understanding of motion.)

The question of projectile motion is a good example of metaphysicians grappling with understanding the causality of motion in Newtonian physics. Is projectile motion as treated by modern physics really motion (i.e., the reduction of potentiality to actuality) requiring a cause, or is it really a static state?

For example, http://www.encyclopedia.com/topic/Emile_Meyerson.aspx#1 E. Meyerson, in his http://www.worldcat.org/oclc/551667775 Identité et Réalité (1908), writes (pp. 132, 134):


  the principle of inertia demands that we view motion as a state; if motion is a state, it must maintain itself like every state. … The principle of inertia demands that we view speed as a substance. Now this is an entirely paradoxical concept for the immediate understanding


Cf. http://www.sljaki.com/ Stanley L. Jaki's "http://scholastic.us.to/The%20Physicist%20and%20the%20Metaphysician%20(Jaki).pdf The Physicist and the Metaphysician" regarding projectile motion, http://www.thomist.org/jourl/1974,%20vol.%2038/April/1974%20April%20A%20Moreno%20web.htm quid quid movetur ab alio movetur ("whatever is moved is moved by another"), and http://scholastic.us.to/note.html the Principles of Inertia and Conservation of Energy. For more on a metaphysician's perspective on the principles of modern physics, see https://www.dropbox.com/s/xyutmgwmv61izg9/The%20Two%20Uses%20of%20Reason%20%28Aversa%29%20%28footnotes%2C%20single-spaced%29.pdf?dl=1 Garrigou-Lagrange's letters to Pierre Duhem (PDF pp. 14-29).

Modern physics is a "scientia media" ("intermediate science"), halfway between the first (physical) and second (mathematical) degrees of abstraction—i.e., modern physics, like ancient astronomy, has "a closer affinity to mathematics, because in its thinking that which is physical is, as it were, material, whereas that which is mathematical is, as it were, formal." (St. Thomas Aquinas's http://dhspriory.org/thomas/BoethiusDeTr.htm#L21 Division and methods of the sciences http://dhspriory.org/thomas/BoethiusDeTr.htm#53 q. 5 a. 3 ad 6). Now,


  By its very nature motion is not in the category of quantity, but it partakes somewhat of the nature of quantity from another source, namely, according as the division of motion derives from either the division of space or the division of the thing subject to motion. So it does not belong to the mathematician to treat of motion, although mathematical principles can be applied to motion. Therefore, inasmuch as the principles of quantity are applied to motion, the natural scientist treats of the division and continuity of motion, as is clear in the Physics. And the measurements of motions are studied in the intermediate sciences between mathematics and natural science [physics]: for instance, in the science of the moved sphere and in astronomy.(http://dhspriory.org/thomas/BoethiusDeTr.htm#L21 ibid. http://dhspriory.org/thomas/BoethiusDeTr.htm#53 q. 5 a. 3 ad 5)


This gives the reason for Meyerson's quote above. Modern physics is "formally mathematical," and mathematics does not "treat of motion, although mathematical principles can be applied to motion."

Leibniz discussed questions regarding causality and force; he, unlike Newton, was not afraid to give metaphysical reasons for Newtonian concepts. (cf. http://www.encyclopedia.com/topic/Pierre_Maurice_Marie_Duhem.aspx Pierre Duhem's https://books.google.com/books?id=mRvTBgQ-hmEC&lpg=PA18&vq=Leibniz&pg=PA16#v=onepage&q&f=false Evolution of Mechanics pp. 16-19f.)

Further reading (from http://scholastic.us.to/ here):


http://www.worldcat.org/title/modeling-of-nature-philosophy-of-science-and-philosophy-of-nature-in-synthesis/oclc/34284430 The Modeling of Nature by http://www.morec.com/nature.htm William Wallace, O.P. (vide http://www.thomist.org/jourl/1997/974AAshl.htm this review and http://home.comcast.net/~icuweb/icu020.htm associated course)
http://www.thomist.org/jourl/1965%20vol.29/Jan%201965/1965%20Jan%20A%20Kreyche.htm Some Causes of the Elimination of Causality in Contemporary Science by Gerald F. Kreyche
http://www.worldcat.org/oclc/533764 Causality and Scientific Explanation by http://www.morec.com/nature.htm William Wallace, O.P.

There's essentially at least two, perhaps three differing systems of morality battling it out here.  The oldest one is based on the idea that there are certain virtues that embody morality, and that living well is to live out those virtues, regardless of the larger picture.  The virtue model is common in traditional warrior cultures, and elements of it are found in Aristotle.  If the virtue of a soldier is to fight bravely and loyally for his country, you would judge him well by this standard.

The second system of morality judges everything against universal standards, so that the act of an individual, such as a soldier, must be judged against the larger impact it has.  A brave Nazi soldier, in this view, is still a bad person in the larger picture.  Plato tried to integrate the first two systems in The Republic, arguing that the virtue of a good person is the same as the virtue of a good city/state --a good person can only be effectively virtuous in a virtuous context.

The third system of morality is existentialist, and argues that we are each individually and irreducibly responsible, in a moral sense, for the world in which we inhabit.  This has origins in some elements of the teachings of Jesus, in Kant's dictum is that morality is what we affirm for all, in the writings of Dostoevsky and more directly in the work of Kierkegaard as developed by Sartre, Camus and DeBeauvoir.  The Nazi soldier is not merely wrong, under this account, he is as morally culpable as is Hitler.
It would make a mockery of the legal system by legally binding things admitted to be immoral.

While I believe what's moral isn't always lawful (and vice versa) there is enough of a connection between law and morality that to blatantly push this dichotomy can raise serious questions about the law.

How would you feel if an authority admitted that the law forced you to do something which that same authority agreed was highly immoral?  It becomes very hard to respect a legal system in which such things take place.
An English translation of the first Notebook is forthcoming: Heidegger, M. (2016) Ponderings: Black notebooks 1931-1938. United States: Indiana University Press.

http://www.iupress.indiana.edu/product_info.php?products_id=807954 http://www.iupress.indiana.edu/product_info.php?products_id=807954
There are so many philosophers who have tackled these topics that to attempt to list them all would be an exercise in futility.  However, I'd recommend researching the Stoics and the Existentialists in particular as offering differing perspectives on how to approach life in the face of death.

It's also explicitly the subject of most of the Biblical book of Ecclesiastes:

https://www.jewishvirtuallibrary.org/jsource/Bible/Ecclesiastes.html https://www.jewishvirtuallibrary.org/jsource/Bible/Ecclesiastes.html
Firstly, what is punishable by law and what is moral are two very different things, and it would be a mistake to conflate the two. While someone may not go to jail for killing an animal, this does not thereby guarantee that it is morally permissible to kill an animal. Going in the other direction, it may be illegal to consume certain substances (for instance), but this does not mean that it is morally impermissible to consume them - the law could just be confused on this mark. Similarly, what is natural is not a good guide to what is moral. Arguably things like rape, murder, war, famine etc. are just as 'natural' as a non-human predator killing its prey for food, but we wouldn't want to say that those things are moral.

Secondly, the answer to the question "is it morally permissible to kill animals?" will depend on the kind of normative ethical theory that one adopts (or, which normative ethical theory is correct). A normative ethical theory is a collection of principles that tell us which actions are permissible, which are impermissible, and which are obligatory. The most common kinds of normative ethical theory are consequentialism, deontology, and virtue-based.

Consequentialism holds that we have an obligation to maximize utility, where we can fill out the notion of utility in a variety of non-equivalent ways (e.g. as pleasure, happiness, flourishing etc.). So, a consequentialist will say that it is permissible to kill the animal so long as you ensure that doing so will maximize utility. Similarly if killing the animal won't maximize utility, then it is impermissible to kill it. See http://plato.stanford.edu/entries/consequentialism/ here for more.

Deontological theories hold that what is (im)permissible is a matter of the kinds of duties and obligations we have to each other. So if I have an obligation (say) not to harm living creatures, then it would be impermissible to kill an animal, because in doing so I wouldn't be observing my obligation not to do so. Similarly, where I have an obligation not to harm living things, living things have a right to not be harmed by me. By killing the animal I would be violating that right. http://plato.stanford.edu/entries/ethics-deontological/ Here's more on deontology.

Finally, lots of ethicists have written on animal ethics - whether they are part of the moral community, and if so, what kinds of moral rights do they have? Peter Singer is probably the most well-known philosopher that works on these problems, and his writing is accessible. http://www.utilitarian.org/texts/alm.html Here is some of his work.
I found it surprisingly hard to find a good general reference about truth trees but here is one: http://pegasus.cc.ucf.edu/~stanlick/truthtrees.htm http://pegasus.cc.ucf.edu/~stanlick/truthtrees.htm

A truth tree is a mechanical method of checking the validity of an argument in symbolic logic.  You start with the premises and the negation of the conclusion, and check to see if any possible interpretation does not lead to a contradiction.  If the negation of the conclusion is inconsistent with the premises under all interpretations, the argument is valid (by definition).

What makes a truth tree work is that every step in a truth tree decomposes a more complex branch into either one or two branches of a lesser order of complexity.  When a branch reaches the base level of complexity (a single statement, represented by a single letter or the negation of a single letter) it can be evaluated as either consistent with everything above it (open) or inconsistent (closed).

From the list of options, we can rule out #1 immediately, because some arguments (invalid ones) do not lead to closed trees.

To rule out number 3 and 4 we reason as follows:  Any finite argument (a finite set of finite premises and a finite conclusion) will have a finite order of complexity.  Since each decomposition of the tree reduces the complexity of a given branch, and adds at most two branches of lesser orders of complexity, only a finite amount of branches can be ever generated for any finite initial level of complexity, and all branches must eventually reach the base level of complexity in a finite number of steps.  Therefore every tree must complete in a finite amount of time.
Well there is a typical consideration , why to not reify concepts (like states). This is the age old Occam's Razor (http://en.wikipedia.org/wiki/Occam%27s_razor http://en.wikipedia.org/wiki/Occam%27s_razor):


  Entities must not be multiplied beyond necessity


In other words, we'd better not reify states, if we don't have to. Syntactically, we can still treat states as pseudo-entities using transformation rules, such as:

alive(x) <-> (Es)alive(s,x)

We seem to need reified events, more than we need reified states. Davidson's main reason for that, if I recall correctly, is that events are needed as the terms of causal relations. There seems to be  no similar motive for reified states, because states are not ingredients of causal relations (at least in Davidson's account of causality ).
You have several questions in here, but I'll address only the title question about the relationship between freedom and democracy.

The definition of "democracy" is "rule of majority". This means, that in an isolated democracy, there is absolute freedom of majority opinion (outside influences such as other countries with military power would change the totality of this statement). This structure  is not tied to any moral standard (although the majority opinion may be guided by moral standards). If the majority decided that speeding should be punished by immediate death without a trial, then that's what would happen.

Democracy is not freedom of any one person (anarchy), democracy is freedom of the majority of a group of people. It should be noted that no democracy can exist in a strictly defined form for long - no current governments are strictly democracies. John Adams said, "Democracy... while it lasts is more bloody than either aristocracy or monarchy. Remember, democracy never lasts long. It soon wastes, exhausts, and murders itself. There is never a democracy that did not commit suicide." http://www.brainyquote.com/quotes/quotes/j/johnadams136320.html REF
Rawls, A theory of justice, provides a Kantian inspired piece of reasoning for such an axiom.

This is his 'veil of ignorance'. Say before you were born, a god asked you what would you wish as the law of the world that you were to be into, given that you do not know to whom you will be born, and nor your social position. Rawls argues under such a presupposition, one should argue that the law should treat everyone equally.
Part of what your friend is saying does have some truth to it. Just because someone believes that X is the case, it doesn't follow that X is actually the case.


  However this led to a backlash from my gym partner and his analogy was
  that "some believe lemsip cures the cold, it doesn't mean its true" and
  then posited that I have a belief system and that "I believe
  creatine helps" in the same manner as a rebuttal to my argument.


However, what your friend seems to be doing is that he is highlighting a feature of your disposition (your beliefs), and then using this in order to refute your argument.

In other words:


  You: My argument is that the calming of your soreness can be best explained by creatine consumption.
  
  Him:  Just because you believe in something, it doesn’t make it true. You believe that creatine helps soreness, this doesn’t make it
  true. Your argument that creatine helps soreness is therefore wrong.


Whilst your friend's ideas about beliefs have some merit, they are actually irrelevant to the argument at hand. Your argument could very well be wrong, but not for the reasons that he has presented. 

This is because he is attacking your beliefs instead of your argument. For your friend's argument to be successful, he needs to attack the notion of creatine helping soreness, rather than attacking the idea that you hold a belief in the utility of creatine with respect to soreness.

Therefore, I would say that he is guilty of ad hominem. Nevertheless, I don't think that hunting fallacies and then accusing something of committing fallacy X is particularly useful. Rather, you should be able to explain where the reasoning has gone wrong, and you don't need to know the name of a fallacy in order to do that.

P.S. What should be noted is that the impact of creatine on soreness is an empirical issue. If you guys intend to debate the notion that creatine reduces muscle soreness, you have to bring some empirical support to the table.
Going on the basis of this passage only; Foucault appears to be describing an ontology of power; which isn't flat ie of one form - only the large institutions of power, whose motions are commonly the subject of politics and history; or the counter-narrative pioneered by Marxism and Social Darwinism which reduces these situations to individuals - to the personal, the self and the body: ie 'each individual'.

His micro-physics of power is situated between these two canonical ontologies of power; so it's in-between, more temporal and transient.
Consider Francis Bacon (1561 – 1626):


  Non inutiles scientiae existimandae sunt, quarum in se nullus est usus, si ingenia acuant et ordinent.
  (Sciences which have no practical use in themselves must not be considered useless if they sharpen and order the mind.)


Bacon was frustrated with speculations which did not lead to improvements in the human condition. And so, he argued that scientia which is not pragmatically useful (or meta-pragmatically useful) must be discarded. The trick here is that "pragmatically useful" depends on the person's desires. A Saturn V rocket is useful if you wish to go to the moon, but grossly inefficient if you want to feed the poor.

Josef Pieper's 1957 http://www.jstor.org/discover/10.2307/1405199?uid=3739560&uid=2&uid=4&uid=3739256&sid=21105248156153 Knowledge and Freedom is a sustained critique of this epitomizing of pragmatism. According to Pieper, when the pragmatic is made the judge, philosophy is murdered and the sciences are hamstrung. True freedom requires freedom from the current desires of mankind. This may seem paradoxical until one realizes that the university cannot exist if the mob is allowed to rule. Neither can it exist if there is no unity whatsoever. The word itself is a combination of 'unity' and 'diversity'.

The actual shift to technological (vs. social) control may be captured by the following from Charles Taylor's http://en.wikipedia.org/wiki/A_Secular_Age A Secular Age:


  The basic idea is that Baroque culture is a kind of synthesis of the modern understanding of agency as inward and poietic, constructing orders in the world, and the older understanding of the world as cosmos, shaped by Form. (795)


Another great book on this topic is Louis Dupré's http://rads.stackoverflow.com/amzn/click/0300065019 Passage to Modernity: An Essay in the Hermeneutics of Nature and Culture. He digs into the old "ontotheological synthesis", in which the ideal is to become conformed to the world (the kosmos), and how that broke down, primarily due to nominalism, especially via William of Ockham (1287 – 1347). Dupré argues that it was really http://plato.stanford.edu/entries/nominalism-metaphysics/ nominalism which laid a significant portion of the foundation for the Enlightenment, partly via detaching words from merely describing reality, to being tools humans could use to exert control over that reality.

As to Enlightenment being intended as a system of control, I suggest Allan Bloom's http://rads.stackoverflow.com/amzn/click/1451683200 The Closing of the American Mind:


      It has long been fashionable in some quarters to treat the thinkers of the Enlightenment as optimistic and superficial. This was a view promoted in the wake of the French Revolution by reactionaries and romantics, the counter-coup of the religious and the poetic, which has had considerable and enduring success. The modern philosophers are alleged to have believed in a new dawn in which men would become reasonable and everything would be for the best. They did not, according to this popular view, understand the ineradicable character of evil, nor did they know, or at least take sufficient account of, the power of the irrational of which our later, profounder age is so fully aware. In these pages, I have tried to show that this is a skewed and self-serving interpretation. No one who looks carefully at the project these philosophers outlined can accuse them of being optimistic in the sense of expecting a simple triumph of reason or of underestimating the power of evil. It is not sufficiently taken into account how Machiavellian they were, in all senses of that word, and that they were actually Machiavelli’s disciples. It was not by forgetting about the evil in man that they hoped to better his lot but by giving way to it rather than opposing it, by lowering standards. The very qualified rationality that they expected from most men was founded self-consciously on encouraging the greatest of all irrationalities. Selfishness was to be the means to the common good, and they never thought that the moral or artistic splendor of past nations was going to be reproduced in the world they were planning. The combination of hardness and playfulness found in their writings should dispel all suspicion of unfounded hopefulness. What they plotted was “realistic,” if anything ever was. (291–92)

A positive duty is an obligation to do something. A negative duty is an obligation to refrain from doing something (http://www.jstor.org/discover/10.2307/2218209?uid=3738328&uid=2&uid=4&sid=21106497132803 link). Thus, a common example of a negative duty is that we have a duty not to murder people or a duty not to tell lies. Conversely, a positive duty would be a duty to save people or intervene if we see a child molester or to serve in the army.

In regards to your first question, based on what you are saying, he is denying that we have a positive duty to help the needy.

In regards to your second question, if we look at what you provide, he is again denying that we have a positive duty.

This is because in both cases, he is saying we are not required to do something. Neither refers to what we are prohibited from doing.
I think that the prof means to distinguish between the term "almost" and the expression "almost everyone".
The expression "almost everyone" is primarily assuring (enlarging).
But the term "almost", within the expression "almost everyone", is guarding (diminishing).
So, the prof's italicized words do not seem to me to contradict his choice of #1 as the answer.
Here is a very provisional answer based on my own reading of Debords Spectacle.


  
  The Spectacle is not the mass dessimination of images...
  


So the technology is of no concern, and nor, surprisingly is the production of images and it's mass circulation.


  It is better viewed as a Weltenschaung that has been actualised, translated into the material realm - a world-view transformed into objective force.


Now, Marx is popularly viewed as turning Hegel on his head ie Hegel saying that mind dictates how matter moves, whereas Marx said it was the material conditions (economics) that dictates. 

Debord turns it around again; his point is that the spectacle has material force, it is the 'mind' of the productive forces; and this is confirmed in the preface when he states the Soviet Union crumbled due to the objective force of the Spectacle. 

But this simple 'upturning' of Marx isn't correct; as thesis 8 shows:


  
  The spectacle cannot be set in abstract opposition to concrete social activity...the spectacle, though it turns reality on its head, is itself the product of real activity. Likewise lived reality suffers the material assaults of the Spectacles mechanism of contemplation, incorporating the Spectacular order and lending it positive support.
  


ie the they form a dialectic, and this should lead one to expect an intensification of the Spectacles 'mechanisms of contemplation' - which is what in one sense Facebook is.



Euclid draws Euclidean lines

In Euclidean sands
This is an accident (also known as "destroying the exception").

An accident is a deductive fallacy in which one ignores the existence of exceptions, and thus obtains a faulty conclusion.

As Ryder notes, the first one isn't a fallacy, as stated, but a slightly loaded question. The appropriate answer is "Yes, in the case of this road, driving at night is safer than driving during business hours".

For the other two, they are good examples of an accident. The example given on wikipedia is another one, which concludes that surgeons are criminals because cutting people with knives is a criminal act and surgeons cut people with knives... ignoring that there are exceptions in which cutting a person with a knife is not a criminal act.
Physiological evidence suggests that removing emotion (not lowering its level, but breaking the mechanism that accesses it) limits one's ability to apply logic to situations.  E.g. the case studies of Antonio Damasio, simplified here: http://bigthink.com/experts-corner/decisions-are-emotional-not-logical-the-neuroscience-behind-decision-making http://bigthink.com/experts-corner/decisions-are-emotional-not-logical-the-neuroscience-behind-decision-making and interpreted in his book "Descartes Error".

The reason for this seems simple and direct, to me.

I would like to point out that the only way we have of realizing that we have decided or admitted that something is logical is a subjective feeling of clarity.  So I would contend that our sense of logic is, in fact, an emotion of its own, and therefore, any preference for acting logically is an emotional decision.  If you accept that, there is only internal competition between emotional forces, not some competition between logic and emotion.

Feeling calm and competent is a feeling, just as feeling shattered and angry is.  Choosing to impose one state over the other is still choosing to be emotional.  By choosing to 'ignore your emotions' you are really choosing the emotional state that has gotten you the best return on your investment in the past, not choosing some state that is not emotional.

I skipped the answer: Based on this, I am assuming that if we were really emotionless, we would not be able to function rationally.  (I am not sure that would make us more like animals.  To me, animals seem, in their own limited way, more rationally motivated than humans.  After all when it is perfectly logical for pig to kill off some of its children, it just does so, with an emotional equilibrium few humans could muster.)
This is called equivocation.

See:

http://onegoodmove.org/fallacy/equiv.htm http://onegoodmove.org/fallacy/equiv.htm

http://en.wikipedia.org/wiki/Equivocation http://en.wikipedia.org/wiki/Equivocation
Both of the terms you're mentioning are odd ways of compressing down what is going on in Kant. Odd enough that I wasn't sure Kant had stated either of them in that way.

"transcendental knowledge" is knowledge that is not of objects but rather about the apparatus of knowledge. In other words, for him, it is the metaphysics of knowing, i.e. it is knowing the categories themselves and the forms of sensibility that condition our knowledge of objects (http://en.wikipedia.org/wiki/Transcendence_%28philosophy%29 http://en.wikipedia.org/wiki/Transcendence_%28philosophy%29 http://www.iep.utm.edu/kantmeta/ http://www.iep.utm.edu/kantmeta/ ).

"a priori knowledge" is confusingly knowledge that is achieved under the conditions of the a priori (rather than knowledge that is about the a priori) and thus refers to the way we know objects under categories and through the forms of sensibility (Cf. http://plato.stanford.edu/entries/apriori/ http://plato.stanford.edu/entries/apriori/) 

Transcendental knowledge is not subsumed under the analytic / synthetic distinction because it is not really subject to the conditions of understanding objects. (Here, I use really as a technical term). However, for us, transcendental knowledge is an ideal, so its possible that the mode through which we encounter it requires us to synthesize features together to grasp what it is (though never perfectly).
The boy explicitly asked for a chocolate cake. 

His implicit wish might have been for any cake that could fix his craving.

Explicit wishes / questions and implicit wishes / questions don't always correspond pefectly, as illustrated by your example.
I think that the difference in standard deviation in a lot of attributes between men and women is huge.  Men are much more likely to be morons or geniuses, for instance.  Our society focusses on the upside, but the downside is equally real.

Part of this is natural, because men have 5% less genetic material.  But far beyond that, the social structure creates excessive competition which via vicious cycles drives the standard deviation between men up and that between women down.

Women are encouraged to complete directly to be appreciated, because this is a social norm, which limits their willingness to be too different and avoids spreading their distribution.

Men are challenged to be good enough at something to be appreciated, and so more often focus on becoming better at what they are already best at.  Also men are both naturally inclined and further conditioned to compartmentalize their self-image in a way that isolates them from awareness of their deficiencies.  These two forces artificially spread their distribution.

As hidebound as these two characterizations may sound, they are not significantly reduced in our modern society.  Women are allowed to compete with men, but they still feel constrained by the expectation that they should at he same time compete directly for appreciation and a different kind of social status -- this shapes the kinds of jobs they go into.  And we definitely see the males in out-groups punished differentially.  (No one is accidentally shooting a large number of black women.  I have never seen a lesbian dragged behind a truck, and there is not open denigration of them embedded in our day-to-day slang.  In any war, we kill mostly enemy men, doing otherwise is immoral.)

The primary driving force that allows us to favor this is the historical disposability of men.  Look at my other answer at https://philosophy.stackexchange.com/a/17817/9166 https://philosophy.stackexchange.com/a/17817/9166 for some potential historical sources of this phenomenon.

Criminality is a specifically awful aspect of a high heteroscedasticity between the genders because its opposite has very little open visibility.  We define crime very closely, and we can see where lying near extremes leads one to alienation from social values and toward obstacles to developing reasonable empathy or accountability.  But there is never any crime in being too average.
Not sure if there's any philosophers that have written on this subject itself, as opposed to knowing things in general.

Practically, Phillip Armour wrote an http://dl.acm.org/citation.cfm?id=352194 article about the levels of ignorance: 


0th Order Ignorance: Lack of Ignorance. I have 0OI when I (probably)
know something. 
1st Order Ignorance: Lack of Knowledge. I have 1OI
when I don't know something. With 1OI we have the question in a
well-factored form. 
2nd Order Ignorance: Lack of Awareness. I have
2OI when I don't know that I don't know something. 
3rd Order
Ignorance: Lack of Process. I have 3OI when I don't know a suitably
efficient way to find out I don't know that I don't know something.
4th Order Ignorance: Meta-ignorance. I have 4OI when I don't know
about the Five Orders of Ignorance.


The first level of ignorance is what people normally refer to. Rumsfeld referred to the second level: there were questions that he didn't know the answer to because he didn't know the question in the first place.
Even forced labor with excellent working conditions and high pay is slavery. "Forced" is all it takes. However, the question is somewhat blunted by the under-scrutinized question of what rights prisoners have. That is, if prisoners do not have the rights that others have, then nothing would follow from this being slavery. I think the key to solving this is to understand that in violating the rights of another and thus ending up in prison, there is a specific and limited restriction on your freedom, which does not in fact include forced labor, at least, not in Canada or the US. Needless to say, not all prisoners are rights-violaters.

[EDIT]
To clarify, the fundamental question is the nature (and source) of rights, and what follows from that. Imprisonment is, properly, a product of a person denying the rights of another, but if A violates the rights of B, that does not mean that A entirely loses their rights. A simple case would be breach of contract, which is a violation of property rights of another, which obviously is not properly rewarded with death. I would say that a concept of "justice" is what both justifies and tempers punishment for rights violations. That concept does not then justify forced labor, at least the indiscriminate kind potentially sanctioned by weak laws of the US and Canada. I don't believe that forced labor is absolutely immoral (because I don't believe that morality can properly be absolute), but it would be tangential to discuss rare circumstances where forced labor would be proper.
How incest as a taboo is defined is dependent upon the culture you are raised in. If I remember correctly, Cleopatra was the result of 27 generations of brother-sister marriages. In some cultures marrying your first cousin is ok, in others, second cousins are ok. In some to marry an uncle is ok. In others, several generations. Every society defines it differently. Why there are rules in the first place has never been settled. Some have held that it was a 'natural' mechanism for not allowing a particular weak DNA characteristic to become predominant. Others have said it is mainly a cultural taboo, that allowing relations between family members would not allow for stable family relationships needed by all societies to develop culturally.

Most of the taboos that are around were first developed when people were living in small to medium sized communities/tribes/villages of 100-200 people. Most pairings would take place within the small community as most people never had any contact outside their own communities during their entire lives. Having family taboos for pairings allowed inter-family bonds to be established which strengthened the community as a whole and at the same time allowed individual families to be stable family units without sexual competitions within an individual family.           
It seems that we have many questions in one here, and my answer is certainly one of many possible.

I'd like to quote Charles Sanders Peirce, from an article called "Some Consequences of Four Incapacities" (http://www.peirce.org/writings/p27.html http://www.peirce.org/writings/p27.html), where he states his disagreement with the Cartesian principle of universal doubt:  


  We cannot begin with complete doubt. We must begin with all the prejudices which we actually have when we enter upon the study of philosophy. These prejudices are not to be dispelled by a maxim, for they are things which it does not occur to us can be questioned. Hence this initial skepticism will be a mere self-deception, and not real doubt; and no one who follows the Cartesian method will ever be satisfied until he has formally recovered all those beliefs which in form he has given up. It is, therefore, as useless a preliminary as going to the North Pole would be in order to get to Constantinople by coming down regularly upon a meridian. A person may, it is true, in the course of his studies, find reason to doubt what he began by believing; but in that case he doubts because he has a positive reason for it, and not on account of the Cartesian maxim. Let us not pretend to doubt in philosophy what we do not doubt in our hearts.


Even if your line of argument does not implicate universal doubt - only the doubt that comes from the absence of immediate evidence from the senses - it is a form of skepticism that can easily lead us to discredit even the "certainties" that come from direct testimony: 

How can you be sure of having counted those eight bars correctly? And those other three on the left side, did you actually count them, or just subitized? Aren't your perceptual certainties also subjective, in the end? A result of insistence, mere beliefs? They certainly are beliefs too, even if that is not all what they are. 

The starting point of knowledge - of science - for Peirce, is the inevitability of hypothetical thinking. Any mind is a factory of hypotheses, unproved assertions, first guesses. This is the ground zero, not doubt. How we respond to the hypotheses we are condemned to make, that's where the problem of method begins.
Knowing or not knowing the consequences of an act is essential to evaluating the morality of a choice. Take one of those contrived examples where some evil genius has rigged explosives to a light switch such that when Mrs. Smith turns on the light, the victim is blown to smithereens. If Mrs. Smith knows of the consequences of turning on the light, and assuming there isn't some countervailing life-or-death paradox involved, then the moral choice is to choose not to turn on the light. If Mrs. Smith has no knowledge of this arrangement, the moral choice is to turn on the light (rather than curse the darkness). In the former case, she is choosing to both turn on the light and to kill the victim (the latter being the immoral choice), but in the latter case she is not choosing to kill, though that does turn out to be an unforseeable consequence of the action. The operative principle is that choosing to kill an innocent person is immoral; not all choices constitute "choosing to kill".

Your two examples have in common killing, but that is not the only domain of moral evaluation. To see whether there is a general principle (i.e. are we just dealing with a viceral reaction against death, are you getting bogged down in death penalty morality or meat-eating), you should look for a less extreme scenario. The proper moral question to ask is whether it is morally justified to choose to execute a person for theft, per se. If it is not, then doing it yourself versus commissioning someone else to do it for you does not change the picture, and it is even less relevant whether you implement the execution yourself.
I take the purpose of the wax argument to be a little different than you're specifying. I take the point of the wax argument to be that what gives physical things their consistency on Descartes' view is an application of a judgment of mind that some given physical object is an instance of an idea (this is highly important to his arguments in the subsequent meditations including his argument about how error works).

Regarding your 3, 4, and 5, I think you may be effectively misunderstanding Descartes. Starting with your comment after five, Descartes does not describe to the "primary qualities" language you are using. In fact, in large part, his point in the entire argument is that "wax" as a category is a category of mind. When you say "Hence, the conception of wax is derived not from the senses", this seems infelicitous as a wording, because for Descartes what is going on ins the mind's conception (that is judgement) of a particular physical thing as wax. And the mind can maintain this judgment over and against changes in the physical thing.  When he speaks of imagination being insufficient, it is that imagination can generate an infinite number of extensions and shapes for the wax, but these do not make something wax.

Your 7 speaks of "ordinary language", but I don't think that is of great concern to Descartes. His point instead is that we should realize "wax" is an idea of the mind that we in judgment apply to physical things. In other words, he thinks that it is wax is not a matter of perception or imagination but the use of a mental faculty.

His point here is to establish the absolute necessity and power of mind for knowledge, i.e., judgments about things we perceive. In other words, something being wax is not a question of what we see but of making a type of judgment relative to thing, and this is the power of mind.



At the point where this argument occurs, Descartes is not emphasizing the possibility of demonic skepticism. Instead, he has already proven what he takes to be the certainty of mind via the thinking thing argument (which is a condensed dilemma whereby whether or not he is deceived, he remains a thinking thing). Meditation 3 will return to the problem of demon skepticism and reject it (on the basis of an argument that it would be impossible for the self to generate an idea of a perfect God), and then through that the senses are guaranteed as well.

But at this point, he doesn't need the guarantee of the senses just the operation of the faculty of judgment itself will show (on his epistemology) that the pattern is that we are making judgments of mind applying an idea to our perception.
The coditional is "inside" a quantifier :


  there is no x (Tall(x) and Rich(x)),


that is equivalent to :


  forall x not (Tall (x) and Rich(x)).


Thus, if we "instantiate" the universal quantifier with John, we get :


  
    not (Tall(John) and Rich(John)).
  


Bur Rich(John) holds, and thus we have to conclude with : not Tall(John).
Basically, the fallacy of accident is one of the https://en.wikipedia.org/wiki/Accident_%28fallacy%29 ancient fallacies identified by Aristotle.  It means wrongly applying a general rule to a specific, exceptional situation.  The cited website (which is more than a bit confusing!) is using Biblical rules as an example because religious laws are often conceptualized as universal and exceptionless. The point is that even a Divine Commandment must be viewed with some kind of context.  Even within the Bible, http://biblehub.com/matthew/12.htm specific exceptions to general religious laws are frequently mentioned.

The other example is meant to show that yes, sometimes general rules can rightly be applied; and therefore that citing a general rule for a specific situation is not intrinsically fallacious; it is often a good and valid form of argument.  This makes sense, since most fallacies are superficially similar to good arguments --that's what makes them attractive.

The overall desired takeaway is that there isn't any one approach to religious laws --that common sense and judgment must be used even in the interpretation of divine commands.
(Personally, I agree with the idea that time is an aspect of change, and not the other way around.  But the physics does not presume that, and still allows for a consistent interpretations.)

None of the above.  You can anthropomorphize the happy photon, but you have to bring the rest of relativity along with you.  And relativity presents an interpretation in which time does stop for the photon, causation is temporal, and events are predictable.

Even if we imagine a the photon has a sense of time of its own, there is still no spatial movement seen by the photon relative to our perceived space.

Taking the same limit that reduces the passage of time to zero, also affects space.  The photon sees our length completely compressed to zero in whatever direction it is travelling relative to us.

We are always completely in its peripheral vision, whenever it might rotate to look at us, we disappear. (We are like Carlos Casteneda's death, always straight left of its line of sight, even when it turns its gaze.)  Whatever direction it might choose to move in our space, the space won't let it.  In effect, then, our whole space is reduced to a single point.  And for it to bounce around our point-sized universe does not ever require, or even allow for, motion with velocity.

To the extent it might perceive the remaining dimensions of our space perpendicular to the one in which it is traveling, there is still no real causal sense.  Things in our universe might cause it to turn, relative to us, but the photon sees this as other things moving around it, and not as causal effects upon it.

Further, those things moving around it are stuck where they are relative to one another, and in some ways relative to it.  They cannot get closer or farther, or change their other effects upon it in any way.  From the photon's point of view, our time is frozen, because when the photon's time passage relative to us became zero, so did our time passage relative to it.  That is what is relative about relativity.  And the bizarre view available to it of our frozen time would have to be one in which everything we see as successive, is actually happening at once.

This does not mean, necessarily, that the photon does not experience causation, but that the causation is independent of anything we might perceive.  It gets its own frame of reference, in which it can be affected by gravity, etc., in ways we would not perceive, just as it sees no effects on or from us, because our timelines are independent.  It sees as static what we see as changing, and vice versus, with each of us having a complete map of the other's timeline embedded in its view of space.

Rudy Rucker imagines extra spatial dimensions in which we might be extended and participate, but never perceive.  Our space would be like that to the photon.  Except that additionally, since our timelines are perpendicular, all side-effects of its participation in our space are frozen, still pending but already done, so there will never be real evidence to the photon that we exist or at least that what happens/ed here ever affects/ed it in any way.

Another way of looking at this is proposed by Wheeler and Dyson.  The slickest way to formulate the change of frames of reference is as four-dimensional rotation in a hyperbolic space.  As we speed up, time rotates (rotatesh ?) relative to space.  Attaining the speed of light relative to us, then, is a (hyperbolically) perpendicular rotation, which takes its time dimension into (a bizarre reshaping of) one of our spatial dimensions.  This indicates all the same effects but is far harder to interpret anthropomorphically.
Might be "perfect solution fallacy" aka "nirvana fallacy", meaning that any solution but a perfect one must be rejected. In this case, rejecting any course of action that doesn't prevent all robberies. 
For a discussion of the Libet experiment see the post   https://philosophy.stackexchange.com/questions/24716/contradiction-between-the-selfish-gene-and-lack-of-free-will-demonstrated-by-lib Contradiction between the selfish gene and lack of free will demonstrated by Libet experiment 

The Libet experiment deals with at least two different topics (Benjamin Libet: Mind time. 2004):


Unconscious versus conscious. Libet derives from the measurements of his experiment: A deliberate action starts as unconscious process and becomes conscious to the proband about 500ms later.
Free will versus determinism. Libet is very cautious whether his experiment rules out free will and can be explained by a deterministic approach only. He does not draw any conclusion on this topic. Until proof of the contrary he votes for the hypothesis of free will. He prefers the latter because it conforms to our sense of self. 


Concerning your considerations on our subjective sense of time which may  differ from the physical time: Please note that all time-related results of the Libet experiment refer to physical time.
While possibleWorld's answer suggests that the basic Utilitarian response would be to  save the cubes because this saves more people in the end, I would suggest that this is not necessarily the case. There's an important ambiguity in John Stuart Mill's Utilitarianism that has plagued the theory since then and is difficult to resolve. 

While the basic point of the theory is to "maximize happiness," there's a root problems here. First, it's not clear whether that is making an effort to maximize happiness or achieving maximal happiness. In other words, is it enough that I tried or do I have to succeed to behave morally? If success is the criterion, then this leads to bizarre consequences such as say stabbing someone randomly maximizing world happiness because of unknown consequences. If trying is the criterion, then something that does not at all increase happiness would be moral on a view that ostensibly thinks happiness = good. 

This problem can be restated as a problem with the future for utilitarian and consequentialist views. (Consequentialism more broadly does not restrict the value to happiness so we can substitute "life" or some other good). In the data cubes versus woman scenario, I want to suggest that the authors of the game are invoking a variant of this such that the actual gains of the woman not dying are valued more highly than the possible future good of saving many using the data cubes.

For Kantians, humanity (with a meaning that extends to other rational creatures) is the top value. As Kant explains, this has worth and other things have price (*Groundwork 4: 431–34). This means the value of rationality is for Kant something that is not tradable and you cannot thus place a data cube as of more worth than a person. 

Whether this would work for Kant's other tests of the Categorical Imperative is less clear. Since it seems that you might be able to universalize the choice to save many lives over the choice to save one, and that you subjectively would prefer this outcome even if you were one of the lives to be lost. (N.b., however that this hinges again on seeing the data cubes as actually saving lives rather than a potential that occurs afterwards.

For virtue ethics, the question would be what sort of moral character this develops. It seems like it develops the sort of moral character who trades one life for potentially helping many lives. It thus weakens courage and strengthens a sort of cold calculation. Conversely, one could argue that it produces a moral character that pursues the greater rather than lesser good. (One challenge for "virtue ethics" is that there are several different species with different accounts of the virtues; a second challenge is that, on for instance Aristotle's version, the only people who could know what the virtuous act is with certainty are phronemos (i.e. the practically wise)).



tl;dr - save one woman is guaranteed to save one life now (= actual). Saving data cubes is making a choice that might save many lives in the future (= potential). actual > potential.
This can be a case of the https://en.wikipedia.org/wiki/Cherry_picking_(fallacy) Cherry Picking fallacy:


  Cherry picking, suppressing evidence, or the fallacy of incomplete evidence is the act of pointing to individual cases or data that seem to confirm a particular position, while ignoring a significant portion of related cases or data that may contradict that position. 


The person making the argument picks a case which supports his point, ignoring the cases that don't support his point.
I certainly think you could have a coherent, viable moral system based around this principle; in many ways we already do in the concept of free will and recognition of and respect for other people as free-willed subjects.  In fact, in the evolution of secular ethics this could be considered a real trajectory.  

I do not think such a system would be acceptable to everyone; it is ontologically problematized in a religious context, if "the consent of those involved" extends into the supernatural.

However, I think many people could frame the exact same moral choices they make already in this context, given the appropriate conceptual framework.  It provides a foundation for moral discussion by making us ask questions like, "Who's consent is required?" (i.e., who's really involved) and "Who's capable of consenting?" (to allow for excluding children and the mentally unfit).  Applying that, I'll tackle a few of the problem/counter cases from elsewhere in this Q&A.


A potperri list from the original question: "incest, [...] mutilation, gay sex, sex with animal".  You can get consent to have gay sex, so that is pretty straight forward. You cannot get consent from an animal to have sex, although whether that makes it morally wrong is complicated by the fact that we do lots of nasty things to animals without their consent.  With regard to incest, the prohibition is largely because we know there is an increased rate of birth defects. 
Hence, this is really a question about whether it is moral to risk pregnancy when there is a significant risk of birth defects; beyond that I don't see anything immoral about incest, if it is consensual, between adults, etc.

Regarding mutilation, this needs to be defined more specifically, but it sort of goes along with freedoms we already consider essential.  Should smoking be completely banned because some people die from it?  Then what about automobiles, electricity, and red meat?  All of these can be fatal no matter how many precautions you take.  Can I safely chop off a finger?  Who's consent do I need if the answer is no?  Who's consent if it is yes?

I also think the mutilation question is more complex than it might seem at first glance.  Can https://en.wikipedia.org/wiki/Th%C3%ADch_Qu%E1%BA%A3ng_%C4%90%E1%BB%A9c self-immolation (which crosses over into the next point...) as political protest be considered a moral act, at least under some circumstances?  If so, couldn't there be forms of self-mutilation that line up with this? What about throwing yourself under a tank? 
That https://philosophy.stackexchange.com/questions/26591/can-any-consensual-activity-be-wrong?noredirect=1#comment62582_26591 suicide cannot be easily condoned because of regrettable errors in judgement, etc. I think the more significant case here is actually assisted suicide, and there's no reason we could not regulate this to mitigate against mental illness, etc. by committing to the consent/judgement of several involved people.

It can still be applied to cases of lone suicide, however. If I provide for my young family, who stand to suffer and perhaps even starve without me, it is possible to inject the concept of consent as a moral responsibility and deal with all the things we would normally want to consider in such a case.
https://philosophy.stackexchange.com/a/26595/6189 "..consensual sex between someone with AIDS and a partner married to someone else" -> As the author points out, this puts a further person at risk, and in-so-far as that is true this other person is involved.  The mistake is to then say that the consent of this involved person isn't necessary.  Likewise, any form of adultery without consent from your partner is a violation of a trust, and any such violation of trust is so because consent was not given in a situation where it might reasonably be considered prerequisite.  I loaned you my car, but you purposefully driving it into a lake was such a non-consensual violation of trust, regardless of whether there were other people in the car who thought it was okay.
https://philosophy.stackexchange.com/questions/26591/can-any-consensual-activity-be-wrong?noredirect=1#comment62593_26595 "Could any reasonable person get consent from everyone else on the Mississippi river?" Implies that if we consider consent to be too difficult to achieve, then we can set it aside without having to feel morally compromised.  But ethics is not something we apply only when it is convenient for us.  Might it become very difficult to get certain things done to the Mississippi River if we had to admit a moral responsibility requiring consent from a large number of people?  It might, but that is sort of the point of morality -- to bind our behaviour to a certain standard.

I'm going to attempt an answer, though it will be---necessarily---incomplete. First, I'll note that there's a kind of ambiguity in the question. The text known to us as "The Monadology" by Leibniz was originally written as a summary of his views for Eugene of Savoy, and not really a full explication of them. Modern editions have tended to add a series of references from one of the drafts Leibniz wrote, largely to his Theodicy. "Monadology" as a text refers to a broader context that we might call "monadology," though few of Leibniz's texts that might be taken as being important to constructing this "monadology" explicitly use the word "monad." The question might be asking, therefore, either does the "Monadology" have a relevance for philosophy today or does Leibniz's philosophy have a relevance to philosophy today.

Both Whitehead and Deleuze, as has been mentioned, make extensive use of Leibniz. I think @jobermark has adequately pointed to the elements of Leibniz in Whitehead. In terms of Deleuze, I'd simply point out that The Fold, his text on Leibniz (which has a rather novel but convincing exegesis of monads) is hardly the only thing he wrote on Leibniz. Leibniz also importantly infiltrates Deleuze's Expressionism in Philosophy: Spinoza and Logic of Sense both of which deal less explicitly with "The Monadology" though they do deal with "monadology." In particular, Deleuze is fascinated by a range of things which culminate around the notion of what he terms in The Fold "perspectivism" and its relation to monads. Perspective winds up defining both what it means for monads to express something and, perhaps more importantly, the notion of event.

Gabriel Tarde, although perhaps not fitting the notion of "contemporary philosophy" for several reasons, was deeply impacted by Leibniz's monadology and saw in something consistently being reconfirmed by the science of his day. 

Walter Benjamin also takes up and reinvigorates the concept of monad in an entirely different way, particularly in his Origin of the Trauerspiel, which continues to have knock-on effects in the spheres that Benjamin influences.

Hiroshi Kojima has published a study that links Leibnizian monads to Husserlian phenomenology.

Leibniz's monadology has to be importantly understood as follow-on to Aristotle's notion of substance. In this sense, Leibniz is often in the cross-hairs of the Object-Oriented Ontology-ish types of recent thought. In particular, Graham Harman has introduced Tristan Garcia recently published Form and Object as an antidote to a certain form of Leibnizianism, though it retains some elements that seem quite "monadological." Thus, for Garcia, each object has its sort of inverse, which is much like Leibniz's notion that every monad expresses the entire universe.

In a less continental vein, it might be worth mentioning Ian Hacking who, early in his career, had attempted, among other things, a Leibnizian account of space based on Leibniz's monadology. More than that, Leibniz's concept of monads, particularly following the ways it can be read into his correspondence with Clarke has become one way that some people attempt to rethink the idea of relativity.

Certainly other aspects of Leibniz's thought, deeply bound to his conception of monads, remain current. "Possible worlds," for example, owns its coinage, and much of its meaning to Leibniz, and is intimately connected to both "The Monadology" and "monadology" more broadly. So, too, his principle of the identity of indiscernibles, important for establishing monads, remains an active area of interest in contemporary metaphysics.

None of this is to say that there is some sort of great Leibnizian moment going on or in the more recent past. Nevertheless, Leibniz's monadology hasn't completely left the world without some sort of traces.

Indeed. A logical implication "p → q" is true if either p is false or q is true - in other words, it is only false if p is true but q false.

This means that we can say that ∀ x; P(x) → Q(x) is equivalent to ∀ x; Q(x) ∨ ¬P(x): for every x either Q(x) has to be true or P(x) has to be false, or both.
In this case, you need to show that the system holds a contradiction. So, if you can arrive at ∃ x; ¬P(x), you have shown a contradiction with premise 3 and you are done.

Yes. In particular, the philosophy of the Australian moral philosopher https://en.wikipedia.org/wiki/Peter_Singer Peter Singer takes this point of view in some cases. He does not argue that, for example, any given pidgeon has a higher 'value' than all human beings but instead that there is a sliding scale where it is possible in some cases for animals to have more worth than humans (essentially that there is nothing inherent in humans that makes us superior).

This has lead to much controversy - Following this principle there may be cases where you should prioritize saving a very gifted chimpanzee from a burning house rather than a heavily disabled human, and this has been interpreted by some people as a complete devaluation (I can't think of a better word right now) of disabled people in general.

EDIT: It really depends on what problems you are trying to cover, but if the above seems relevant to your interest then you might want to take a further look into the areas of (moral) utilitarianism, bioethics and discussions on the intrinsic value of animals/people.
I don't think that it is necessary to motivate your choice from 'how important' human lives are.  But from the fact that humans are responsible for other humans, in a different way from the way they are responsible for other animals.

Species naturally advance their own genes.  By the standards of many biological theories, that is what makes a species a species.  And one of the most logical ways to advance them is to preserve them.  So any non-domesticated animal would choose to save the members of its own species before those of another.

[By domestication I mean when one species -- any species (we are not the only species that domesticates others http://www.nytimes.com/2002/10/15/science/before-adam-and-eve-the-farmers-were-termites.html http://www.nytimes.com/2002/10/15/science/before-adam-and-eve-the-farmers-were-termites.html) -- adapts the behavior of another for its own ends.]

We are non-domesticated animals.  So it is only natural for us to do the same.

Your own dog is one of the very few exceptions.  Heavily domesticated species like dogs and horses have been bred and raised to value humans above themselves.  The dog might save the human.  Well-bred horses do quite uncomfortable or dangerous things to protect their riders or drivers.

But rather than some absolute 'value', I think this is better looked at as the contents of a certain kind of social contract.  We agree to take care of our own lives, and to trust nature more with the lives of everything else.

The dog has been brought into that contract in another way -- its existence is due to our choice.  The entire species would not exist, had we not adopted it as our own set of personal servants.  (We have evidence that even the most peaceful, primitive, vegetarian societies did so, keeping them as sanitary agents -- garbage disposals and walking napkins).  Likewise, the cows people eat would not exist if we had not fostered them, and we feel entitled to determine what purpose they should serve.

I think we should avoid a moral basis in absolute human value, which seems to me to culminate in the tradition of trophy hunting, where human comfort and enjoyment is of value, and the lives of animals hold little intrinsic value.  Instead, we should look at the agreements 'negotiated' by our societies and our genes, and consider where everyone's best interests lie, but accept that we will always value those more like ourselves higher than others to some degree.
I could probably give a better answer if you gave  a reference to a text (though it may not be all that clear in Epicurus' writing style if memory serves), but generically the need for food and the need for sex are pretty different.

The words "natural" and "necessary" can have several very different meanings. I take "natural" in the context of Epicurus to mean that which we have without any external imposition, i.e. things he things humans just have.

For "necessary" in this context, I take it to mean that without this thing an individual person (n.b., the inclusion of the term individual here) will not be able to live. In other words, that which is necessary here is that without which I will die.

I then take the two following sentences to be true:


  If I were to not eat, I will die.
  
  If I were to not have sex, I will not die.


Translating them into the terms of necessity:


  Eating is necessary.
  
  Sex is not necessary.


Changing contexts:


  I want to eat.
  
  I want to have sex.


become = I have desires for eating and sex. (and we can also adduce for most humans these are natural desires).

Thus, eating is "natural" and "necessary" but sex is "natural" but "unnecessary".

For the record, I don't think he's wrong.
The Greek terms are: good = agathos, beautiful = kalos, just = dikaios, the one = to hen, god = theos.

I do not remember any passage from Plato where he equates to hen = theos. I assume the apotheosis of to hen is due to later new-Platonic philosophy.
See https://en.wikipedia.org/wiki/Jabberwocky Jabberwocky; it is a "well written" nonsense poem, full of suggestive rhymes and words, like :


  All mimsy were the borogoves.


It is readable and enjoyable, and it has beeen translated multiple times.

So what ? From a solipsistic point of view, how it is possible that your own mind can 'write up' a poem that makes no sense, yet built on syntactical and rethorical rules that you are mastering ?

Thus, if you do not "understand" it, someone outside you must have "created" it, and thus solipsism has been defeated...

This looks like a variation of http://press.princeton.edu/chapters/i7298.html Descartes' argument base on doubt.
I'm not sure that the phrasing of question makes a great deal of sense. I'll try to explain why below.

When Foucault is developing the concept of biopower (most particularly in is lectures published as "Society Must Be Defended") he traces it as a transformation in the classic definition of sovereignty from "the right of life and death" (Hobbes uses this as a defining facet of sovereignty in Chapters 21 and 30 of the Leviathan). Classically, according to Foucault, this meant the sovereign has the right "to take life or let live" ("Society Must Be Defended", p. 241). This classical right, in the 19th century, comes to be "complemented by a new right which does not erase the old right but which does penetrate it, permeate it:" "the power to 'make' live and 'let' die" ("Society Must Be Defended", p. 241) It is this last which is most properly biopower. Properly speaking, therefore, biopower cannot "lie under" any of Hobbes powers of sovereignty because biopower names a form of power that, for Hobbes, has not yet been invented.

That said, there is a perhaps more interesting relationship between biopower and Hobbesian power. It is important to note that the power of the sovereign you have enumerated are powers which Hobbes describes as "annexed to sovereignty" (see Chapter 18 of Leviathan). Being "annexed to sovereignty" means that they are powers belonging to the sovereign, but that they can be shown to follow from the purpose for which the sovereign was created, that is, the peace of the commonwealth. The relevant question is, however, to what are these powers annexed? 

What I suggest is that the core of sovereignty to which these powers are annexed is the right over life and death, which, on my reading of Hobbes, is the basic inalienable right of sovereignty, the very core of what it means to be sovereign. Thus, in Chapter 18 of Leviathan, Hobbes argues the sovereign can never cause injury to his subjects. He goes on, in Chapter 21, to use this argument to show that the liberty of the subjects must be consistent with the sovereign's right over their life and death. Moreover, he uses this argument—still in Chapter 18—to argue that the sovereign may never be justly killed nor otherwise punished. In this sense, there is a form of proto-biopower at work in Hobbes, albeit in the sense of the control of bodies, rather than the control of populations. 

This proto-biopower does not belong to any of the enumerated forms of the exercise of sovereign power. Rather, it forms their source, the basis on which they can be derived. Hence, whereas Hobbes argues for these other forms of power being annexed to the core of sovereignty, but he makes no argument for the sovereign having the right of life and death. The right of life and death is the most basic meaning of sovereignty. The development of biopower from out of this proto-biopower, therefore, does not represent a change in any of the powers enumerated to the sovereign nor the assignment of yet new powers to sovereignty. Rather it is, as Foucault argues, a shift in the very meaning of sovereignty itself, i.e., in the core from which all these powers are derived.

Biopower, as a shift in the core of sovereignty, is exercised through all the various classical forms of power Hobbes enumerates, but because it has changed the very idea of sovereignty, it necessarily changes how those powers are exercised and even what those powers designate.
I think that according to special relativity, there is no now somewhere else, regardless of the distance, at least as long as we are not talking of quantum mechanics scales, "now" is only "here".

This is how Feynman puts it: "Alpha Centauri 'now' is an idea or concept of our mind; it is not something that is really definable physically at the moment." but the point is that it does not matter if we are talking of another galaxy or another continent, or another city or another room.

the tip of a light cone is just a point in space-time and any event outside the light cone has no fact-of-the-matter time order relation to it, regardless of the distance.

it is actually a mind blowing property of space-time, that we do not appreciate enough.

naturally, we have a little complexity in that the brain is about 10-15 cm in diameter, a problem that can be exploited for an indeterminate amount of contemplation.



you may be thinking now, "what is this guy talking about? didn't he hear of relative simultaneity?"

The planes of relative simultaneity are useful as mathematical abstractions but do not fit the bill of a Now. try to think about it while walking back and forth in your room, contemplating deeply what some hypothetical aliens are doing right now 2.5M light years away (on the axis of your march), but note that this remote Now of yours absurdly shifts back and forth days or even weeks each time you change direction. this problem is called the https://en.wikipedia.org/wiki/Rietdijk%E2%80%93Putnam_argument Andromeda Paradox.



as for presentism, it seems to me that things that have ceased to exist in your past light cone, like Socrates, do not exist (anymore), and things that will come into being in your future light cone, like the philosophers of the 31st century, do not exist (yet), and as for events outside your light cone, it is meaningless to ask if they have already happened or not. 

there is no such thing as a snapshot of existence in the universe.
In mathematical logic P → Q must be read : 

"if P, then Q", as well as : "P only if Q". 

Thus "Q only if P" is Q → P, which is not equivalent to P → Q.



You can see also this https://math.stackexchange.com/questions/617562/conditional-statements-only-if post as well as this https://math.stackexchange.com/questions/311192/how-does-if-p-then-q-have-the-same-meaning-as-q-only-if-p one.
There are many confusing definitions and ideas about what Metaphysics consists of; I like Aristotles, where he also calls it First Philosophy; he takes it to be an investigation into the first principles of nature.

To give an account, then of human nature, must then ground themselves into what these first principles are.

If a first principle is God, or Brahman or Allah; it seems that we must ground human nature there; and this is done, for in Christianity - we have the nature of the soul, in Hinduism - atman, in Islam - ruh.

In a framework where these are not accepted, say modern empiricism, then they are grounded naturally - in the mind, or subconscious: so Freud, Jung and Lacan.  

So yes, by definition a full account must take metaphysics into account; a partial account will not, neccessarily.
Agent intellect is the faculty that mediates between concrete things and abstract concepts. Avicenna and Aquinas represented opposite positions on its nature, and we do not know which position is closer to the truth even today, although the issues are phrased very differently of course.

The notion of agent intellect goes back to Aristotle who distinguished active and passive aspect in the intellect, just as matter and cause are in reality. He was very vague on the nature of this intellect "by virtue of making all things" however. Most Greek and Islamic commentators interpreted Aristotle to mean something transcendent to the human soul, as opposed to "potential" or "possible" intellect of humans, which is part of it. Avicenna elaborated and clarified this transcendent interpretation: agent intellect is the lowest in the series of spiritual substances that rules the sublunar world and illuminates possible intellect with intelligible forms. As such, it is the source of abstract concepts and first principles of science. This dovetailed nicely with Augustinian illumination of the soul by God, and some Christian commentators even identified the agent intellect with God himself.

Aquinas rejected this transcendent interpretation altogether, and gave an elaborate theory of agent intellect as human soul's own power. In particular it is responsible for stripping sense perceptions of their "material" contents, turning them into universals and making them accessible to passive intellect. Although he had precursors in Bonaventura and Philip the Chancellor, in contrast their accounts like Aristotle's were quite obscure. Here is Aquinas in his own words:"I say together with Avicenna, that the possible intellect... is different in different individuals... But I add that also the  agent  intellect is different in different individuals... It is necessary to postulate a power, belonging to the intellect, to create actually thinkable objects by abstracting ideas from their material conditions. That is why we need to postulate an agent intellect". 

See http://www.verbum-analectaneolatina.hu/pdf/6-1-06.pdf Nejeschleba's Thomas Aquinas and the Early Franciscan School on the Agent Intellect for more context.
The fundamental meaning of a fallacy is that it is a defective piece of reasoning that conforms to an identifiable pattern such that all instances of that pattern are defective. If you are asking, is there a single kind of pattern that all fallacies conform to, then no, there are many different types of defective reasoning. 

What fallacies do have in common is that they are not cogent, or that the truth or probability of their premises does not provide support for the truth or probability of their conclusion. 

It is important to note that the defective nature of fallacies is not the same as invalidity. Being invalid is neither necessary nor sufficient for an argument to be a fallacy. 
The question is actually a fundamental and nuanced point of science.  It goes from "this looks like it might be a good way to think about things" to "this is how things are."  The way one thinks about that transition varies by time period.  Because of the way you are phrasing the question, I have a feeling the modern scientific approach is the most meaningful, so I will do it first.

Modern Scientific Approach

In this scenario, we're going to need to define a few more propositions to properly capture the entire process, step by step.  First, we are going to start with Pp, the "prevailing hypothesis."  This is the presumed answer, based on what people knew at the time.  It's also something we'll be overturning, so don't be surprised that you already know its false:


  Pp: "Because there are two parents for each offspring, the experiment crosses should always exhibit 1:1 ratios."


Let's presume Mendel was a modern scientist, perhaps a grad student.  His adviser had noticed that Pp just didn't feel right in a few interesting corner cases, so had Mendel do a quick screening test.  Sure enough, Pp felt off.  The proportions were off by a long shot.  In fact, it looked like a 3:1 ratio in some situations, not a 1:1 ratio that Pp claimed would be found.  Mendel then develops his new theory of genes (rules L, in your phrasing) which predicts a 3:1 ratio.  Now he needs to "prove" it.  To do so, he sets up an experiment and makes a hypothesis H1:


  H1: "This experiment should reveal that my model is correct, including its prediction of a 3:1 ratio"


Now, to follow the scientific method, he sets up a null hypothesis, H0, with the intent of rejecting it via statistics


  H0: The experimental results will not consistent with a 3:1 ratio plus some random noise due to experimental error.


Mendel then does the experiment, analyses the results, determines that H0 can be rejected because it's just too statistically unlikely that the results had anything but a 3:1 ratio.

Now, if enough others agree with the work (potentially running their own experiments), the model Mendel proposed becomes the accepted theory, overturning Pp, and prevailing at least until someone starts playing with multiple alleles and publishes their own work.

Now he goes about "falsifying" Pp, by demonstrating that his new [conflicting] theory, P1, fits the data cleanly while Pp looks less reasonable because it is a poor match to the data.  To "prove" P1 using the scientific method, he uses one final proposition, the null-hypothesis, P0: "Pea plant crossings do not operate near a 3:1 ratio with a random variance around that point."  If the data from his upcomming rejects this P0, then the only valid theory remaining is P1. 

Now Mendel runs his experiement, and sees that P2 is true.  This allows him to reject P0, the null hypothesis, leaving only P1 (3:1 ratios) as potentially true.  He has also rejected Pp in the process, because Pp (1:1 ratios) is part of P0 (anything but 3:1)

More intuitive

Of course, all of the modern scientific approach involved statistics and falsification of hypotheses, and all sorts of things that are relatively new phrasings.  More traditionally, the process may be simpler.  The fact that Mendel's model does a good job of explaining the experimental results and the prevailing model does a very poor job of explaining them may be sufficient to argue that Mendel's new model should be considered "right," without all the formalism of null-hypotheses and other features of modern scientific methodology.

More pedantic

An alternative approach is to observe that the prevailing model does a poor job of predicting experimental results, scratch together a theory, and state "The results of pea plant crossings is well modeled using the model, Rules L."  At this point, no statement of ontological truth is made, just an empirical announcement that this model fits the data so far.  Over time, if no better model comes forth, people may elect to change the phrasing to "The model is true," not because any additional evidence had come forth, but simply because it is convenient to think that way.  This may be very unsatisfying mathematically, but linguistically it captures how ideas evolve, simplify, and are passed on to new generations.
Ideas are not actors. Actors in ethics are always persons. 

Hence moral ideas cannot corrupt themselves. 

But persons can misinterpret or abandon ideas. Or as you axpressed it "we [are] the ones to corrupt moral ideas."

Added after your comments:
As I understand from your comments, your question is:


  Why is it that "The followers of [american] dream will even commit crimes and sins in order to get money. This contradicts the main principle of the dream."


There are several possible explanations:


Those who commit those crimes are not the same as those who subscribed to the dream.
Those who subsribed to the dream changed their opinion.
Ideas and dreams are not strong enough to suppress actions which offend these dreams. Actions can be induced by more powerful drives and desires. 

You are only equivocating on two very very different usage of the same word.

In mathematics, "http://plato.stanford.edu/entries/type-theory/ type" is a syntactical or ontological category, while in psychology is a descriptive term that can have a "specific" meaning in the context of a https://en.wikipedia.org/wiki/Psychological_Types typology theory of human personalities.



Your example regarding :


  a jolly man "wearing a hat" that act as a catalyst on a social environment and then cause the environment to be "wearing a hat" as well


is a reasonable example of syntactical type theory : stretching it a little bit, we can say that the phrase "a social environment is wearing a hat" is a case of https://en.wikipedia.org/wiki/Category_mistake category mistake, because it is not "grammatically correct" to predicate "waring a hat" of "objects" like social environments. 
There is a couple of fallacies at play here, all variations on the https://en.wikipedia.org/wiki/List_of_fallacies#Red_herring_fallacies red herring. Bringing in racism is a red herring because racism is irrelevant to religion as  Jack himself acknowledges. He then says that those who so argue might still be racists nonetheless. This is a valid point, but it is irrelevant since the original argument was to show that they are racists in view of their criticism of religion, not to refute someone who claimed they can't be that in addition to it. This is https://en.wikipedia.org/wiki/Ignoratio_elenchi ignoratio elenchi, a.k.a. irrelevant conclusion or ignoring the issue. And finally the claim of racism is more emotionally charged than being anti-religion, so it is likely used to discredit the opponents personally and dismiss their arguments, which is a case of https://en.wikipedia.org/wiki/Poisoning_the_well poisoning the well.
Kant explicitly writes on suicide in his Metaphysics of Morals, Part Two, Ak. 421-24.

As I have no translation in English at hand, I will paraphrase the main point:

Suicide is a crime (murder). It is a violation of a perfect duty against oneself (First Part, First Book is named "Perfect duties against oneself").

So there can be no categorical imperative commanding suicide, no matter how casuisticly sophisticated the situation may be. Because duties are imposed by the categorical imperative (singular!).

For further arguments on this, feel free to read by yourself, especially the casuistic questions 423-24.

In German, the text can be found http://www.korpora.org/kant/aa06/421.html here.
Given that p or ¬p is a classical tautology, and normal modal logic has all tautologies as theorems and the necessitation rule, then □(p or ¬p) is a theorem, so yes. Having said that, modal logic refers to a whole family of logics, rather than a single logic, so it is perfectly possible to set up a logic in which this doesn't hold, such as constructive modal CS4. 
I don't know what I would call this fallacy but I would say we can generalize the problem as confusing credentials with knowledge.

In other words, it doesn't matter what letters come before or after someone's name as to whether or not they might know (or not something).

At first, I want to say this is an informal fallacy, but I think we can actually describe it as a formal fallacy.

First off, I think we can reorganize the argument as follows:


If you were a lawyer, you would (could) know what you are talking about.
You are not a lawyer.
Ergo, you do not know what you are talking about.


This is a formally fallacious argument that is an example of "denying the antecedent." The fallacy is that there are of a number of ways that an individual could know what they are talking about which is not restricted to "lawyer" (substitute as necessary). In this case, the conditional identifies a sufficient condition.

In terms of informal fallacies happening in the argument, I suppose we could name this either an ad hominem insofar as it does not deny the claim of C but only attacks C's ability to make the claim or still an "argument from authority" or possibly an inverted argument for authority (to mark that the assertion is that an authority is necessary and absent). 

Similar arguments need not necessarily be fallacious. For instance,


* If you speak Japanese, you understand what he is saying.
You don't speak Japanese.
Ergo, you don't understand what he is saying.


But here the language is deceptive, because in general we would take the first claim here not to be a simple conditional but rather a claim that is a necessary condition for understanding what he says.

I've marked the first sentence with a * because as far as formalizations go, this is an incorrect use of a conditional in normal logic, but it seems completely natural as English.

I mention this second example in part to state that similarity in the structure of the English alone is not enough to prove that the argument is fallacious.
Some of the answers to these set of questions you raised reside on Mill's principle of harm. According to this principle, the state can rightfully exercise its power over any citizen in order to prevent harm to others. So, if pornography causes harm to others, it follows, such principle could be justly activated. Another principle sometimes made use of is what may be called legal moralism, and yet another is what may be called legal paternalism. According to the latter, the state can rightfully exercise its power over any citizen in order to prevent him/her from harming themselves; and according to the former, the state can rightfully exercise its power over any citizen in order to protect what is plausibly assumed to be standards of community morality. 

There seem to have been consensus that pornography involving children - either as acts or as consumers - should justly be prohibited. But the question of whether censorship of pornography may be justified in the case of consenting adults has portrayed disagreements. Moral Conservatives, for instance, contend that the state should prohibit pornography as part of its responsibility for its citizens. They advocate legal moralism and legal paternalism. Liberals on the other hand perceive pornography as a manifestation of freedom of speech and advocate the principle of harm - the only grounds that may be legitimate for state limitation on individual's freedom is in order to prevent harm to others. Feminists too voice their position. Some feminists, e.g. Catharine MacKinnon, argue that pornography should be justly prohibited as it violates women’s civil right (of equality); it is a political practice defining the treatment and status of all women, establishing them as inferior to men. Yet other feminists contend that pornography rather liberates women and assists in establishing gender equality. 
The ability to 'hear' ones own thought and emotions is possible in Douglas Hofstadter's https://en.wikipedia.org/wiki/Strange_loop strange loops and https://onphilosophy.wordpress.com/2006/08/28/two-problems-facing-self-representational-approaches-to-consciousness/ self representational theories of consciousness. 

The idea is that living beings first developed the ability to sense and to represent outside objects. At one point, their sensing abilities got advanced enough that they could sense and represent themselves as well. That is when they developed consciousness.  
In my opinion, the modern understanding of "deserving punishment" goes something like this:


The emergence of https://en.wikipedia.org/wiki/Third-party_punishment altruistic punishment (as distinct from vengeance) can be explained by https://en.wikipedia.org/wiki/Evolutionary_game_theory evolutionary game theory (which btw can also explain spite, vengeance etc.), and in particular, by models and simulations based thereon. I don't know the details, but you can run off and learn the math and even write your own computer simulations to convince yourself of this.
Presumably, the idea of "deserving punishment" then naturally emerges in order to help people view themselves in a positive light (I am moral, I am decent etc.) despite that we sometimes punish other people with ridicule, ostracization, and even violence, etc., and despite that we often authorize others to punish people by these and other means. I recall reading that social psychologists have long known that if you can convince someone that they are a bad person, you make them more likely to behave badly, so this is actually pretty important.

One way of thinking infinity is that it measures an https://en.wikipedia.org/wiki/Arbitrarily_large arbitrarily large set (or characterizes arbitrarily fast speeds, etc.) 

The basic idea is that a given property is true regardless of how large something becomes or to what speed it accelerates.

Let's consider this object continuously increasing in size over time. It is clear it can become however large we want it given we are willing to wait some amount of time for the object to grow. 

This seems to my mind to imply that any description of the size of this object will be correct at some point (given it is larger than the "initial" size of the object.) So, for instance, if we wish to assert of the size of this object will be 15m, and assuming the object began growing when it was smaller than this, at some point in time it will be 15m exactly. 

As you suggest, if the object is enduring a continuous transformation of its size, while there is a precise moment when this property is true of the object, nevertheless its duration is infinitesimal, and it is already past before any indication is possible.

However it is of course possible to formulate rigorous functions to show the continuous variation in size, inferring a derivative rate of growth, which constitutes an adequate description at every moment in time. Functions are universal in this sense; they can capture transformations as  variable maps. 

As a quick commentary here, we are very close to the heart of science and mathematics: that is to say, the creation of functions, delicate sieves capable of capturing every point in a continuous transformation through abstraction, implicating the distribution of points in depth. Creating a function means plunging into chaos and extracting free variables to assemble new equations, isolating time or space, extruding the imperceptible lines of structural or genetic organization which articulate all the moments of growth and development.
The idea of applying radical alterity to aesthetics is a partial theme of the work of http://rads.stackoverflow.com/amzn/click/0823231089 Jean-Luc Marion.

He calls these sorts of things saturated phenomenon.

For him, these reach their culmination not in art but in religion, but he does talk about art as saturated phenomenon. See for instances Chttps://books.google.co.jp/books?id=cjshCgAAQBAJ&pg=PA51&lpg=PA51&dq=art%20as%20saturated%20phenomenon&source=bl&ots=SMyDrPjXtP&sig=LtM_cNiFMJttIjh7Wty3bY3cWOk&hl=en&sa=X&ved=0ahUKEwjK0Ozd9frJAhXC2qYKHR1yAdwQ6AEINTAE#v=onepage&q=art%20as%20saturated%20phenomenon&f=false rina Gschwandtner book.
The answer lies in the Phaedo,  not much after the passage on suicide, to which you referred. The issue of suicide arises in the context of the question, put to Socrates, why he seemed to favor death, rather than struggling to avoid it. And a part of his answer was, that the knowledge which the philosopher seeks all his life, seems to await him after death. Because the body is mainly an obstruction to true knowledge.  Knowledge is,  then, not merely possible after death. It seems to be more possible after death than during life. At least so for someone, like a philosopher, who has been preparing himself during life... 


  It has been proved to us by experience that if we would have pure knowledge of anything we must be quit of the body—the soul in herself must behold things in themselves: and then we shall attain the wisdom which we desire, and of which we say that we are lovers, not while we live, but after death; for if while in company with the body, the soul cannot have pure knowledge, one of two things follows—either knowledge is not to be attained at all, or, if at all, after death. For then, and not till then, the soul will be parted from the body and exist in herself alone.

No gay marriage in itself does not justify incest marriage per se. Both are quite different. However reasons why gay marriage is justified may apply even to incest.

For example in Obergefell v. Hodges -

"The Court listed four distinct reasons why the fundamental right to marry applies to same-sex couples. 

First, "the right to personal choice regarding marriage is inherent in the concept of individual autonomy."

Second, "the right to marry is fundamental because it supports a two-person union unlike any other in its importance to the committed individuals," a principle applying equally to same-sex couples.

Third, the fundamental right to marry "safeguards children and families and thus draws meaning from related rights of childrearing, procreation, and education"; as same-sex couples have children and families, they are deserving of this safeguard—though the right to marry in the United States has never been conditioned on procreation.

Fourth, and lastly, "marriage is a keystone of our social order," and "[t]here is no difference between same- and opposite-sex couples with respect to this principle"; consequently, preventing same-sex couples from marrying puts them at odds with society, denies them countless benefits of marriage, and introduces instability into their relationships for no justifiable reason.

(https://en.wikipedia.org/wiki/Obergefell_v._Hodges#Majority_opinion https://en.wikipedia.org/wiki/Obergefell_v._Hodges#Majority_opinion)

These are also commonly given justifications by gay rights activists. Incest marriage satisfies all these criteria. 

Although court opinion in itself is not a best philosophical foundation, I think but it is most popular one.
Because the argument makes an explicit appeal to societal definitions of virtue, and does not attempt to claim an ontically objective one, it does seem to be valid and sound under those assumptions. It would me much harder to defend ontic objectivity in any ethical theory.

1) In our society, respect and empathy are virtuous. 
2) A doctor respecting the wishes of a patient is empathetic.
3) Therefore, the doctor is acting virtuously.
This question is phrased as a legal question, which belongs on Law.SE.  However, it seems as though the philosophical question you are getting at is whether you are responsible for your actions if someone else tells you to do it.

Obviously there's many opinions in philosophy about this, but the current prevailing opinion would be "yes, you are responsible for any action you do under your own free will, even if you are being asked to do so by someone else."  This opinion is maintained solidly by the result of the Nuremberg trials, which made it very clear that a solider could not simply claim they were "following orders" and get off for crimes against humanity.

There is an exception for duress, which is when the person asking you to do something is sufficiently forceful that it is reasonable to say you had no freewill.  For example, you are rarely considered responsible for actions done while a gun is held to your head, up to but not including murdering someone yourself.

The person doing the forcing is responsible for their actions, of course, but not automatically responsible for yours.  They become responsible when it becomes clear that they took responsibility away.  This is the basis for arguments regarding statutory rape.  The idea is that it is too easy for someone older to take over a situation thoroughly enough that a younger person cannot be deemed responsible for their actions, because they simply didn't know better.

There are always grey areas.  However, hopefully those cases will be enough to frame any future questions you might have about responsibility.
Both statements [2.] and [3.] are statements about the net (aggregate) amount of gas used by the vehicle classes -- therefore they are statements about the collective properties of those classes.

A hasty generalization would be more of the form "therefore for all individual cars the amount of fuel consumes is less than for any individual fire truck".
Here is a relevant link and quote contrasting instrumental reason with substantial reason.  As you say, practical efficiency can be simply instrumental.  The moral qualities of reasoning require a higher order.

https://ldmac5.wordpress.com/2011/10/17/max-horkheimer%E2%80%99s-critique-of-instrumental-reason-and-the-domination-of-nature/ Max Horkheimer’s Critique of Instrumental Reason and the Domination of Nature


  The capitalist system can be seen to have a directly proportionate
  relationship with instrumental reason. Horkheimer goes beyond this
  comparison to suggest that the gradual reification – the mastery of
  nature – has its end in Fascism, however liberal democracy and
  capitalism were very much the target of the Frankfurt School. The
  capitalist agent looks to commodify the world toward his/her own ends,
  primarily the individualistic pursuit of self-preservation.
  Instrumental reason is the greatest tool in this process. In any case,
  substantial reason, which guides moral judgements and values, a
  veritable looking glass into reality and the truth of the world, has
  in today’s capitalist society become obsolete.

There are a few limitations that are worth mentioning:


Arithmetic is not a trivial thing.  In particular, one has to deal with the axiom of induction, which is metaphorically quite similar to a tower of Babel argument.  It took a lot of work to develop meanings of logical concepts which could reach to infinity with the finesse mathematics does.
For many, "right" and "wrong" are fluid concepts, unlike the "true" and "false" of strict mathematical speak.  Right and wrong have developed to deal with situations where the information is not always perfectly known, so they have subtle shifts in "flavor" when compared to "true" and "false."  There are those, of course, that want such distinct "right" and "wrong," so Godel's theorem may apply to them.
You need a concept of negation with strict enough properties to permit the diagonalization lemma to take charge.


One thing I do think Godel's theorem does is it shows that there are a class of formal ethical systems which cannot be provably consistent.  If someone is peddling their ethics system as being provably consistent, and you can find a way to prove arithmetic in using their ethics system, you demonstrate they must have one of the flaws associated with Godel's incompleteness theorem.

This would be very similar to what Godel did to Russel.  He took Russel's system for Principia Mathematica, and stood it on its head, using it to prove its own limitations.

When it comes to ethics systems, I find Tarski's non-definability theorem more useful than Godel's incompleteness theorem.  They are cut from the same cloth, as both use the diagonalization lemma to prove a system cannot be provably consistent, but Tarski chose to direct his proof towards any formal language.  He proved that it is impossible for a formal language which includes negation to define its own semantics.  In particular, it cannot define a concept of "True(x)" where x is a sentence in that language.  The particular implications of Tarksi's proof are remarkably similar to that of Godel's Incompleteness theorems, but their focus on formal languages is more effective for showing limits of ethics systems.  It basically forces any system of ethics which has negation and is strong enough to admit the diagnoinalization lemma to have its semantics defined by a metalanguage (which itself might be corruptible).
To give an immediate answer, yes philosophers have traversed the subject. Epicurus was the first to pose the challenge of non existence after death, Thomas nagel more recently discussed the subject, that's a good place to start.
Instead of looking at temporal statements as playing out in a time-indexed classical logic, it is more realistic to look at temporal logic as a variety of (time-indexed) modal logic.

I disagree that your statement most naturally breaks down to "If it is tomorrow, it is raining."  There is no world in which such statements have any purpose.  What is missing is not logical segmentation of this sort.  What is missing is the mood or modality in which you are putting forward the otherwise meaningless statement.

One simply cannot meaningfully say "Tomorrow, it will rain", where the will is an expression only of the future tense of 'is'.  You cannot ever know this fact, so it is not a realistic approach to the use of language.  Instead, here, 'will' is a modal verb, the future tense, not directly of 'is', but of 'must', the complement of 'can', indicating your prediction or belief, and not an indicative connector.  You mean that for some refinement of 'can', it cannot fail to rain.

But in that translation, 'some refinement' is very important.  "If the laws of physics are correct, it cannot fail to rain tomorrow" is very far from "If the wind does not shift unexpectedly, it cannot fail to rain tomorrow." or "If the way my knee aches right now is just the right way that faithfully represents a given barometric trend, and I have correctly assessed the degree of the pain, it cannot fail to rain tomorrow."  Or even "(Without external basis) I believe that it will rain tomorrow."

There is an intermediate position between modal and ordinary logic that considers all modal propositions vacuously true.  But only until enough context is supplied.  One has enough context once the premises are supplemented enough to express the mode asserted at least well enough to give a notion of probability to the statement.

For some notion of 'can' -- in which you personally simply cannot every be wrong -- it is surely true that whatever you say, including 'It will rain tomorrow' is true.  That world accords with the non-modal world where the statement is vacuously true.  But if you live in that world you are megalomaniacally psychotic.  Instead, all of us are supposed to guess by context the particular refinement of the meaning of 'can' involved in making sense of your statement.

That refinement is made up of a bunch of premises, the likelihood of each of which we can assess.
Your idea that religion is a reaction to fear is echoed in Marx's famous "Religion is the Opium of the masses". Marx didn't reference fear of death in particular, but suffering and pain in general, which presumably includes fear of death. The idea is that life is painful, and people use religion as an anesthetic, the way they use drugs to forget physical pain.

Freud later develops ideas very similar to yours in his writings on religion, especially in his book "The Future of an Illusion". He states that religion was created as a form of wish fulfillment. People invented religion because they were desperate to believe that their soul was immortal and was going to live on after their death. Another reason Freud mentions, related to the one you mentioned, was that people created gods as a longing for father figures. As adults people subconsciously longed for the protection provided for them as children by their parents, but their parents were dead (or dying soon), so they invented gods as transcendent and eternal father figures who they felt would always be their for them. 

More generally Freud saw religion as a defense mechanism agains the sheer randomness and cruelty of life, a way people coped with the fact that nature was far more powerful than they were.

Ernest Becker takes this a step further, and claims in his 1974 "Denial of Death", that most of human civilization, not just religion, is one elaborate psychological response to the fear of death. 
Regarding the actual judgement and its philosophical background

The constitutional court in fact had three main points in its justification of the judgement:


Because you can never now if your actions provide the desired outcome, i.e. if the alternative would really have been worse. You produce facts and exclude the possibility of probably better alternatives by this. Do you know that the torture will make him speak? Do you know it does help anything because the victim is not already dead? Do you know that the men and women on this plane will not decide themselves for sacrificing their lives or can overcome the terrorists? No, you do not. You only claim to have this knowledge you cannot possibly have. 


=> Kantian point against consequentialistic reasoning, e.g. in his On a supposed right to lie from philanthropy


Because you would help them show that you are in no sense morally superior, giving up your values you pretend to hold higher than anything else if it fits you. Therefore it would help the cause of the terrorists. 


=> I am not aware of any particularly philosophical writing that adresses this argument, but it seems good.


Legally, there is no alternative! The highest value of German constitution (Grundgesetz) is human dignity (article 1, sentence 1), not human life (article 2, sentence 2). Torture as well as weighing up lives (plane example) does not take human dignity as absolute value for granted, it rejects it. The German constitution (alongside with the Universal Declaration of Human Rights and the Charter of Fundamental Rights of the European Union, btw!) forbids calculus like this. 


=> This also corresponds with the strong kantian tradition in Germany, because the human dignity as inviolable, absolute value is a kantian concept  first expressed in his Groundwork of the Metaphysics of Morals (1785), Ak. 435. The very first formulation of dignity in this sense is by Samuel von Pufendorf more than 100 years earlier in De iure naturae et gentium libri octo (1672) as a reaction to Hobbes' Leviathan, so that v. Pufendorf can be considered the ancestor of dignity and public international law [Völkerrecht] in the modern sense.

Regarding the ethical principles and consequences

So the philosophical point behind this reasoning (and the three probably most influential formulations of human rights of the last 70 years) is that while other fundamental/human/basic rights (whatever they are called) can be relativised, the one that constitutes humanity as such, human dignity, must not. And this is in the end a thought that emerged in the era of Enlightenment, embodied by kantian moral philosophy.

Yes, there may be consequences that some individuals hold to be unintentional and undiserable. It will have negative consequences for the welfare of individuals that seem injust. But this is the framework of values we have given ourselves and made them constitutional for our society exactly because it is thought (with Kant) to be the best for human society as a whole. In fact, injustice will be done either way, because the causing acts are injust. This is why we need law (this is the standard thought e.g. of Kant, Fichte and Hegel in their philosophy of law).

Books on that topic

Regarding Kant's own reasons for and conceptions of human dignity:
Sensen, Oliver: Kant on Human Dignity

Regarding the kantian understanding of human rights and the relations between these frameworks:
Follesdal, Andreas and Maliks, Reidar (eds.): Kantian Theory and Human Rights

Appendix regarding the German constitution in particular

The only way to change this in Germany is making a new constitution, because article 1 and 20 cannot be changed and are eternal as long as this constitution is in effect (Article 79, sentence 3 Grundgesetz). This includes the human dignity as highest, absolute (i.e. not to be relativised) value (article 1) and the main rules for the government: Federal structure, rule of law, democracy, representative government, sociality and the right to resist against anyone who actively tries to change any of these (article 20). Perhaps you can now conceive how important it was for the men that formulated the Grundgesetz not to open dignity for ethical calculus, especially after the cruelties and terror of WWII that showed how important the absoluteness of dignity is.

If you are interested to learn more about the structure and values of the German Grundgesetz (basically, the first 17 articles represent the valuation of basic rights according to their order), there is an official translation available (also in PDF) http://www.gesetze-im-internet.de/englisch_gg/ here.
Let R(d) be the proposition "I ride/rode on day d", and D(d) the same for driving.  Then the statement "I ride or drive every day" can be expressed as ∀ d : R(d) ∨ D(d).  Negating this just gives ¬(∀ d : R(d) ∨ D(d)).

Language does not always give you a convenient form in which to state your logical formulas.  In English, the preferred form would be "It is not the case that I ride or drive every day."  (You need all of "It is ... the case that" to make it clear that the "not" is modifying everything else you're saying--it serves, by virtue of length and custom, the same role as the parentheses.  Note that there are not very many levels of parentheses that you can effectively convey in English prose this way--this is one reason why people use mathematical notation for logic.  But here, it suffices.  Merely, "I do not ride or drive every day" may also work, but it could also be parsed as "I do not ride every day and I do not drive every day", which is a different statement.)

However, since you are trying to express a logical formula, you can always use identities to transform your logical statement into a form whose English (or other language) representation feels more natural.  For instance, using the identity ¬∀x p(x) iff ∃x ¬p(x), an equivalent statement is ∃ d : ¬(R(d) ∨ D(d)).  The English version of this is "Some days I neither ride nor drive."  (The existential mapping to "some", and the negation of an "or" mapping to the "neither/nor" structure.)

Note that in English, the implication of "some days" is that there are other days that are different. If you said "some days it rains" but it actually rains every day, it would be technically correct but the listener would feel justifiably misled.  Almost every phrasing carries some impliciation like this, though: "there has been a day where I neither rode nor drove" suggests such days are rare; "it is not the case that I always ride or drive" suggests that riding or driving is the norm; "sometimes I avoid driving and avoid riding" again suggests rarity; "usually I drive or ride" suggests that sometimes (rarely) you do one but occasionally none; "usually I avoid riding and avoid driving" suggests that you sometimes must do at least one but usually don't.  Language is rich with implicature, and if you wish to avoid this, you generally must stick with logical formalism.
After the edit, here's the Updated Answer:

Film making itself isn't unethical, there's a clear distinction between what's a truth(fact)/lie from what's ficticious.


  A lie is defined by A false statement deliberately presented as being true http://www.thefreedictionary.com/lie (Dictionary Entry)
  
  A film or a book, or in other words, a story is An account or recital of an event or a series of events, either true or fictitious http://www.thefreedictionary.com/story (Dictionary Entry)




Every story has to define it's scope/universe. It could be a complete new universe (such as Star Wars) or a spin-off from reality (such as How to Get Away with Murder). Neither events are true, but also neither are passed on as true. Both are ficticious. It doesn't matter what the story is, as long as it doesn't try to pass it on as a fact.



Since ficticious stories don't try to pass as facts, or truths, they cannot be lies. Due to that, film making, writing and other forms of storytelling cannot be unethical in that aspect



However, this case is an exception. While I've personally not seen the movie in question, I've made a quick research on it, and found out similarities with 'Paranormal Activity', which I am much more at ease discussing. Interestingly enough I tried to look up a bit of information regarding Paranormal Activity's marketing campaing and I found an http://www.ew.com/article/2009/10/07/paranormal-activity-marketing-campaign article that draws paralels between 'Blair witch' and 'Paranormal Activity'.

According to the article (emphasis mine):


  Most obviously, Blair Witch was one of the first films to exploit the viral power of the Web to stir up word-of-mouth. What made the campaign brilliant, however, is the way that it took full advantage of the murky/underground/conspiracy-theory side of the Internet to imply that the movie was “real.”


That's the key. If we assume that "implying", or rather, not flatout lying, but intentionally misdirect people into believing in something that is a lie. According to the 'lie' dictionary page,


  To present false information with the intention of deceiving.


That pretty much sums up the whole ordeal.



Conclusions to be taken

Fictional Works are not true. Fictional Works do not try to pass on as true. Since lying is considered unethical, and Fictional Works do not lie (as they don't attempt to be passed on as truths), then Fictional Works cannot be unethical for not being true (Note that they could be unethical for other motives).

The Blair Witch Project as a work of fiction is therefore not unethical for not being true.

The same cannot be said about it's marketing campaign. Due to being a work of fiction, 'The Blair Witch Project' is not true (while it may be based on some truths, it isn't 100% real). As such, it would be unethical to try to pass it on as something that is true. Since apparantly the Marketing Team who campaigned this movie did attempt to pass the movie as being akin to a Documentary, rather than a work of fiction, then they were attempting to intentionally deceive the target audience, and as such, (assuming lying is unethical)  the campaign was unethical
From an existential point of view: Death is the knowledge that our life will end. 

After some time it will be for the generations then living, as if we had never existed. And after 5 billion of years, when the sun expires, all life on earth and all memory and documentation will vanish as if it did never exist. 
ad 1) On the base of human rights there is no justification for the suffering of people. In addition, history has shown that the promise of a better future can be a dangerous delusion and invites to misuse.

ad 2) We cannot be certain: History has shown that the ideal Marxist state is a theoretical fiction.
What you describe is called in theology and philosophy of religion "The problem of evil" and has been discussed by many theologians. A counter argument to the problem of evil is called a https://en.wikipedia.org/wiki/Theodicy Theodicy. There have been many notable theodicies throughout history. A notable historical theodicy was presented by Augustine of Hippo (St Augustine), in his works "Confessions", "The Enchiridion" and "City of God". 

St Augustine's theodicy can be broken down into two parts: 


Men have freewill, and it is their actions that cause evil, not God. 
Evil doesn't exist independently, evil is only the absence of good. 


From the Enchiridion: 


  What is Called Evil in the Universe is But the Absence of Good. -- Enchiridion, Chapter 11.  


Alvin Plantinga presented a modern version of the Augustinian theodicy, which he called the free will defense (Plantinga, Alvin. 1977. God, Freedom, and Evil. Grand Rapids, MI: Eerdmans.). It can be summarized in the following way: 


People who are compelled to do only good do not have freewill. 
Actions are not considered moral or good unless people have the freedom to behave otherwise. 
For there to be good in the world, freewill is necessary. 
In a world where freewill is possible, evil is possible. 
Therefore a world where evil is possible is better than a world were evil is impossible. 
Evil occurred because humans were free to commit it, and freewill was necessary for a better world, not because God willed it. 

There's two different things going in your question.


Should there be "exceptions" to the classical laws of logic (identity, excluded middle, and non-contradiction"?
How should we treat conditionals with false antecedents? (i.e. is the classic solution of considering them True problematic and if so what should be done instead).


To answer your question, I'm going to first give a brief overview of what the law of the excluded middle is (in part in order to say what it is not).

The principle of the excluded middle is an axiom of certain forms of logic. This has its origin in the West at least with Aristotle as one of his first principles (http://plato.stanford.edu/entries/contradiction/ http://plato.stanford.edu/entries/contradiction/).

This joined with a principle of identity and a principle of non-contradiction give us a very simple system for logic that is largely effective. But these principles also by their very nature limit its applicability.

In other words, it's a tool that's part of a tool kit, and it's one we don't use constantly in our lives at every moment. Instead, it's useful when trying to resolve certain problems in a logical way.



How to treat conditionals with false antecedents is one of the places that shows a limitation with logic built around these three laws. I don't think it's best to think of it as requiring an "exception." But rather showing a potential limitation of logic built on three laws.

Logicians have suggested several solutions to the problems raised by the three law logic of Aristotle. One solution is to have a NULL value for things that do not compute. Another solution would be to refuse to translate English language conditionals for which if the antecedent is false, the value is undefined in this way. A third solution is to use different operators along with the three laws to avoid this problem. 

In other words, you can either say that three laws are inadequate to conditional sentences or you can say that we need operators other than conditionals and must take great care in our translations of sentences into propositional forms.



Does this require there to be "exceptions" to the law of the excluded middle. On my view, the answer is no. It requires us instead to understand that the logic built around the three laws is a tool, which is very helpful when dealing with the right sorts of problems and very unhelpful when dealing with the wrong sorts of problems.

For instance, per the law of the excluded middle, the sentence "it is raining" (or more accurately the proposition behind the sentence) must be either true or false. But what if the reality is that it's "sleeting" or that there's some other form of wet precipitation happening or there's a mixture of rain and something else. Since the law of the excluded middle requires us to go two-valued, we have to make a choice between whether this is true or false.

Again, there's more than one way to fix it and make it more applicable. We can add many fine-grained distinctions to our definitions (narrowing or broadening the identity of "raining" vis-a-vis normal language) or we can refuse to try to fit that reality into propositional form.

Much of the reasoning we do is loosely bound by the three laws rather than strictly bound by the three laws. I take this to be the case, because it's a very clear way of making deductions and drawing conclusions. 

But I think if we start making "exceptions", then we're breaking the tool where it works. Instead, we need to keep our heads on our shoulders and know when to apply this sort of logic strictly (for instance in a large amount of programming and math), when to apply it loosely, and when not to apply it all (arguments with my wife, perhaps?).
If X is a sufficient cause of Y, then X will show a correlation with Y.  However, the opposite is not always true.  A correlation can be a general trend with a few exceptions, but a sufficient cause must be true in all cases.  Your example demonstrates why.
It's an interesting question to apply the Categorical Imperative and Kant's moral philosophy to the case of slavery and specifically as to whether one should manumit one's slave.

There are several difficulties in how you formulate the question that also need to be addressed:


  ... I trying to act according to the categorical imperative.


While I understand what you mean by using this construction, there's something inadequate about this wording if we're going to be true to Kant and his idea of the enlightenment. The main potentially problematic phrase is trying to act according to. 

There's two issues, first, this seems to present the Categorical Imperative as something outside yourself. For Kant, this is the definition of "heteronomy" an issue he addresses indirectly at one point in the Groundwork (the shopkeeper example) and directly in the "autonomy" formulation of the CI.

The second issue is that this presents this as "trying." This might be okay depending on what exactly is meant, but for Kant, there is no standard of trying, there's do and don't. A basic reason is that for Kant, the Categorical Imperative is nothing other than what pure reason dictates in the moral realm (this is stated in the preface to the Groundwork).


  ... I am considering the action of releasing this slave so that he is happier.


There's a big problem here in a place you might not expect -- "so that he is happier." In the Groundwork, Kant distinguishes between hypothetical imperatives and the Categorical Imperative (he's confident there's just one and it's unified and its complete based on pure reason). "happiness" is, http://myweb.ecu.edu/mccartyr/GW/hypotheticalimperatives.asp according to Kant, an end everyone has but that remains hypothetical.

A second thorny issue also occurs in passing. For Kant, the form that an individual's will takes is maxims which it imposes. When these maxims accord with universal reason (either reason itself or reason in the form of laws of nature applied to the world), then they are moral and agree with the categorical imperative.

But as indicated, the maxim "I will make someone happy" cannot be perfectly identical with the categorical imperative -- since happiness is not for Kant a moral end per se (things get very complicated when you add Religion within the bounds of reason alone and Metaphysics of Morals into the mix on this question).

In terms of slavery, Kant does mention it briefly in some places, namely Metaphysics of Morals 6:330. Elsewhere, there's some work on his view of https://www.academia.edu/1802951/Kants_Racism racial slavery. I must admit I'm not particularly up to speed on those particular references -- but since your question is about the CI we can skip many of them.

The way you formulate your question, you're operating from what is called the "universalization formula" of the categorical imperative. This has two specific forms -- logical contradiction and contradiction in a world where this is a law of nature. "world" for Kant is a technical term referring to something we make with our minds.

In Critique of Practical Reason, Kant says lying is wrong because we owe the truth to others. This can also be formulated in terms of universal law -- viz., that if we lie, then the purpose of communication disappears since we cannot discern anything from the gibberish that we are hearing. Admittedly, this isn't a pure logical contradiction (and several articles have critiqued him on this front). But it's hard to imagine lying being effective in a world if there was a rule that people never told the truth.

The same sort of universalization can apply to slavery in a world, but it might be a stretch for us to believe this works. This points to three things -- one that what we can conceive of as being possible might different from the Prussian Kant. Second, Kant tells us that there are other formulas of the categorical imperative -- some of which are going to get us much clearer accounts of why slavery is wrong. Three, that the equivalency claim in the previous sentence is somewhat dubious (an angle to critique Kant on).

The easier place to get the wrongness of slavery is "The Formula of Humanity." Here, Kant again gives two articulations (I don't have the page numbers with me), but they are: treat rational beings as ends-in-themselves and never merely as means and treat humanity in yourself and other as ends-in-themselves and never merely as means. (I argue in an unpublished paper that we shouldn't put so much stock in his use of "humanity" since he clarifies elsewhere this doesn't have much to do with being homo sapiens and just means rational beings again).

Slavery is treating someone as a means to your ends. In so doing, it fails to respect the rationality they have.Thus, it is wrong. 

Lying can also be shown to be wrong on the same logic -- as used Critique of Practical Reason: that when you lie, you are misleading a rational being and using them for your own ends. (N.b., that in Metaphysics of Morals, Kant changes the argument against lying to one where the problem is that you are not respecting rationality in yourself by engaging in this irrational behavior).

As I mentioned above, Kant claims that the Formula of Universalizability and the Formula of Humanity are identically the CI -- along with a third formula about the Kingdom of Ends.

Regarding manumission, Kant has an ought implies can view. Meaning, if we can identify what is the right thing to do, we should do it -- practical difficulties be damned.
Here is Cantor in his own words (from his influential 1887 letter to Weierstrass):" I begin from the supposition of a linear magnitude ζ which is so small that its product by n , ζ · n, for every finite whole number n however great is smaller than unity, and then prove, from the concept of a linear magnitude and with the help of certain propositions from transfinite number theory, that then ζ · ν is less than every finite magnitude however small, where ν is an arbitrarily great transfinite ordinal number (i.e., cardinal or type of a well-ordered set) from any arbitrarily high number class. But this means that ζ cannot be made finite by any actually infinite multiplication of any power, and hence surely cannot be an element of finite magnitudes. But then the supposition made contradicts the concept of a linear magnitude, which is such that every linear magnitude must be thought  of as an integrated part of other ones, and in particular of finite ones... Hence, the so-called Archimedean Axiom is not an axiom at all but a proposition which follows with logical necessity from the concept of linear magnitude". See a modern reconstruction of the argument, and how it fails for infinitesimals of the non-standard analysis, in http://link.springer.com/article/10.1023%2FA%3A1021204522829 Moore's Cantorian Argument Against Infinitesimals.

Peano in 1892, and Russell in 1903 gave their variations on the theme, but according to Moore "in none of its incarnations is the argument particularly easy to follow, and though there are resemblances among the three versions it is not even clear that they are in fact versions of a single argument". In the passage Cantor's issue seems to be that the infinitesimals, traditionally intuited as the "inverses" of infinities, can not be the "inverses" of his transfinite numbers, which in the time honored tradition he saw as the only true infinities. In other words, one can not produce a finite magnitude out of infinitesimals, even concatenated transfinitely many times. As Moore shows this is essentially because Cantor's ordinals only allow well-ordered concatenations. It is hard to argue with Cantor's "concept of a linear magnitude" excluding infenitesimals, just as it is with Kant's "pure intuition of space" excluding multiple parallels. With concepts like that, to each their own. From the modern point of view, Cantor is conflating cardinality with measure, but then the modern concept of magnitude isn't Cantor's, just as the modern concept of geometry isn't Kant's. At the time axiomatic method and measure theory were still in the womb, http://citeseerx.ist.psu.edu/viewdoc/download?rep=rep1&type=pdf&doi=10.1.1.116.8707 du Bois-Reymond's non-Archimedean speculations weren't up to Weierstrass's standards, and Weierstrass's analysis had no use for infinitesimals.

In addition, infinitesimals had a bad reputation among philosophers since (even before) Berkeley's Analyst declared them "ghosts of departed quantities", and Cantor did not want them mixed up with his transfinite numbers, acceptance of which as respectable mathematical entities he was advocating at the time. Contra the scholastic concept of numbers derived from Aristotle and supported by arguments like "annihilation of a number", see https://philosophy.stackexchange.com/questions/24640/how-does-infinity-of-numbers-and-of-space-work/24644#24644 How does infinity of numbers and of space work? The irony is that, as Dauben writes, "Cantor condemned this kind of argument... on the grounds that it was fallacious to assume that infinite numbers must exhibit the same arithmetic characteristics as did finite numbers". Yet he indulged in the same sort of reasoning when it came to infinitesimal magnitudes.
Informal fallacies have fuzzy boundaries.  This seems to me to be a variation on "https://en.wikipedia.org/wiki/Cherry_picking cherry picking," which is basing a general conclusion on a few carefully selected data points (which may not reflect the larger trends).
(s → p) → r, ~r, (p → q) → (t → r) : ~t


{1}      1.   (s → p) → r                      Prem.
{2}      2.   ~r                               Prem.
{3}      3.   (p → q) → (t → r)                Prem.
{1,2}    4.   ~(s → p)                         1,2 MT
{5}      5.   p                                Assum.
{6}      6.   s                                Assum.
{5,6}    7.   p & s                            5,6 &I
{5,6}    8.   p                                7 &E
{5}      9.   s → p                            6,8 CP
{1,2,5}  10.  ~(s → p) & (s → p)               4,9 &I
{1,2}    11.  ~p                               5,10 RAA
{12}     12.  ~q                               Assum.
{5,12}   13.  p & ~q                           5,12 &I
{5,12}   14.  p                                13 &E 
{5}      15.  ~q → p                           12,14 CP
{1,2,5}  16.  ~~q                              11,15 MT
{1,2,5}  17.  q                                16 DNE
{1,2}    18.  p → q                            5,17 CP
{1,2,3}  19.  t → r                            3,18 MP
{1,2,3}  20.  ~t                               2,19 MT


Abbreviations:


&I = & Introduction
&E = & Elimination
DNE = Double negative elimination
MP = Modus ponens
MT = Modus tollens
CP = → Introduction
RAA = Contradiction

Foucault's own thoughts for or against things that he writes about, are often difficult to unravel. His first goal in writing is to allow us as readers to see something clearly which at first is not particularly clear. For example with the recent publication of his lectures, particularly, "Society Must be Defended", "Security, Territory, Population", "The Government of Self and Others" and "On the Government of the Living" one sees with great detail his concerns with the material developments and evolutions in the exercise of and contest of power, not just at the level of the State, but as diffused throughout our societies. Where his opinions really come to fore are in interviews, especially when being pressed for his perspective. So there's an answer to your question, but I wouldn't advocate looking for it in his books. 

Insofar as power is never one-sided in his perspective, it applies to the dominated as much as to a dominator. In each field of genealogy or archeology of statements and events that he goes into, there's never much of an reason to say "yes this was good" "that was bad". That type of evaluation becomes irrelevant to his overall objective of letting you know the origins of where you are today. 

So he engages, Plato, Artistotle, Cicero, Polibyus etc on their views on democracy, republic compromises etc, but it is to explore both the context of the relations of power in their epochs but also the types of knowledge they developed to deal with those concrete situations. So with everything he approached, there are situations, and how subjects deal with those situations. Its a massive project for the articulation of spaces of freedom. 

So would he advocate democracy? He would ask what one means by that term. As he himself states in an interview, "you can’t provide a definitive formula for the optimal exercise of power." Part of his perspective is that it's not a question simply for him to decide. Power-relations are negotiated in the moment, in society, BY society, not just any singular individual, and always in our every-day present. What is, or what becomes, are the results of "relations of force" a la Nietzsche.

It's a somewhat vague response that never satisfied many of his peers and critics while he was alive. But it's something he doesn't veer from. He sees a great danger in trying to prescribe for the world particular a priori formuli and spent the entirety of his philosophical career fighting against such a priori's. His ultimate goal is freedom: 

In an interview from Nov. 1980: 


  We have to rise up against all forms of power—but not just power in
  the narrow sense of the word, referring to the power of a government
  or of one social group over another: these are only a few particular
  instances of power.
  
  Power is anything that tends to render immobile and untouchable those
  things that are offered to us as real, as true, as good.


What form that freedom takes is a matter of what we're capable of, not any preconceived notions. Maybe it's somewhat ironic, but Foucault was a philosopher that spends his career digging through the past, but was not in the slighted bit interested in living in it. 
Let me first state Zeno's Arrow Paradox more clearly:


Time is composed only of instants.
At any single instant, an apparently moving arrow doesn't travel any distance, i.e. the arrow is at rest during every instant.
That means that the arrow is at rest for the entire time period.
Therefore, the arrow cannot be moving at all.


I think your explanation is on the right track.

Possible solutions

One approach to resolving the paradox is this: it's false that the arrow is at rest during every instant of time, since motion is not something that occurs during a single instant of time. If we calculated the speed during an instant it would come out 0/0, and so Zeno is not justified in calling that rest anymore than calling it motion.

Along these lines we could look to calculus to get a precise notion of speed at an instant so that it doesn't come out 0/0. In this case we will find out that the arrow has a certain speed at every instant of time.

There are other possible approaches. We could deny that time is composed only of instants or even claim that instants have a certain finite duration. If true, these also show that the argument is not sound.

Paradoxes in general

One final comment regarding your last sentence. That something is called a paradox does not mean that it has no solutions. A paradox is something that leads to contradiction from apparently true assumptions. Often these assumptions are much debated and reasonably shown to be false. Yet we still call these things paradoxes since the assumptions retain their apparent truth.

Read more http://plato.stanford.edu/entries/paradox-zeno/ here and http://www.iep.utm.edu/zeno-par/ here.
Not all of my answers are sourced, some are based on my general reading of ethics courses, they are still accurate.


  I want to prove it is immoral.


You can never prove that it is immoral. Immorality is only relative to the ethical system chosen. You can however argue, that even for a consequentialist, slavery can still be immoral. 


  What more could I say?


Different possibilities:


You can't say anything: A hardened consequentialist might simply bite the proverbial bullet and say that, yes indeed, a small amount of slave labor is justified for the greater good.  This for example http://www.ushistory.org/us/27f.asp was one of the justifications that pro-slavery "intellectuals" in the American South advanced in the lead up to the American Civil war. 
You can argue that in theory a consequentialist should accept a small amount of slavery if it benefits the greater good, but that in practice it is impossible to perform the necessary utility calculus that allows her/him to determine that the greater good achieved by the building is indeed higher than the harm inflicted on the slaves. Similarly, using probabilistic reasoning, you can argue that the harm coming from slavery is predictable and certain, while the benefit coming from the building is uncertain and liable to change (if for example a fire or an earthquake destroys the building right after it is built). Once you factor such probabilities/risk analysis into your utilities calculus, then slavery becomes an immoral choice. This what an Act Utilitarian would do (http://www.iep.utm.edu/util-a-r/#H2 See IEP article on Utilitarianism, Act Utilitarianism vs Rule Utilitarianism) 
You can argue that the harm of slavery is incomparable to the harm caused by the lack of such a building, or that even if it was numerically comparable, it so much greater that it counts as infinitely greater harm, and therefore can never be justified. See Alastair Norcross, http://www.colorado.edu/philosophy/heathwood/6100/Norcross%20-%20Comparing%20Harms--Headaches%20and%20Human%20Lives.pdf “Comparing Harms: Headaches and Human Lives”, Philosophy and Public Affairs, 1997. Sections I and II.  
You can argue that the long term harm of slavery due to its detrimental effects on society far outweigh any benefit coming out of the building. For example in a society where slavery is permitted, those who are enslaved or who fear enslavement might resort to violence and terrorism to avoid enslavement. Such a society would be living in constant fear of the violence that might erupt because of slavery, and this cancels any beneficial effects that cheap and efficient slave labor might provide. This would be a variation on Rule Utilitarianism, http://www.iep.utm.edu/util-a-r/#H2 again see IEP.   
You can argue along the lines of G.E. Moore's http://www.iep.utm.edu/moore/#SH3c ideal utilitarianism("Principia Ethica", 1903), that although a small amount of slave labor does allow us to achieve a greater good from constructing the building, there is a scenario of even greater good, where the same outcome is achieved without using slave labor. Per Moore, if such a scenario is possible, then using slave labor to complete the building is immoral. You can then argue that there will always be situations where the outcome is achievable without resorting to slave labor, hence the scenario you describe is immoral.  

Several perspectives can be offered (as a complement to Cort Ammon's answer): 

Kantian perspective

It is possible to examine the question from the perspective of Kant's categorical imperative: 


  "Act only according to that maxim whereby you can, at the same time, will that it should become a universal law." - Kant, Immanuel (1993) [1785]. Grounding for the Metaphysics of Morals. Translated by Ellington, James W.. 


So what happens if cheating becomes universal - that is if everyone starts sleeping with whomever they wanted to, regardless of relationship status? The concept of romantic relationship would loose its meaning altogether. One could then argue that given how much of our society is built around courtship and relationship status, this would be an overall bad thing. 

Consequentialist/Utilitarian perspective

John Stuart Mill says in "Utilitarianism": 


  It is quite compatible with the principle of utility to recognise the fact, that some kinds of pleasure are more desirable and more valuable than others. It would be absurd that while, in estimating all other things, quality is considered as well as quantity, the estimation of pleasures should be supposed to depend on quantity alone.


Per this principle, sexual pleasure/connection, although desirable, is not as valuable as friendship at the level of ideas and emotional connection. Then, based on utility, one could argue that sexual relations might interfere with real friendships, and so friendship should be prioritized over sex whenever the two compete.    

Evolutionary perspective

One could argue that there is nothing morally wrong about "cheating" but that fidelity makes sense as an evolutionary strategy. According to Strategic Pluralism Theory for example, it makes sense in many environments for people to restrict themselves to one partner and focus on maximizing the care provided to the offspring of that relationship. In other environments, other strategies might used. See http://www.ncbi.nlm.nih.gov/pubmed/11301543 Gangestad, S. W.; Simpson, J. A. (2000). "The evolution of human mating: Trade-offs and strategic pluralism". Behavioral and Brain Sciences 23: 573–587.

Religious/Social perspective

I can't provide a source for this. It was an answer provided by my high school religious studies teacher. The Abrahamic religions had very strong rules against promiscuity because of the need to preserve family lineage. In societies were family and clan relationships were very important, people had to be sure that siblings and cousins were indeed who they claimed they are. At a time when contraception or DNA testing weren't available, the only way to insure the "purity" of family ties was by restricting the sexual partners that people could have.   
To the more general problem, there are cases where people all face the same (or very similar) incentives to not act for the general good, and this leads to bad behavior. See the https://en.wikipedia.org/wiki/Tragedy_of_the_commons Tragedy of the Commons for different ways of thinking about this problem.

There are three problems with the particular problem of voting:

The marginal utility of voting/not-voting in your argument is implicitly uniform. Instead, consider what happens as there are fewer people vote: the marginal benefit of voting is actually quite high (one individual vote is more likely to have an impact on the outcoming). So, as fewer people vote, there is increased incentive for more people to vote - this is the opposite of what happens in the tragedy of the commons, where as more people fail to act for the common good, incentives increase to act against the common good.

The argument also assumes that the utility is uniform across members of society, but the utility of voting that we observe is actually very different across people (eg: people who are retired are more likely to vote, people with strong affiliation to a particular candidate or party are more likely to vote, etc.). 

Voting is also (mostly) independent - the decision of one person to vote seems to have a small impact on whether others vote (at least, in comparison to the analogous action in the Tragedy of the Commons).

Taking these together, there are stronger arguments for the ethics of voting than "what-if-no-one-did-it" type arguments.
The unit of knowledge-that is proposition, expressed linguistically in declarative sentences, the unit of knowledge-how is skill. The use of "knowledge" here refers to non-propositional uses like "know how to ride a bike", which are often passed over in the traditional position, which Ryle called intellectualism when he introduced the distinction between https://plato.stanford.edu/entries/knowledge-how/ knowledge-that and knowledge-how in 1940s. A similar distinction between https://en.wikipedia.org/wiki/Tacit_knowledge tacit and explicit knowledge was introduced by Polanyi in 1958, he was inspired by Merleau Ponty.

Intellectualism reduces skills to propositions, namely sets of instructions on how to do it, methodology. But intuitively, skill fuses structure and action, it is implausible that one can spin action out of structure alone. In Shaw's Man and Superman one of the characters says "those who can, do; those who can not, teach". What those who "do" have is something beyond methodology, a coach may be able to teach an athlete how to perform a stunt without having the skill to perform it himself. One defense is recourse to tacit knowledge-that, the coach knows but he is unable to explain. But there are other problems. Lewis Carroll described a tortoise which was given p, p → q, and the description of how modus ponens works, but still failed to grasp the inference of q from p. Skill is not reducible to instructions, in fact applying instructions properly is in itself a skill. Ryle developed a regress argument against intellectualism based on iterating this, similar to Wittgenstein's rule-following regress. See https://www.academia.edu/371704/Knowledge-how_Linguistic_Intellectualism_and_Ryles_Return Löwenstein's Knowledge-how, Linguistic Intellectualism, and Ryle's Return for a critique of the more recent version of intellectualism defended by Stanley and Williamson. 

An impressive slew of 20th century philosophers, in addition to Ryle and Wittgenstein - Heidegger, Merleau-Ponty, Sellars, Quine, Davidson, Brandom (Kant and Hegel can be named as precursors) - concluded, coming from very different backgrounds, that it is pragmatics that underlies semantics, i.e. knowledge-how is more fundamental than knowledge-that. Perhaps the most ambitious and developed is Brandom's reduction of knowledge-that to knowledge-how:


  "While the meanings studied by semantics may not consist in the roles played by expressions in linguistic practice (meaning need not be identified with use), according to this view those roles must at least establish the connection between contents, meanings or semantic interpretants, on the one hand, and linguistic expressions on the other. The semantic pragmatist’s basic insight is that there is nothing apart from the use of expressions that could establish such connections".


Brandom's project in Making It Explicit is to present linguistic practice in non-semantic terms, and then reduce semantic categories to them, http://link.springer.com/article/10.1007/s11406-010-9289-y Szubka's On the Very Idea of Brandom’s Pragmatism is a very short summary of the project's motivation and challenges. Brandom takes advantage of Ryle's and Wittgenstein's insight that skill is not just an ability to act, it is an ability to perform a task, "up to standards", "according to rules". This does not imply some spelled out or even spellable standards, that would be like reducing skill to instructions, but communal judgement of a perfomance as adequate or not. "Knowing a rule is knowledge-how... performances come up to certain standards, or satisfy criteria", says Ryle, "there has to be a way to grasp a rule which is not an interpretation", says Wittgenstein. Language is a rule-governed, normative, activity. So semantics is to be reconstructed not from purely behaviorist or functionalist descriptions of practice, pragmatics must be normative. Specifically, he singles out one special type of linguistic practice as responsible for generation of semantics, where performances are assertions governed by (informal) rules of inference:


  "The game of giving and asking for reasons is not just one game among others one can play with language. It is the game in virtue of the playing of which what one has qualifies as language (or thought) at all. I am here disagreeing with Wittgenstein, when he claims that language has no downtown... This is a kind of linguistic rationalism. Rationalism in this sense does not entail intellectualism, the doctrine that every implicit mastery of a propriety of practice is ultimately to be explained by appeal to a prior explicit grasp of a principle. It is entirely compatible with the sort of pragmatism that sees things the other way around".


The mechanics of Brandom's reduction of semantic content to inferential roles is complicated, and involves scorekeeping of commitments and entitlements by fellow speakers, see simplified version in his Articulating Reasons, and discussion in http://ndpr.nd.edu/news/24555-reading-brandom-on-making-it-explicit Reading Brandom. Of course, pragmatist proposals do not have to be as rationalist (Heidegger and Merleau-Ponty emphasize "embodied-embedded" aspect of meaning), or as structured as Brandom's (late Wittgenstein was sceptical of any linguistic generalities). One can also moderate by granting certain autonomy to knowledge-that in advanced stages of linguistic practice. Davidson can be interpreted along such lines, but he does not use the terminology.
One problem here is that you are acting outside the structure everyone else relies upon to make decisions and create efficiency.  The fact this may be an unfortunately corrupt and inefficient mechanism to begin with does not prevent you from making it worse.

Even if your intention is good, the odds are that you are encouraging enough institutional waste to offset your net effect.  And it may be pointless.  After all, it may be perfectly possible to simply get the money given voluntarily by the corporation.  PR pays off in spades.

I would argue from the Agile principle that lack of transparency ultimately displaces energy from the task at hand into power-mongering.

If this is something that happens -- employees simply reallocate funds on purpose to what they prefer -- then money might go missing from any part of the enterprise for no reason at any time.  So the company cannot trust its control of where its money goes, and it has to pad allocations to many areas, lest they get stolen from at a bad time, causing delay or missed opportunities.

But then that is money that is sitting stagnant in reserve, in case something bad happens.  It cannot be stored and leveraged, because its use cannot be planned.  And it does not flow out into the economy.  So it does not create employment opportunity that might eventually propagate out to create positions for those refugees if they want to establish residence in their new host countries.

One of the main reasons companies are larger and more rigid than necessary  is to provide ballast meant to forestall risk.  By creating unseen and unpredictable risk, you are encouraging them to be larger and for more of the bulk to be waste.
To whom was the information necessary to avoid the ticket provided?  You.  So there is your answer.

Responsibility presumes power, she had no power over whether or not to register your car.  For her to do so would not have been legal.

It was irresponsible for you to give her permission to break the law.
Evolution works at the species level, not at the individual level. Altruism evolved because it gave certain groups advantages aver other groups competing for the same resources. If an individual sacrifices their own personal needs for the group so that the group as whole thrives, the end result is still the propagation of that individual's genetic information (as a subset of the group genetic information), and evolution is driven by genes.  

An extreme example of this are female worker ants and worker bees: These individuals do not mate at all, only the queens in their colonies/hives do, and yet they spend their whole lives dedicated to the well being of the group. 

See the http://plato.stanford.edu/entries/altruism-biological/ SEP article on altruism for further information.  
Death may be final because it can be defined to be as such.  One of the most powerful aspects of death is its finality.  Once something dies, it can not be brought back.

However, this is only part of the story.  How do we know something really died?  We used to declare something dead when it's heartbeat ceased, but modern medicine has since demonstrated that it can bring someone back from beyond that point.  The medical definition of dead is a nuanced definition that science is constantly refining.

Maybe it is the trickiness of these definitions that lead people to use phrases like "you will always live on in my heart."  Perhaps people seek a more refined concept to describe what they experience while they are alive.
In my experience, it is virtually impossible to argue that being truthful is indifferent to values.  If you choose to tell the truth in a situation where definite harm will result from your truthfulness, then preventing that harm is either a value, or it is not.

Choosing correctness over responsibility to protect others is a value judgement in-and-of-itself, and cannot be values-neutral.  Sissela Bok makes a thorough and convincing (if a bit boring) analysis of the balance point in http://rads.stackoverflow.com/amzn/click/0375705287 http://www.amazon.com/Lying-Moral-Choice-Public-Private/dp/0375705287.  But it is obvious that the methodology of balance already requires a value system.

I am a gay man who has worked professionally with people with Borderline Personality Disorder.  Consider two scenarios directly related to my real life:

'Outing' people who are married because one feels that telling the truth has value in-and-of-itself can be harmful to their children.  Waiting until it is safe to do so generally requires lying now and apologizing later.

Some 'borderline' people purposely do damage through the emotions of others, by choosing the right time to make them angry or to scare them into behavior that is unwise.  They commonly arrange to use truth as a tool.  (One way they do so is exposing truths to authority in a way and at a time that will cause harm, say precipitating depression and a return to drug use, or deepening isolation of a fragile family member who will consequently not get help when they need it.)  The easiest way to circumvent such tactics is to make it obvious up front that you know when to lie.

It is hard to imagine making the estimates of damage involved in ways that would not change as my values shifted.  Utility remains subjective and our deliberations are always incomplete.  (Thankfully, the moment is the moment, and self-forgiveness does not depend upon being right.)

I have argued elsewhere (https://philosophy.stackexchange.com/questions/34736/is-truth-telling-important-or-just-having-good-intentions-is-enough/34738#34738 Is truth telling important or just having good intentions is enough?) that truth is not the right standard for ethical judgment, but instead, respect for individuals is.  But what action expresses respect does change with your values.  Respect motivates protection if you value safety highly, and may motivate withdrawal of that same protection if you value autonomy and independence more highly.
A lot is hidden in the claim that it improves the overall utility assuming we are all equal.

There's several variants of Utilitarianism and these all fall within a larger family of consequentalist views (views that see the evaluation of actions in terms of some quantitative object and that see morality as about the maximization or minimization of this object). Many of these problems are inherited from Bentham and Mill's formulations of Utilitarianism.

The simplest type of these theories are called act utilitarianism. In such views, we evaluate individual actions to see whether they would maximize (hereafter I'll skip minimize) a desirable outcome. On such an analysis, it's conceivable that someone killing their own self could maximize utility, whether utility is understood as (a) pleasure, (b) people who are able to live freely, etc.

There are, however, severe problems with act utilitarianism that make it highly unattractive as a moral theory. First, there's a problem with outcomes -- namely, whether we can predict outcomes with such clarity and then whether we are responsible for maximizing actual outcomes or picking what appears maximal based on what we know. (E.g., presumably some doctor at some point vaccinated Pol Pot and this saved his life -- seemingly a good thing, but then consequences go screwy and he ruins a country murdering its best and brightest).

Second, there's the "drifter problem." A pure act utilitarian view with only an eye to optimization cannot consider anything else. Thus, if it the loss of your life or happiness would be less than the gains realizable by your death, it also follows that the same can be applied to a protesting subject who does not have the will to die to save others. (In other words, there's no room for a consent condition).

Within consequentialism, it's far more common to support a type of rule utilitarianism where we make calculations about what is beneficial away from the actions themselves. Thus, we can have laws against acts that are of doubtful consent even if we acknowledge that consent might be possible, because we calculate that the policing costs of distinguishing cases outweigh any good of allowing consent cases. To restate that more clearly, we can decide that it is not licit to take lives to save lives based on generalized calculations, acknowledge their might be exceptions, but include in our calculation that these too will be illicit due to the odds of consent being low).

So if you're a pure act utilitarian optimizing "survival" or "life" and you have no epistemic concerns about whether you can know outcomes, then it might be possible on your view to see donating your life to save others by dying as licit.

Bentham, for instance, draws no distinction between types of pleasure and doesn't have much talk about consent (from what I gather). Mill, on the other hand, incorporates a "harm principle" wherein in our maximization we cannot cause harm or violate the autonomy of others. Moreover, Mill distinguishes types of pleasures and doesn't see things as a pure optimization. In this way, he avoids the second objection. (His view is ambiguous about whether we are optimizing actual or expected consequences).
The conclusion is not "I am not guilty". That would be nonsense. 

And it's not an appeal to pity. It is a claim that the punishment would in this particular case be much harsher than for many other people. If I get a £50 ticket, it costs me £50. If this person gets a £50 ticket, they claim that it costs them £50, plus loss of use of the car. 

So that person is trying to convince the police officer that in his particular case the ticket together with its consequences would be an unacceptable harsh punishment, and therefore letting the guilty person get away with it. 

Not dishing out an overly harsh punishment is not "pity". Whether the argument is convincing is another matter. 
First of all, let's consider what an "appeal to pity" is: its an attempt to win support for one's argument by appealing to the sympathies of one's listeners. It's considered a logical fallacy because, of course, one's listener's sympathy should not be involved in the (logical) validity of the argument (i.e., one should be able to agree that an argument is valid, even if one doesn't find it persuasive). It's nevertheless a powerful and effective rhetorical tool.

Second, let's consider the "deductive argument" you adduce. While it's possible to impose such a structure on his remarks (mutatis mutandis), it doesn't get you to the right place. If Clarence Darrow were simply trying to establish the proposition "I trust farmers," he wouldn't be doing a very good job as a defence lawyer. After all, whether or not Clarence Darrow trusts farmers is entirely irrelevant to any conceivable defence. What he is trying to do is win farmers over to his side. The force of his remarks isn't to establish any particular proposition, but rather to create a connection ("he gets me!") intended to provoke them to be more sympathetic for the case he's trying to make. It is the goal or object of these statements that makes them an appeal to pity, regardless of whether or not one might discern some deductive argument within it.
You're reading the Knox translation. Nisbet (the more recent Cambridge translation) translates as follows


  It is impossible to break into the inner conviction of human beings; it is inviolable, and the moral will is therefore inaccessible. The worth of a human is measured by his inward actions, and hence the point of view of morality is that of freedom which has being for it self.


I think a good question here is why Hegel writes this as assertion. To decipher this, we need to look at a few things at once and consider these theories.

First, as an Addition, the remarks are meant as a comment on section 106 to expand on section 106 itself, which claims "subjectivity now constitutes the determinacy of the concept and is distinct from the concept as such..." 

The "now" hints that this is a moment in Hegel's dialectical logic.  What sort of moment? This is almost immediately after the end of the section abstract right (abstrakte Rechte). There, Hegel is looking at something roughly like our concept of rights but in abstraction from a system of law or society. Motivated by the problem Unrecht, this leads to "civil society" wherein rights are granted a social and legal existence that we will learn is incomplete.

Moving more broadly within Hegel, we can note that Hegel is referencing the "concept." The concept [der Begriff] is a term for that which Geist comprehends in all of its distinctions. Thus, a big part of what is going on in here is that in Moralitaet the concept is taking on subjective form, because as Hegel has stated earlier on freedom, reason, and will are for him identical. At the point of this passage, the case is not fully distinguished, but the bald manner in which the addition asserts it makes it seem like Hegel does it take it to be quintissential to the self to have complete control over will.

Further in the text, this subjectivity is joined to a more robust idea of community and freedom, but I'm not aware of anywhere in which Hegel denies the idea that human selves qua rational beings have control over their inner convictions (a position that sounds thoroughly Kantian and Cartesian to me).
Chomsky does, he has a book where he describes what he calls the 'manufacture of consent'.

It's implicit in Mills argument that one can have a legally enshrined freedom of speech, but that this doesn't get past the 'tyranny of opinion'.

Chomsky is describing how in the era of mass media this tyranny of opinion is manufactured; and how opinion that is nonetheless well-argued so not eccentric is marginalised; and by being marginalised is rendered invisible and impotent.

I'm not sure that eccentric in Mills time has the same meaning as it does now; perhaps diverse might be a better choice; so Mill is arguing that a diverse range of opinion shows implicitly that the tyranny of opinion doesn't hold sway tyrannically.

It might be worth pointing out that Arendt regards the arts as a barometer of the range of opinion.

It's also worth pointing out, I think, that some such tyranny has to hold away, as not all opinion is created alike; the question is its range, width or diversity.
I classify your example as ad hominem. Some other formulations similar to your example are:


An opinion about war held by one who has not served in the military or fought in combat is not legitimate.
Intellectual insight from one who is not college-educated is invalid.
An understanding of the rigors of pregnancy on the part of a man is erroneous (male medical doctors who practice obstetrics and gynecology would disagree).


This is a particular flavor of ad hominem that I have encountered often, and I sympathize with the urge to distinguish it from other fallacies. It certainly seems like a counterpart to the appeal to authority, a variation in which one does not vouch for an argument on the basis of alleged expertise, accomplishment or privilege but instead dismisses an argument on the basis of an alleged lack thereof. This illustrates where the confusion lies, and also identifies an important fact. Appeal to authority is an affirmation, while ad hominem is a negation. I view the two fallacies as counterparts, though I never encounter them characterized as such. They each shift emphasis and scrutiny away from an argument and toward an arguer. Your example, however, is properly termed ad hominem because it fallaciously negates an argument, and does not fallaciously affirm one.

Put more simply, a reversal of an appeal to authority would be an impugning of authority, which would be to say that one's position is invalid because one lacks the knowledge or experience to hold an informed opinion or put forth a cogent argument, which is to attack the arguer and evade the argument, which is ad hominem. Q.E.D.

I think it is more to the point to identify that ad hominem and appeal to authority are the same device applied in two different but complementary ways. Each approach says, essentially, "when evaluating this idea or opinion, consider the source (and only the source)."

One can appeal to authority (a privileged position or frame of reference) in many ways: an argument about patriotism must be true if it comes from a Republican; an argument about morality must be true if it comes from a Christian; an argument about economics must be true if it comes from a successful capitalist; an argument about law must be true if it comes from an attorney; an argument about climate change must be true if it comes from a scientist. If each of these is reversed, I find ad hominem attacks: an argument about patriotism must be false if it comes from a Democrat; an argument about morality must be false if it comes from an Atheist; an argument about economics must be false if it comes from a welfare recipient; an argument about law must be false if it comes from a criminal; an argument about climate change must be false if it comes from a layman. Therefore, while it is technically correct to term your example ad hominem I assert that your identification of your example as a reversal of or opposite to the appeal to authority is essentially correct, and that perhaps new organization and nomenclature, which would identify these fallacies as two sides of the same philosophical coin, is in order.
Yes, you may help someone self-actualize, even though you are not "fully" self-actualized. Further more, you may even do it without you being aware you are doing it.

  Does this mean that the most beauteous we can be when we are filled
  with alcohol or see the world through the lens of alcohol?


Funny. I would say your are accidentally falling into a trap by using today's modern terminology to try to understand a translation of an idiom from a long time ago. 

Wikipedia does a good job describing the original intended meaning, saying:


  In vino veritas is a Latin phrase that means "in wine, truth",
  suggesting a person under the influence of alcohol is more likely to
  speak their hidden thoughts and desires.
  https://en.wikipedia.org/wiki/In_vino_veritas Source


It is important to point this out because it is a VERY COMMON thing people do. For instance people do this with historical documents such as the constitution and accidentally (or purposely) twist the meaning and are misled. This underscore the criticality of reading something in context of the time it was written as intended by people of that time, with the context of what was known at that time, with any known biases the author may have had in the back of your head.

We have all made this mistake, so it's a good thing to have a chance to learn from it.

  It seems to me that a person of one gender would usually treat a
  person of a different gender differently (e.g. more respect or less
  respect, to name a quick example).


That qualifies as sexism, yes. So, if your assumption is correct that a person of one gender would usually treat a person of a different gender differently, that would imply that interactions between members of a different gender/sex are usually sexist.

However, this does not mean that all interactions between members of a different gender/sex are sexist, unless everyone always treats a person of a different gender differently from those of his/her own. I'm not convinced that is  accurate.

I believe it is most definitely possible to treat people of different genders in the same way, at least in most contexts.
Insanity has come to signify a psychological concept that is defined by social acceptance and scientific discovery. In this case, if ultimate reality equates to an individual's perceptual experience, then the only way to prove an ultimate reality (on any socially or scientifically acceptable level) is through corroboration.

There are still issues with how individual's interpret, and therefore corroborate, observable phenomenon. Consider a single event with 10 witnesses and their 10 interpretations of said event. Imagine a UFO sighting, for instance. There may be stories of angels or aliens, military activity or simple mass hysteria. So... was it real? These people can corroborate the occurrence of event, at least. But, society and science may still call them insane, depending on the circumstance.

From an individual's perspective, regardless of social acceptance, the question becomes about the nature of reality. If perceptual experience is the ultimate reality, then whatever one experiences IS real (even when it cannot be corroborated.) Barring any debates on the nature of existence, this concept can't easily be disregarded. For advanced as our science is, we should all still be aware of it's limitations; the same goes for our bodies and perception!

Humans perceive only a percentage of https://en.wikipedia.org/wiki/Visible_spectrum light and sound. So it could be argued that certain stimuli, such as a state of extreme fatigue, might simply allow one to perceive sensory information that is not observable in an otherwise healthy state.

Our generally minimal knowledge of the brain and human behavior is constantly changing our definitions of insanity... Consider the https://en.wikipedia.org/wiki/Female_hysteria hysteria fiasco. Likewise, our theories on reality cannot determine what is real through abstract concept alone... So, this question may well be, ultimately, unanswerable for now. :S
Simply put, no. Or maybe to reword that, doing so is non-trivial much in the way replacing the word "God" with "non existence of God" in Dawkins God Delusion would not produce an argument for God's existence. ( I assume this is so -- and assume why is that presumably his sentences and arguments hinge on "God" bearing non arbitrary signification and working with other ideas he's using). I take the key features to be those below

The basic mistake in what you're suggesting is that you're importing a modern picture of being and perfection into a classical and medieval argument.

On the classical picture, evil is a privation not a thing. (evil does not exist in things). 

A privation of what you might ask? A privation of being.

Goodness in turn is correlated to having being. Or to word that differently, anything is good to the extent that it has (in classical language "participates in") being. Thus, on this picture, a serial killer has some intrinsic good that he puts to evil use. Evil in this account is always parasitic on good, because only good properly exists (we can judge that some event that occurred or behavior is evil, and thus we consider something to be evil but the actual existing of things is good). 

Anselm's argument depends on God being the being of which no greater can be conceived. Such a being would be perfect in every way -- he tells. Joined to the definition of good such that to be is good. Consequently, Anselm's argument automatically incorporates the perfect being being good because being good is coterminal with having being at all. And if a being is perfect, then for Anselm it would also be perfect in goodness. 

In other words, his argument cannot be easily converted to "a perfectly evil God" because perfect and evil are contrary terms within his metaphysics. It's not until the modern era that we begin to have elocutions like "perfectly evil" in a way that we can mean "evil to the ultimate degree" or "epitome of evil."

One could argue that Anselm's argument is question-begging in that the classical terms are pretty well designed for this conclusion or alternately raise the modern objection that existence is not a predicate, but the objections you suggest for the most part wouldn't work because of the meanings in the classical system of "being" and "good."
"The sun rises every morning". That is a fact about reality. But it is a string of letters unless one understands what "sun", "rises", "every" and "morning" mean. What they mean has to be established and learned prior to producing the fact, and does not belong to reality alone, it depends on human categorizations of it, to which alternatives are possible. It is tempting to assume that reality is pre-categorized "in-itself", and this is the position of naive realism and its traditional philosophical expressions, e.g. Aristotle's. But this overlooks the more recent analysis of how categorizations work. Because we routinely model reality to understand things we instinctively try to model the relation between a model and reality on a relation between two models, one refining the other. But this is exactly the case where the trick can't work, the relation between any two of our models is of a completely different nature than between any of them and reality, and no amount of refining helps with that. 

At issue is the thorny http://www.iep.utm.edu/intentio problem of intentionality, how one thing (category, concept, idea, proposition) can "stand for", "represent", another. It can not be solved by representing the representing, as the naive approach attempts, that is circular. The only way we understand how it can work is when there is a "subject" (not necessarily live or sentient) that connects the two through some action, category is a "rule" for such action (if only for recognizing). Without a "subject" it is unclear what it even means that concepts or propositions "correspond" to reality (or anything "corresponds" to anything else). There is a similar deficiency with the meaning of "something about reality that is literally unknowable". Negation ("un") here can only be conceptual, yet it is used to negate conceptuality as such, the result is a "dry wetness".

One traditional solution, chosen by Plato and Aristotle, is to postulate that things of a kind with concepts (i.e. idealities) already exist out there. Since the middle ages this led to the intractable http://plato.stanford.edu/entries/universals-medieval problem of universals as to how these idealities are supposed to relate to particulars they are "about". Another way was to invoke an ideal subject, the God of epistemology. It is commandeered to do the "true" categorizations, and the suggestion is that human ones resemble God's in some way, or approach them in the fullness of time. But this "solves" the problem with a bigger problem, the only work "God" is doing here is converting "reality in itself" into something of a kind with human concepts, and our lack of understanding of what that means does not improve when we call it God. It is also unclear why there should be a "best" way to categorize reality at all, only one "God", as opposed to many excelling in their own ways but unmergeable. 

Since Kant there has been a growing realization that the prospects of traditional, a.k.a. https://plato.stanford.edu/entries/realism-sem-challenge metaphysical realism, and of correspondence theories are problematic. A very influential modern critique of them is given by http://www.iep.utm.edu/sellars Sellars's Myth of the Given, see also the more recent discussion stirred by http://ndpr.nd.edu/news/23295-reading-mcdowell-on-mind-and-world McDowell's book Mind and World. Both Sellars and McDowell nonetheless identify as realists. This cuts across the analytic/continental divide, Peirce, Husserl, Wittgenstein, Merleau-Ponty, Quine, Putnam, are all realists, or perhaps quasi-realists, in the following sense.

They draw a distinction between two different senses of "mind-independent", semantic and substantive. Any proposition or fact is mind-dependent semantically, a subject has a pre-acquired array of categories, whose availability is needed to even enable facts. But what the subject then "detects" as fact is not up to it, "he" has no control there, it forces itself upon "him", even as it is coached in "his" categories. Reality acts as a constraint on pre-formed propositions, not as a source of them, there is no "reality intake". In this sense facts are discovered, but at the same time their "building materials" are invented, in our human case, historically. And for reasons explained above "separating" the two into "pure reality" and "interaction with the subject" is not a meaningful suggestion. We can certainly build meta-models where objects are equipped with "potentialities" that are "grasped" by us, as we do with great benefit, and in such meta-models the facts will appear as correspondences to (modeled) reality. But all of this takes place within an already pre-conceptualized space, and is enabled by our role as subjects, it is therefore implicitly relativized to us.
Yes, the given argument involves a formal fallacy, https://en.wikipedia.org/wiki/Denying_the_antecedent denying the antecedent, which goes like this:


  
  p → q
  ~p
  Therefore, ~q
  


The conclusion doesn't logically follow from the premises.

The given argument is clearly an instance of that:


  
  ∀x(Bx → Qx) [all babies are querulous]
  ~Bd [David is not a baby]
  Therefore, ~Qd [David is not querulus]
  

The murdered person would count, in Utilitarianism, (positively or negatively according to the happiness he contains, even if he produces no effect on anyone else), and his absence would change the sum of "utility" in the world.

I think what you are after is material consequentialism.

There are very few absolute holders of this position in the Western tradition, since it means that no act is moral or immoral when you do it.  So it is impossible to behave morally.  You just have to behave, and find out later whether it was moral or immoral.  Which is not how we intuitively want the world to be.

Even the most absolutist Utilitarian wants to be able to compute the odds of his action helping or hurting, and have the decision matter.  So most forms of consequentialism involve the intent or expectation behind the act, rather than the material outcome.

Skepticism and the associated (but better developed) Hindu or Buddhist traditions that encourage peace through detachment sometimes favor material consequentialism as a criterion for morality, as a way of making the point that these computations really do not matter and just encourage worry and other investment in suffering that might limit equanimity or compassion.
I will take this up from a Kantian point of view

Bob simply cannot morally know the necessary outcome of John's actions, but more relevantly, John is a moral agent, still completely capable of not pursuing any prediction that can be made by Bob.  If you cannot be defied, you have stripped other agents of their autonomy, in a way that no human wants to be limited.

Even if my future is knowable, for it to be known, is immoral.  To have free access to prediction universally, the prospect of the delight of surprise would be removed entirely from the universe, and we would not will that: it is not compatible with how humans need the world to be.  So whatever thaumaturgy might determine John's fate is not moral, and even if we have done it somehow by accident, we should not act on it.

(Kant wants morality independent of species, and this sometimes clarifies arguments.  For Kantians who share the religious context of his upbringing, this is the argument against angels, who live outside time and carry the agenda of limiting sin, simply fixing history so the world is perfect.  To do so, whatever sin it might prevent, would crush human will.)

So, if Bob knows John's future and acts on what he knows, he is already in the wrong.  But that puts Bob in the position of acting inauthentically while trying not to lie.  He needs some boundary around what would be expected of him.

Even in the case with no magic, relying on your guesses about someone else's behavior based on observation too much is prejudice, and becomes immoral quickly.  John must be treated as a moral agent with autonomy.  If we do not allow in every way for the possibility of basic moral change, John can no longer participate in a 'Kingdom of ends', his autonomy is reduced to logic and he becomes a mechanical part of the universe, a mere means.

John then needs to be treated in a way that intends to work out for the best even if he deviated from any prediction we can make, at any point.  The only point where Bob can safely control the possibility that John will change the script, and obviate the predicted good outcomes, is in deciding to give or not to give John the money to start with, and that is the only act with predicted consequences that can be judged for moral content.

Giving is a good thing.  But it is sometimes not a good idea to just give people money at random.  If I gift the policeman who pulls me over with $100, this is not to his ultimate benefit, especially if his camera is on.  It proposes moral hazard.  So automatic generosity in complete generality is not universalizable.

Any rule about giving needs to incorporate the proviso that it should not cause undue foreseen risks for the recipient that outweigh the gain of the gift.  But with this addendum, the idea that giving away resources that you do not need does seem universalizable.  So this condition is a natural part of the proper maxim.  Kant's "Kingdom of Ends" is a community of ends, and mutual protection at a respectful and sustainable level is part of the package.

John is at risk here, and Bob, knowing of that risk, should provide a level of protection from it that we can generally expect from others, but he should not force John's hand.  If John is someone to whom Bob would ordinarily give help, that help should take a less risky form than cash.

This is a risk of which John himself would be aware, so accounting for it, and purposely treating him differently, is not a prediction of the future, or a stereotyping of John into a box we assume he cannot get out of.  It is acting on a motive that John should have and would approve of, were his functioning not impaired.
Pragmatism in philosophy is much like behaviorism in psychology.  So I like to think of this in terms of a behaviorist bird box where a pigeon gets food for learning to peck a given button under given circumstances.

Do you honestly think the bird attaches meaning to the buttons, and thinks of itself as communicating the wish for food through the button?  The odds are that the bird is doing something less conceptual.  And in fact, in responding to language, so are we.  But we have a built-in sense of meaning that reinterprets what we are doing in terms of representation because we as humans have learned a theory of mind.

We think of language in terms of representations because we experience life in stories, so that we can share it more readily.  But we know from physiology experiments that the representation comes alongside rather than before the act.  We start acting before we are done forming the representation.  The two processes happen in parallel, rather than representation happening and then meaning being ascribed afterward.  Therefore, our activity is not a result of representational manipulations.  Unless it is really complicated, it is basic response, like the bird's.  This means there is something below the level of representation that makes language work.

We do still form and store the representation, anyway, describing our own action to ourselves in case we need to share it or reprocess it later.  But the representation is really just about storage and communication, and is not a basic aspect of meaning.

This means that our words 'attach to reality' not to interpret the world to ourselves, but to render it communicable.  And again, we learn communication and interpretation by success and failure, and not by parsing.  We impose that later, and it is a separate refinement process.  Think about how many turns of phrase you use that make no literal sense.  If you needed to have the language parse out right and find representations in your mind for the respective parts in order to use it to listen or speak, those would disappear from the lexicon.

And from an "ontogeny recapitulates phylogeny" point of view, look at how languages evolve from more complex grammar (like Sanskrit and Attic Greek) to more analytic grammar (like English and Chinese).  This tells us that the process of rationalizing and representationalizing language progresses over time from a muddled parallel process (where the meaning is distributed over many words via endings or combinations) into a more linear one (where words more and more stand alone and take the same consistent, uninflected form).  Languages get richer in vocabulary and connotation over time, but less complex and nuanced grammatically.  If we really relied upon representationalism to base our use of language, we would need to start simple and build.
Following Clausewitz "War is politics by other means", I think war shows that it is ultimately impossible to limit the will to power.

In the Genealogy of Morals, Nietzsche points out how even the commonality of losing, and being oppressed, can be leveraged into a source of power if the ranks of those oppressed are large enough.  And even that power can be abused.  Christianity has been used to convince a lot of people to kill.  And it is based on empathy for suffering and oppression, at its origin.

War is, most basically, the mass refusal to obey the taboo against killing for power.  And the fact it still exists shows us that ultimately, no rules apply.  We have to choose them.

If, as in the case of cultures wholly alien to one another, there is no means of negotiation, there are no rules.  Politics has no limits because it can create 'alternative means' when it finds the ones it has are not working.

Enders Game cheats, because it shows only effective war.  Card has his protagonist isolated first from knowledge of his actions, and then from their consequences.  So it is also, to some degree, a good illustration of how horrifically easy asymmetric warfare is.  Even though the sides are somewhat easily matched, the bugs elicit no sympathy, so Ender's sentimentality is not confronted with actual suffering.  By completely failing to identify with one's enemy, one can be led to drastic means easily.
I believe you're thinking of the https://en.wikipedia.org/wiki/Anthropic_principle anthropic principle. There are actually two variations:

The strong anthropic principle says that the universe was made the way it is so that humans could exist.

The weak anthropic principle says that the universe must be such that humans could exist, since here we are.

The strong anthropic principle is considered fallacious. The weak version, however, is perfectly valid. It's used e.g. in cosmology to dismiss hypotheses that would exclude the possibility of human life ever developing.
I believe what you describe is close to what is called https://en.wikipedia.org/wiki/Anachronism#In_academia anachronism (literally, out of time -ism):


  "In historical writing, the most common type of anachronism is the adoption of the political, social or cultural concerns and assumptions of one era to interpret or evaluate the events and actions of another... Arthur Marwick has argued that "a grasp of the fact that past societies are very different from our own, and ... very difficult to get to know" is an essential and fundamental skill of the professional historian; and that "anachronism is still one of the most obvious faults when the unqualified (those expert in other disciplines, perhaps) attempt to do history". Anachronism in academic writing is considered at best embarrassing..." 


This is usually used more broadly and on a greater time scale than yours, but in its ethical application the idea is the same: decisions and actions should be judged based on information and context available at the time, not from the all-knowing perspective available with the benefit of hindsight. "No second guessing", "no Monday morning quarter backing" are lower brow expressions of this sentiment.

The OP abortion example puts an additional twist on it, however, and a controversial one. What it seems to argue is, roughly, that abortion is not murder because the "person" in question (fetus) is only a potential person, and therefore does not yet exist. This sentiment is also expressed as "to die is not the same as never to be born". This is a defensible position, but one will have harder time dismissing the opposite one as anachronistic. 

There is a well established school of thought which holds that potential existence is also a kind of existence, and it carries moral value with it. In other words, it is not necessary for people to place themselves in two different time frames at once, they can argue from a single time frame, but acknowledge its potentialities in addition to actualities. The device of subjunctive mood and the use of counterfactuals you mention evolved precisely for such purposes. Felt discusses potentiality in the context of free will and responsibility in http://www.anthonyflood.com/feltimpossibleworlds.htm Impossible Worlds:


  The actualists are therefore right in denying an independence to the possible.  On the other hand, to be potentially is really a way to be, even though it is not to be actually.  And this of course is just what Aristotle said in response to Parmenides, who conceived of only one way of being, being in actuality. 


The real issue, as you yourself point out at the end, is how to balance the likelihood of consequences against their utility and/or moral value, and that is the crux of most intractable problems in https://en.wikipedia.org/wiki/Consequentialism ethical consequentialism. It would be so much easier if we had a firm grasp and consensus on what is or is not a "sufficiently removed" consequence, and how much of an existence potential existence is. https://en.wikipedia.org/wiki/Slippery_slope Slippery slope is another kind of situation, whose being or not being a fallacy turns on such judgements.
The most straightforward way out of this conundrum is to localize logic.  There are a lot of variants of this (my favorite being neo-https://en.wikipedia.org/wiki/Intuitionism Intuitionism) but most of them can be captured by the notion of http://plato.stanford.edu/entries/fictionalism-mathematics/ Mathematical fictionalism.

The idea here is that reality, in particular math, as the elaboration of basic intuitive logic, (though it is really just an example) may not be a pure and consistent whole, but it has large consistent pieces that apply in a broad range of domains, as we witness when we actually do it.  So only use the large consistent pieces, and retain isolation from the paradoxes.  Don't allow your logic to make inferences too close to a paradox, by limiting how the effects of a paradox would 'spread' and contaminate your consistency.

This involves abandoning various aspects of classical logic, and which aspects you choose becomes a very interesting question, but one that is hard to find agreement upon.  (After all, it involves violating a strong and appealing intuitive system in favor of choosing which intuitions we should apply where.  The obvious answer, that all intuitions should apply everywhere, failed, and, as you note, that leaves us kind of at sea.)

The Intuitionist form looks at mathematics as the oldest form of psychology:  Starting from the http://plato.stanford.edu/entries/kant-spacetime/ Kantian base that space is an aspect of human understanding, and not of reality, the subject matter of mathematics in general is not what is real, but what we as humans can readily understand, and what we find interesting about the patterns in our world.
Two thoughts.

First, for Kant himself, it is never the case that the categorical imperative comes in conflict with itself. In other words, Kant rejects the idea that there are conflicting moral principles and we must choose among them.

Second, the problem you pose does not even demonstrate conflicting moral principles or a situation where the categorical imperative obliges us to take two mutually-exclusive courses of action. You seem to be misunderstanding the categorical imperative when you equate it with the imperfect duty to help others.

In the situation, you describe the moral self has a choice between two courses of action both of which are possibly moral if performed in accordance with the categorical imperative. The wording in the preceding sentence is obtuse but important -- For Kant, actions do not necessarily qualify as moral even if the end agrees with morality. Instead, they only qualify as moral when the maxim of my action corresponds with a law that could be universalized or that treats humanity (rational nature) as an end rather than a means or as a possible law in a kingdom of ends.

Stated another way, helping either person for the right reason would qualify as moral as a fulfillment of the imperfect duty to help others. BUT it does not follow that you have failed in this duty because you did not help all others at all occasions. (Kant explains this in more detail in the Metaphysics of Morals: Doctrine of Virtue and Critique of Practical Reason). 

One major reason for this is that Kant is not concerned primarily with the consequences of our actions. Thus, for Kant, that one person died because I saved someone else does not ever impute a moral wrong to me. In fact, it is possible to act morally in ways that lead to the deaths of others on the Kantian picture. Specifically, because my duty is to pursue a maxim that treats others as ends -- and these others can include those with immoral goals. (Cf. https://philosophy.stackexchange.com/questions/6611/why-shouldnt-you-lie-to-the-future-murderer-of-your-children Why shouldn't you lie to the future murderer of your children?) 

To answer one formulation of your question towards the end, you ought to save one of the lives because each life is an end of infinite worth rather than price. You need not feel any guilt that saving one meant you couldn't save the other.
There were no citizens of Greece at the time.  So no, it makes no sense.  Greece, like Germany, and post-Roman Italy, was not a single nation until after the culture already had a long shared history.  (This has led German philosophers like Hegel and Nietzsche to link and contrast the psychological foundations of the Greek, German, and renaissance Italian cultures.)

Socrates was a citizen of Athens, one of the few Greek states that had citizens at all.  The rest were run by Tyrants, and therefore had subjects.

Also, Socrates was not exiled or ostracized, he was sentenced to death and followed through on the sentence instead of taking on a different nationality.  He insisted on not requesting exile, despite that he could have counter-plead for that punishment, and would probably have gotten it.  And he insisted on not escaping, even though some of his students seem to have arranged a way for him to do exactly that, and offered him the option.  So he was Athenian to the end.
I do not think that there is any issue with viewing signification as an activity, in fact this is how pragmatists view it since Peirce. In modern terms, pragmatism asserts semantic and epistemological priority of knowledge how over knowledge that, so representation is viewed as a special kind of performance. The problem begins when we look into the status of what it is representational of. This brings up the issue of reference, correspondence to "reality", and is closely related to the realism/anti-realism divide. http://ceur-ws.org/Vol-444/paper19.pdf Szubka in On the Very Idea of Brandom’s Pragmatism speaks directly to your suggestion:


  "Perhaps anything which is performed by us will count as a doing or action, and it won’t be necessary to describe it in some selected vocabulary (e.g. in physical, biological, behavioral, or functional one) in order to invoke it in understanding or explanation of propositional contentful beliefs and knowledge. However, by allowing advocates of global pragmatism such a latitude in this matter, one puts their view at risk of being easily trivialized."


https://books.google.com/books?id=5grK3Rt2NboC Brandom in Pragmatics and Pragmatisms describes pragmatism "as a movement centered on the primacy of the practical, initiated already by Kant, whose twentieth-century avatars include not only Peirce, James and Dewey, but also the early Heidegger, the later Wittgenstein, and such figures as Quine, Sellars, Davidson and Rorty". Ryle, Merleau-Ponty, Polanyi and Dreyfus can also be added to this list. This is a big tent with plenty of room for realism, even materialism (Davidson), quasi-realism (Kant, Quine, Sellars), and anti-realism all the way (Rorty). Moreover, Dummet, who believes that representational and pragmatic aspects are too intertwined for either to have primacy, is nonetheless an anti-realist. 

However, pragmatism is opposed to "intellectualism, the doctrine that every implicit mastery of a propriety of practice is ultimately to be explained by appeal to a prior explicit grasp of a principle". Since reference to a conceptualized object does involve "grasp of a principle" any description involving it can not be first in the order of explanation. So it is not that representational use "offends" pragmatists, it is rather that its representational character is in a sense moot. Let me give an analogy: classical objects are interesting and practically important formations in quantum theory, they are unsuitable for formulating the quantum theory.

Representational activities, truth-conditional semantics that articulates them, etc., are certainly rich and interesting topics, but they are secondary in semantics/epistemology. As Brandom puts it, “explicit theoretical beliefs can be made intelligible only against a background of implicit practical abilities”, it would be self-defeating to reference a pre-conceptualized ontology of said beliefs in articulating the said background. The "first" task is to describe how conceptualized ontologies are formed, then we can decide how seriously to take them, and how much, and in what contexts, to use them. Quine, who is a more sympathetic to your position kind of pragmatist, splits it this way in https://books.google.com/books?id=YxWOMUguw5sC&source=gbs_navlinks_s Theories and Things:


  "The scientific system, ontology and all, is a bridge of our own making... But I also expressed my unswerving belief in external things — people, nerve endings, sticks, stones. This I reaffirm. I believe also, if less firmly, in atoms and electrons and classes."


Davidson, Quine's student, developed a truth-conditional semantics which is broadly pragmatist. See http://www.yorku.ca/cverheg/publications.htm Verheggen's How Social Must Language Be? on his triangulation procedure, which ties practice to reference along the lines that you may perhaps find more agreeable.
In 2. There seems to be a http://en.wikipedia.org/wiki/Fallacy_of_division fallacy of division.

"My life's contribution to the world was positive, so every contribution I made in my life has to have been positive for the world" is a traditional logical misstep.
Not a terribly philosophical answer here...

Animals in the wild are actually quite efficient at resource utilisation. For example, a lion kills a zebra, the pride eats enough food for them to last for 2-3 more days without any other food. The scraps and bits the lions don't want to eat get eaten by smaller animals (and so on until literally there's nothing left of the zebra but bones). All those animals leave dung which dung beetles like and helps plants to grow, etc etc. Thus, one zebra feeds an awful lot of the food chain.

Humans are bad at the same resource allocation. If we were to lab-grow meat, we'd expend more energy than it took to grow the zebra, and we'd only feed the lions. We'd need to do it all over again to feed the smaller animals, and the insects, birds, bacteria and whatever else that feeds on a dead zebra. Further, we'd be likely to over-feed or under-feed the lions, which then changes other behaviours.

Further, the animals involved all know the 'rules of the game'. Zebras have multiple young to compensate for the odd one getting eaten here and there (just as humans once did, by the way). Lions occasionally come off worse when they get too desperate for food and take unnecessary risks, thus providing a different form of meat for the food chain to enjoy. And so it goes on. For the most part, any animal killings are usually pretty quick (as opposed to slow and torturous 'playing'). As such, I'd say it's probably better to leave nature to do it rather than try to impose our human will on it (side point: what's to say our way is actually the 'right way'? Nothing empirical has been proven, so it's mostly a position based on arrogance).

That said, animals under human control are (in my opinion) a different matter. I'd say we humans do indeed have an obligation, for example, to make sure our pet dogs don't maul the local cats or chickens or whatever. Indeed, I'd go further to say we have an obligation to train pets not to pursue such behaviours in the first place. We also have some obligation not to breed dogs (etc) in artificial/forced ways specifically with the aim to make them more aggressive and therefore likely to be violent towards other animals. One could also argue we shouldn't artificially breed them specifically to be more passive either, but that's a different conversation.

So in summary: yes and no ;-)
I think this is less ambiguous than it seems.

Not all killing is murder. For example killing in self-defense is defined as not being criminal.

Adding a detail like criminality only to remove it at the same time is a red herring.  It cannot help clarity, or contribute to meaning, as it is canceled out before any logic is applied to it.  So it must be present only to incite its emotional effect, and skew opinion.

Modifiers like 'just' in the form that does not take a pronoun phrase, is also a signature here.  The 'just' means that we should rule out alternatives, and minimize the content, but left alone, it does not specify the kind of alternative we are meant to rule out or the excess we should be seeking to minimize.

Something like 'just that part of the argument that can be transcribed into formal terms' might appear in a technical speech.  But without that kind of follow-up criterion, 'just' is basically a dismissive interjection that emphasizes the supposed simplicity of the statement, and invites us to not look too closely.  This is emotional reassurance rather than logical information, and it is therefore usually rhetorical manipulation, or at least editorializing, neither of which belong in an objective definition.

So this is a 'persuasive' definition.

It does not persuade me, and I even accept the point to which it is aimed.  So I don't like your author's vocabulary.

The term of art in rhetoric as I was taught it, and the one used in some theories of law, is just 'suasive' as in 'moral suasion'.  The separate term helps because it means folks diagnosing arguments that do not have force but still may affect judgment do not appear to be admitting 'truthiness' by calling the argument 'persuasive'.
It is specifically just the http://rationalwiki.org/wiki/Genetic_fallacy genetic fallacy: assuming an argument is erroneous on the basis of its provenience.

But I prefer to see it as what CS Lewis named 'https://en.wikipedia.org/wiki/Bulverism Bulverism' -- diagnosing an argument as a symptom of something (the speaker's known or presumed bias or ignorance, the social or political context, any psychological dynamics at play, concurrent rhetorical manipulations, etc.) instead of considering its content 

The specific diagnosis given in both of your example cases is 'ignorance through lack of exposure'.

However you choose to identify it in particular, as @elmer007 notes above, you can tell this is a fallacy right away because it is ad hominem -- pointed at the speaker and not the content.

Even if ignoring or being especially dubious about an argument for some contextual reason is sensible, it is always still a fallacy.  Logically, one can only reject an argument based upon its content -- which involves actually considering the content.
"Conceiving" is the activity of the mind (compare it with "perceive"). 

We may rephrase it as "To understand, to think something truly." Thus, I would chose 3. understand.

Understanding is the "basic" act of mind. The term is from Descartes; see http://www.wright.edu/~charles.taylor/descartes/meditation5.html Meditations, V, 9 : 


  "I cannot conceive a God unless as existing", 


translated from the http://www.wright.edu/~charles.taylor/descartes/meditation5l.html original Latin : 


  "ne possim quidem cogitare Deum nisi existentem".

Second guessing your question away, I think this is not about collectivism or individualism, it is about sense and sentimentalism.  The most collectivist thing one might be able to do in a world where everyone encounters the dying might be to tone down your reaction to death, so that fewer of those around you feel guilty about it.  This may not be individualism, and in fact, expecting others to care about your dead may simply be selfish, if there is no chance that all the dead can be adequately considered.

You might want to look at this from a sort of 'https://en.wikipedia.org/wiki/Desmond_Morris Desmond Morris' point of view.  What is the cultural and biological message sent by sensitivity to the dead and dying.  I would suggest that it is a sign that one's lineage has the luxury of demanding unnecessary actions of individuals to protect the group.  We carefully process the dead and dying because they are a public health risk.  But sending that signal is unwise when it stops being true.  At that point, it is equally collectivist to require this natural impulse be suppressed.

We like to think of the harshness of traditional masculinity as a form of individualism, if not outright selfishness.  But from a point of view put forward by pacifist feminists like http://starhawk.org/writing/books/truth-or-dare/ Starhawk, it arises most clearly in the context of the service of war, which is a gift of oneself to an extraordinary level of collectivism.

Putting these two (who would probably never agree) together, sentimentalism has a social purpose, and so does resisting it.  The former sends the message "We have resources that will go to waste unless we share them", the latter sends the message "We need to ration logically, even if that logic is somewhat inhumane."  They are both empathic, one directly and the other less so.  But both are collectivist impulses, and both limit genuine individualism.

Returning to the original perspective, modern cosmopolitan society is, from Starhawk's point of view, brainwashed into an impending feeling that scarcity lurks around every corner, and efficiency is therefore required for the common good.  So it chooses the path of limiting oneself prematurely, chooses rights over responsibilities, and delays attending to broader shared needs.  But that is a communal position.  It presses those with more idiosyncratic individual impulses to deny them.

These people are in line to get money, they have resources, they could spend those resources on removing the obstacles presented to those who are more subject to their externalizing impulses.  But they don't -- it would not be efficient, there will just be more dead bodies tomorrow -- and they have the right to pursue their chosen responsibilities first.  They are communally choosing to make a space where that right is honored.  If one of them broke from the mold, they would all be less stable.

So I would argue that we are not already desensitized, or that everyone is, and the choice indicates something different.

As the documentary 'Cowspiracy' demonstrates graphically, those in prior eras would expect any grown man to be able to behead a chicken without a second thought.  And most of our modern, cosmopolitan peers just can't.

At the same time, most ancient folks would never sacrifice their religious solidarity and walk past a dead body just to get something practical, unless under severe duress.  Their shared responsibility to God trumps efficiency, the same way our shared responsibility to efficiency makes us explain away our natural revulsion as superstitious.
Strictly speaking, it's not a fallacy. If something really doesn't affect me, there's no reason I should care about it. Thoughts:


This isn't a true-or-false statement, so you really can't apply the concept of logical fallacy to it. He's not saying "it doesn't affect me, therefore it's false [or true]". That WOULD be a fallacy. Simply saying "I don't care about it" is probably the truth.
You can attack the antecedent of this argument by showing that the issue actually DOES affect him, either directly or indirectly.
You can attack the conclusion of this argument by showing that he cares about things that don't affect him. Most people have fundamental concepts of morality: you might ask him if he cares about people starving in Africa or other suffering in the world. If he's a true hedonist, he might not. However, if he cares about those things, you've show his statement is false for him: it's possible for something not to affect him but for him to still care about it.
When attacking the antecedent, you can take the extreme position that everything affects everything, so "it doesn't affect me" can never be true.
When attacking the conclusion, you can point out that by discussing it with you, he does "care about it" to some sense. IE, the issue has entered his life in some way and has some effect on him.

Aristotle's views on artifacts are a little bit hard for us to accept in the contemporary era, but the basic idea is that a substance is unified by a single form which is its activity and the thing that organizes it.

So I think it's slightly off to say Aristotle assumed humans, animals and even plants to be substances. On Aristotle's picture, this is not an assumption -- it's an observation from empirical data. To give a couple of examples by differentiation:


A plastic plant and a living plant differ in two significant regards. First, if I snap a branch off the plastic plant, it forever remains damaged in that way. Conversely, if I snap a branch off a living plant, then the plant will either suffer a catastrophic death and stop being a plant (i.e. die) or it will adapt and recover around that and continue its life. In other words, the plant-like artifact just takes damage, but there's no life to take away from it. Second, the real plant is growing and to keeping being a plant needs soil, light, and air. 
A taxidermed wolf vs. a dog. We have the same image of damage vs. injury. In addition, the real living animal moves around under its own utility whereas the taxidermed one can't do that. Even if we stick something in it and let it be remote-controlled, then that's not really the taxidermed thing moving around on its own.


To put it another way, a substance has one thing that makes it tick and that orients the rest of what it's doing. And when we break that, for living things we have to kill them, and for other things they completely lose their unifying coherence.

I raise this because the point isn't that Aristotle is pulling a random assumption out of thin air and holding to an irrational faith in substances. At least on his own view, he thinks he's using his mind to intuit forms from living things (and some non-living things).

One contrastive category is the "aggregate" or the "pile." If we think of a mound of sand, it doesn't really have anything that 'unifies' it in a deep sense. There's nothing that keeps the pieces of sand grouped except that they are pieces of sand.

If we think about a CPU, it's really a very well organized aggregate. It's billions of little tiny transistor gates that we've piped together to work out as a super abacus. But it's still a bunch of parts that don't have a fundamental unity or maintain themselves. They have a very useful unity in that we put this tool to good use, but the unity is not fundamental to the thing.

In other words, I deny that a CPU is a "unified material object". The key is to grasp that what is meant her by unity is not "to be in one piece" but rather to be one coherent whole where the parts necessarily compose the whole.

Now, the article Mauro mentions raises some similar objections to what you're raising but does so in the context of a good understanding of the Aristotelian vocabulary and how it works.



If we want to lend credence to your idea, a better example might be an android of some sort. Here, it seems to have internal motion and coherence around its android existence. In other words, it would be better approaching that unity even with non biological parts.

(ps in passing I do want to mention that this is a good question despite my disagreement with your claim -- in large part because Aristotle's view and claim are hard to understand).
This is the scenario. As I was happily enjoying a meal at a downtown restaurant, I suddenly noticed that a person is falling onto me from the forth floor of the building. The restaurant is so crowded that I cannot get away from my seat (= I am trapped). Incidentally, I had a pulverizer and could use it to render the falling person into particles, thus saving my life. I am a Kantian, and avowed that I would never treat others as mere means. But using the pulverizer onto the person is treating him like a rock, i.e., as a mere means. On what moral ground, can I, a Kantian, use the pulverizer?

While Kant himself does not discuss this type of scenario (called self-defense under an innocent aggressor), a moral theory informed by the Kantian maxim (Never treat others as mere means) has been developed in the name of the doctrine of double effect (DDE).  DDE states that it is morally permissible to cause a foreseen harm as a side effect of bringing about a good result, but it is morally impermissible to cause such a harm as a means to bringing about the same good end. 

Applying DDE, my goal is to save my life from a falling object, which many believe is morally permissible. Pulverizing the object is the only means to achieve the goal. When I pulverize the object that is falling onto me. I can foresee that the action has the side effect of killing a person. But clearly, I did not intend to kill the person, His death is a mere foreseen side effect. I never used the person as a mere means for my survival. Thus, I am morally permitted to use the pulverizer. 
Jeremy Bentham's (https://en.wikipedia.org/wiki/Jeremy_Bentham https://en.wikipedia.org/wiki/Jeremy_Bentham) 
"pleasure calculus" (https://en.wikipedia.org/wiki/Felicific_calculus https://en.wikipedia.org/wiki/Felicific_calculus) 
lists seven elements to consider when making a choice between 
pleasurable or potentially pleasurable activities: 


Intensity: How strong is the pleasure? 
Duration: How long will the pleasure last? 
Certainty or uncertainty: How likely or unlikely is it that the 
pleasure will occur? 
Propinquity or remoteness: How soon will the pleasure occur? 
Fecundity: The probability that the action will be followed by 
sensations of the same kind. 
Purity: The probability that it will not be followed by sensations 
of the opposite kind. 
Extent: How many people will be affected? 


If you accept this system, the "propinquity or remoteness" criteria 
suggests that short-term pleasure is more beneficial, whereas the 
"duration" criteria suggests that the long-term pleasure is more 
beneficial. As with any philosophical system, Bentham's calculus 
doesn't tell you which choice to make (you have to weigh the two 
possible criteria yourself), but it does answer your question: if you 
accept Bentham's view of utilitarianism, yes, it's quite possible that 
propinquitous (immediate) pleasure outweighs long-term loss. 

Your post strongly suggests that you prefer long-term benefits over 
short-term ones, since you use phrases like "skimp", "cut corners", 
"not healthy", "incurable", "is it still right" (suggesting it's not), 
etc. Here are some balancing thoughts for propinquitous pleasure(in no 
particular order): 


Certainty: while most people agree that fast food, skipping health 
maintenance appointments and casual sex are "bad for you" and reduce 
your life expectancy, this is a vague and undefined risk. There's no 
guarentee that any of things will give you displeasure in the future 
and there are things you can do (eg, safe sex) to make future 
displeasure even less certain. 
Intensity: the benefits of cooking your own food, health 
maintenance, avoid sexual disease, and perfectionism are fairly 
mild. Living a long and healthy life is nice, but that kind of 
pleasure is passive and not intense. Perfectionism for the sake of 
perfectionism may give you pleasure in knowing you did the job 
"right", but this, again, seems like minor self-satisfaction, not 
intense pleasure. In this case, I'd say it's a tie. 
Intensity: Sexual pleasure is intense, even when repeated, but the 
intensity of avoiding sexual disease is very low, as above. 
Fecundity: casual sex probably will lead to more casual sex as you 
build up a group of casual sex partners. 
Purity: perfectionism may give you some pleasure, but 
perfectionism is often annoying to coworkers and employers, because 
perfectionists often inefficiently spend too much time working on a 
single project, focus too much on a small detail, don't accept help 
from others, or even come up with rigid designs that are hard to 
change in the future. Perfectionism is an impure pleasure because it 
leads to things that may cause you displeasure. 


You note "even if it works for the majority", but I don't think that's 
an argument to "extent", and seems a little out of place. 
Utilitarianism deems something "right" if it works, almost by 
definition. Therefore, if something works for the majority, 
utilitarianism says it's "right" for the majority, which would seem to 
work against your arugment that it's not "right" under 
utilitarianism. However, I don't think what the majority does affects 
your decisions that much, nor vice versa, so it seems irrelevant. 

In conclusion, I'd say you can make a strong argument that immediate 
pleasure can outweigh long-term benefits under the doctorine of 
utilitarianism, even though your question appears to be biased against 
that position. 

  
  Death is nothing to us; for that which has been dissolved into its elements experiences no sensations, and that which has no sensation is nothing to us.
  The magnitude of pleasure reaches its limit in the removal of all pain. When such pleasure is present, so long as it is uninterrupted, there is no pain either of body or of mind or of both together.
  Continuous bodily pain does not last long; instead, pain, if extreme, is present a very short time, and even that degree of pain which slightly exceeds bodily pleasure does not last for many days at once. Diseases of long duration allow an excess of bodily pleasure over pain.
  


...and such from http://www.epicurus.net/en/principal.html Principal Doctrines putatively by http://www.thefamouspeople.com/profiles/images/epicurus-2.jpg Epicurus himself.

You could also check out "The Conquest of Fear and Worry" by John Herman Randall from his 1909 "https://books.google.com/books?id=A2cAAAAAMAAJ&pg=RA5-PA17#v=onepage&q&f=false A New Philosophy of Life"

More recently from the University of Glasgow, there's also http://www.euthanasia.cc/telfer.html Elizabeth Telfer's article, "http://www.euthanasia.cc/telfer.html Philosophical Approaches to the Dilemma of Death with Dignity" which addresses utilitarian and consequentialist positions.

And this dissertation may be what you are looking for: "https://philpapers.org/rec/BLOIDO-2 In Defence of Euthanasia: The Epicurean View of Death" by Andreas J. M. Blom

I would question, however, the notions you mention in your bounty notes that "pain is innately bad" and from the question that pain is "evil" - and this whether "https://plato.stanford.edu/entries/pain/ pain" in the sense of physical, emotional, mental pain or what have you. Pain is ontologically subjective. In that sense, consider http://www.ted.com/talks/john_searle_our_shared_condition_consciousness/transcript?language=en Searle's distinction of "feeling a pain and engaging in pain behavior", per his recent Ted Talk.


  "My pains have a subjective mode of existence in that they only exist
  as experienced by me, the subject. But mountains and molecules have an
  objective mode of existence because they exist whether or not they are
  experienced by any subject. It can be an epistemically objective
  matter of fact that I have a pain even though the mode of existence of
  the pain is ontologically subjective."
  John R. Searle, https://books.google.com/books?id=EW521XwPg-EC&source=gbs_book_other_versions Philosophy in a New Century, 2008)

So the Cartesian project of complete foundationalist certainty has been abandoned my contemporary epistemologists. Nevertheless there are still foundationalists, and they tend to really stress the defeasibility of their knowledge. Foundationalists hold that relative certainty of the truth is achievable through some starting point. A domain-non-specific global skeptic would say there can't be any certainty as to the truth. Praxeologically we may all act on some sort of implicit foundationalism, but that doesn't mean we aren't skeptics, however. It depends on how we think the world discloses itself to us.
The fallacy that is made by person B is the fallacy that is known as ignoratio elenchi, which is Latin for "ignoring criticism", but is a fallacy wherein, for any pair of interlocutors interlocked in an argumentational dialectic, interlocutor A states X and interlocutor B responds to X with a statement that is immediately irrelevant to X such that B "misses the point".  
To give a terribly unphilosophical answer informed rather by economics:

As purely mathematical and for wealth distribution only (as per OP) the https://en.wikipedia.org/wiki/Gini_coefficient Gini coefficient and normally nothing else, really.

It is http://www.fao.org/docs/up/easypol/329/gini_index_040en.pdf the most popular inequality index.
https://en.wikipedia.org/wiki/Wittgenstein's_ladder Wittgenstein's ladder may be the ladder you need.  It's a process, described in the Tractatus Logico-Philosophicus, where each step prepares you to understand the next step, but should be discarded after you've gotten past that point.  It's similar to Plato's idea of the "noble lie," a deliberate falsehood meant to move you closer to a deeper truth, but with the explicit addition that the lie is only a step on the way.

If so, then your friend is saying that Judeo-Christian morality has gotten us somewhere, if not directly to the dissolution of itself, then perhaps to a material society where it's no longer needed.

Whether or not your friend means this I don't know.
You are correct about the definition of validity, but actually 'tautological sentence' is defined in the way regardless of premises or conclusions. A 'tautological sentence' is one that is always true regardless of the truth of 'atomic sentences (ex. 'A','B',...)' that consist of the sentence. It is not originally defined in the context of premise-conclusion as you said.

However, it can be proven that tautological sentences as defined previously is always the 'true conclusion' of any argument regardless of truth of the premises. Therefore, tautology is always valid. (In the rigorous manner, 'tautology' usually refers to the logical sentence, not argument. However, it can differ from how the person defines each terminology.)

-I used the terms from Elementary Logic by Benson Mates.
Whether or not time can extend infinitely into the past depends on the truth of a particular theory of time. 

Parmenides denied the reality of space and time, which is a metaphysic that begot Zeno's paradox. "If time extends infinitely into the past, this conversation would never occur" is a variant of Zeno's paradox because, in order for the hypothetical proposition to be true, time cannot be real. If the proposition is true and time is unreal then the occurrence of the conversation is an illusion.

Presentism denies the reality of past and future (but affirms the reality of the present) such that nothing real is referred to by the statement "time extends infinitely into the past". Therefore, presentism can accept that time truly can extend infinitely into an illusionary past and the conversation truly can occur in the very real present, because the present is the only time anything real happens.

Perdurantism and B-theory of time affirm the reality of past, present, and future but deny that there is any objective location in any B-series (or C-series) ordering of time. These metaphysics of time suggest that time is a 4-dimensional object, but not necessarily finite in its dimensions. It is possible that aleph null is the cardinality of the set of a all B-series (or C-series) statements completely describing a 4-dimensional object.

An endurantist non-presentist A-theory of time affirms the reality of past, present, and future and it affirms that there is an objective location in an A-series, B-series, and C-series ordering of time. Yet, the aforementioned point about cardinality applies here as well.

What would make an infinite past impossible? Why would statements about a real infinite past necessarily not be true?
From the Stanford Encyclopedia of Philosophy on indexicals: 


  https://plato.stanford.edu/entries/indexicals/ Some theorists hold that sentences containing verbs in the present tense vary not only in truth-value from time to time, but also in content, from context to context. For example, some hold that the content of ‘Fred is hungry’ at a context whose associated time is t is (roughly) that Fred be hungry at t. The variation in content is said to be due to the occurrence of ‘is’, which is context-sensitive. More generally, tense markers and tense morphology are claimed to be context-sensitive expressions.


From More Structural Analogies Between Pronouns and Tenses by Angelika Kratzer: 


  http://semanticsarchive.net/Archive/WY1NDFkM/Tenses.and.Pronouns.pdf In the early seventies, Barbara Partee suggested that tenses in natural languages might not be operators, but pronouns. Like pronouns, they have indexical, anaphoric, and bound variable uses. In this short presentation, I will discuss one more parallel between tenses and pronouns.


Her paper then goes on to outline the other parallel she discovered. Additionally, the original paper she is referencing is https://www.jstor.org/stable/2025024?seq=1#page_scan_tab_contents here.

If we take a philosophical stance, as opposed to a purely linguistic stance, and we concern ourselves with https://plato.stanford.edu/entries/propositions/ propositions, then it is widely accepted that present tense propositions are inherently indexical. As outlined above, the argument is that the truth value of something such as "my dog is happy" can change from time t-naught to time t-one. Therefore, the propositions carry some indexical, and in certain circumstances anaphoric, structure to them. If a present tensed proposition is expressed without a use of indexicality it would still need to account for the fact the that its truth value could change at some later time, or have been different at an earlier time. The arguments above support the idea that this is synonymous with indexicality and therefore that would be impossible. 
I am just answering the question since there is no reason to leave this question marked as being unanswered.

The given set of statements does not constitute an argumentum ad hominem fallacy. For the fallacy to occur, the following two conditions must be met:


An irrelevant appeal is made, and 
An argument must be present. 


The given set satisfies neither. The appeal to him being a lawyer is relevant (violating 1) to the conclusion (criticism) of his inability to present an argument (violating 2).  



Thanks, Dwarf, for the comments! I offer a reference for the two conditions of mine. This is an article you can assess on the internet: "Argumentation Schemes and Historical Origins of the Circumstantial Ad Hominem Argument" (http://www.dougwalton.ca/papers%20in%20pdf/04historical.pdf http://www.dougwalton.ca/papers%20in%20pdf/04historical.pdf) by D. N. WALTON


  Not all attacks on character should be classified as ad hominem
  arguments. To qualify as an ad hominem argument, the character attack
  must be used in a dialogue in a certain way. One party must use it to
  attack an argument put forward by the other. (italics mine) (p.362)


Walton provides the basis for the two conditions:


The character attack must be based on the irrelevant aspect of the presenter relating to the argument. 
The character attack must be used as an attack of the argument put forward by the presenter.


Walton's idea is commonly adopted in academia. For example, 
http://philosophy.lander.edu/logic/person.html http://philosophy.lander.edu/logic/person.html 
in II, A, 2 asserts as follows:


  Note that for the argumentum ad hominem fallacy to occur, (1) an
  irrevelant appeal is made and (2) a (logical) argument must be
  present.

The context is human action and its "rationality":


  [ http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0054%3Abekker+page%3D1113b%3Abekker+line%3D1 1113b.1 ] The activities in which the virtues are exercised deal with means. Therefore virtue also depends on ourselves. And so also does vice. [...] if it is in our power to do and to refrain from doing right and wrong, and if, as we saw, being good or bad is doing right or wrong, it consequently depends on us whether we are virtuous or vicious. 


Human action must be "rational"; i.e. if an action is neither forced, nor performed because of ignorance, it is is "voluntary" performed.


  actions of which the origins are within us, themselves depend upon us, and are voluntary. 


In his discussion of courage [ http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0054%3Abekker+page%3D1115a%3Abekker+line%3D1 1115a.1 ] Aristotle distinguishes true courage, where someone remains at his post in battle simply because to do so is good and admirable [ http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0054%3Abekker%20page%3D1115a%3Abekker%20line%3D20 1115b.1 ] (the voluntary one):


  Courage is shown in dangers where a man can defend himself by valor or die nobly,


from false forms of courage, like facing a disasters like shipwreck (the "forced" one).


  The courageous man then is he that endures or fears the right things and for the right purpose and in the right manner and at the right time, and who shows confidence in a similar way. 
  
  [ http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0054%3Abekker%20page%3D1116b%3Abekker%20line%3D1 1116b.1 ]  A man ought not to be brave because he is compelled to be, but because courage is noble.
  
  [ http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0054%3Abekker%20page%3D1116b%3Abekker%20line%3D20 1116b.20  ] wild animals [...] are not to be considered courageous for rushing upon danger when spurred by pain and anger, and blind to the dangers that await them; [...] the form of courage that is inspired by spirit seems to be the most natural, and when reinforced by deliberate choice and purpose it appears to be true Courage. 
  
  [ http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0054%3Abekker%20page%3D1117a%3Abekker%20line%3D1 1117a.1 ] Nor yet again is the boldness of the sanguine the same thing as Courage. The sanguine are confident in face of danger because they have won many victories over many foes before. [...] When however things do not turn out as they expect, the merely sanguine run away, whereas the mark of the courageous man, as we have seen, is to endure things that are terrible to a human being and that seem so to him, because it is noble to do so and base not to do so. 


We see here again Aristotle's stress on the "right middle" :


  [ http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0054%3Abekker%20page%3D1116a%3Abekker%20line%3D1 1116a.1 ] The coward, the rash man, and the courageous man are therefore concerned with the same objects, but are differently disposed towards them: the two former exceed and fall short, the last keeps the mean and the right disposition. 


In conclusion, courage is not "lack of fear"; the rash man falls short of fear, as well as the coward exceed in fear.

The truly courageous man is that able to "balance" fear and confidence.
The duty of the father is not towards the mother, but towards the child. The father clearly participated in the decision to create the child. And since the natural consequence of creating the child is a birth (unless there is an accident or illness), the father should pay. 

There are many anti-abortion ("pro-life") groups, those groups in particular would say that since a child shouldn't ever be aborted, that initial decision of the father is the only one that he could make. 

  A superior says: "all orders of a superior are lawful because they have been given by a superior."


By itself, this is not necessarily false. In the military, for example, this is the standard of conduct - a superior's orders are lawful except in extreme cases which the code of conduct addresses. In such cases where this presumption of lawfulness is not the case, you might be thinking of the https://en.wikipedia.org/wiki/False_equivalence false equivalence where the law and the a superior's orders are presumed to be the same. Note that this is distinct from http://www.txstate.edu/philosophy/resources/fallacy-definitions/Equivocation.html the fallacy of equivocation where an argument is made using one sense of an ambiguous term and concluding something using a different sense of the same ambiguous term.


  But the policy in this instance states, "employees shall promptly obey any lawful order given by a superior." Which implies that the law and superior are not one and the same.


In that case we have two statements:


Policy: "employees shall promptly obey any lawful order given by a
superior."
Declaration: "all orders of a superior are lawful because they have
been given by a superior."


In this case, if the authority issuing orders is not authorized such that their authority is inherently lawful, then the declaration is http://grammarist.com/rhetoric/begging-the-question-fallacy/ begging the question. For example, if the manager at Chuck E. Cheese's pulled this on their subordinate employees and used it as a justification for ordering them to hand over their weekly cash tips it would be an example of petitio principii argumentation (i.e. assuming the initial point).


  I know this is a pretty basic logical fallacy but I can't put my finger on the name. What is the fallacy called?


You may also be thinking of the https://en.wikipedia.org/wiki/Argument_from_authority argument from authority or appeal to authority when the authority in question is not related to the argument at hand, e.g. the opinion of a c.e.o of a tech company may be authoritative in their field, but irrelevant to a heart surgeon performing a transplant. You may also be thinking of the argument from false authority, but in the case of military commands, if the authority is your superior, then their authority is not false. For example, argumentum ex cathedra, or, argument from the chair, i.e. the seat of authority, is a valid argument when the authority is valid such as a Papal decree or a judicial ruling.

It's all well and good to "question authority" but to sincerely question authority, one has to be ready to listen to the answer from authority.
There seems to be no established explanation of Wittgenstein's rhinoceros, just speculations. I may as well add one of my own :) (I don't know whether the following explanation has been offered before)

Wittgenstein was http://www.independent.co.uk/life-style/when-ludwig-wittgenstein-met-bertrand-russell-1596995.html reported to have asserted, at the incident, that "nothing empirical is knowable". To me this immediately reminded one specific item of philosophy: David Hume and https://en.wikipedia.org/wiki/Problem_of_induction the problem of induction. Where else, in the history of philosophy, we have such a prominent argument against empirical knowledge in particular? Hume famously and convincingly argued, that all empirical knowledge was based on induction, and that induction had no rational basis. Induction had a merely psychological basis, in the psychology of habit. Hence Wittgenstein's "nothing empirical is knowable".

But how, you might ask yourself, is the problem of induction related to Wittgenstein? Well, it's rather straightforward really. Wittgenstein is endorsing Hume's position on induction (and on causation) in the http://www.kfs.org/jonathan/witt/t6363en.html Tractatus. This happens in the somewhat wild sixth part of the Tractatus, where Wittgenstein applies the philosophical view, which he develops in the previous parts, to a host of philosophical issues. One of his theses there is that there is no necessity except logical necessity. Here is how it goes:


  6.362 What can be described can happen too: and what the law of causality is meant to exclude cannot even be described. 
  
  6.363 The procedure of induction consists in accepting as true the simplest law that can be reconciled with our experiences. 
  
  6.3631 This procedure, however, has no logical justification but only a psychological one. It is clear that there are no grounds for believing that the simplest eventuality will in fact be realized.
  
  6.36311 It is an hypothesis that the sun will rise tomorrow: and this means that we do not know whether it will rise.
  
  6.37 There is no compulsion making one thing happen because another has happened. The only necessity that exists is logical necessity.


This view of Wittgenstein's will provide us with an easy explanation to his rhinoceros assertion. For just as we cannot know, according to Hume and Wittgenstein, whether the sun will rise tomorrow, or not, we cannot know whether there was a rhinoceros on Russell's room, or not. When e.g. Russell looked under one table, it is perfectly possible (that is, logically possible) that the rhinoceros popped up under the other table, and vice versa. The rhinoceros could have been on the ceiling, it could have been very small, it could have been invisible, etc. There would be no way to know otherwise. And so the riddle may be solved.
The thing is, that for the early Wittgenstein the Cogito Ergo Sum was just http://www.kfs.org/jonathan/witt/t563en.html not true. So the Cogito could not be true a priori for him.

Like David Hume, Wittgenstein believed that the Cartesian Ego, the thinking subject, was nowhere to be found.


  5.631 There is no such thing as the subject that thinks or entertains ideas. If I wrote a book called The World as l found it , I should have to include a report on my body, and should have to say which parts were subordinate to my will, and which were not, etc., this being a method of isolating the subject, or rather of showing that in an important sense there is no subject; for it alone could not be mentioned in that book.


The self exists only as an external "limit" of the linguistically constructed world. (Perhaps akin to Kant's Transcendental Subject)


  5.632 The subject does not belong to the world: rather, it is a limit of the world.


And again


  5.641... The philosophical self is not the human being, not the human body, or the human soul, with which psychology deals, but rather the metaphysical subject, the limit of the world — not a part of it.

If to reincarnate I must forget everything I know, in what sence it can be said that "I" am reincarnating? What is left of "me" if all my memories, ideas, feelings, etc., are gone?



If there is something as reincarnation, then, whether or not this is "necessary", total amnesia is the rule; nevermind whether it is possible or impossible to remeber former lives, the fact is that we do not remember them.
It's complicated

But you should probably tell them.

Theoretical underpinnings: Some kinds of utilitarianism

There are two species of utilitarianism: https://plato.stanford.edu/entries/consequentialism-rule/ indirect or rule utilitarianism, which is a species of deontology, and direct or act utilitarianism, which is a species of https://plato.stanford.edu/entries/consequentialism/ consequentialism (I don't agree with the SEP's labels here, but ah well).

In rule utilitarianism, you set out a series of laws, the general observance of which would result in the greatest good for the greatest number of people. So if it were better for everyone if nobody ever stole, you should never steal.

In act utilitarianism, you consider the marginal effect of individual actions. So even if humanity would be better off in the event that we never stole, if stealing this one thing this one time would be better than not stealing it, you should steal it.

Finally there's https://plato.stanford.edu/entries/utilitarianism-history/#IdeUti preference utilitarianism: essentially that you should treat other people as they would like to be treated. This is in fact not in conflict with either of the two forms mentioned above, but it's an important enough school of thought to warrant its own mention.

Arguments for either kind

As long as you are convinced that the source of goodness is human happiness, there are some pretty good reasons to be either a rule or an act utilitarian.

On one hand it's much easier to follow determinate laws than the calculate the effects of individual actions. In fact this is so hard that it may even be the case that humans will generally be happier in a rule-utilitarian society than in an act-utilitarian society. It's rare that we want to only look at the marginal effect of an action, because it's really generally impossible to examine the complete marginal effect of an action. Besides, the effect of allowing people to view their actions solely on the margin is huge; you're essentially saying goodbye to law as such.

On the other hand we might question why we should refrain from doing something that we know will lead to a better outcome right now. Of course we're not really telling other people anything with our decision to be an act utilitarian, unless we in fact tell them that we are being act utilitarians. So we don't set any precedent, because nobody has to find out.

OK, but what should I do?

If you're a rule utilitarian, then you know that lying is bad, so you shouldn't lie. Therefore tell them the truth.

If you're an act utilitarian, you have to ask: will this in fact do more harm than good? For instance, you're suffering right now because of your guilt. And there's an outstanding chance that your spouse will find out that you have cheated anyway and be even more devastated because you have hidden it. If you really don't care about having cheated on them, and you are confident that they won't find out, then, fine, I guess, don't tell them. Those last two statements probably aren't true though. So you should confess.

But! If you are a preference utilitarian, we have a neat solution! Ask your spouse, "Would you rather live a lie or learn some terrible news?" And if they say they would rather learn the news, even if they would be less happy, then tell the truth.
The question is how much well-informed citizens need to be to exercise their democratic rights. The answer depends on theories of democracy. The duty of citizens to be well-informed can be very demanding or not demanding at all. Some theories even require moral duty not to vote. I explain these three views  in the following.

J.S. Mill's theory can be argued to be very demanding. In Considerations on Representative Government, Mill posits the thesis of the inequivalence of the peoples. The thesis states that the people of democracy are intelligent and active while the people of a benevolent despot are vapid and torpid. The thesis then entails that, even if the outcomes of the benevolent despot and democracy are the same in terms of promoting the public good, there is reason to choose democracy. According to Mill, democratic citizens are active and intelligent since, to exercise their rights to vote, the people of democracy will have to study political issues and agenda in search of the public good. For this reason, Mill called democracy the school of public spirit. To ensure this, Mill ponders on public voting that people can see how each others voted. Mill infamously proposed plural voting (that the educated should have more votes than the uneducated) to ensure that voting results reflect the public good.

Many theories of voting are not as demanding as Mill's. Reasons vary, however. I offer two scholarly arguments for the reason. One is to view the democratic decision making process as a machine. That is, by merely implementing universal and uniformly-weighted franchise, correct answer for the public good will rise to the top. The idea is most strongly supported by Condorcet Jury Theorem, which states that, if each voter is more likely to vote correctly, and independently, then the majority will find the correct answer with certainty as the number of voters increases. James Surowiecki popularized this idea with The Wisdom of Crowds. Under this view, individuals can just vote based on what they already know, and there is no need to be well informed. StackExchage and Quora utilize this wisdom of crowd to find the truth (or knowledge).

The other reason is based on non-epistemic nature of democratic decisions. This view used to be popular thanks to the contribution of social choice theories. The premise for this view is that the core of democratic decisions is empty (non-epistemic due to cyclicity). Then why should we vote? Their answer is similar to the way moral anti-realists explain our moral responses: "Hooray" or "Boo." We vote to approve or disapprove policies or representatives. Under this view, voting is like cheering for your favorite football team, and thus unnecessary is being well-informed. A San Diegan would cheer for Chargers however poorly they performed in the games.  

The third view is that it is morally impermissible for uninformed citizens to vote. This view is articulated by Jason Brennan in his The Ethics of Voting. To him, voting is to find the correct answer for public policies and thus ignorance will get in the way in this pursuit of the truth. The argument clearly assumes the correct answer and can be unpersuasive to political skeptics.  
As some of the comments point out, this question seems mostly historical in nature, and arguing over "what-if" scenarios is shaky ground. However there is a possible philosophical reply to it based on the Frankfurt school view of history. 

In the comments you say: "[...] seems like a philosopher may have something to say on barbaric tendencies, how they relate to democracy etc." 

Horkheimer in "The End of Reason" and Adorno and Horkheimer in "Dialectic of Enlightenment" argue that "barbaric tendencies" - or more specifically dictatorships, fascism, etc... - are the logical end result of the Enlightenment doing away with myth and religion and leaving instrumental reason as the only remaining value.  In the "End of Reason", Horkheimer states: 


  Reason, in destroying conceptual fetishes, ultimately destroyed itself. [...]
  
  When even the dictators of today appeal to reason, they mean that they possess the most tanks. They were rational enough to build them; others should be rational enough to yield to them. [...] 
  
  Whoever desires to live among men has to obey their laws—this is what the secular morality of Western civilization comes down to. … Rationality in the form of such obedience swallows up everything, even the freedom to think.


And then, writing with Adorno in "Dialectic of Enlightenment", he states:


  Enlightenment, understood in the widest sense as the advance of
  thought, has always aimed at liberating human beings from fear and
  installing them as masters. Yet the wholly enlightened earth is radiant with
  triumphant calamity. Enlightenment’s program was the disenchantment
  of the world.* It wanted to dispel myths, to overthrow fantasy with knowledge. [...]
  
  What human beings seek to learn from nature is how to use it to dominate wholly both it and human beings. Nothing else counts. Ruthless toward itself, the Enlightenment has eradicated the last remnant of its own self-awareness. Only
  thought which does violence to itself is hard enough to shatter myths. [...]
  
  On their way toward modern science human beings have discarded
  meaning. The concept is replaced by the formula, the cause by rules and
  probability


For Horkheimer and Adorno, "barbaric tendencies" are a consequence of the Enlightenment and the triumph of rational and scientific thinking. Deprived of meaning by the removal of myths and religion, yet provided with awesome technological means through science, human societies have nothing left to do each other than violence and attempts at domination through the tools that reason has provided them with. Reason itself lead to an irrational desire dominate.  

Horkheimer and Adorno's ideas are dense and complex, besides Marxist theory, they include ideas from Freudian psychology, Nietzsche, social commentary, greek mythology, art and even music theory in their approach. 

But I will try to give a simplified summary of their thought with regards to your question: 


The Enlightenment made reason (i.e logic and empirical knowledge, the scientific world view, etc...) the main human values. 
This was supposed to be a good thing, since it freed people from dogmas and superstitions, but instead it had a negative effect. 
Reason slowly destroyed mythology and religion, and all other sources of meaning, until eventually only reason remained. 
Reason by itself cannot provide any meaning, reason is just a tool - in this sense reason "destroys itself" and becomes irrational. 
Modern 20th century man, has all the tools of science and technology, but no longer has any meaning or values, because those have all been destroyed by reason. 
With meaning and values removed by reason, man reverts to his primitive instincts to try to dominate other men and use violence whenever he feels like it, except that now he has all the tools of modern industry and technology to do so. 
This is what leads to oppressive dictatorial states like Hitler's Germany or Stalin's Russia, the holocaust, the gulags, etc...the root cause of this goes all the way back to the Enlightenment, not the Russian revolution or the economic conditions that prevailed in Germany after WWI or anything like that.  


For them, Hitler and Stalin were just too sides of the same coin. In Russia there was a communist revolution, in Germany there was none, yet the end result was the same: Murderous totalitarian states, concentration camps and gulags. 

So to answer your question directly: What happened in Russia would likely have happened anyway, Bolshevik revolution or not, because the same thing happened in Germany, where there was no communist revolution. 

Some things should be noted: 


Horkheimer and Adorno were themselves Marxists who started out as part of the Frankfurt school in the 1920s, which was trying to answer the question: "Why has there not been a communist revolution in Germany yet?" - the subsequent events in the 1930, WWII and the Holocaust lead to their later thoughts on the subject. 
The Frankfurt school were occasionally known as Western Marxists, because they wanted to disassociate themselves from "Eastern" Soviet Marxism, which they saw as violent and dictatorial. For them, there was still some valuable lessons to be learned from Marxism, even after all that had gone down in the U.S.S.R. Marcuse and Habermas are later well known representatives of this school. In the 60s they became known as the New Left. 
Horkheimer talks more about this in his later "Eclipse of Reason", but I haven't read it, so I can't quote it directly.    

There is no logical requirement for negative utilitarianism to support anti-natalism. So if there is something wrong with negative utilitariansm, support for antinatalism isn't it. 

Negative utilitarianism requires that the elimination of pain or at least the causing of the least possible amount of suffering be given priority over the maximisation of happiness or pleasure. It does not require that the injunction to eliminate pain or cause the least possible amount of suffering replace totally the injunction to maximise happiness. 

It is true that if humankind were eliminated, pain (or at least human pain) would be eliminated along with it. Pain would not be eliminated, however, since animal pain would remain.

Three more comments are in place : 


No reason is given for according priority to (a) the elimination of pain over (b) causing of the least possible amount of suffering while still accommodating unpreventable suffering. ('Least possible' implies 'some'.) Where's the argument for this priority ? Very clearly, causing the least possible amount of suffering does not mandate the elimination of humankind since even the smallest amount of suffering requires the existence of humankind if any suffering is to be experienced. 
There remains a requirement, of lower priority, to maximise happiness or pleasure. This cannot be fulfilled, as far as concerns humankind, if humanity is eliminated. The co-existence of the least possible amount of suffering with the maximisation of happiness might produce a better state of affairs on balance than the total absence of both pain and pleasure. If there is an argument against this, I can't see what it is. 
There is a false dilemma in opposing the elimination of suffering to the maximisation of happiness or pleasure. Every utilitarian I know includes the elimination of pain in the requirement to maximise happiness or pleasure. It is obvious why this should be so : neutral states aside the less pain there is, the
more pleasure. 

A person who managed to become rich and therefore happy hasn't bought happiness. "Buying happiness" would mean giving away money in exchange for happiness. This person hasn't been giving away money in exchange for happiness. 

And a person who has only one goal in life wouldn't be automatically considered a happy person when that goal is achieved. They would be considered a person who achieved their goal, that's all. 

Quite the opposite, people who have only one goal in life may be very happy while they are working hard to achieve that goal, and when it is achieved, they have nothing to achieve, no goal, and may be unhappy because the goal in their life is gone. 
I think Philip's answer elsewhere does address this issue and quite well.

To complement that here if you're finding it inadequate. There's three things to consider:

(1) Kant does not provide a very clear explanation of all the rules about what can and cannot be a maxim. Put another way, Kant just starts talking about maxims in Groundwork and there isn't much prior to that which operates from this framework. He doesn't give us rules for them even though universalizing them is central to morality.

(2) There's some recent literature (though going back to at least Allen Wood's earlier work) that suggests that (a) you're not allowed to maxim shop and (b) your maxim should reflect your subjective reason for acting. 

(3) Kant's later works, especially Metaphysics of Morals: Doctrine of Virtue demonstrate some flexibility towards social niceties and mistakes that might apply here (for instance, you don't have to tell someone that they are ugly).  (Here, the main thing is the quodlibetal questions where we seek Kant fielding harder things like -- is sacrificing yourself in battle suicide? or what makes lying wrong (again)?). This almost directly addresses your lying example.

A further consideration is that dealing with a shopkeeper is one of the examples in Groundwork (though there is disagreement whether Kant refers to three or four different accounts of doing so). Another point is that Kant claims that the different formulas are the same CI such that whatever maxim we come up with it should be identical with treating people as ends not means and identical with citizenship in the kingdom of ends.

All of that to say, I think Kant could come up with something that covers most cases along the lines of:


  Always attempt to deal honestly in transacting with others.


Because the negation of this or the loss of this seems to be problematic to universalize (assuming we ignore Hegel's critique that whether or not we have a system of transactions and property is a priori arbitrary).

It does not guarantee success but Kant's morality is never about the guarantee of success. It's about the purity of the act of willing such that its accords with pure reason.

Suggested References


Immanuel Kant, Metaphysical Principles of Virtue in Practical Philosophy ed. Allen Wood
Thomas Hill, Blackwell Guide to Kant's Ethics (see https://books.google.co.jp/books?id=Bqemab0aXx8C&pg=PA57&lpg=PA57&dq=maxim%20shopping%20kant&source=bl&ots=ExBSoajU7y&sig=bfv8DRnp6yN6oNKKNmjOaXm1dXo&hl=en&sa=X&ved=0ahUKEwiDvpfUs_PTAhUKTrwKHRtVCTsQ6AEILzAC#v=onepage&q=maxim%20shopping%20kant&f=false this page)
Allen Wood, Kant's Ethical Theory
Marcia Baron, Kantian Ethics Almost without Apology

This is really a question about English, but I'll try and answer anyway.

In your example, differential means discriminating (not necessarily in a negative sense), that is, applying differently to different things.

In this sense, differential punishment means giving different punishments to different crimes, i.e. punishing differently in different cases, as opposed to giving the same punishment in all cases. I hope you can see that different punishment, by itself, won't do here.

This is crucial for understanding the passage your quoted. It argues for differential punishment in general, and not for punishment that is different from some other punishment.
It would never be rational to be irrational.  In Rational Actor Theory, a rational actor is goal-oriented, reflective and consistent.  If you act irrationally, you will not achieve one of these goals.

However, there is nothing which prevents a player from choosing to act in a way that appears irrational because they perceive a long term benefit to others perceiving that apparent irrationality.  This would qualify as rational thought.  The phrase for this is "there is method to my madness."

The uncertainty about players would be most likely incorporated into the reflective side.  If it is clear that the other players are not being affected by your apparent irrationality, and you do not believe you are going to be able to sell it any better than you already had, then it is irrational to continue acting apparently irrational.  You would either no longer be goal-oriented or you would no longer be reflecting enough to realize that your actions were not accomplishing the goals.
Claiming to know that something is unknowable does not seem to be a paradox straight away, but it can become a paradox if the ground for the unknowability claim is itself umknowable, by its own standard. One famous example from 20th century philosophy is the https://en.wikipedia.org/wiki/Verificationism verification principle of the https://en.wikipedia.org/wiki/Logical_positivists logical positivists. The verification principle asserted that only statements that can be empirically verified have meaning. It was soon suspected, however, that the verification principle itself cannot be empirically verified, and must be therefore meaningless, by its own standard...
Civil society is founded on two notions of justice: absolute justice and proportional justice, a la Aristotle. Absolute justice applies to all members of society for the reason that they meet certain qualification. For instance, any human objects are guaranteed with basic rights for the reason of being human. Proportional justice is founded on the idea of deservingness: those who work hard or are efficient users of their talents should be rewarded more than talent wasters or unincentivized people.

Historically, people have disagreed on the domains of absolute and proportional justice. The right to vote for representatives, for instance, used to belong to the domain of proportional justice (19th century). Many now think the right to a democratic say (voting right) belongs to the domain of absolute justice. The marriage right of same sex people is only recently viewed as the basic right: previously same sex marriage was viewed as an inefficient use of talents (no offspring). 

Naturally, the delineation between basic and non-basic rights is circumstantial, cultural, sociological, political. Now, to answer your question, most present societies view matters relating to knowledge and talent belong to the realm of proportional justice. Thus when universities and businesses treat individuals preferentially based on talent and knowledge, they are not viewed as discriminating people on these bases. 
I think you are referring to a moral dilemma like the one proposed by https://www.unc.edu/courses/2009spring/plcy/240/001/Jim_and_Indians.pdf Bernard Williams:


  Jim finds himself in the central square of a small South American
  town. Tied up against the wall are a row of twenty Indians, most
  terrified, a few defiant, in front of them several armed men in
  uniform. A heavy man in a sweat-stained khaki shirt turns out to be
  the captain in charge and, after a good deal of questioning of Jim
  which establishes that he got there by accident while on a botanical
  expedition, explains that the Indians are a random group of the
  inhabitants who, after recent acts of protest against the government,
  are just about to be killed to remind other possible protestors of the
  advantages of not protesting. However, since Jim is an honoured
  visitor from another land, the captain is happy to offer him a guest’s
  privilege of killing one of the Indians himself. If Jim accepts, then as a
  special mark of the occasion, the other Indians will be let off. Of
  course, if Jim refuses, then there is no special occasion, and Pedro
  here will do what he was about to do when Jim arrived, and kill them
  all. Jim, with some desperate recollection of schoolboy fiction,
  wonders whether if he got hold of a gun, he could hold the captain,
  Pedro and the rest of the soldiers to threat, but it is quite clear from
  the set-up that nothing of the sort is going to work: any attempt at
  that sort of thing will mean that all the Indians will be killed, and
  himself. The men against the wall, and the other villagers understand
  the situation, and are obviously begging him to accept. What should
  he do? 


However, it sounds like your example differs from Williams in that this part is not true:


  Jim, with some desperate recollection of schoolboy fiction, wonders whether if he got hold of a gun, he could hold the captain, Pedro and the rest of the soldiers to threat, but it is quite clear from the set-up that nothing of the sort is going to work: any attempt at that sort of thing will mean that all the Indians will be killed, and himself.


If that's the case then there is no moral dilemma; clearly killing the captain is the optimal solution. The reason that Williams thinks his version is a true dilemma (and one which utilitarianism gets wrong) is because, in his version, you don't have this easy out.
This is well-known in ethics, but not as a flaw of argumentation, rather as the problem of causal resposibility. The problem is thorny because drawing the line depends on resolving highly controversial issues in ethics and metaphysics, free will, attribution of agency, efficacy of proximate vs mediate causes, etc. http://sartorio.arizona.edu/files/cr.pdf Sartorio's Causation  and  Responsibility and https://www.academia.edu/2642530/Chapter_5_Commitment_and_responsibility Del Coral's Social Commitment and Responsibility are recent works that discuss it. 

To see why deciding what does or does not count for responsibility is challenging recall that there are causal chains connecting any event to multiple past actions, by people and not. Where in those chains, and how, do we place the responsibility or blame? Is this placing somehow objective or does it entirely depend on social conventions, context-specific interests, etc.? How much of responsibility/blame goes to various links in the chain? If one accepts causal determinism it is not clear that the blame can be apportioned at all, as Del Coral points out:


  "...by analysing the causes of the agent's actions, we pass the buck backwards and relieve the agent from her responsibility. The buck would stop by showing that the agent acted freely (this is, she could have chosen not to act). Determinism and free will, enter into conflict."


Even if we accept free will or some form of compatibilism, there is no consensus (or even majority) solution to the resposibility apportionment, and therefore there is no sure fire way to resolve even your example. Here is one way, sketched by Valentyne:


  "To be agent-responsible for an outcome, the agent must be causally responsible for the outcome and the outcome must be “suitably reflective” of the agent’s autonomous agency. There is much debate about what exactly determines when an individual is agent-responsible for something, but it’s clear that one can be causally responsible for harm without being agent-responsible for it.


Presumably, your cousin's death is not “suitably reflective” of your intentions for you to be held responsible for it. This reliance on intent generally guides common-sensical and legal assignment of responsibility. But it is not without its pitfalls due to the general obscurity of "intent", and the phenomena like  https://en.wikipedia.org/wiki/Transferred_intent transferred intent,  https://en.wikipedia.org/wiki/Willful_blindness willful ignorance, https://en.wikipedia.org/wiki/Irresistible_impulse irresistable impulse, etc.

And under some ethical positions you do share a portion of the blame, say because you failed to resolve your gambling problem, and knew, or should have known, that it might put people who care about you in harm's way. This would be a case of "absence causation", responsibility for inaction/omission. This notion is problematic even in more straightforward examples than yours, as Sartorio points out:


  "If we were to say that my failure to water a plant that I promised to water is a cause of its death, then we would probably also have to say that the Queen of England’s failure to water the plant is a cause of its death (because it is also true of the Queen of England that, had she watered the plant, the plant would have survived)."


But there are responses to such skepticism. Woodward, for example, exempts agents without a serious opportunity to act, which would exclude the Queen of England. But I am not sure if it entirely rules out the blame in your situation. In the legal system there is a notion of https://en.wikipedia.org/wiki/Felony_murder_rule "felony murder", which classifies accidental killing in the commission of another crime as murder, even when the person did not even physically do the killing (but, say, an accomplice did). It is of course a long way from your scenario, but is there a difference in quality or merely in magnitude? 
I think desire and intensity are Proportional to each other. so focus should be gain and become more steady. it could only happen if you have multiple desire that are non-relative to each other and that leads to confusion.

And your confusion is the factor that dropping your focus. you need to prioritize your desire and than determined your objective so you can gain focus to achieve satisfaction to your desire.

good luck.
I think to deny friendship with nice people is not regarding to morality. may be ignoring someone`s honest effort could be.

if someone if interested in you for friendship and being nice to you than you can give them reason that how are your perspectives are different so your friendship is not could be possible. so in this case you are not ignoring or disrespecting anyone you are just expressing your thoughts so there is nothing wrong on moral basis.
but if you are abusing and disrespecting and arrogantly shouting or badly behave with person than it will be wrong on moral basis.

for me if you doing something to someone that will feel bad if that same someone will do to you than it is wrong. and otherwise there is no issue with morality.
Is it morally wrong to deny friendship with nice people? answer is depends how (by what kind of behavior ) you deny it.

hope that helps.
There seems to be some limited evidence that your second group lead more fulfilling lives. http://www.sciencedirect.com/science/article/pii/S0001879117300039 This meta analysis paper ultimately tests thousands of people on various metrics of happiness and intelligence and the authors conclude that the more intelligent (I'm vaguely linking intelligence with your second group) are marginally more happy by the various metrics included in the papers.

In essence leading a fulfilling life requires that opportunities for fulfilment arise, and that you make a successful plan to obtain them. The first will affect the first and second groups at random, being mediated by socio-economic group, and luck. So the second is the only factor that your modelling behaviour can affect. The paper seems to support the theory that reaching fulfilment is sufficiently complex that extracting it from the opportunities that arise via a carefully thought out plan is only marginally more likely to work than just "winging it".

From an evolutionary perspective, of course, it could be that our intuitions hold more information about how to achieve fulfilment that we give them credit for and so planning and modelling could undermine latent strategies for fulfilment that would otherwise emerge without the "clutter" of all that planning. Many papers, https://www.ncbi.nlm.nih.gov/pubmed/26847844 this one for example, link happiness with remnants of our ancestral needs. Personally, however, I see it as unlikely that we would be able to extract out ancestral needs from a very different modern society without a little planning.

The anthropologist Clive Finlayson divides early humans into "Innovators" and "Conservatives", which I think matches your two groups well. The conservatives benefit from the low calorie requirements of simply copying others in their day-to-day choices, thereby not having to waste time and effort modelling the world and constructing solutions to problems that have already been solved. They lose out, however, if the environment changes and their solutions no longer work. They also lose out if they're in too large a group as the effect of "Chinese whispers" renders some of their copied solutions flawed. The innovators have to contend with the consequences of wasted calories working out solutions to problems which may have already been solved, but benefit in a changing environment.

Society as whole benefits from a mix of the two. Too many innovators and the total resources needed is too large, to many conservatives and the society is not adaptable enough to change. Since it is reasonable to presume that the happiness of each individual is linked to the happiness of the community they are in, it is likely that a proper mix of both groups is best for all regardless of what group they're in. Of course, this only applies to the small communities in our palaeolithic past. I doubt it would work as well in our massive modern communities.
In origin, https://plato.stanford.edu/entries/aristotle-logic/ sllogism was defined with https://en.wikipedia.org/wiki/Categorical_proposition categorical proposition i.e. for proposition like "All men are mortal" with class terms.

The extension to singular terms can be managed considering the https://en.wikipedia.org/wiki/Singleton_pattern "singleton" formed by the class containing the single individual: Socrates. 

In this way, we can translate "Socrates is a man" as "Every member of the class containing only Socrates is a man". 
"I continue to live because I can always die, but not always live" in and of itself doesn't answer the question, but it implies an answer: "I continue to live because I am unsure if I will get to live again once I die." Most people would sum this up as "I continue to live because I'm afraid to die."
A related question would be "have we arrived at deductive principles?"  If we were to start in the context which assumes humans evolved (and we'll hand-wave assume we both agree on what "evolved" means), can we justify the claim that we have arrived at deductive or inductive principles?

Both deduction and induction operate on absolute truth.  Evolution is not actually inductive in that it never fully assumes that "because A worked in the past, A will always work."  It never reaches that level of certainty.  It's always trying new things.

Indeed even the concept of "knowledge," as typically defined in philosophy, is tricky with a fallible evolved brain.  All sorts of philosophical terms require special treatment if we get rid of the foundations those terms are built on.  Instead we might have to talk about the terms as "ideals," which cannot be achieved by an evolved brain but which may have meaning in some "perfect" sense.

It's not that it can't be done.  It just requires a bit more care to avoid these sorts of paradoxical results that arise from assuming we have achieved certain ideals, such as those of truth.
Assuming the "peanut actually causes cancer" is true and the focus is on B:
then B would have used Moral equivalence to misdirect attention from the original "problem" by saying "Pollution also causes cancer. Lots of things cause cancer.", trying to decreasing significance of the original statement. The part "We can't avoid them" is true however followed by "Non sequitur" (unrelated conclusion).

And for the sake of the argument as to the response.
A: Its true lot of things cause cancer, we don't have control over. But we can choose if we eat peanut butter or not. Giving us the change to increase the risk of cancer or not.
Well the whole struggle of the renaissance and the Enlightenment was really over the issue of faith and reason. So your question has a lot of history behind it. The "modern" struggle for reason probably began with the rediscovery of (really transportation of) Aristotle from east to west in the 12th or 13th century. And overall this would be characterized as looking to the world for answers, and not just for empirical answers but for ideas that might spur developments in mathematics also. 

At the end of the day Fred has faith in a book or a tradition, the words and promises of the book or tradition, or he might just have faith in himself. The book, tradition or Fred is set up as the authority.  George challenges Fred's faith-based arguments using reason and Fred says, well you just have faith in reason and you are the same as me. 

I would not call what George has faith, I would call it a justified belief based upon the principles of reason, which combines both the empirical and the rational. 

Now looking at this problem from where we stand in 2017, and making the assumption that Kant's transcendental aesthetic failed, which I think it did, then I think George's real justication will rest in pragmatism. To (boldly) paraphrase William James, George's way of thinking pays better than Fred's does, and by this he means pays in a broad way. Crude, but it gets the point across. (Certainly not all pragmatists are as crude as this, personally I'm not entirely sold on pragmatism, I rather agree with some of the remarks B. Russell made about Dewey. Nevertheless pragmatism is strong today). 

The problem we have is that reason today does not rest on a good foundation (to the extent that matters anymore).  But the age-old fight is against bare authority and superstition, and I believe that still stands.  The definition of reason in the dictionary is good enough for me. I still think a historically informed reason is the way to go. I think a narrow, historically uninformed reason is deadly. Sadly, most of what we have today falls into the latter category. 
Your attitude, is of paramount importance if you want to avoid the "negative" effects of "sharing your achievements" (or anything else). You will never go wrong with an attitude of humility, even to the point of giving the credit for your achievement to somebody else!  You should avoid "coming across" as a conceited person, or one that feels superior to others, because of the achievements.  But, most of all, avoid false humility! 
We aren't civilized enough to set rules about war, we are only civilized enough to punish behaviors that we find repugnant, afterward.  We would like to believe that we have control over the behavior of human beings at their worst.  But obviously, U.S. soldiers in our very most recent war committed severe breaches of the supposed rules of war, e.g. at https://en.wikipedia.org/wiki/Abu_Ghraib_torture_and_prisoner_abuse Abu Ghraib, for which we then had to make reparations.  And those were not even soldiers in combat, making unfortunate decisions due to unbearable pressure.  They were just bored prison guards.

Being able to set accounts into balance after the fact is not the same as having actual control.  The Salic code of https://en.wikipedia.org/wiki/Weregild weregeld did not end revenge killing centuries and centuries ago.  It made it unprofitable for the clans that did it.  But that did not stop anything.  Killing for revenge still goes on, not just in France, but everywhere.  

Rulers do not really have control over their subjects -- no human being really has control over another human being.  We like to think that we have established control over the emotional excesses of our fellow human beings, but this is only a rationalization.

So obviously, if we chose to replace war with something else, real war would happen in an unsanctioned manner anyway, and we would end up having hypocritical clean-up mechanisms that then 'undid' it.  Given the dramatic cost of modern war, and the fact that we still engage in it anyway, it is not clear that an additional 'wrapper' mechanism that made it even more inefficient, would do anything productive at all.
This is a tricky one to answer because the association of youth with life and old with death is not an absolute truth in our own society, much less across multiple societies.

https://i.stack.imgur.com/qX8uTm.jpg  https://i.stack.imgur.com/YdkEWm.jpg 

Accordingly, there will be no answer which definitively pins youth to life and old age with death.  However, there is a strong tendency to make this correlation.  We can discuss why this pattern shows up again and again in cultures, we just won't arrive at the conclusion that it is a universal.

I think one of the most pronounced reasons for associating life with younger age is that one of the defining characteristics of life is its potential.  Things that are living have the potential to to great vibrant things.  A seed has a potential to become a great tree, while the tree is already what it is going to be.

We can see parallels to this idea in the Chinese concept of https://en.wikipedia.org/wiki/Qi#Philosophical_roots xue-qi.  If I may grossly simplify the translations, xue is the Chinese word for blood, and Qi is their word for one's life energy:


  The [morally] noble man guards himself against 3 things. When he is
  young, his xue–qi has not yet stabilized, so he guards himself against
  sexual passion. When he reaches his prime, his xue–qi is not easily
  subdued, so he guards himself against combativeness. When he reaches
  old age, his xue–qi is already depleted, so he guards himself against
  acquisitiveness. - Confucius, Analects, 16:7


In this, we see a pattern of starting with potential and ending with depletion which strongly correlates to the idea of "life" and "death" as you refer to them in your question.

As a general rule, the young are more likely to still have great potential in them, simply due to having more time and having a body that hasn't worn out yet.  There are absolutely counter examples in the world, but the general rule is strong enough to create a general pattern of associating youth with life and old age with death.

As for the idea of life coming from death, as you mention, this does indeed occur in our lives.  It's why we have wakes to celebrate people's lives rather than funerals.  It's why we appreciate what a forest fire does to revitalize a Forrest.  However, these are generally more difficult concepts to work with.  It is much simpler to draw the connection between youth and life.

And finally, the answer may simply be "because it works."  Cultural patterns like this often form because they worked well for many generations.  Perhaps it helped with the grieving process.  Perhaps it spurred youth into action while they still had the bodies to act.  Never forget that cultures are living breathing organisms in and of themselves.
Kant wrote in his first critique:


  Space is not a discursive, or as one says, general concept of relations of things in general, but a pure intuition.


This is simply saying we shouldn't confuse the immediate experience of space with the concepts that we use to talk about it; this actually has been important in both physics and geometry, especially because of the popularity of the Cartesian notion of describing space, where one imposes a system of axes and then gives the coordinates of space; instead, when we look at space we see no cartesian grid, taking this cue leads to the notion of general covariance in physics, and describing geometry intrinsically.


  it follows from this an a priori intuition (which is not empirical) underlies all concepts of space.


He's elaborating here what he means by a pure intuition - it's an 'a priori intuition'. 


  Similarly, geometric propositions, that, for instance in a triangle two sides together are greater than the third, can never be derived from the general concepts of line and triangle, but only from intuition,  and indeed a priori with apodictic certainty (A24-5/B39-40)


This is where Kant opens up the possibility for non-Euclidean geometry; if we exchange the axiom he mentions with a similar one (that is easier to work with, and changes nothing in what Kant wrote): that the angles of a triangle need not add upto 180 degrees; then, if they add up to less, we get hyperbolic geometry, and if they add upto more, we get elliptic geometry.

Gauss was known to have read Kants first critique where this extract is taken from (at least five times, according to one source) then one could conjecture that this - which is talking about geometry, his speciality - opened up for him the possibility of making a definite mathematical model of non-Euclidean geometry. Sometimes in mathematics all one needs is a hint or a cue, and Kant may, and more than likely to, have provided this for him.
The problem of hierarchy of being dates back to Plato, who introduced "becoming" to reconcile the sensible world with Parmenides's prohibition on change in being. In Aristotle, who recognizes forms and matter as aspects of being this leads to the problem of universals, prominent in the medieval scholastics, if they are not Platonic entities what is their mode of being? I will not go over the general http://www.iep.utm.edu/universa problem of universals since the subject is vast and well-covered by encyclopedias. The closest positions to the OP description I can think of are Peirce's theory of entia rationis ("creations of mind", as he also calls them), and Meinong's theory of non-existent objects. Peirce directly studied scholastic doctors involved with the problem of universals, especially Duns Scotus, and so did Meinong's teacher, Brentano, who introduced https://plato.stanford.edu/entries/intentionality/#2 intentionality and intentional inexistence into modern discourse (Husserl was also a student of Brentano's).

According to Peirce, entia rationis are initially introduced by nominalizing predicates into subjects, "the new individual spoken of is ens rationis; that is, its being consists in some other fact". So we pass from "x is red" to "x has redness", this is called nominalization or hypostatic abstraction, and the new "object" fully supervenes on whatever underlies the pre-nominalized facts ("representations", "manifestations", etc.). It is the latter, which determine its mode of being. Frege had a similar notion of abstraction based on his https://plato.stanford.edu/entries/frege context principle, but he did not develop it into a hierarchy of being for abstracta. Peirce uses entia rationis to even partly rehabilitate Molière's famous satire of a scholastic "explanation" that opium puts people to sleep because it has dormitive virtue, "for it does say there is some peculiarity in the opium to which sleep must be due". While it is ridiculous as a final explanation it is exactly the kind of operational that-which definition that sets off a scientific inquiry. At their inception, temperature (that which thermometer measures), weight, voltage, etc., were such hypostatic that-whiches. I'll quote from https://books.google.com/books?id=NyGsVelOwKYC&source=gbs_navlinks_s Peirce's Theory of Signs by Short:


  "These same examples teach us to be cautious about denying reality to an ens rationis. For all of the quantities mentioned – voltage, temperature, and so on – are consequential: all explain a range of effects. And each exists or obtains at a particular place and time (however vaguely these may be defined). Thus they have a physical reality, even if that reality
  ‘consists’ in the reality of certain other facts. Some entia rationis have only the being of a mathematical abstraction;
  others have physical reality; and some entities introduced by hypostatic abstraction are not entia rationis at all, for example, the alkaloid that is opium’s dormitive virtue..."
  
  "If the inference is from true premisses and apodeictic, the introduced entity is an ens rationis, and it will have the same mode of being as those entities represented in the premisses: ideal in the case of pure mathematics, real in the case of empirical science. If the inference is not apodeictic but is abductive, then the entity introduced may fail to be real at all (even though the premisses be true), but, if it is real, it will be as real as the effects in terms of which it is introduced, whether it is an ens rationis or not".


In other words, the process of introducing entia rationis converts indefinite descriptions into objects on which they supervene, the description then may or may not have a further supervenience base in reality, and different kinds of bases are possible. In particular, numbers supervene not only on natural occurences (sticks, grains of sand, etc.), but also on facts of our use of them as tools in counting, measuring, etc., i.e. on artifacts. This brings about the distinction between "physical" and "mathematical" numbers. The former can be empirically repudiated if certain class of real objects fails to conform to arithmetic, but not the latter. As our tools numbers are as "indestructible" as our cultural memory. So Ptolemy's epicyclics converted into a piece of pure geometry is still alive today, and so is Euclidean geometry, their revision as physical theories notwithstanding (Resnik calls such conversion "Euclidean rescue").

What about unicorns? They are also a that-which, that which is like a live horse with a horn. Does this make them akin to numbers in their artifactual role? Indeed it does. The distinction, if any, is to be found in our use of them as tools, on which their being supervenes. Unicorns are loose, poetic tools, their being is accordingly ephemeral, numbers, on the other hand, supervene on facts of a highly regimented practice, now supported by symbolic formalisms of decimals, Peano arithmetic, etc. Hence, facts about them have a much firmer hold on reality, although the difference is of degree and not of kind. And of course numbers do also have physical base of supervenience from which they were abstracted, while unicorns, so far as we know, do not.

Meinong goes even futher than Peirce in diluting being, he allows even inconsistent objects (e.g. round squares) to "subsist", if not to "exist", see https://plato.stanford.edu/entries/nonexistent-objects SEP's Nonexistent Objects.
John Stuart Mill in On liberty will say that what makes a violent speech violent (construed as 'legally violent', that is, a legally impermissible act to do onto others) depends on the context. What makes a violent speech legally violent is a matter relating how or when to legitimately exercise freedom of speech. Mill's answer to this question is the Harm Principle, which states, "The only purpose for which power can be rightfully exercised over any member of a civilized community, against his will, is to prevent harm to others." Stated equivalently, individuals have the freedom to do everything which injures no one else. 

To explain how the Harm Principle works, Mill applied it to the case of the Corn Laws which excited the minds of the 19th century British people. The laws imposed restrictions and tariffs on imported grain, which entailed the enrichment of the landowner class and the starvation of the British laborer class. Mill maintained that if you made a violent speech like, "Corn Laws are evil. They kill people!" in a Parliament session, your speech is an exercise of the freedom of speech. However, if you uttered the same speech in front of a landowner's house with an angry mob, your speech is counted as the legal violence since it could endanger the safety of the landowner's family.

Many legal scholars employ the harm principle to understand the limits of individual freedom and the limits of legitimate forceful interference of the government in the actions of individuals. 
Aristotle refers to this in  Politics. For example, in Book IV he states that a society with a large middle class (i.e. low inequality) is the "best constitution" of society. A quote in extenso below (from http://www.online-literature.com/aristotle/politics/4/ here):


  It is therefore the greatest happiness which the citizens can enjoy to possess a moderate and convenient fortune; for when some possess too much, and others nothing at [1296a] all, the government must either be in the hands of the meanest rabble or else a pure oligarchy; or, from the excesses of both, a tyranny; for this arises from a headstrong democracy or an oligarchy, but very seldom when the members of the community are nearly on an equality with each other. We will assign a reason for this when we come to treat of the alterations which different states are likely to undergo. The middle state is therefore best, as being least liable to those seditions and insurrections which disturb the community; and for the same reason extensive governments are least liable to these inconveniences; for there those in a middle state are very numerous, whereas in small ones it is easy to pass to the two extremes, so as hardly to have any in a medium remaining, but the one half rich, the other poor: and from the same principle it is that democracies are more firmly established and of longer continuance than oligarchies; but even in those when there is a want of a proper number of men of middling fortune, the poor extend their power too far, abuses arise, and the government is soon at an end.


A more thorough analysis of Aristotle and his thought on inequality is https://underground.net/aristotle-and-the-middle-class/ here.
(This answer elaborates a bit on R. Barzell's answer.)  

Charles Mills takes up this question in his book The Racial Contract.  The title of Mills' book refers to a kind of inversion of classical social contract theory.  Classical social contract theory uses a hypothetical, general social contract (involving "everyone," in some sense) to characterize a just society.  Mills' inversion emphasizes actual agreements among small groups of people (namely, powerful white men in Europe and, later, the US) to treat certain other groups of people (namely, indigenous peoples and African or African-American slaves) as inferiors.  For example, at the infamous https://en.wikipedia.org/wiki/Berlin_Conference Berlin Conference in 1884-5, a group of European diplomats gathered together and established the "rules" for the colonization of Africa.  

Mills emphasizes that the idea of a biological hierarchy of races developed at the same time as Enlightenment and nineteenth-century ideas of democracy and equality.  Indeed, white/European supremacy was often defended by the same individuals who argued for democracy and equality.  For one example, Immanuel Kant — one of the most important and influential Enlightenment philosophers — was also http://blogs.umass.edu/afroam391g-shabazz/files/2010/01/Kant-on-the-Different-Races-of-Man1.pdf one of the first people to articulate biologically-based racism.  For another, early in On Liberty, John Stuart Mill writes that


  It is, perhaps, hardly necessary to say that this doctrine is meant to apply only to human beings in the maturity of their faculties. We are not speaking of children, or of young persons below the age which the law may fix as that of manhood or womanhood. Those who are still in a state to require being taken care of by others, must be protected against their own actions as well as against external injury. For the same reason, we may leave out of consideration those backward states of society in which the race itself may be considered as in its nonage.


Note that this means that we can indeed blame philosophy for promoting white supremacy and biologicaly-based racism.  

To reconcile this apparent contradiction, Mills develops the provocative notion of Herrenvolk democracy.  "Herrenvolk" is a German term, associated with Nazism; it's usually translated into English as "master race."  On Mills' view, most European and American political philosophers since the Enlightenment have implicitly assumed that, when we're talking about democracy or justice or equality, we're only talking about white people.  White supremacy is silently assumed as given, fixed, or the background condition for society.  

Finally, let me also recommend https://jacobinmag.com/2017/04/biological-determinism-science-innate-ability-capitalism/ this essay by geneticist Richard Lewontin.  Lewontin argues that notions of biologically-based differences — between sexes and between races — have an important ideological function in liberal political philosophy:  


  The bourgeois revolution succeeded because it was only breaking down artificial barriers, but the remaining inequalities cannot be removed by a further revolution because what is left is the residue of biological differences that are ineradicable. 

In a lot of ways, what you're talking about here is best described by https://en.wikipedia.org/wiki/Game_theory game theory which touches on rule-following and rule-breaking decision making.  There are arguably plenty of examples in real life, but not necessarily any uncontroversial ones.  It's easier to find cases where it's to your personal advantage to break a rule, but not if everyone breaks that rule.  This is known as the free rider problem.  It's generally accepted, however, that being a free rider is unethical.  Conversely, if we're dealing with an unjust law, then it's arguably ethical for everyone to break it.  It's more difficult to find a situation where breaking the rules is ethical for a few, but not for all.

One easy way to do it is situational.  Jaywalking is generally illegal (although not often enforced) for everyone's greater safety.  But no one would blame someone for jaywalking to save someone's life, or to escape danger.  These, however, are just cases of one standard superseding another.  

The best example I can come up with to match your ask is one I'm not entirely sure even I agree with, but it has the right structure.  Suppose country A has a limited immigration process for people from country B.  If everyone from country B broke the rules, and immigrated illegally, it would be bad for everyone, but there might arguably be net positive value for everyone in the fact of some people evading the process. 
Since this is the philosophy stack exchange site, I will answer somewhat from a scholarly philosophical context.  The answer, I think, is that it depends on what philosophical context you choose to think from.

Some possibilities:


  The meaning of life is just to be alive. It is so plain and so obvious and so simple. And yet, everybody rushes around in a great panic as if it were necessary to achieve something beyond themselves. - Alan Watts


So we spend a lot of time worrying about what we ought to do and what we ought to not do - following our current whims and beliefs about what is right and what is wrong.
Driven to act, it is often our ego dictating what we ought do or not do.  If we believe x, are we morally bound to act upon x, and what must we do?
Often acting from this context, we will end up doing more harm than good, and become fixed on our beliefs, certain that what we are doing is right.  One need look no further than Twitter to see how dangerous that can be.




  Act only according to that maxim by which you can at the same time will that it should become a universal law. - Immanuel Kant


Do you believe that your position on abortion is something that you believe should be a universal law?  Do you think it is an unquestionably good position for all?  Does it align with a pre-existing universal truth? (For example, Never Lie).
If so, you are, for the most part, obligated to act in accordance with your position.
Kant is too heavy to capture in a paragraph, but the above isn't a terrible intro.  I think Kant is limited and deliberately ignores realities that he doesn't like, but for the most part, you won't damage yourself following his philosophy.




  What destroys a man more quickly", he asks, "than to work, think, and feel without inner necessity, without any deep personal desire, without pleasure—as a mere automaton of “duty”?"  - Friedrich Nietszche


When obligated to act there is no creative, joyful purpose in one's actions.  Are we not just slaves to some master (internal or external)?  We are bound and thrust into our action, and as it is our duty, we will die defending it.  The people who followed Hitler's dictates often thought they were in the right.  They believed in the cruelty suggested by their master, and believed they had a duty to follow that master.  It is easy to blindly obey under the guise of loyalty.




  Love God and do. what you will - St.  Augustine


As long as your cause is square with your God, then do it joyfully.



Ultimately, I think my position lies somewhere in the middle.  While it is fine to act in accordance with a spiritual guide, and "do what you will", if you want to make a difference, you won't get far that way.  You won't always feel like it, and your emotions will be conflicted, and for myself, the only way I will ever have the discipline to do anything is to set up my actions as a matter of my own integrity, and force myself to act no matter whether I feel like it or not.

But at the same time, the internal pull to a particular cause can be dangerous, because coming from the wrong place, it can harm and ignore others, and even produce unexpected and undesirable results.

I think it is essential to act on that which you choose to act on, not from obligation, but from a place of creativity and honesty.  For example, it is not, "I think x about abortion, so I must do y".   It is more like, "Given I think x about abortion, what could I do that would make a difference?  What is the intention behind my opinion? Do I think that all people should be treated equally?  Or is it that I want people to experience being taken care of?  What can I do to fulfill on that?  Do I want to do anything?  Does it light me up?"

It is essential to examine ones own motives.  As Werner Erhard said, (paraphrasing):


  I am distrustful of all motives, especially my own.

Interestingly, this sounds similar to a thought experiment I've developed to explain some concepts around machine awareness. To summarise;

If you were able to put all your memories and experience on a hard disk, would that HDD be aware? No, because it's a static collection of data that does not have any interactions with the external world.

If (on the other hand) you put a new born baby in a sensory deprivation chamber so that it has no contact at all with the outside world (we're assuming it's fed intravenously) and leave it there for 20 yrs, is the resultant human sentient?

The answer is no. The resultant human is conscious, but possesses no information upon which it can build the concepts of meaning that we take for granted as humans. In that sense, sentience (I'm using this word because I use 'awareness' in very specific ways in this topic) has to be a combination of consciousness (an 'always on' ability to sense new information and integrate it with existing internal concepts) and those same internal concepts that build up over years of development.

In our early development as children, we learn language, mathematics and several other structured concepts for categorising the sensory inputs we receive and building an internal ontology of meaning from it all. It's not until about the age of 2 (in normal development) that a child begins to see him or her self as separate from the parents; that sense of self awareness is not 'programmed' into the human brain. Also, unless you've seen death or had it explained to you, how would you know it exists?

Based on these aspects, if a human child does not have the ability to interact with its surroundings in any form, the sense of self and the understanding of death are unlikely to have formed.
'Rich' and 'poor' are mutually exclusive (one can't be both at the same time in the same respect). But one doesn't have to be one or the other - one can have a median position in respect to wealth. That's to say, neither rich nor poor but in the middle of the wealth distribution. 

@Alessio Eberl. I don't think you need to prove that there is actually such a group of people as the medians. Conceptually it's enough to prove that logically there could be. 'Rich' and 'poor' are https://en.m.wikipedia.org/wiki/Opposite_(semantics)#Gradable_antonyms contraries, not https://en.m.wikipedia.org/wiki/Opposite_(semantics)#Complementary_antonyms contradictories. If one is not rich it doesn't mean one is poor; there's at least a third possibility, namely that one is of median wealth. (Whether medians exist is an economic question, nothing to do with conceptual analysis.)  Why assume that rich/ poor is a dichotomy ?  'True' and 'false' are dichotomies if one assumes bivalence - mutually exclusive and jointly exhaustive. 'Rich' and 'poor' are nothing like this, just points on a continuum : rich at one end, poor at the other and other states of wealth in-between.                   
The work of Paul Ekman on https://en.wikipedia.org/wiki/Paul_Ekman#Visual_depictions_of_facial_actions_for_studying_emotion depictions of facial actions for studying emotion was the gold standard for many years, and lent a lot of credence to the hypothesis that human expressions were universal, regardless of culture. Recently, the work of Lisa Feldman Barrett (et al), has upended this hypothesis,  causing her to state in a https://www.theguardian.com/lifeandstyle/2017/mar/26/why-our-emotions-are-cultural-not-hardwired-at-birth Guardian article that: "Research has not revealed a consistent, physical fingerprint for even a single emotion." A general take on how she came to this conclusion is laid out in the article https://www.chronicle.com/article/the-secret-history-of-emotions/239357 The Secret History Of Emotions, and links to many of her academic articles and other pieces on the web are available https://lisafeldmanbarrett.com/articles/ from her website. Basically, her experiments and meta-analyses of the literature have shown that the classical view of emotions are a fiction.

What does this mean for the film industries? Not much, for the moment. As our perceptions of emotions are part and parcel of a social construct, and have been for so long that they appear to be intuitive, the construct itself would have to suffer quite a blow, and social behavior (and knowledge of what emotions actually are) would have to be altered drastically for it to effect the film industry in any meaningful way. We should really feel sympathy towards the taxpayer, more than the moviegoer. That "the U.S. Transportation Security Administration spent nearly a billion dollars training airport security agents to detect signs of deception in airline passengers"? Ouch.
You are correct that syllogistic, which corresponds to monadic predicate calculus in modern terms, is insufficient for doing mathematics. Modern formalisms use polyadic calculus. However, Euclid does not use syllogistic alone (in fact, he does not use it at all), but rather synthetic construction, which makes up for the lack of polyadic (many-place) predicates and quantifiers. Kant, like Locke before him, saw that analytic, i.e. derivable in syllogistic, consequences were insufficient to prove even theorems of Euclidean geometry, let alone calculus, so he introduced synthetic a priori constructions to explain how non-trivial mathematics was possible. But the motivating distinction between "logical" (analytic) and "geometric" (synthetic) arguments in Euclidean geometry predates even Euclid himself, and occurs already in Aristotle, who went as far as to say that all thinking requires building images (phantasma) in De Memoria et Reminiscentia:


  "An account has already been given of imagination in the discussion of the soul, and it is not possible to think without an image. For the same effect occurs in thinking as in proving by means of diagrams. For in the latter case, though we do not make any use of the fact that the size of the triangle is determinate, we none the less draw it determinate with respect to size. [quoted from https://www.jstor.org/stable/41134289 Euclid's Pseudaria by Acerbi.] 


The difference in logical strength is discussed at length in https://www.jstor.org/stable/2185244 Friedman's Kant's Theory of Geometry:


  "Our distinction between pure and applied geometry goes hand in hand with our understanding of logic, and this understanding simply did not exist before 1879 when Frege's Begriffsschrift appeared... Euclid's axioms do not imply Euclid's theorems by logic alone. Moreover, once we remember that Euclid's axioms are not the axioms used in modern formulations... it is easy to see that the claim in question is perfectly correct. For our logic, unlike Kant's, is polyadic rather than monadic (syllogistic); and our axioms for Euclidean geometry are strikingly different from Euclid's in containing an explicit, and essentially polyadic, theory of order. The general point can be put as follows. A central difference between monadic logic and full polyadic logic is that the latter can generate an infinity of objects while the former cannot.
  
  [...] Does this... show that Euclid's axiomatization is hopelessly "defective"? I think not. Rather, it underscores the fact that Euclid's system is not an axiomatic theory in our sense at all. Specifically, the existence of the necessary points is not logically deduced from appropriate existential axioms. Since the set of such points is of course infinite, this procedure could not possibly work in a monadic (syllogistic) context. Instead, Euclid generates the necessary points by a definite process of construction: the procedure of construction with straightedge and compass."


After the onset of modern logic the issue of analytic vs synthetic got reformatted. Frege and Peirce, for instance criticized Kant for holding that mathematics is synthetic or for defining "analytic" too narrowly, their notion of analytic was of course much stronger than his because of much stronger logic, and on it classical mathematics is indeed analytic. Interestingly enough, while Frege thought that this made construction wholly unnecessary according to Peirce modern logic simply codifies it:


  "But neither Kant nor the scholastics provide for the fact that an indefinitely complicated proposition, very far from obvious, may often be deduced by mathematical reasoning, or necessary deduction, by the logic of relatives, from a definition of the utmost simplicity, without assuming any hypothesis whatever (indeed, such assumption could only render the proposition deduced simpler); and this may contain many notions not explicit in the definition.
  
  [...] But Kant, owing to the slight development which formal logic had received in his time, and especially owing to his total ignorance of the logic of relatives, which throws a brilliant light upon the whole of logic, fell into error in supposing that mathematical and philosophical necessary reasoning are distinguished by the circumstance that the former uses constructions. This is not true. All necessary reasoning whatsoever proceeds by constructions; and the only difference between mathematical and philosophical necessary deductions is that the latter are so excessively simple that the construction attracts no attention and is overlooked.

@MauroAllegranza gets at why we need second-order quantification, let me say a bit about why some want this to be interpreted as introducing plural quantification.

The main motivation for proponents of plural logic — why they want plural quantification instead of singular quantification over sets or properties — is ontological/conceptual. Their thought is that representing, e.g., the cereal on your spoon as a set distorts what’s intuitively the case: you’re eating bits of cereal, not a set of cereal.

There’s also a dialectical component. Insofar as second-order quantification involves quantification over sets or properties it has ontological commitments — sets or properties have to exist. Some, like Quine, have thought that logic must be “ontologically neutral” and dismissed second-order logic as “set theory in sheep’s clothing”. Part of Boolos’s aim in introducing the plural interpretation was to get rid of these extra commitments. “Pluralities” are not (meant to be) additional objects over and above their constituents, whereas sets are.

Finally, some have hoped that a plural account of quantification could be use to avoid set-theoretic paradoxes that result from quantification over all sets if the domain of quantification is itself a set. On this question, see the https://plato.stanford.edu/entries/plural-quant/#SetTheo subsection on Set Theory at the SEP article on Plural Quantification. Boolos originally mentioned this idea in passing in his "To be is to be the value of a variable (or some values of some variables)". The main proponent of this view is Timothy Williamson, who staked out the position in his 2003 article http://www.jstor.org/stable/3840881 "Everything". (He has expanded on the view in subsequent works as well, such as his book Modal Logic as Metaphysics.) 

Øystein Linnebo was an early critic of Boolos's suggestion, notably his http://www.jstor.org/stable/3506205 "Plural Quantification Exposed" (2003). The objections can be framed in a variety of ways, Linnebo attacks the supposed ontological innocence of plural quantification. The crux of his criticism is the question of the coherence of higher-order plural quantification -- plural quantification over pluralities. He argues that a "full semantics" for plural quantification, like full second-order semantics more generally, needs a determinate notion of "arbitrary sub-plurality" (like arbitrary subset). It's well known that, in first-order ZFC, the powerset operation is not absolute and admits of varied interpretations (hence why you can establish the independence of the Continuum Hypothesis). If the Axiom of Constructibility is true, then V=L and "arbitrary subset" coincides with the constructible sets. But it's also consistent with ZFC that there are many more sets than the constructible sets -- in fact, that V != L seems to be the more prevalent view among those who think there's a unique universe of sets. Linnebo argues that the plural logician has given no reason to think "sub-plurality" is any more absolute/determinate. Essentially, the plural comprehension axiom is no more trivial than set/class comprehension and cannot be assumed to be ontologically innocent.

More generally, it might be worried whether the plurality/class distinction is one without a difference. Simply saying that a plurality isn't one thing isn't very reassuring considering that we can and do use singular terms and descriptions to refer to them -- cf. "the universe of sets". Even if you were to grant that maybe class-talk was best understood as plural-talk all along, there remains the fact that the orthodox ZFC-ist denies the existence of proper classes. They are a mere façon de parler, a view articulated by Allen Hazen in his 2012 "Reflections on Counterpart Theory" where he says that the proper ZFC-ist attitude towards proper classes is a sort of fictionalism where "esse est definiri" (to be is to be defined; a play on Berkeley's Idealist principle "esse est percipi","to be is to be perceived"). Such a view would place sharp limitations on the power of plural comprehension principle, essentially restricting you to a predicative plural logic. That predicative restrictions are needed is more or less explicitly conceded by Williamson. This just amplifies the need for a clear comprehension principle and clarification of the expressive power of plural logic -- the intuitive picture won't suffice.

In short: it gets complicated quickly and this is still an area of active research and controversy. I think most people who don't reject plural quantification as unintelligible -- which isn't an unpopular position, particularly among the older generation of philosophers who grew up with set-theory as the gold standard for regimentation -- find some intuitive appeal to the idea that domains of quantification could just be pluralities of the values of the variables. But while the literature on the topic presses the need for a precise comprehension principle, it seems that's where opinions diverge substantially -- to the extent that there are firmly held opinions. For some (disclaimer: I'm one of these folks), the lesson to take from various "paradoxes" like those generated by the Löwenheim-Skolem theorems (among others) is that our efforts at formalization are doomed to failure -- http://www.columbia.edu/~hg17/nonstandard-02-16-04-cls.pdf Haim Gaifman has a good paper on the significance of non-standard models, which explores these issues. While we can speak of "everything", any attempt to provide a well-behaved regimentation of this talk (particularly any recursive axiomatization) will fall short in one way or another.
▻

First question : When Socrates and Meno discuss virtue (arete) what Socrates has in mind are the major, high-level virtues such as wisdom (sophia), courage (andreia), temperance or self-control (sophrosyne), justice (dikaiosyne) and piety (hosion). These are non-gendered. If child-bearing were to be virtuous, then it would probably fall under justice as something due from a woman to the city (polis). Equally arms-bearing in Greek warfare might be a masculine activity but it would fall under courage. Women and men might fulfil justice and courage in different ways but there are no separate, gendered major virtues. Justice is justice and courage is courage for woman and man alike. 

It is true that Meno produces a 'swarm of virtues' - virtues for women, children, slaves, the elderly (Meno : 71e - 72a) - but Socrates dismisses this list. He doesn't want a list of virtues and does not endorse this list. He wants to know the nature of virtue. What exactly is it ? What is its essence, its essential nature, whenever it occurs and whoever possesses it ? 

It is questionable whether the various virtues, even if we confine ourselves to those recognised by Socrates and Plato, share a common nature, a real essence, by reason of which they are all virtues. Ask if you want anything further on this.

▻

Second question : On your other point, that of no person's desiring evil, a central point is the status of virtue as knowledge. Knowledge here means knowledge of excellence, of what is best for one as a human being. Socrates seems to make the intellectualist assumption that if one knows that X is better than Y then one will prefer X to Y; and if one prefers X to Y then one will choose X rather than Y. This is dubious for all the reasons you suggest. Even if one does know that X is better than Y as a matter of moral fact and as a product of knowledge or rationality, there can be and are non-rational motivations, appetites and passions, desires, emotions, which can neutralise or over-ride that knowledge in situations for action. The alcoholic may know that staying sober is better for her or him than downing another litre of vodka yet be so fixated by the desire to drink and sink into temporary oblivion that down the drink goes anyway. Aristotle is much more sophisticated and nuanced than the Platonic Socrates in Nicomachean Ethics, VII.  

  Many men sneer at virtue, because it makes vice uncomfortable. (Venerable Fulton J. Sheen)


If regarded as a  true statement, the suggestion is that virtue and vice are mutually exclusive, leading to the conclusion that virtuous relationships are free of vice.


  Aristotle defines moral virtue as a disposition to behave in the right manner and as a mean between extremes of deficiency and excess, which are vices. http://www.sparknotes.com/philosophy/aristotle/section8.rhtml http://www.sparknotes.com/philosophy/aristotle/section8.rhtml


https://www.cwu.edu/~warren/Unit1/aristotles_virtues_and_vices.htm https://www.cwu.edu/~warren/Unit1/aristotles_virtues_and_vices.htm

ARISTOTLE'S ETHICS 
TABLE OF VIRTUES AND VICES (linked above) provides a handy reference guide for identifying virtue in various spheres of experience, including that of social conduct, which lists "friendliness" as the key ingredient for a virtuous relationship.

However, limiting social behavior to "friendliness", while recommendable as a general rule, isn't quite enough for maintaining lasting friendships with people whose company you really enjoy. Or for general diplomacy, either.

To form real friendships of virtue, or have virtuous friends -- you must practice impeccable virtue in all spheres of being, feeling and action. In other words, you must be virtuous.

Being virtuous involves excluding vice from your life. It involves spending more time in quiet, solitary contemplation.
The context is Stevenson's 27 August 1952 address to the American Legion Convention :

http://www.adlaitoday.org/articles/think3_patriotism_08-27-52.pdf http://www.adlaitoday.org/articles/think3_patriotism_08-27-52.pdf

'We talk a great deal about patriotism. What do we mean by patriotism in the context of our times? I venture to suggest that what we mean is a sense of national responsibility which will enable America to remain master of her power—to walk with it in serenity and wisdom, with self-respect and the respect to all mankind; a patriotism that puts country ahead of self; a patriotism which is not short, frenzied outbursts of emotion, but the tranquil and steady dedication of a lifetime. The dedication of a lifetime — these are words that are easy to utter, but this is a mighty assignment. For it is often easier to fight
for principles than to live up to them.'

The context doesn't seem totally to clarify the final sentence. The best sense I can make of it can perhaps be illustrated most readily by an example. In the 1950s and 1960s there was a concerted effort in the US at least at a legislative level to secure the principle of equal rights for black citizens. There was a real struggle to secure this principle, which met with hard resistance particularly in states such as Mississippi. But eventually a raft of legislation went through. The principle had been fought for but securing it in practice - getting US society and the political system to live up to the principle of equal rights - proved a much more difficult task. One in fact that still remains to a very significant extent unaccomplished.

Perhaps others can throw more light but this is as clear as I can make Stevenson's quote. He has a powerful point. 
Self-ownership is sometimes considered the fundamental commitment of political libertarianism.  For example, consider the opening sentences of https://plato.stanford.edu/entries/libertarianism/ the SEPH article on "Libertarianism": 


  In the most general sense, libertarianism is a political philosophy that affirms the rights of individuals to liberty, to acquire, keep, and exchange their holdings, and considers the protection of individual rights the primary role for the state. This entry is on libertarianism in the narrower sense of the moral view that agents initially fully own themselves and have certain moral powers to acquire property rights in external things. [emphasis added]


It's controversial whether self-ownership implies that you have the right to sell yourself into slavery.  Usually this possibility is treated as an objection to libertarianism.  (Search in the SEPh article for "voluntary enslavement.")  I seem to recall that Nozick had some cryptic remarks in Anarchy, State, and Utopia that suggest but don't state outright that you do have the right to sell yourself into slavery. 

Libertarians generally agree that prostitution is morally permissible and should be legal.  

Two non-libertarian discussions of self-ownership are worth mentioning.  In the introduction to Self-ownership, Freedom, and Equality, G.A. Cohen observes that self-ownership seems to be a central commitment of both libertarianism and the kind of Marxism he was raised with in working-class Montréal.  Over the course of the book, he critically engages with Robert Nozick's version of libertarianism, and ultimately rejects self-ownership.  

In Justice, Gender, and the Family, Susan Moller Okin presents a deep feminist critique of libertarianism.  Okin's argument has been neglected in the literature; I wrote https://drive.google.com/open?id=0B6oYmzobonqoUC1qZUc5MVJfQ1U a comprehensive review of replies to Okin, with responses to those replies a few years ago.  (I discussed every reply to Okin that I could find, except for one very technical reply that had to be cut due to word limits.)  Specifically, I argued that Okin's argument creates a fundamental dilemma for libertarianism:  either people do not own themselves or justice requires at least some involuntary transfers of property.  

  According to the first admendment [sic] in US, an individual has the right to say anything he wants


According to the first amendment, the US Congress does not have the right to abridge freedom of speech. It explicitly does not say that individuals have the right to say what they want. Also, it has never been taken to mean that and a number of exemptions have been determined by the US Supreme Court over the years.


  This also implies that he has the freedom to offend whoever he wants


No it doesn't although it's probably fair to say that most attempts to legislate against giving offence have been struck down. 


  Doesnt [sic] this spread hate and tension between people?


No. One can argue that a different interpretation than is current may reduce the amount of public hate speech. Given that a number of countries do make hate speech illegal, it's not entirely obvious that this has led to less hate and tension.
No.

See https://plato.stanford.edu/archives/sum2015/entries/plato-aesthetics/#Pha Plato's Phaedrus:


  Madness comes in two general forms: the diseased state of mental dysfunction, and a divergence from ordinary rationality that a god sometimes brings (see 265a–b). Divine madness in turn takes different forms: love, Dionysian frenzy, oracular prophecy, and poetic composition (244b–245a). In all four cases the possessed or inspired person (enthousiazôn: 241e, 249e, 253a, 263d) can accomplish what is impossible for someone in a sane state. All four cases are associated with particular deities and traditionally honored.
  
  The madness of the Phaedrus is separated from ordinary madness as the Ion's version is not, and pointedly called a good derangement. 


See Phaedrus, http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0174%3Atext%3DPhaedrus%3Asection%3D244a 244a-on.
"What defines us [humans] is primarily the attempt and desire to escape mortality and not the ability to use reason to escape it".

You can see: Paul Gorner, https://books.google.it/books?id=GK6GJTR816IC&printsec=frontcover Heidegger's Being and Time: An Introduction, Cambridge UP (2007), page 125-on:


  Death is a possibility of being. This plainly does not mean that death is something merely possible. Death is certain though the moment when it comes is indeterminate.
  
  One dies. Death is levelled down to an occurrencewhich affects, befalls,
  concerns Dasein – but nobody in particular. This attitude to death which dominates everyday being-with-one-another is an evasion of death which conceals it as a possibility of my being, a possibility which is most my own, is non-relational and unsurpassable.
  
  Such concealing evasion can also be seen in the way we comport ourselves to the dying. We try to persuade them that they are not going to die and will soon return to the familiar world of their concern. In thus seeking to console the dying person we are helping them to conceal from themselves the possibility of being that is most their own. And in helping to conceal death from the dying person we are also seeking to conceal it from ourselves.

Smith is writing in the context of a capitalist market economy. He makes three major claims : 

▻ THE GENERAL ECONOMIC CLAIM

The basic idea is that if each individual pursues his or her own interest, without regard to the public interest, the 'invisible hand' of the free market will promote the public interest. This appears a paradox. Let's see how Smith tries to remove the paradox : 


  'As every individual, therefore, endeavours as much as he can both to employ his capital in the support of domestic industry, and so to direct that industry that its produce may be of the greatest value, every individual necessarily labours to render the annual revenue of the society as great as he can. He generally, 
  indeed, neither intends to promote the public interest, nor knows how much he is promoting it. By preferring the support of domestic to that of foreign industry, he intends only his own security; and by directing that industry in such a manner as its produce may be the greatest value, he intends only his own gain, and he is in this, as in many other cases, led by an invisible hand to promote an end which was no part of his intention.' (Wealth of Nations, 1776, IV.ii.9)


So, for instance, if I pursue only my own economic interests, investing my capital in a free market, I will prosper only if I produce goods and services that consumers want. I reap the economic rewards and this is all that matters to me but the public interest is served. I have to employ my capital efficiently in order to make a profit; this is in the public interest as much as the resulting profits are in my own. The 'invisible hand' is just the mechanism of a free market. 

Smith can also be less abstract and more homely in talking about the promotion of the public interest by the pursuit of self-interest: 


  'It is not from the benevolence of the butcher, the brewer, or the baker, that we expect our dinner, but from their regard to their own interest. We address ourselves, not to their humanity but to their self-love, and never talk to them of our own necessities but of their advantages. (WN I.ii.2)


In a free market the butcher, the brewer and the baker are business from self-interest but they will fail unless they provide quality goods at competitive prices - this promotes the public interest.

I am not presenting the situation in a sophisticated economy but trying simply to convey Smith's basic point. We'll see next how he complicates the picture. 

▻ EXCEPTIONS TO THE GENERAL ECONOMIC CLAIM

Smith distinguishes three economic classes : (a) labourers, (b) landlords, and (c) manufacturers and merchants. He is doubtful whether the invisible hand argument works very effectively in the case of manufacturers and merchants : 


  'The interests of this third order [those who live by profit], therefore, has not the connection with the general interest of society as that of the other two [labourers and landlords]. . . . The proposal of any new law or regulation of commerce which comes from this order [those who live by profit], ought always to be listened to with great precaution, and ought never to be adopted till after
  having been long and carefully examined, not only with the most scrupulous, but with the most suspicious attention. It comes from an order of men, whose interest is never exactly the same with that of the publick, who have generally an interest to deceive and even to oppress the publick, and who accordingly have, upon many occasions, both deceived and oppressed it. (WN I.xi.)


Smith offers an explanation for why the self-interest/ public interest equation readily fails in the case of merchants and manufacturers but to go into this would take us into economic analysis. The main text is WN I.xi. Suffice to say that for whatever reason Smith does not think there is an automatic interlock between the pursuit of self-interest and the promotion of the public interest across the piece. 

▻ THE MORALITY OF SELF-INTEREST

Smith does not promote a morality of self-interest. He does not endorse the pursuit of self-interest in general. Self-interest serves a particular purpose in the context of economic activity, and even there only under free market conditions and with a cautious eye kept on merchants and manufacturers. The pursuit of self-interest is morally neutral. Smith invokes a doctrine of 'natural liberty' to block any automatic right of the state or society to interfere with it as such. But his view of the moral life as set out in 'The Theory of Moral Sentiments'. 

Smith regarded viewed sympathy as the foundation of virtue. In his ethical theory he is not concerned with the promotion of the public interest. The virtues are to be pursued for their own sake; and the central moral virtues derive from sympathy (a concept he took over and re-worked from David Hume). Sympathy is the root of beneficence (altruism or concern for the interests of others), self-command ('propriety') and justice. Where his economic theory uses the metaphor of the invisible hand, his ethical theory employs that of the impartial spectator : 


  'We either approve or disapprove of the conduct of another man according as we feel that, when we bring his case home to ourselves, we either can or cannot entirely sympathize with the sentiments and motives which directed it. And, in the same manner, we either approve or disapprove of our own conduct, according as we feel that, when we place ourselves in the situation of another man, and view it, as it were, with his eyes and from his station, we either can or cannot entirely enter into and sympathize with the sentiments and motives which influenced it. We can never survey our own sentiments and motives, we can never form any judgment concerning them; unless we remove ourselves, as it were, from our own natural station, and endeavour to view them as at a certain distance from us... We endeavour to examine our own conduct as we imagine any other fair and impartial spectator would examine it. If, upon placing ourselves in his situation, we thoroughly enter into all the passions and motives which influenced it, we approve of it, by sympathy with the approbation of this supposed equitable judge. If otherwise, we enter into his disapprobation, and condemn it.' (Smith, TMS, 1759, III.1.2).


It is clear that there is no necessary coincidence between self-interest and the judgements of the impartial spectator. Whatever role Smith assigns to self-interest in the economic realm, it is not self-interest but sympathy adjudicated by the impartial spectator that dominates - to an extent does, and totally should - in the moral life.
At the present time, we do know not 2, but 4 kinds of physical interaction forces:


Gravity 
Electromagnetism 
Weak interaction (explaining phenomena from radioactivity) 
Strong interaction (explaining phenomena from particle physics)


The latter three kinds of interaction have been successfully unified within the standard model by a common type of mathematical formalism (gauge theory). The same formalism could not be extended to include gravity.

Therefore we now have a dichotomy between on one hand quantum field theory which unifies the latter three kinds of interaction. And on the other hand the theory of relativity which covers gravity. Both theories follow quite a distinct paradigm.

During its early phase our cosmos was a world with extremely high energy enclosed in an extremely small domain of space. There was the very active interaction of particles and radiation in a highly curved spacetime. These phenomena cannot be explained by applying the theory of relativity and quantum field theory in separation.
We need a unified theory, a theory of quantum gravity. Such theory does not exist up to now. The two main candidates for a theory of quantum gravity are loop quantum gravity and string theory.
First, not all species are capable of breeding. How are we then to distinguish different species of bacteria?

Secondly, what about a continuum of mammals, of which each animal can breed with the animals close to it in the continuum, but not with others farther off? Where are we then to set the boundary?

For example, I could hypothetically breed with all of my female ancestors down to a certain time in the past, but not beyond. Should that be the point in time when my ancestors became a different species? But my ancestor X, who lived just after that time, could breed both with me and with some of her ancestors, down to our ancestor Y, etc. If all my ancestors are one species, I should be able to breed with Y's sister, which I cannot. If they are not all one species, the boundary of my species must lie somewhere between X and Y—but then, given the above, X and Y would be able to breed across species. This leads to contradictions.
This may be well be one way to state what phenomenology does; although not an explicit return-to-the-Greeks, Husserl and other phenomenologists are definitely engaged with the first person study of the structures of qualia (and more generally of consciousness and intentionality.) There is a decidedly "inward" or subjective bent to these analyses:


  Basically, phenomenology studies the structure of various types of experience ranging from perception, thought, memory, imagination, emotion, desire, and volition to bodily awareness, embodied action, and social activity, including linguistic activity. The structure of these forms of experience typically involves what Husserl called “intentionality”, that is, the directedness of experience toward things in the world, the property of consciousness that it is a consciousness of or about something. According to classical Husserlian phenomenology, our experience is directed toward — represents or “intends” — things only through particular concepts, thoughts, ideas, images, etc. These make up the meaning or content of a given experience, and are distinct from the things they present or mean.


(From the http://plato.stanford.edu/entries/phenomenology/ Stanford Encyclopedia of Philosophy entry on Phenomenology.)
Singer outlines his claim in a little more depth in the next paragraph:


  Of course, it would be impossible to get agreement on universal sterilization, but just imagine that we could. Then is there anything wrong with this scenario? Even if we take a less pessimistic view of human existence than Benatar, we could still defend it, because it makes us better off — for one thing, we can get rid of all that guilt about what we are doing to future generations — and it doesn’t make anyone worse off, because there won’t be anyone else to be worse off.


My sense is that this is mainly a rhetorical flourish to provoke thought as he ends up arguing life is indeed worth living after all in his conclusion:


  I do think it would be wrong to choose the non-sentient universe. In my judgment, for most people, life is worth living. Even if that is not yet the case, I am enough of an optimist to believe that, should humans survive for another century or two, we will learn from our past mistakes and bring about a world in which there is far less suffering than there is now. But justifying that choice forces us to reconsider the deep issues with which I began. Is life worth living? Are the interests of a future child a reason for bringing that child into existence? And is the continuance of our species justifiable in the face of our knowledge that it will certainly bring suffering to innocent future human beings?

So there seem to be two major ways to address this question.

First, if one takes the definitions as proposed above, the truth of the statements seem entirely tautological. If happiness is that thing that every action strives toward, then by definition, every action is motivated by a desire for happiness.

Alternatively, if one defines happiness without directly referring to it as the intended end of other actions, then a more interesting project can proceed. 
For example, some evolutionary psychologists might suggest that an increase in the likelihood of the gene is the motivational force behind many (if not every) action. If one identified happiness as the successful reproduction of one's genetic code, and these evolutionary psychologists were correct (which, in my opinion is questionable given the known influence of genetic drift on phenotypes such as behavior), then again this claim is likely true but in a much less uninteresting way.

If you wanted to disprove it, however, this would also require defining happiness. For example @Keith statements above are only true if biological need is not a subset of "happiness". If happiness is merely "the motivation of every man['s actions]", then the tautology still holds and if @Keith's statements are true, then either biological need should be thought of a subset of happiness, or a contradiction is reached.
It's a "http://en.wikipedia.org/wiki/Catch-22_%28logic%29 Catch-22" http://en.wikipedia.org/wiki/Paradox paradox!

The Catch-22 paradox is that behaving a certain way because of some desire is a sure way to not get what you desire.  If you want to be declared insane in order to be discharged, you can't act insane as only sane men can act insane.  But if you don't exhibit signs of being insane, you will not be discharged.  The same analysis works for the witch paradox.

One point of Catch-22 is to illuminate the dysfunction of the US Army and bureaucracy in general.  The continual use of paradox accomplished that goal in my opinion.

The point isn't to prove the original statement, but to demonstrate a particular type of absurdity.  Another example that I've seen professionally is that in order to get a job that requires a http://en.wikipedia.org/wiki/Security_clearance security clearance, you need to already have clearance, but you can't get clearance until you have a job that requires it.  In the entertainment industry, blockbuster movies require a lead actor who is a "name", but the only way to become a "name" is to star in a blockbuster movie.  The usual solution to these conundrums is to fudge the rules a bit: interim security clearances are granted to people who start jobs that require clearances and actors can star in blockbusters if they are particularly well received in smaller roles.
The primary philosophical justification for this in governmental or political terms is http://en.wikipedia.org/wiki/Social_contract social contract theory. Essentially, social contract theory is the viewpoint that both a person's moral and political obligations derive from an agreement that they have made (implicitly or explicitly) with their society (or government). Social contract theorists argue that this type of agreement is vital to forming a society, and in fact, much of modern Western moral and political theory is derived out of the work of the famous triumvirate of social contract theorists: Thomas Hobbes, John Locke, and Jean-Jacques Rousseau.

There are various justifications for this theory, depending on which of the three social contractarians you choose to read. But most of them agree that humans abandoned the natural state (where all beings were completely free to do as they liked) in order to enter into an ordered society that revokes some of their natural liberties in order to provide them with collective benefits such as security and protection. The fundamental idea is essentially that the rights required to be given up have less net desirability than those rights that can only be ensured and protected by a government or society.

Now, you might argue, that this is all well and good for the parents of that child, but what of the child itself? It clearly isn't old enough at the time that it is born to have agreed to comply with any type of contract, social or otherwise. You seem to be driving at exactly this argument when you say:


  Although his or her parents could have freely decided to become members of a certain nation, to what extent is the country legitimized in extending its authority over the newborn?


Well, the answer is a fairly simple one, and certainly one that the social contract theorists anticipated. There have been only very few instances in which human beings have actually gotten together and signed a written contract—in keeping with the relatively Western nature of this philosophy, the http://en.wikipedia.org/wiki/Magna_Carta Magna Carta is often cited as the canonical example. In the rest of cases, indeed the majority of cases, the consent to this social contract is assumed to be implicit, or tacit.

The argument usually goes something like this: By virtue of choosing to remain in the territory which is under a particular social contract, the individual has tacitly given consent to be governed by that society and therefore give up his/her individual rights as required to gain the larger benefits of that society. In fact, the argument goes, this tacit consent (the ability of people to "vote with their feet") is what gives the government its legitimacy.

To answer your question more specifically, I'll focus on the writings of John Locke, arguably the most influential and widely-known of the social contract theorists.

Locke says that simply walking along the roads of a particular jurisdiction is tantamount to tacit, or implied, consent to the laws of that government. A person agrees to obey the laws of a society while it is living within the territory of that society. He says that this explains why resident aliens are obligated to obey the laws of whatever state in which they reside, even though they are not naturalized citizens of that society. This obligation no longer holds when they return to their country of origin, but it necessarily attaches while they take up residence in that particular jurisdiction.

And the idea of property ownership (a recurring theme throughout Locke's philosophical views) becomes important here as well. If a person inherits property, then that works to create an even stronger bond between the individual and the commonwealth. The idea is that the original owner of that property agreed to place this property permanently under the jurisdiction of the commonwealth or society, and that by inheriting this property, the inheritor also inherits the obligation to follow the laws of that society.

So when children accept the property of their parents, they also tacitly consent to the jurisdiction of the commonwealth and agree to abide by the rules of the social contract. 

For more on this line of thought, I highly recommend picking up a copy of John Locke's seminal work, http://rads.stackoverflow.com/amzn/click/0915144867 The Second Treatise of Government. It's a relatively short little book (as these things go), and well worth a read for anyone curious about the nature of governmental sovereignty and implied consent.
Of course language is somewhat of a barrier when it comes to describing many things, certain things more than others. For example, the aesthetic beauty of a sunset or the exquisite taste of a fine wine to a seasoned taster might be somewhat difficult to convey, particularly to people who aren't familiar with such things. But I'm pretty sure there's less impediment with things like basic mathematics and physics, although I'm sure there are somewhat arcane concepts in either field that are equally challenging to express.

However, unless you are some sort of dualist, physicalism—which accounts for all mental states as physical brain states—leaves the door open for accessing what goes on in people's minds (in other words, our minds are not inherently available only to ourselves). Concordantly, with sufficient understanding of the brain and sufficiently advanced technology, it should in theory be possible in the future to record the precise brain state of a person, and translate the exact representation of that idea into a form that is more readily exchanged (whether by viewing it in a home theater system or experiencing directly via an analogous process). Some might dismiss such ideas as speculative, and we are admittedly quite far from this technology and understanding, but it is logically sound. That is, if you are a physicalist, you have to believe this is possible.
What you are describing is not in the philosophical literature, but is found in the blogosphere under the name http://answers.yahoo.com/question/index?qid=20081012134755AAnFxdL Special Snowflake Syndrome.  I suspect you picked it up there.

  But the question I wish to ask is then how, if the material the constitutes the heap exists, and existence is not a predicate, can that material only be a heap when we say it is?


The easy answer here is to say that the material exists, but "heap-ness" is a predicate which may or may not be present. 


  If this is true, which is to say, if there is only a heap when we say there is a heap, then it seems that there must be borderline cases of existence, i.e., collections of sand that are possible but not actual candidates for extant heaps of sand, no?


Not borderline cases of existence, but borderline cases of definition.  Something exists; the question, in this context, is whether or not it should be classified as a "heap."


  Is existence to be understood as something like "red" or "bald", which is to say, only true as specifiable by human beings? Is this a deflationary take on truth? I don't know where to go next.


No, heapness is to be understood as something like "red" or "bald"; even in classical cases of the http://plato.stanford.edu/entries/sorites-paradox/ Sorites Paradox, the existence of the possibly-bald-man (or the would-be-heap) is not in doubt.
I'll try to expand on Michael's terse answer.

Basically, no. Objectivity is usually taken to mean something like "mind-independent". Exactly what this means is up for some wiggle-room (what is objective? or what kinds of things can be objective?), but in most cases it's fairly clear cut. 

Subjectivity, conversely, is "mind-depended". Again, it's pretty clear, but it's up for debate precisely what this means (is the existence of emotions subjective? Some would say trivially so, because there has to be a mind to have an emotion, but subjectivity is usually taken to be an epistemic notion, so it's not so clear if this is a good use of the term).

Even if something is dependent on 6 billion minds, it's still mind-dependent. 

What are the intuitions behind your question? I can't say for sure, but I can speculate:

(1) Objectivity/subjectivity has a functional flavor to them. It's almost like an objective truth is one that you can 'bump into', that resists you or imposes itself on you. Mass agreement might have this kind of cognitive 'substance'.

(2) A subjective truth is one where there can be genuine disagreement, because the facts of the case depend on an individual's standpoint, and disputes can't be resolved with reference to external facts. If everyone agrees on something, then there can't be disagreement.

(3) It might have the same impulse as "If everyone decided that cats were dogs, then they would be." If we take this to be a definition, then yeah, if everyone decided cat meant dog, then cat means dog. It doesn't mean that cats are the same as dogs, it's just that we've decided to use the words in that way.

This question could have other possible motivations, but I think these cover the most likely ones. Feel free to explain more or ask questions.

wikipedia, as much as it is a supposedly self correcting medium, is not necessarily a definitive account of anything.
your question could just as well be about motion or space (based on the given definitions) or even more so about how definitions of physical phenomena work. At some point, to make definitions usable, they refer to others definitions and those to others, but it is not so easy to deal with infinite regress ("turtles all the way down"). There needs to be an 'end' which is either explicitly terminal,that is, altogether undefined (or rests on intuition or inarticulate gesture) or is circular (as on your given example).
whether defined well or not, does a definition make time(or space or motion) exist? No, I think things 'exist' without needing to be defined by humans (for some intuitive notion of 'exist'). Time will elapse without our awareness of it. Or more relevantly, we can have a concept of time that works for others who may not be aware of the concept.
more generally, does a concept need to be well-defined to exist? I think that is really dependent on what you want 'exist' to mean. Suppose you consider a circular or an inconsistent set of definitions is taken to be nonexistence. That doesn't mean there's not a better workable definition.

Directions require a relation; they are entirely meaningless without any context. Typically however, the notions of left and right are used in relation to oneself: my right or my left. In this regard and only in this regard would I still be able (in the middle of empty space) to know the direction of left and right (in relation to me).

If you remove all objects of relation, then yes, any directional terms would be meaningless. There is no intrinsic North or South of the universe ("North" for humans typically just means in relation to the geographic North Pole).

This is fairly simple to conceptualize if you remove yourself from the picture, but that can be somewhat challenging to do. I can provide an example but I hesitate in that it may only pollute this otherwise pure notion. But for example, if you were driving and came to a fork in the road which split off in four different directions all facing left, and your friend asked you to "take a left", would that help you? How would you know whether it meant to "take a left" in relation to you ("your" left) or in relation the number of lefts ("first" left) or to one of the roads or a sign or what? Without knowing the relation, the term "left" is without meaning.

.
Even though his style is rather simple and direct, Wittgenstein is not someone whose writing you can just pick up and 'dive into.' I'd recommend familiarizing yourself with a lot of context before even beginning to read - not only as far as the content is concerned but also as far as the dialectical form. The Investigations for instance make little sense unless you take care to separate the three interlocutors in the text - separated by single, double or no quotation marks. Similarly, you need to be aware of the separate, stand-alone arguments that litter the Tractatus - that is the only way to give the text structure (the numbering of the propositions creates the illusion of a single strain of thought, pursued to completion, but that's not at all how one should read the Tractatus (if, that is, one wants to avoid frustration).) So, in summary, read a lot of exegeses and/or introductory essays (e.g. Anscombe or Potter for the Tractatus, the Routledge volume for the Investigations etc.) before engaging with the texts, make sure you are familiar with the structure and, finally, a lot of historical context won't do any harm, although that is advice true of any text, philosophical or otherwise.

Now on the other hand, to answer your actual question, are there any other philosophers who write more clearly on the issues Wittgenstein himself was concerned with? I don't know - clarity is certainly a matter of taste. I for one think Wittgenstein is by far the clearest philosopher I have ever read and find a lot of the analytic philosophers that succeeded him and who wrote in the name of 'clarity and rigour' much harder to penetrate. I will suppose that you are interested in (what is now labelled) the philosophy of language. The problem is no-one has written like Wittgenstein or about the things Wittgenstein wrote about in much the same way - and I am guessing you are not after scholarly appraisals or purely exegetical works. The only contemporary philosopher I would recommend in that vein is Richard Rorty - he is a great writer and very lucid and, in my opinion, holds ideas that Wittgenstein would surely have sympathized with.
Yes I believe it is his counterfactual statements.

One’s true belief that p is knowledge if and only if the following two conditions hold: 


if p were not the case, one would not believe that p, 
and if p were the case, one would believe that p.


I've only just started to research it myself, but this might help:
http://www.iep.utm.edu/epis-clo/#SH3b http://www.iep.utm.edu/epis-clo/#SH3b
My own interpretation is that Plato actually argues that although a regime decays, the human soul is perpetually rising toward its best state. For this interpretation, I first determined that Plato thinks of the level of education in a city (or soul) as analogous to its regime. Therefore, the more education there is in a city, or the better educated a soul is, the closer that soul/city is to the top of the regimes (the aristocracy).

I think this point is fairly obvious in any reading of The Republic, so I'll skip over explaining why I think education is proportional to regime for Plato (if you would like me to explain it, please leave a comment and I'll add it in).

Regarding my interpretation that Plato sees the soul as in perpetual growth, the following quotation is very relevant:


  Education is not what the professions of certain men assert it to be. They presumably assert that they put into the soul knowledge that isn’t in it, as though they were putting sight into blind eyes…but the present argument, on the other hand, indicated that this power is in the soul of each, and that the instrument with which each learns…must be turned around from that which is coming into being together with the whole soul until it is able to endure looking at…the good. 518c


The last portion is particularly important, where Plato asserts that the human soul must be educated and developed "until it is able to endure looking at... the good." Thus, unlike the decay of education in the city, which leads to a decay of regimes, the soul is actually accumulating a better education, and therefore state with time. 

Why do I say that the best education is required for the best soul (the aristocratic)? Well, when discussing the oligarchic man (of the 3rd regime, that which values money), Plato comments:


  I don’t suppose such a man has devoted himself to education. (554b)


Therefore, if somebody does devote themselves to education, they'll end up better than the oligarchic man, higher and closer to (if not at) an aristocracy.

So, it appears that Plato argues for the importance of constant education of the soul, and for the value of education is achieving the aristocratic mind. Crucial to my interpretation that he sees the soul as perpetually improving is the following:


  ’Does anyone, either god or human being, willingly make himself worse in any way at all?’‘It’s impossible’. (381c)


This is a bit of dialogue where another (I don't recall who) is actually replying to Socrates' question, and here we can see that Plato doesn't think anyone would ever make themselves worse (i.e. decay the regime of their souls). So, since they are always being educated, and education leads to a better soul, and they will never make their souls worse, the human soul must always be in a state of improvement.

However, through his regular arguments, Plato determines that the city in speech, the actual nation, is always in a state of decay. This, too, I take as apparent, so I won't cite quotations unless requested.

This, though, is just my interpretation of the difference between the soul and city for Plato, which was why I posed this question; I was (and still am) looking for other ideas and also an answer to what happens when a city bottoms out as tyranny. 
This is more of a biology question (or possibly English, if it's a definitional issue) than philosophy, but:

The change in genetic material is still random.  Selection has been nonrandom since the beginning: if it's too cold for you, you are unfit; if it's too hot, you're unfit; if it's too acidic, you're unfit; if you can't escape predators, you're unfit; etc. etc..

Organisms all react to their environment in non-random ways in order to try to increase their fitness (this ability having been selected for).  Learning Chinese and upregulating your lactose metabolism enzymes are really just two different instances of this.  So, no, the mere fact that ideas are important for our reproductive success these days does not fundamentally change how purposeful evolution is.

If we started making genetically modified humans, then one could argue that we're bypassing the random component of evolution and making it purposeful instead.  Or if our ideas were focused on how specifically to breed ourselves to encourage certain traits, then one could argue that we are injecting purpose into the process in a novel and nontrivial way that might warrant our changing the way we speak of it.  But we're not doing either of those things routinely.

Culture and technology are not (biological) evolution, nor is learning.  These things are alternative ways to become better adapted to one's environment than random genetic changes.  Biologists maintain a distinction between these concepts (where evolution is restricted to mean change in allele frequencies across time).
At what point does "reform" become "revolution"?

Nietzsche's critique is quite radical, and ruthless, and I suspect that "Christianity" one would arrive at by following his critique would be so unrecognizable as to render the name suspect.
A good introduction I found is http://plato.stanford.edu/archives/win2004/entries/time-experience/#3 The Experience and Perception of Time in SEP.
On those matters, I know of Augustine of Hippo:


  Edmund Husserl writes: "The analysis of time-consciousness is an age-old crux of descriptive psychology and theory of knowledge. The first thinker to be deeply sensitive to the immense difficulties to be found here was Augustine, who laboured almost to despair over this problem."


You can see more references, mentioning Augustine and others (like Kant), at

http://en.wikipedia.org/wiki/Augustine_of_Hippo#Influence_as_a_theologian_and_thinker http://en.wikipedia.org/wiki/Augustine_of_Hippo#Influence_as_a_theologian_and_thinker

Personally, I can't realise conscientiousness without a dimension of time. Without time there's no dynamical causality, there's no movement. 

Once there's time, the personal passage of time is subjective to the living creature. Without a reference for the passage of time, 30s can be whatever. A given unknown living creature may evolve in such a ambient where in our 30 seconds it would be able to consciously process much more causal relations than our brain is able to do in the same time.

You talk about a person that can only remember its last 30s, while also able to feel pain and pleasure. You must realise that such a feelings are higher brain functions that come up after a long process of evolution, and ultimately these feelings are evolutionary memory, it is unconscious brain work compelled by the evolutionary process.

The 30s memory you talk about is the conscious one. Whatever short memory one gets, once the living being is able to take any action foreseeing its consequences it has a self.
Once it takes action to stop a continuous pain of hungriness, for example, it must have a self sense.
Fichte, Schelling, and Hegel were divided in most significant respects by their views about realism and naturalism, the status of subject-object identity, and the nature of rationality and intellectual intuition. The differences between Fichte and Schelling are explored in depth in Hegel’s Differenz des Fichteschen und Schellingschen Systems der Philosophie (for a more contemporary review of the issues you might look into The Philosophical Rupture between Fichte and Schelling: Selected Texts and Correspondence, 2012, edited by Michael Vater and David Wood). 

To state things crudely, the main difference is that Fichte upholds a more or less Kantian interpretation of the subjective status of transcendental idealism, whereas Schelling and Hegel defend an objective interpretation of absolute idealism. In particular, Schelling and Hegel criticize in different ways the Cartesian tradition of foundationalist epistemology, where we need to begin philosophy from the presuppositionless starting point of the knowing subject or res cogitans. For Schelling and Hegel, however, this involves an unacceptable form of solipsism or dualism that is created by falsely abstracting the knowing subject from nature itself. 

Roughly put, if we start from the subject and work outwards toward nature we are forced to postulate an unbridgeable gap of intelligibility between subject and object. The problem being that the transcendental ego cannot reach outside itself to make contact with an externally standing reality. In order to escape this both Schelling and Hegel attempted to reconceive the knowing subject in terms of the natural history of consciousness; the ultimate end of the organic powers of nature. Their claim boils down to the idea that if the subject is internal rather than external to nature then all of nature is contained within itself; a notion similar to Spinoza's substance. 

Apparently, this form of idealism is called objective and absolute since subjectivity emerges from a pre-existing objectivity which is dissolved in a relation of self-identity; meaning that self-consciousness coalesces out of nature and returns to nature in order to become something that is neither nature nor self-consciousness alone, but is rather the undifferentiated unity of the absolute.  
There are two things here.

First, maybe ad hominem was historically born for using with individuals (I don't know about its history). But since its purpose is to point to a logical error, I don't see why one could not use it to refer to the same error, only applied to institutions. I don't know of a fallacy specific to insitutions, so I don't see a problem in referring to ad hominem.

Well, that said, there's the second thing. A fallacy is usually an error in argument. In this case, if one says "A because of B", I'm commiting ad hominem if I argue that it is wrong because the person saying is ugly. That is because the ugliness of a person is irrelevant to the argument's truth and validity.

Even if we use "stupid" instead of "ugly", it would still be a fallacy. Even though "stupidity" may hinder someone's capability of argumentation, their argument should be considered on its own merits.

But that doesn't mean that anything citing someone's characteristic is an ad hominem.

For example, if I say "A because of B", there is the premise B and the conclusion A. You could challenge my conclusion by saying that A logically does not follow B. Or you might challenge my premise B.

If you know I'm a pathological liar, and you don't know if B is true, then you have all the right to question my premise B based on my history of lying. So you can't say that my logic is wrong because I'm a regular liar, but you can doubt the truthness of B.

Yes, it is "against the person" in a way, but not necessarily irrelevant to the argument. Hence an ad hominem fallacy does not necessarily apply.

  Is it ever morally ethical to override the majority in a democracy if you feel morally obliged to do so?


If you're phrasing it like that the answer seems pretty clear to me. The answer is 'Yes' if you admit to morality being subjective, for if you do feel the moral obligation to counteract the majority's wish, why wouldn't you?

The majority vote is interesting in itself. Rousseau said that the agreement on the majority vote is the only necessary concordant vote, because otherwise we cannot explain why pretty much always and everywhere, the majority vote is considered to be fair and valid, and why the minority should be accepting the majority's will (Social Contract, Chapter 5 [I think]).
But even Rousseau, a radical democrat (let's call him that for reasons of simplicity), can't refute the possibility of the majority being wrong in exercising the collective will. 

I am mentioning this because even if you're not a subjectivist, the argument that majority is always just or that the will of the majority must be considered 'right' is impeachable.
The argument that "money equals speech" is actually fairly narrowly drawn; it is not a global philosophical principle.  The notion is that because the US Constitution prohibits the restriction of free speech, any restriction on the amount of money one could use to take out political advertisements would be an unconsitutional restriction of the freedom of expression.

This means that the assertion concerning "participation in an institution" being the same as or different from "speaking or using language" is irrelevant to the matter at hand-- what is at stake is limitations on the ways one can choose to make public one's speech acts.  Taking out an advertisement endorsing a candidate is viewed as a speech act, but not "participation in an institution" generally.

Furthermore, this entire problematic is orthogonal to the question of whether or not corporations have free speech rights.  Since time immemorial, groups of citizens have banded together (ad hoc, or in unions or other organizations) to pool their resources to support a candidate; the question of whether corporations should be excluded from this practice has nothing to do with the "money equals speech" doctrine.  If the Supreme Court had ruled that money did not equal speech, and that limits could be placed on the amount of money that an individual or group could spend on political advertising, this would have no bearing on whether or not corporations would be allowed to spend up to that limit.

If the Supreme Court were to hypothetically rule that "speech" for the purposes of the First Amendment applied only to individual mental acts, then any kind of statement on behalf of an organization or political party could be restricted, including documents that begin "We the people..."
Inductive inference. All humans have died so far, therefore (in all likelihood) all humans die at some point. You are human, I take it, so there you go.
The problem with some instances of the kind of reasoning you are deploying is that, in a train of thought of the kind If A, then probably B; if B, then probably C; if C..., then probably Z, although every of the single steps might be likely and compelling, the transition from A to Z might not be.

Suppose that each transition has a probability of 0.99 -- that is, P(B|A)=P(C|B)=...=P(Z|Y)=0.99. Then, for all the argument tells us, the probability of Z given A is the much lower 0.99^28 = 0.75

This doesn't mean that, in general, reasoning from causes to consequences is fallacious. As you rightly point out, it is not.



Now that we are at it, it is probably useful to note that hardly any of the traditional fallacies single out reasonings that are never compelling. Take, for example, post hoc, propter hoc: while, indeed, in general it is not true that if an event follows another they are both related causally, noting a strong temporal correlation between events of one type and events of the other can provide very good evidence of the existence of a causal process. Ignoring it on the grounds that post hoc, propter hoc reasonings are fallacious would be very silly.
Yes, "not all are American" would still be true if we found out the one we assumed was actually wasn't. 
As Sindikat already pointed out: 


  "Not all", ~(x), is right-open, left-closed interval - the number of animals is in [0, x), 0 ≤ n < x.


Which means that n, the number of individuals that have the attribute x, can be 0 or positive. 

But in this case (we find out he actually wasn't American) your second premise "stops" being true, because: 


  "Some", (∃x), is left-open, right-closed interval - the number of animals is in (0, x], 0 < n ≤ x


Which means, there must be one individual with that attribute, it can't be 0.
http://www.acrwebsite.org/search/view-conference-proceedings.aspx?Id=7580 Baker and Kennedy list several causes. Among them:


The more drastic the change in a current life role, the more nostalgia, or symbolic reflection, will occur.
The more satisfied individuals are with their perceived quality of life in the past, the more nostalgia, or symbolic reflection will occur.
The more direct the experience, the more vivid the memories.


See the paper for a more full discussion.
As leancz said, there isn't one answer to this. An argument I heard from extreme vegans: 

The difference in your example is that bees are often harmed when collecting honey, and also you "steal" the basis of their habitation as well as for new offspring, while relying on their pollination 1) is indispensable and 2) doesn't harm them but, on the contrary, it's essential to their survival (while the crop of the pollenised plant isn't).

Edit: I hereby present a somewhat suspicious link to http://www.vegetus.org/honey/honey.htm Why honey is not vegan. Please don't hold me liable.
Sure.  At some point the cost of saving a person could cost so much that it could endager the lives of other people.  Imagine a government that needed to pay for a water purification system and the cost of saving someone in a specific emergency situation would effectively remove their water purification capability, and that would undoubtably endanger many lives.  In such a case it could be argued that it is ethically right to not spend the money to save the one person.

Lives are weighed when money isn't an object, in cases where the danger to the rescuers becomes very great the rescue can be called off, effectively condemning the person in need of rescue to death.  If money becomes a true factor in the weighing of life and lives, as disturbing as it seems, someone will be forced into a hard decision.  But the fact that money is involved in the weighing of lives doesn't in and of itself make the decision immoral.
Any system becomes a thesis which, inevitably, creates its own antithesis, thus continuing the dialectical process.  So the mixing of many cultures and points of view does not lead to a meanings mushy soup but rather a richer source of new theses, which can keep the process alive.   So, somewhat as the world views of women, of cultural minorities, and of differing orientations enriches a culture, so the world-wide interplay of knowledges (there's not just one) make for an exciting, if not simple, future.
Singer already addresses the issue, albeit indirectly:


  it follows that I and everyone else in similar circumstances ought to give as much as possible, that is, at least up to the point at which by giving more one would begin to cause serious suffering for oneself and one's dependents - perhaps even beyond this point to the point of marginal utility, at which by giving more one would cause oneself and one's dependents as much suffering as one would prevent in Bengal


That is, it's not that children are more or less important than each other, it's that you and those who depend on you can suffer too, and they will if you give too much.  (He also addresses the issue of needing to maintain yourself/the economy in a state where you can generate a surplus to give away.)

The text of Famine, Affluence, and Morality is http://www.utilitarian.net/singer/by/1972----.htm available online.  This and other questions and objections one might have are likely to be addressed there; it's a well-written article and Singer is (as typical) thoughtful.  One may ultimately disagree with him, but it will probably be for relatively complex reasons (or because of complexity in the world that Singer failed to adequately account for).

  What are the key differences between (classical) pragmatism and
  conventionalism?


Conventionalism doesn't refer, at least in itself, to practical bearings. It is the view according to which there is no valid certain knowledge, no informational content about the world but only inter-subjective conventions. So it is essentially a claim regarding the nature of what we call knowledge; it is an epistemological stance.

Pragmatism, on the other hand, is an all embracing approach for philosophical conduct. To relate to Conventionalism, one might inquire into the epistemological conclusions of Pragmatism:


As a theory of justification, it is the thesis that statements are justified only as a function of how consistent they are with the other statements in the system. Not true for Conventionalism, as the latter isn't dependent upon the consistency of the conventional content; inconsistent as it may be, a convention could still hold firm.
As a theory of truth, it is the thesis that the property of Truth of a statement is essentially the property of being practical (or useful) to hold. Again, not true for Conventionalism as it isn't necessary for convention to be practical; it only has to be mutually accepted, be it practical or not.



  Both descriptions refer to the practice of human discourse ("practical
  bearings", "agreements in society")


Good point :)

Premise 1: No home cooked food is good. 
Premise 2: No good food is wasted.
Your conclusion: No home cooked food is wasted.


There are three possibilities given the two premises. Graphically through Venn-diagrams:





Your conclusion isn't necessarily false, as the top Venn-diagram shows, but it does not logically follow, as the bottom 2 Venn-diagrams show.
Mobius strips aren't that interesting physically because they're all just rings.  Everything is three-dimensional.  If you took a noodle and squashed it so it was 30% wider than it was tall, and instead of doing it flat you did a half-revolution so that one edge of the bulge went from left to up to right, you'd have a "Mobius strip" of a sort.  But it's a really unimpressive one because the curvature is still nearly constant as you travel around (rather than along) the noodle.

Doing it with paper just makes the aspect ratio more and more extreme.  You can squash your noodle into a Y shape had have it rotate by 120 degrees also to get an interesting shape (if you forbid yourself from going over the high-curvature "edge").  I don't think any of this proves anything about the reality of objects, but it demonstrates that practical geometry is fun.
You have observed that there are days in which it snows, but does not rain — where ~r&s, in direct contradiction (twice!) with the axiom (2). We can only conclude that those days are not modelled by that axiom, or any system of axioms which include it.

On days conforming to (2), it is not snowing, and so s→r holds vacuously. (See towards the end of a related post https://philosophy.stackexchange.com/questions/4089/how-can-we-reason-about-if-p-then-q-or-p-only-if-q-statements-in-proposition/4091#4091 on conditional propositions in sentential logic).
One should distinguish between the representation (declaration) of truth and truth itself. Perhaps this is as true in love as it is in philosophy.

Plato touches upon this in his dialogue (or philosophical play) the http://en.wikipedia.org/wiki/Symposium_%28Plato%29 Symposium. A Symposium now means an academic conference - then it meant a drinking party. They (amongst them Socrates) are in the male chambers at the house of Agathon, a playwright. Seven participants are asked to discourse and eulogise on love in turn. 

Aristophanes first skips his turn as he had a bout of hiccups, when this passes he expresses a little nervousness. He says:


  I fear that in the speech which I am about to make, instead of others laughing with me, which is to the manner born of our muse and would be all the better, I shall only be laughed at by them.


Like Phaedrus, the first to make a speech he relies on myth to express his thoughts. But unlike him this is appears to be a whimsical, absurd and made up myth. 

Wikipedia admits that:


  his speech has become a focus of subsequent scholarly debate, as his contribution has been seen as mere comic relief, and sometimes as satire


In brief, his myth explains the origin of love in man & woman - and in fact of man & woman too. He suggests that originally that human beings were doubled creatures with two pairs of everything - hands, feets & heads. They were enormously powerful and attempted to scale heaven so Zeus threw a thunderbolt down and sundered them in half. These became man & woman. They then ran looking for their other half and if they found them they proclaimed themselves whole.

If we take this myth seriously rather than comic relief, then Aristophanes is suggesting that true love is true and makes wholes out of halves. He doesn't suggest that these halves (men & women) will always find their other halves. 
I think this is a VERY interest question that gets to the heart of what logical positivism, and empiricism in general, are really about.

I remember Peter Hacker in his book 'Wittgenstein's Place in 20th Century Philosophy' explaining the Marxist leanings of some (but not all) of the leading positivists.  Logical positivism was a epistemic reductive movement with a tendency to elevate the status of science culturally and strongly reject metaphysics/religion. In this respect logical positivism broadly belongs to empiricism. Ever since Hume, empiricism has had a strong connection with atheism, the rejection of religion, and the promotion of science in its place.  Logical positivists tended to continue in this tradition.  To some positivists, Marxism probably seemed more compatible with their empiricist dogmas than the free market based political systems that most Western democracies had implemented in the early 20th century.  There was certainly a strong cultural match between the progressivism towards science inherent in logical positivism and the cultural progressivism of Marxist idealology. The http://en.wikipedia.org/wiki/Vienna_Circle#Manifesto positivist manifesto seems to be influenced by the earlier Communist Manifesto, for example.

I think it is reasonable to say that to this day there persists a general cultural connection between scientific materialism (the successor to positivist scientism), atheism, and Marxist/socialist ideologies.  But this is my own humble opinion which needs further argument.
To answer the question directly: yes, it should be completely disregarded except as an intuitive historical artifact.  We have a much better understanding of what things are composed of now, and those details are within even a child's capacity to understand (e.g. phases of matter, "metal", "plastic", "glass", etc.).

However, the elements are fun in stories.

  But if there is no God, could we say that there is Substance at all?


Yes, we could: Take the universe itself as substance. The universe satisfies your definition as "the underlying being that supports, exists independently of, and persists through time despite changes in its accidental features".

If you accept the hypothesis of a Big Bang, then there was no time before the existence of the universe. Spacetime is a feature of the universe. 

  But when we look at energy as a substance, that is something that remains conserved through accidental change, we see in fact that the conservation of energy is a law that characterises the meaning and the substance of energy.


Conservation of (mass-)energy isn't the only http://en.wikipedia.org/wiki/Conservation_law conservation law. IMO, that wrecks the second paragraph of the question.
Modern social contract theory is said to begin with Thomas Hobbes' Leviathan (not Rousseau). I will give an overview of the main points in all social contract theories, including the one in Plato's Crito (although technically, it is not a social contract theory!). Note that this is a broad and very brief overview. For a more nuanced view, I recommend the SEP or IEP.

Form of modern social contract theories

State of nature and its problems (=reasons to have the social contract)

They always begin with an hypothetical state of nature. What would happen if there were no laws, no form of government intervening?

Ways to resolve these issues (=the social contract)

After describing the state of nature, it is clear that there should be an intervening body (=government). For this government to work, the citizens should hand over some of their freedoms to this government. Social contract theories include which freedoms and justification for handing these over (i.e. the role of the government).

Plato's Crito and Republic

Socrates says in Crito he cannot evade the laws of the city, because they made his life possible. You can't avoid laws when you want to; you were born in a city with laws that gave you protection and possibilities. You also have to accept them when they 'turn against you'. Citizens here, however, can leave the city when they grow up. If you stay however, you must accept the conditions (laws) of that city. So Socrates accepts his death sentence.

In the Republic, he outlines his 'state of nature' (though he doesn't call it as such). People would do all sorts of unjust things if they can get away with it (Ring of Gyges). They also want to avoid being treated unjustly and not be able to treat them unjustly back. A just society is one in which these extremes are avoided. Justice is worth having for its own sake (i.e. needs no external justification). The dialogue goes on to justify this view. It is not a social contract theory, because it does not say you give up something to a governing body for certain reasons; you just live a good life because it's worth it by itself.

Hobbes' Leviathan

(As a side note, Hobbes (and other after him) believe all men are made by nature to be equal. This is very different from the notion in Greek Philosophy, where for instance Aristotle said some people are just meant to be slaves and some people are meant to lead.)

During Hobbes' time, religious civil wars were commonplace. It is no surprise then that Hobbes' main goal of the social contract is public safety. Hobbes' state of nature was a place of war of "all against all" (he clearly was influenced by these religious wars). It was the most pessimistic world view of any contract thinker. 

To resolve this, he proposes a social contract. The citizens sign the social contract, however, the sovereign is not a party of the contract (the citizens can't abandon the contract because the sovereign breaks the rules (compare with Locke!)). You can never resist him; he is the only thing that stands between this state of affairs and the awful State of Nature! For Hobbes', the sovereign is the most powerful man on Earth (even a God on Earth). The title page of the book says "There is no power on earth which can be compared to him". 

My thoughts: this absolutist point of view may seem extreme nowadays, but consider the religious, civil wars he lived in. He wanted peace and was willing to give up a lot to get it, even if that means giving up your freedom to be ruled by a potentially tyrannical sovereign. A lot of people in the same situation nowadays would agree. Think of the people in the Middle East who prefer Saddam Hussein's (horrible) regime to the unrest of having no real leader, for instance.

Locke's Second Treatise on Government

Locke's State of Nature is very different from Hobbes'. The State of Nature is not that bad. Contrary to Hobbes' version, it is not a lawless state (there are indeed morals). There is the Law of Nature (which has been given to us by God). We all belong equally to God, we can't take away what is God's, so we shouldn't harm each other. As long as you don't harm others, you are free to do as you please. It is, in other words, a peaceful place. But, conflicts may arise. A property dispute, for instance, and there is no government to intervene. There is no civil authority, so disputes are likely to escalate (instead of being resolved by a third party). This is the main reason for the social contract.

For Locke, to goal of the government is to serve the citizens (i.e. those who signed the contract). The sovereign has fiduciary power (we entrusted him with this power); if the sovereign doesn't do this, the citizens can rebel (i.e. abandon the contract). The role of the government is to protect"life, liberty and estate, which I call by the general name of property" (property in Locke is much broader than what it means to us today); to protect our rights which are given to us by the Law of Nature.

Rousseau's Social Contract

Rousseau has two social contract theories. The first is a descriptive theory, outlined in his Discourse on the Origins of Inequality. The second is a normative theory, outlined in his Social Contract.

Rousseau's develops his idea of the State of Nature historically. In the beginning, with few people, everyone was happy on their own. They had few needs which were easily satisfied by nature (an abundance of resources for few people). Since people lived solitary lives, there was no competition, no fear, etc. Since pity is an essential trait of these people, they would not harm each other. 

But, times change. More people were trying to get by with the same amount of resources, so they had to start living differently. They started living in small communities and divisions of labour were introduced. Living in communities, however, had some nasty side effects: people started comparing themselves with others, which lead to shame, contempt,... It was in this phase that private property came into existence, which led to competition, greed, and inequality.

Those who came out best in the competition think it's a good time to start a government to protect their rights. It is supposedly to guarantee that everyone is equal, but in reality it just makes sure the inequalities that benefit those with private property are kept in place. 

His normative social contract theory is attempt to deal with this state of affairs. His book begins with "Man was born free, and he is everywhere in chains"; he clearly wants to offer a solution to resolve this issue. Since the State of Nature is so peaceful, it would be reasonable to expect that he would propose to go back to this state. He does not do this; it would not only be not feasible, but also not desirable, he says. So the question becomes: how can we live together (in a community), but at the same time be free (i.e. avoid the nasty side effects I talked about). This is where he introduces his very controversial concept of the General Will. We submit our own, individual wills to the General Will. This General Will is not just the sum of particular wills; individual persons become a people and this becomes the foundation of society; this is the sovereignty, which, according to Rousseau, is undividable. The General Will is directed towards the common good. 

This concept has been very controversial, especially when Rousseau said that people who do not comply must be "forced to be free". This all sounds very totalitarian and even gives justification for it. Robespierre was influenced very much by this book and said to lead his Terror regime "in the name of the people"; it was the "will of the people". Rousseau also said the "General Will cannot dwell"; for some, this is even more evidence of his totalitarian viewpoints, for others, it just follows from the definition of the General Will, namely that it is the norm, what reasonable people ought to want.
This is more on the lines of commentary on your question, rather than an answer.

If one uses the usual natural numbers then yes. But are there other systems of natural numbers?


There are non-standard integers: The integers are axiomatically defined by the Peano Axioms. We actually obtain them in a model. The question here is are all such models isomorphic - that is categorial. If we use second-order logic, they are; if first-order then not. But the initial segment of any non-standard integer model is isomorphic to the usual ones. So this doesn't work.
We could use topos rather than semantics for the PA axioms, this means the axioms are interpreted in a topos. This means that the topos must have a Natural Number Object. Whether there is a topos, within which the condition you specify fails, is for me, an open question.

The connection between the idea of the Good and the idea of the Triangle is that, in the physical world, there exist neither things that are perfectly good nor things that are perfect triangles. Even if you help someone, you probably do it because it gives you a pleasant feeling, or because it's just a habit, so it's not pure good. Similarly, the triangle drawn in my book is not a perfect triangle: it is ever so slightly deformed by the structure of the paper and the inaccuracy of whatever drew it.

I could say, "I have this figure here in my book that looks like it has three angles, but it's ever so slightly off; I will refer to it by describing the little quirks in its outline that come closest to its actual shape". Alternatively, I could simply say, "granted, it is an imperfect triangle; but we all know what a perfect triangle would be in theory, and it's more efficient to refer to this perfect triangle when discussing mathematics rather than this imperfect drawing in my book, so I will just call it a triangle, keeping this perfect theoretical triangle in mind". The same can be applied to a good action.

The concept of an ideal, perfect triangle or the perfect good can be helpful and efficient in our daily tasks. But Plato went farther: he held that their perfection makes them in a way divine. They are different from physical triangles and good behaviour in our daily lives; they're something of a higher order. That's why he postulated that these idea(l)s must exist in some way in a higher plane of existence, that they must transcend their imperfect physical copies.

The fact that we, humans, could in some way touch on or even comprehend these ideas must mean that we, too, must be or have been connected with the divine. For how can we have a perfect triangle in mind if we have never seen one? A being in no way connected to the divine surely could not distil a perfect triangle out of the imperfect physical things in the temporal world.
Quick and dirty:

(Blunt) criticism (or punishment) is itself an essential element of these dynamics. This evolutionary approach to explaining morality doesn't need to postulate a higher end or purpose. It just happens and if it doesn't then a species might very well be "worse" off.
I don't think that B has made a fallacy. What A is really saying (imho) is this:

(1)If Biologically men and women are different then it shouldn't surprise they have different salary on average.

It is a conditional. Additionally, A is also stating that the antecedent is true.

(2) Biologically men and women are different.

Now what B does is to say that while (2) is true, (1) is not true.
Since one answer has provided the idealist perspective, I'm going to discuss its polar opposite: https://en.wikipedia.org/wiki/Logical_positivism Logical Positivism. Before I do that, though, an important disclaimer is that despite logical positivism's status as the paradigm of natural sciences, it has long been dying in philosophy and is far from a popular position at the present. However, it is historically significant and a useful juxtaposition with idealism.

The primary principle of logical positivism is that apart from the fundamentals of mathematical and linguistic logic (hence "logical"), all truths must be derived from empirical observation. 

Perhaps the biggest consequence of this is that to a logical positivist, if the answer to a question does not have any observable effect, then that question is meaningless. Thus, the question "does an inert metaphysical entity exist" is meaningless, because regardless of the answer our observations in the physical world will be precisely the same. The same physical observations means the same truths in logical positivism, so the answer to the aforementioned question becomes irrelevant. 

In the same way, a logical positivist would call your question meaningless, because whether or not objects exist when nobody is around, they always exist when someone is observing (at least in our experience), so the answer has no bearing on empirical observation. In terms of logical positivism, it's pointless to ask whether an unobserved object exists or not, precisely for the reason that nobody is looking at it. Regardless of what happens when everybody is dead, our observations won't change by definition of the situation (we only observe things when not everybody is dead), and so the question becomes meaningless.

Of course, a big objection to all this is that these things should matter, and this is part of the reason logical positivism isn't doing so well. In some ways it dismisses ideas because "they don't affect me" when lots of philosophy is the pursuit of knowledge for its own sake, regardless of practical implications. On the other hand, logical positivists generally take the hard-line that in truth these things are literally meaningless because truth depends fundamentally on empirical observation.

  If we took a modern society and erased all memory and knowledge of religion and somehow lengthened the life span to be infinite. And then allowed that society to progress, would religion be recreated?


Death is a constant in the world. All things eventually pass away. Even the world will one day. You may as well decide to legislate gravity away. 

Religions do not explain everything in detail, they give meaning & coherence to a people. This though has to be qualified - it's not a objectified meaning - that can be grasped and looked at, and understood as this is what gives meaning; its pervasive and wrapped up in their being. 

Although Dawkins made a fashionable that religion is a 'viral parisitical meme', anthropologists who actually study human beings & their culture as opposed to their biology, say the opposite - which is that its central to culture & society. It can take many varying & subtle forms within the same people, but of course central body of ritual & myth remains. In Lacanian terms - this is the Big Other.

One could argue that as European Christianity is slowly melting away that new religions will either be taken up or evolve. John Gray, the British philosopher for example understands Marxism & Neoliberalism as forms of political religion. Auguste Comte, the sociologist & one of the primary advocates of positivism attempted to turn this into a religion of man.   


  There is a recurring theme of some sort of life after death in all religions, would there be a need for religion if there was no fear of death?


One could argue, if one was to take an external objective view of religion, that to have a notion of life-after-death is to have no fear of death. Of course no religion justifies itself in that way. 


  There is a code of ethics built into religion, would there be a need for it with a code of law in the society?


Where do codes of law come from? Shelley said that Poets were the unacknowledged legislators of the world. This begins to make sense When you understand that in antiquity prophets & poets overlapped. Plato makes the same point when he has the philosopher-king as lawgiver & ruler of his republic; this should also be understood as the priest-king.


  Religion is a way to explain the unexplainable, would it be needed if everything could be explained?


What makes you think we can possibly explain everything? Blake, the poet said 'the world is infinite in all directions'; meaning there is no end to its depth. We only see so much; and what we see we explain, so it appears to us we see & explain everything. In the last century when such a basic idea as counting or addition has undergone convulsions how can we conceivably say we understand the world completely?


  Does man have in innate need to have an abstract figure which is greater then himself? Or without it, would we strive harder to be greater?


Gods do not have to be abstract. That is a relatively recent innovation. For most of the life of humanity icons & idols have been used. If a people was to worship a god(s) - is it conceivable that they would worship something less powerful than themselves? It cannot even be remotely be close. A veil of mystery must hide the divine from the material - for familiarity breeds contempt. All religions have a cosmology, within which is embedded an originary myth. Despite all the rhetoric of kings, emperors, philosophers, artists or physicists - no man has the creative power to make even a stone or a grain of sand. Their creative power is in refashioning - the fundamental creative act is transcendentally beyond them - to make a stone, to create a world takes a god or gods. 
The type of answer you give ideally would reflect what your instructor wants.  One can approach this sort of thing at many levels of detail.  In the absence of a clear idea, I suggest dealing with ranges and conditions.

For example, you can contrast the number of child-abuse cases he would see in the case where he does vs. does not report abuse, and compare the fraction of time the abusive situation stops when he deals with it vs. when it is reported.  Then you can come up with inequalities that must hold in order for it to be a net positive to abused children; if the inequality fails, you then will have a formula for the relative benefit to adults vs. detriment to children that must hold for it still to be a net positive.

For example, let's suppose we're using red-light cameras on all intersections, and let's suppose that when a car runs a red light it has a fraction c of being involved in a fatal collision.  Let's further say that your average citizen has a rate r of running a red light (measured in number of red-light runs per unit time), which will drop to rw if you send them a warning; rt if you send them a ticket; and 0 if you take away their license.  If you see a car run a red light, what should you do?

Well, you could consider taking away the license of anyone who runs a red light.  Presumably it is better to have a license than not--let's call that benefit L, but it's also better to be alive than dead, and we'll call that benefit A.  If we take a license away the cost is -L*T (where T is the elapsed time) and the benefit is A*T*r*c.  Thus, we should do this if and only if

A*T*r*c - L*T > 0
r*c > L/A


Even though A is presumably much larger than L, if rates are sufficiently low and the chance of a fatal collision is sufficiently low, it may yet be worth it.  Now, in reality we could actually always send a warning (let's assume the cost is negligible), so we can always have rw instead of r.  If we give someone a ticket, presumably that has some negative cost to them of -V.  Is it better to send a ticket?

A*T*rt*c - A*T*rw*c - V*T > 0
(rt-rw)*c > V/A


So the answer is that firstly, you should only send a ticket if it's actually more effective than sending a warning (since V and A are both positive), and secondly it'd better be the case that the inequality above holds.

You can then, with conditions clearly mapped out, suggest which category things are likely to fall into.  For example, in the traffic ticket case you might argue that a well-designed warning could be more effective than sending a ticket at correcting behavior, so that should always be the first resort.

Of course you can always make things more and more complicated, but this sort of reasoning is about the bare minimum you need to make sense of utilitarian considerations.
An example of a moral quandary from inconsistent obligations

I would think that the point of obligation is a calculus of fulfilment of duty. That is: if O(p), and if q is a necessary condition for p, then O(q). That is, O(p), p⇒q ⊨ O(q). This is neither factual nor deontic detachment, of course, but in effect an evaluation of contractual dependencies. 

I will borrow https://philosophy.stackexchange.com/a/7439/757 my example of the nature of social dimension of propositions to produce a paradox having nothing to do with factual detachment, but having the same character of the interplay between actions and obligations. If p = "you lie" and q = "you hurt other people's feelings", 
then we would normally say that O(¬p) and O(¬q). However, there are some circumstances where telling people the truth would hurt other people's feelings: where ¬p⇒q; and in particular to avoid hurting people's feelings you must lie, ¬q⇒p.


From O(¬p) and ¬p⇒q we infer O(q);
From O(¬q) and ¬q⇒p we infer O(p).


Thus we obtain the absurdities O(p) & O(¬p), and O(q) & O(¬q).
This conundrum arises from simple reasoning regarding the nature of obligation: the absurdities seem to arise unavoidably due to the interplay between truth-values of propositions and obligations to fulfill them — of the same sort that factual detachment gives rise to, but by a different means.

On the interplay between is and ought in deontic logic

In the comments above, I have of course tried to suggest that simply being interested simultaneously in the truth of propositions and the obligatoriness of propositions, as deontic logic is, introduces an element of multidimensionality. The conundrum of my example above does not require one to entertain multidimensional values for propositions, but it does strongly suggest that any usable logic of obligations which also concerns itself with the actions actually taken must take into account the interplay between truth-values p and obligation-values O(p). That is: the contradiction arises not because O(¬p) and O(¬q) are inherently contradictory, but because introducing the purely logical ¬p⇒q introduces a contradiction of obligations, in precisely the same way that one observes in the Gentle Murder and Chisholm "paradoxes". That is: in my example above, we should infer


  O(¬p), O(¬q), ¬O(p) ⊨ ¬(¬p⇒q)


or possibly instead either


  O(¬p), ¬p⇒q ⊨ ¬O(¬q)


or 


  O(¬q), ¬p⇒q ⊨ ¬O(¬p).


If we adopt the corresponding purely syntactical developments as rules of inference, the absurdity of the situation would then be manifest by a logical contradiction of the form O(¬p) & ¬O(¬p), or something similar.

One might think to formulate a second-order obligation — ∀p:O(¬[O(p) & O(¬p)]), which states that there should be no catch-22s — no inconsistent obligations. We then observe the absurdities as failings of the system itself to be "morally-satisfiable" (i.e. for all of the obligations to actually be fulfillable). Formulating such a meta-obligation would not prevent the inconsistent obligations from actually arising, however; it would just make the failure slightly starker, because it would represent a transgression of the form A & O(¬A), where in this case A = O(p) & O(¬p).

It seems quite clear to me that in these cases, the absurdities O(p) & O(¬p) arise due to the fact that some non-obligational premise — "you will kill someone" in the Gentle Murder Paradox, "you will not go to the party" in Chisholm's paradox, or "telling the truth will hurt someone's feelings" in my example — is incompatible with the imposed obligations. One might say that the conditional in my own example is somehow substantially different, but I don't see how this helps. Indeed, if we recognise that the material implication ¬p⇒q is equivalent to p v q, we may immediately see that


  (p & O(¬p)) v (q & O(¬q))


follows by dilemma. Thus the social transgressions p & O(¬p) which are blatant in Chisholm's and the Gentle Murder paradoxes are all but immediately manifest in my example as well: but it only uses the rule of contractual dependency. This suggests that the moral dimension of propositions, which is introduced on a syntactical level by deontic logic, is accurately represented by factual detachment.

I find it is simply more efficient to recognise that if one wishes to have operators which project propositions to truth-values regarding their social acceptability, it essentially follows that one has constructed a logic in which absurdities of a moral character might arise from statements of fact which transgress the boundaries of social acceptability. Indeed, it is difficult for me to imagine how to avoid this possibility if one wishes to reason simultaneously with propositions p and their obligatoriness O(p) in any way where one could have any bearing on the other.
You are making a moral point I guess, which is: "why should we completely reject what is referred as prejudice, if it is a probabilistic guess based on statistical data".

I think I can see your point clearly.

Let's reformulate it for an event which does not require any morality, for example: the throw of a 6 faces unbalanced die.  Let's assume the die is made in a way that you can not know the real probability of every number as an outcome, but you have the following statistics:

1000000 throws,


5%
6%
16%
67%
3%
3%


If you had to bet any money on the outcome, anyone that is able to see these statistics and know they are actually trustful, will point the money on number 4.  No one will ever say: "Hey, what is this prejudice, we should also point money on other number because it is moral".

Let's translate the above back to the black/stealing example, and assume we have these statistics about thieves (i am giving random numbers):


black: 60%
white: 40%


With the same reasoning, for the next thief that will be brought to jail, one could make a bet on whether s/he is black or white, and based on the above statistics, I would bet on black.

So what about all these fighting prejudice?
That is probably another story.

There are many ways of interpreting the sentence "fighting prejudice", I will try and think about some of them:

First representative one is the one about misunderstandings and false assumptions.  For example, let's say that you have the above statistics, and you randomly take a black person, citizen of the U.S.

Would you bet money on the fact that he is a thief?

If you would that would be a false assumption, because you assume that the probability of a black person to be thief is more than 50%, which can not be deduced by the above statistics.  The percentage could be arbitrarily small (5% for example).

This is a common mathematical error that people easily do, and this kind of thing should be fought, because it is just mathematically wrong.

The second fight is against the mental process that changes a probability to 'certainty'.

This is more a psychological issue rather than a logical one, but it happens quite often that people, relying on the above statistics (let's even assume that the probability of a black being a thief is more than 50%), transform this statistical data into a full-generalization ("every single black person on earth is in reality a thief").  This should be fought as well because it is arbitrary.

Third fight to be done is a moral issue that might be claimed, which is:

Let's pretend you were born in a world where everyone who has the same skin color as you (let's say, green) is also a thief, except for a minority (say 3%) which includes you.
You, on the other hand, did never steal anything in your entire life and can not explain to yourself why other greens have to steal at all.

In this case:


Is it morally correct to you that a person that meets you for the first time probably thinks of you as a thief?
Shouldn't be it more correct that you were given the benefit of the doubt? If it were me, I would probably ask for a par-conditio (50%/50%), because it is not my fault that other greens steal, I am myself and I am extraneous to the process.


For this 3 types interpretation, I think "fighting the prejudice" is morally correct.  I do not think though, that it can be achieved by completely throwing away useful statistical information, that can be always used to predict probabilities of similar events, and only for that.
This would be a great question for the proposed http://area51.stackexchange.com/proposals/45110/buddhism?referrer=yutbd_VLml9XXXrT0S5BhA2 Buddhist site.

In Tibetan Buddhism (what I practice), it has been explained to me that all killing is wrong.  By strict interpretation, yes, killing mosquitoes is wrong.

However, if we dig a little deeper, the answer is more nuanced. Yes, the killing of anything is wrong, but the degree of karma is proportional to the wrongness.  By killing a mosquito, there will be consequences to that action.  Yet those consequences are smaller when compared to killing a human, for example.

(One could have a side discussion about what is worse, but the best answer is you don't know, only Enlightened ones could know.)

There is a parable that the Buddha in a previous incarnation http://www.khandro.net/dailylife_war.htm killed a pirate to protect other people.  This damaged his karma terribly, but he did this for the greater good.  He, being on the path to Enlightenment, knew the consequences.  So can we say this as kill only when necessary?  The issue is not knowing what necessary really is, so be safe, and don't kill.

As for the question if this is "allowed", again, my understanding is you are certainly allowed to do anything; there are just consequences to your actions... always.  

I am sure I didn't explain that as well as a Buddhist scholar, but hopefully it gives you an indication that Buddhism deals with relative concepts as an answer to absolute questions.

Bonus answer: Mosquitoes are definitely sentient to Buddhists.
We can argue that we only exist in the present, our future-selves and past-selves are other people.

At the same time we can argue that our present self doesn't exist at all, because present is an instant, there is no such a thing as "present" me.

If I am typing now, that's because my brain (of my past-self) told the hands of his future-self (aka current present-me) to type a few milliseconds ago.

That works the other way around. You cannot feel your present-self either, all you feel is your past-self, if you feel pain in some part of your body that's because that part sent the message through the nerves a few milliseconds ago. If you feel sad or happy or anything, it's because of memories of your past-self.

Physics about space and time tell us that there is a continuum (or a quantum succession) of your past-selves. Relativity tells us that there simultaneity is relative at two distant points, so there is no objectively simultaneous you to call "present you".

So you don't really ever worry about your future self, but about how your future-past-past-self will make feel your future-past-present-self.
Old question, slightly changed. The answer is: 
Person A should have never given that promise to Person B. Person could and should have known that the two promises could end up contradicting each other, and that therefore giving both promises is a (or could lead to a) http://en.wikipedia.org/wiki/Performative_contradiction performative contradiction (For Person A, in that moment after B committed the crime, has to want to both report and not report). 

I guess one could also say that keeping the promise to the shopkeeper is more important. First of all, it's a positive duty, and Kant held that positive duties are fundamental (cf. Metaphysics 2.I.I §4). Second, giving a promise to keep a secret about a crime is an action not generalisable, so the promise shouldn't have been given in the first place.

I found http://www.google.de/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&cad=rja&ved=0CFAQFjAE&url=http%3A%2F%2Fwww.colorado.edu%2Fphilosophy%2Fcenter%2Frome%2FRoME_2009_full_papers%2FMichael_Cholbi_K%26MoralDilemmas(ROME).doc&ei=pgUnUqSSD4PKhAedmoGQCw&usg=AFQjCNFte2woiyACzMNhGBUqho2b69uYzQ&bvm=bv.51495398,d.ZG4 this paper on moral dilemmas, maybe it's helpful. Otherwise, consider reading the chapter in the original, Kant explains why there can be no opposing duties. Also you might find more information in the answers to https://philosophy.stackexchange.com/questions/259/what-would-kant-do-when-two-categorical-imperatives-conflict-could-he-ever-just?rq=1 this question.
The language is relatively unimportant.  That is, the rights stated may be nominally "natural", but the document itself was a social contract of sorts.  Moreover, it was constructed so as to be changeable later as part of that ongoing contract.  The framers were well aware of these concerns.  

A commentor added that the emphasis was on rights derived from nature, presumably like the right to own slaves.  Plato thought that it was natural for people to serve the state (and to own slaves).  The only right actually given by nature seems to be the right for the powerful to do what they want.. we have to take the other rights ourselves.

So, we have to be careful when we speak of Natural Law.  For many enlightenment intellectuals, Natural Law was the law discovered through reason.  The recognition by the framers that our reason may reach better conclusions implies that the eternal and immutable kind of natural law is not the same kind that they were referring to.
I'm going to say this might be what in Syllogistic logic would be called a very subtle http://en.wikipedia.org/wiki/Fallacy_of_necessity fallacy of necessity, and if so, it's actually a very neat example!

Let's try to phrase the argument formally like this:


  
  All murders are crimes. 
  No Crimes occur when they are legalized.
  Therefore, no murders will occur when they are legalized.
  


The problem with this is that the second premise seems tautological - it's just a simple matter of semantic fact that anything that's legal isn't a crime.  But this doesn't extend to murders being crimes.  It's not simply semantically tautological that a murder is a criminal act - it has to be a matter of legal fact that says that it is indeed illegal to commit murder.  But the conclusion doesn't take this specifically contextual aspect of the first premise into account.  What will happen when murder is legalized is that it will cease to be a crime, and that the second premise, while still true, will cease to be contextually relevant.

As discussed in the Wikipedia article, what seems to be going on is that there is an equivocation between the De Dicto necessity of premise 2 (something that is simply logically necessary) and the De Re necessity attributed in the conclusion 3 (something that is a proposed necessary property of acts of murder).  If we were to interpret the necessity of premise 2 in a De Re manner (asking about necessary properties of acts that are criminal), we would simply say it was false - it's just not necessarily true of anything that in fact is a crime that it wouldn't happen any more if we legalized it.
It depends on what you mean by "correct".

Personally, I take it to be more a rule of thumb for making judgments about the utility of statements in ethics than a strict logical assertion relating to actions. In Critique, Kant writes that "the action to which the ‘ought’ applies must indeed be possible under natural conditions". In other words, we might say that a moral agent can be obliged to do something only if that action is within his capabilities of doing, both physically and in terms of whatever free will we might believe him to possess. So a more precise, albeit longer version, of ought implies can is perhaps if a valid moral judgement or command is made conerning an action, that action must be possible.

Is this correct for all such possible actions? Yes, trivially.

Is this correct for all (imagineable) actions? I'd say no. It is instructive to play around with the statement logically. Try to express the contrapositive (transform "a implies b" into "not b implies not a", which are logically equivalent). It will be can't imples shouldn't... does that make any sense? Not much in my mind. Treated literally, prohibiting something that is not possible seems meaningless. For example: you can tell a child that it ought not to fly around in the sky as a bird, but this will not have much influence on whether the child does takes your advice, since it is not within the child's abilities to fly at all. Because it has no choice.

On the other hand, there are valid oughts and invalid oughts and the consequences of their proclamation. That is, in the real world moral commands and judgements are actions in themselves, at least to the extent they influence other actions. The child may well try to fly when told not to, even though the shouldn't you dispensed is not rooted in a physically possible action. This can easily get complicated by what the child knows and believes to be true and how it reacts to our invalid ought. Is the essence of the prohibition (the ought) in our not wanting the child to fly, or in the child's understanding that we do not want it to try to fly? Who's ought is it? The same questions arise for valid oughts (you should not run near the pool), but the point here is that judgements made about impossible actions are real and affective. If we choose not do classify such judgements as real oughts, or widen the range of possible actions to all in principle possible actions including ones we can only imagine, then we return to the trivial case I started out with.

So, I think things eventually boil down to a consideration of the definition of possible actions, and therefore of the nature of human (free) will. From there it is only a step to how this is related (or not) to determinism, which has been written about a lot.

One interesting observation here is that if ethical statements are possible only for actions that are within the attainable realm (the strict sense), then increasing the possible also increases that domain on which we can make ethical statements. So, as the possibilities grow with technological progress, humans in that sense become "more ethical". I find that intriguing, and there is some discussion of this aspect in Dennett's book Freedom Evolves.
In roman languages, the "sun" has masculin gender (il sole), while the moon is feminine (la lune). Yet, in german, it is the other way: "die Sonne", "der Mond".

If gender in language was not (only) a grammatical category, one would have to explain this.
Whether or not teaching mathematics to children helps to spread the idea of equality is an empirical, psychological, maybe educational question. It is not a question of philosophy.

That said, why should mathematics help with equality? There will be a whole bunch of counterexamples, crazy murderer that are genius at math, early civilizations (most notably ancient greece) that thought a lot about math while celebrating slavery. 

And then, as Rex Kerr pointed out, humans are, according to all our sciences, not identical. Therefore invoking the "=" seems to be out of place. 

As to your question, I don't know about any philosopher who thought about this, but maybe you will find someone in gendertheory/sociology who writes about this.
I should preface this by saying I haven't read Kant and this is just an account of Popper's position, including his position on Kant and not an account of Kant's position in its own right.

Popper's position on how knowledge is created goes like this. You start with your current ideas and look for problems with them: clashes among those ideas, or clashes with results of experiments. You then propose ideas that might solve those problems. You criticise the proposals until only one is left, at which point you have solved the problem. The criticism might include doing an experiment for which your chosen idea predicts a particular result, or looking for clashes or whatever. But the criticism is aimed at eliminating some proposed solution and that helps you decide what to do. If you didn't have any such standard you couldn't make a decision about what to test or why. Then you look for new problems.

In the first paragraph you quoted, what Popper is saying is that Kant is right that you need to have an idea before you can do an observation. In the second paragraph you quoted Popper is saying Kant was wrong to think that the idea you had before the observation must be correct. So what observations you do are determined by what ideas you have but the results are not. (I think it's a bad idea to describe this as imposing our ideas on nature.)

Now you say


  However, given Kants starting point I think Kant means knowledge of a more basic kind - measuring a metre, measuring a second, that there are two bottles there; or taking that this has caused that. 


You can measure a metre while having a bad idea about what should count as a metre. The standard for what counts as a metre has changed over time. It used to be defined in terms of a special type of metal bar under suitable conditions and is now set in terms of the speed of light. The metal bar standard was good enough for a lot of measurements but not good enough for some more recent measurements, see

http://www.nist.gov/calibrations/upload/4998.pdf http://www.nist.gov/calibrations/upload/4998.pdf.

The only question to ask about measurements in terms of the old standard is whether they require the accuracy of the new standard or not to test the ideas they were supposed to test. For example, if a biologist measures the length of a dolphin to presumably does not care about the length down to the last angstrom so then he shouldn't care about the change. 

Next you write:


  But then what does he mean by this? If Nature 'resists' our laws forcing us to 'discard' them; then it appears he is describing the scientific laws - those of chemistry or physics, say. Surely he cannot mean the laws grounding experience - those of space, time, causality & quantity?


I don't understand what "laws grounding experience" might be. Knowledge is not grounded on anything: it consists of guesses controlled by criticism. Our ideas about how to measure quantities have changed. Ideas about space, time and causality have also changed, e.g. - relativity of simultaneity.

Now, with respect to the last paragraph you quote from Popper, what he is saying is this. Suppose that we can impose our ideas on the world, in the sense that those ideas actually have to be true. Then why is it not the case that every idea we come up with is true? This makes sense as an objection to the idea that Popper attributed to Kant.
What do denominators have to do with medians? Your question is confusing to me. However, I'll just talk about medians. 

The median of a (finite, ordered) set is the value with an equal number of elements of the set above and below it.

If you want to extend the notion of median to infinite sets using the same definition, that's ok ... but you have to be careful. Here are two examples.

You can say that any member of the countable set of integers is the median. That's because there are always countably many numbers less, and countably many numbers greater, than any value. 

So if you say that the median of the ordered set ..., -3, -2, -1, 0, 1, 2, 3, ... is zero, you're right. And if you say the median is 47, or -119, you're still right! Every number is the median, by the definition of median.

But if you take the ordered set of natural numbers, 0, 1, 2, 3, 4, ... then NO number is the median. Because if you take any number like 47, only finitely many numbers are smaller than it and infinitely many are greater. So no number is the median. 

You see that the notion of median depends critically on the particular order that you put on your set. 

Countability doesn't have much to do with this, the same discussion goes through for uncountable sets. And remember, there are lots of different ways to order sets of a given cardinality; and the median is a notion based on assigning a particular order to the elements of a set.
There are lots of problems with representative democracy as a means of finding an optimal governance solution.


Voting drops the distinction between strength of preference, so 6 people who mildly prefer A to B can outvote 4 people who would be devastated by A.
Representation requires one to select from a small number of options that may not allow one's preferences to be represented.
There is no requirement for expressed preferences to be based in reality; people may prefer things based on confusion and misunderstanding and thus not even want what they would actually prefer if only they paid attention.
There are few mechanisms for long-term consistency of goal, so short-term interests capture an excess of attention.


In addition to these, I think you're correct that there's an inherent tension between individual happiness as the metric of success and any sort of government or social arrangement (democratic or otherwise) since sacrifice can improve overall happiness at the expense of individual happiness.  If the culture admires sacrifice, the individual who sacrifices can regain some measure of satisfaction; if not, they are culturally perceived as a loser.  Studies have backed up that http://amj.aom.org/content/34/4/827.short cooperation is more difficult to achieve in a more individualistic cultural tradition.

Despite this, it's not clear what alternative might be superior.  It is hard to beat individualism for an easy way to motivate people, and with regard to Democracy we must remember Winston Churchill:


  No one pretends that democracy is perfect or all-wise. Indeed, it has been said that democracy is the worst form of government except all those other forms that have been tried from time to time.

"made up stuff" - in a sense, all stuff is "made up" :)

What do you mean by "manufacture emotions" - these are still emotions. Even more, they are shared emotions. I get the notion that you want to ask:

Why people care for things that don't have direct implication to life?


The answer is because people are social beings, and caring is sharing. People get invested in holidays and sports because it's a way to share experiences, and fulfill their social needs.
Put simply, it would only be "immoral" to people who hold that the act of becoming a hermit resulted in violates a moral principle. If you are asking whether most people in this day and age believe it is immoral, I would imagine most people would see nothing intrinsically wrong with it. I feel like — in developed nations at least — we generally don't mind people doing what they want to do as long as it doesn't harm others. If you believed it did harm others (regardless of whether or not you could prove that), then you would hold it to be immoral. Otherwise, you would not.

If you believe that it is everyone's moral duty to further the human race, and by becoming a hermit a particular hypothetical person would less likely be in a position to do so, then yes it would be immoral for them to do so according to your beliefs. But perhaps the person in question is a serial killer; alienating themselves from society in such a case would be a moral good.

Does the number of people who take an action make it more or less moral?

You bring up sizes in your question which I find interesting (sizes of groups who make decisions / take actions which would be judged morally). I've seen https://philosophy.stackexchange.com/questions/8655/a-contradiction-in-kants-universalizability-principle this kind of reasoning in other questions as well:


  does the question of immorality depend on the number of people with
  the potential to commit the so-called immoral act?


Moral acts are judged in and of themselves. The number of people who take a particular action (moral or not) is irrelevant to rightness or wrongness of any individual moral act to both deontologists and consequentialists alike. Each and every action is its own case. For example, consider a scenario where there are 10 pieces of bread and 11 people to feed. One of these people to feed is a starving 6 year old girl (where as everyone else is not in such dire circumstances). To take bread from the pile at all before letting the starving girl have her piece (assuming 1 would fill her) is likely seen as immoral to most people. Now if 10 people do it and leave the girl without a piece of bread, is it less immoral for any of them to have made their decision to take a piece of bread because many people did it? No, and I think most moral people in the moral community I live in would agree. It doesn't matter how many or how few people did it, the actions of each and every one of them was immoral. 

Using your Hermits scenario as an example of why numbers don't matter

It is either immoral for a particular person to be a hermit under the circumstances they are in, or it is not. The quantity of people partaking in a particular action only matters in that it may change the circumstances, but that doesn't affect the morality of the decision. For example, let's say I hold that:


human happiness is the greatest moral good, and
humans must be alive to achieve happiness.


I also believe that:


becoming a hermit (unless you are a serial killer) reduces productivity and social cohesion and therefore marginally reduces happiness.


The act of becoming a hermit would be then immoral, logically. It would be odd, I think, to have a moral framework in which you thought,  "It's not immoral for the first several thousand people to be hermits because their actions don't affect us much, but once more than 50% of the population is hermits, that really starts to tax our well-being as a species so anyone who decides to be a hermit after the 10001th person (assuming that's half the population) is now considered immoral." No, the actions of the first people to become hermits were immoral before, they were simply ignored because their impact was lesser.
As far as violating codes goes though, it matters what kind of code it is and I think that that should be noted. If violation of the law or code is malum in se (or wrong in and of itself based on individual belief) then that is when the reality of the group is dissolved as there can be no union if individuals are divided from one another.

However, if the violation is only determined as being wrong by the group, and any violation of the "code" is not a violation of individual beliefs, only group beliefs, then the group would not really dissolve as the individuals themselves are not yet divided.

One final note, an individual or even multiple individuals can be severed from a group, and yet the entirety of the group can retain its solidity though some members no  longer are really a part of the group. 
The German is not at all ambiguous. But part of the problem is that they involve a play-on-words and work from the most basic parts of the German language.

I wouldn't get too caught up on holding on to the terms specifically. After all, those are just what one translator decided to go with. As long as you grasp the concept, you can reword this in other ways. For instance, you can call ready-to-hand hammer-ready without losing the Heideggerian reference. And you could present-to-hand something like obnoxiously present -- in reference to the way the object is now before us as something precisely because it is broken or not working. 
Yes, we live in one. What was regarded as mathematics 2000 years ago is not what we regard as mathematics today. Gauss published the first acceptable proof of the Fundamental Theorem of Algebra; but Gauss's proof would not be acceptable from an undergrad today. Standards of rigor, as well as our understanding of the topology of the real line, have changed considerably since then.

Mathematics is a historically-contingent activity of humans. Not only could mathematics be different on a different planet or in another universe; which are of course unprovable one way or the other; but mathematics could and actually has been different at different eras on this planet.

Just consider the rise of computers, experimental mathematics, machine proof systems, and computatibility theory. It's likely that math in 100 years will be very different than math is now. Zermelo-Fraenkel set theory is less than 100 years old. What if on some other planet they never discovered it, but rather skipped to some other framework?

Now, you may be referring not to the mathematics as a historically and culturally contingent human activity; but rather as some sort of Platonic thing that is "out there" that we can discover. To which I'd ask: Where is your evidence that such a thing exists? And if it does, then which human mathematics is the one, true mathematics? The math of 1000 years ago? The math of today? Or the math of 1000 years from now?

I do realize that you're asking if it's possible that in some other universe, 2 + 2 is 3. I have no idea. I don't think the question is meaningful. I think I'm wearing my formalist hat today. 
Looking through McGee's list of counterexamples to modus ponens on 462-463, I think what you are saying is fundamentally correct. Or to put it another way, McGee is objecting to drawing an equivalence between how the conditional operator works and how if-then works in natural language. But I think he's mis-locating the problem.

I have found students are often uneasy with the truth table for the conditional operator:

    A   B   A --> B
    T   T      T
    T   F      F
    F   T      T
    F   F      T


For this, students are often bothered by the truthfulness of the operator when the antecedent is false. They wonder legitimately why a false antencedent should be able to give us a truth value for the entire claim.

What McGinn seems to have missed which many of my students understood is that if-then != -->. One must keep in mind what is meant by the natural language before translating it into  symbolic language. And that means sometimes that one must translate it to something else. In other words, I think McGinn is creating his own problems by imagining a strict correlation where there is a strong one. I place the problem not with modus ponens but with thick-headed symbolization.
Hopefully the following will give you some food for thought.  In brief, and somewhat loosely, for the greeks essence preceded existence, but the romans turned it around :-

n.b. eidos = form or essence


  In Greek thought energeia means “standing in the work,” where “work”
  means that which stands fully in its “end.”  But in turn the
  “fully-ended or fulfilled” [das “Vollendete”] does not mean “the
  concluded,” any more than telos means “conclusion.”  Rather, in
  Greek thought telos and ergon are defined by
  http://en.wikipedia.org/wiki/Eidos eidos; they name the manner
  and mode in which something stands “finally and finitely” [“endlich”]
  in its appearance. ...
  
  Aristotle says this in his own way in a sentence we take from the
  treatise that deals explicitly with entelecheia (Meta. , 8, 1049 b
  5): fanerin oti proteron energeia dynameis estis: “Manifestly
  standing-in-the-work is prior to appropriateness for....” In this
  sentence Aristotle’s thinking and pari passu Greek thinking, reaches
  its peak. But if we translate it in the usual way, it reads: “Clearly
  actuality is prior to potentiality.” Energeia, standing-in-the-work
  in the sense of presencing into the appearance, was translated by the
  Romans as actus, and so with one blow the Greek world was toppled.
  From actus, agere (to effect) came actualitas, “actuality.”
  "Dynamis became potentia, the ability and potential that
  something has. Thus the assertion, “Clearly actuality is prior to
  potentiality” seems to be evidently in error, for the contrary is more
  plausible. Surely in order for something to be “actual” and to be able
  to be “actual,” it must first be possible. Thus, potentiality is prior
  to actuality. But if we reason this way, we are not thinking either
  with Aristotle or with the Greeks in general. Certainly dynamis also
  means “ability” and it can be used as the word for “power,” but when
  Aristotle employs dynamis as the opposite concept to entelecheia
  and energeia, he uses the word (as he did analogously with
  xathgoria and ousia) as a thoughtful name for an essential basic concept in which beingness, ousia, is thought.


http://religiousstudies.stanford.edu/wp-content/uploads/1940-PHYSIS.pdf M. Heidegger: On The Essence and Concept of Physis in Aristotle's Physics, pages 26-27

Note, the Roman Catholics have for God, http://www.newadvent.org/cathen/01125b.htm Actus Purus.
I'm new to the site and don't exactly understand how strict is your classification of "ethicists", so I'll name the philosophers I know.

The first viewpoint is definetely older and extends from divine right to authors such as Carl Schmitt. Schmitt said that the POWER is in the hands of he who DECIDES about the state of exception. The sovereign is the one who decides when the law system begins, ends and of what it is constituted. If he can suspend the efficacy of norms, he is outside of their power, above them.

Hanks Kelsen, who spent a great part of his life arguing against Schimmit, believed that the legal system derived from the Grundnorm, or basic norm. Those who create a state's Constitution, based on the common perspectives of the people it affects, are but representing their will. After it's completion, the Constitution is above all. If that text grows old, it can either be changed, reinterpreted, or put aside for the making of a new one.

What I think is most important is not choosing sides, but understanding that, no matter what, Law is violence, as Walter Benjamin noticed, and is always the imposition of the will of some over the majority.

I don't think I've answered your question and I know I have serious english issues, but I recommend the reading of Kelsen's "Pure Theory of Law", Schmitt's "Political Theology" and Benjamin's essay "Critique of Violence".
First, you must be using humanist in an idiosyncratic way. There's no necessary exclusion between believing in God or having the values of society and being a humanist.

There's a speech that gives at least one humanist's answer: Existentialism is a Humanism by Jean-Paul Sartre. You could start there. In that text Sartre argues that even if there is an external meaning to life as supplied by God or some other force, it is ultimately still up to each individual to figure out why they are alive and what purpose they think life has. Using the example of a solider who needs to decide whether to fight for France as a patriot or to obey the [Roman Catholic] Church and his mother and become a priest, Sartre points out that all of our beliefs are to some extent chosen.

I'm not necessarily in agreement with Sartre, but I think Sartre does make the valid point that we shouldn't identify choosing as the problem with purposes in life. Your critique might still remain but with the qualification mere choosing.

Sartre's account is actually quite similar to Heidegger's in this respect since for Heidegger the meaning of our lives is that we die. Thus, the challenge is that we need to accept that and still live. (For this reason, Heidegger was sometimes called an existentialist though he denied the label).

For a more elaborate version of the critique you raise against choosing one's own purpose in life, I think the best choice is to turn to Kierkegaard's pseudonym Anti-Climacus and specifically the text The Sickness unto Death. While not denying the need to make a choice towards a purpose in life, he points out that choosing a purpose of one's own origination lasts only as long as the willing toward that purpose lasts. i.e., as long as I want to find my identity in my political persuasion, I can do so. But the moment I stop doing so, the entire identity is lost. For Anti-Climacus, the solution is to find your identity in how God identifies you and depend on that fixity. (I have a forthcoming paper on this topic in IPQ later this year).
According to Transcendent School of Philosophy founded by Mulla Sadra, a 16th-17th century muslim philosopher, mental entities or spiritual forms that comprise human soul, contrary to material forms, are static. Material forms are subject to an inherent universal gradual motion taking on a different form at each instant after dropping their previous form, whereas human soul absorbs infinite number of forms during a lifetime without loosing the previously absorbed forms. Human soul is thus a constantly developing and expanding being. I recommend you to study this exposition of Mulla Sadra's innovative theory of human bodily and spiritual becoming: http://www.scribd.com/doc/34529566/The-Soul-as-Barzakh-Substantial-Motion-and-Mulla-Sadra-s-Theory-of-Human-Becoming The Soul as Barzakh: Substantial Motion and Mulla Sadra's Theory of Human Becoming

You might as well be interested in https://philosophy.stackexchange.com/a/10124/5577 this question and my answer where I also explain the metaphysical agent responsible for human development.
Inconsistent theories do not mean that they are trivial, but they may be - for example in classical propositional logic, once a contradiction is admit the theory becomes trivial and essentially useless.

To adopt a non-classical logic may mean that one is simply playing games with the formal structure of the logic, which one adopts or tires to adopt an interpretation of what the logic means - its semantics. For example classical logic uses Tarksian semantics, the same semantics that Wittgenstein adpted in his *Tractatu*s, which is that truth corresponds to existance, and falsity to non-existence. 

The BHK semantics, first put forward by Brouwer, is to assign truth the semantics of proof. It turns out then that the most natural logic is intuitionistic where the excluded middle is denied, and it is a multi-valued logic. This is not a logic that allows contradictions, but its dual does.

One might adopt such a paraconsistent logic to model epistemological truth where what is known may be in conflict with each other, so boundaries between domains of knowledge need policing. One might then argue that this is knowledge as represented to us, not truth in itself. This leads us into the deeper waters that Feyerabend is pointing out. If one adopts the principle that the Jain philosophers expounded on, anekantevada, or more than one point of view as being absolute, in a similar sense that Wittgenstein said of the logic being the limits of our world and not the world, then inconsistent logic is inevitable; Feyerabend took his at least some of his cues from Hume and Kant who placed logic in the realm of human knowledge, as opposed to being a property of the world itself. 

This is still different from dialethism or true contradictions - formally for example one solution of the liars paradox is to adopt a three-valued logic which allows a statement to be both true & false.  One still of course has to explain quite what this means. The Dao begins with a famous line:


  The Tao that is the Tao is not the True Tao


One can take a dialethist position on this statement, or an epistemological position, in that truth that can be grasped by the human mind is always partial, and 'true' truth always eludes us. The Catuskoti/tetralemma shows also that the formal consequences of considering dialethism was considered formally - and one possible consequence of this, in at least Buddhist logic, is that an atom can be both a point and not a point. This gives a new picture of the continuum, which as Aristotle argued against the Greek atomists must have cohesion; this is not far from the conception the intuitionistic continuum where a similar view is taken.

Hegel is probably the major philosopher in the West to have taken contradictions seriously - from Being & Non-Being comes Becoming is one of his early triads that sets his Geist in motion, or one might say is his Geist, for  contradiction is "the root of all movement and vitality."
Regarding the statement


  Mathematics is rife with contradictions


Most people would say that this is wrong. Certainly: this is not known to be true. Indeed, if you could show that this is true, you'd become world famous.

For a decent first discussion of possible inconsistency "of mathematics" -- rather: one of its widely used foundations -- , see this MathOverflow thread:

https://mathoverflow.net/questions/40920/what-if-current-foundations-of-mathematics-are-inconsistent What if Current Foundations of Mathematics are Inconsistent?

And notice the punchline, modulo a bunch of qualifiers and subtleties: there is no particular indication that common foundations are inconistent, but also no proof that they are not, either. In any case, it is not true that known mathematics is rife with contradictions.

Moreover, the typical mathematician, ideal or not, is in fact very much disturbed when confronted with the claim that mathematics might be inconsistent. When Vladimir Voevodsky publically and prominently talked about this possibility in 2011, several people were quite dismayed. You can find long discussion of this on the "Foundations of Mathematics" mailing list, starting with http://www.cs.nyu.edu/pipermail/fom/2011-May/015407.html this thread, continuing with http://www.cs.nyu.edu/pipermail/fom/2011-May/015442.html this one and many followups (unfortunately the list is not usefully indexed or easily searchable, you have to click yourself through the archives...).
This is a top Google result for "Phaedo 75a" and only other answer is unhelpful, So I will respond despite the fact that this question is over a year old. 

Socrates argument is that we always have knowledge of the Forms, but that we are not always conscious of that knowledge (we are conscious of knowledge when we "...hold it in [our] grasp and not have lost it" Phd. 75d). Specifically Socrates argues that we forget our knowledge of the Forms when we become embodied at birth (76c-d) Recollection is not a process through which a person grasps new knowledge, but a process through which a person becomes conscious of the knowledge which they always have but previously forgot.

Now to address your specific concern. The Greek word translated 'it' in "we have not gained knowledge of it, and... it is impossible to gain this knowledge, except by... the senses." (75a) is αὐτό (auto) which is neuter singular and therefore must refer to "the Equal" in Socrates' proceeding statement, "All these are seeking to be like the Equal, but fall short of it." (75a) because the Greek word translated as "the Equal" is ἴσον (ison) which is a singular neuter substantial adjective (meaning it function as a noun).

Your problem then is how to make sense of the fact that Socrates later says that "... before we began to... use the... senses we must have gained a knowledge of abstract or absolute equality..." (75b)

The answer to your problem is that the translator is translating two different Greek words as knowledge. In the first sentence (74a) knowledge translates the Greek word ἐννοέω (ennoeō) "have in one's thoughts, consider, reflect," and Socrates' point is that while embodied we can only become conscious of our knowledge of the Equal through sense perception (Socrates thinks this is the case because, while embodied, the soul is not free to consider the Equal by itself, 79a). In the second sentence (74b) knowledge translates the Greek word ἐπιστήμη (epistēmē) which is the standard Greek term for "knowledge." Socrates point here is that we can only gain knowledge of the Equal before we are embodied, when our soul is not burdened by a body.

Thus your problem can be resolved by distinguishing the two terms. Socrates does not contradict himself because he states that we cannot gain knowledge in the same way that we become conscious of knowledge while we are embodied. Of course you could never have known this without knowing Greek, and it was a failure on the translators part not to make the distinction clear in English. The English translations on Perseus are generally bad because they are old (Perseus is a free service mainly for the Greek text so they don't want to pay contemporary publishers for the rights to better English translations since they aren't charging for anything).

But good job watching out for contradictions! That's exactly the right way to read a philosophical text, and it's especially good philosophical practice to investigate whether you're right that something is a contradiction or whether there's something you don't know that resolves the problem. Sorry that I wasn't here to help you when you asked the question, but hopefully this will be useful to another careful reader, like yourself, in the future. 
First, selfishness does not mean putting yourself before the masses. It means doing what is in your rational self interest. In order to make your life better you have to try to discover stuff about how the world works and how to change the world to make your life better. Rationality is about accepting the responsibility of judging issues yourself, rejecting contradictions, not trying to get away with faking and other stuff. Objectivists think there is no conflict between the rational self interest of different people.

Second, rich people who are not rational may lose money under capitalism. There are many characters in Rand books that fit this description, e.g. - James Taggart in Atlas Shrugged. Only rational rich people, such as Rearden in Atlas Shrugged, will make money. So capitalism is in their rational self interest, acting on whatever whim the rich person wants to indulge will go badly for him.

Third, rational poor people will also do well under capitalism but not under socialism. Under capitalism they will be able to improve their lives. Under socialism their interests will be sacrificed to those of people who are irrational. Socialism is not in anybody's self interest since it involves sacrificing rational people to appease incompetent and irrational people. And incompetent and irrational people who refuse to improve are going to have a bad time anyway since they are trying to evade reality.

See "Atlas Shrugged", "Capitalism: The Unknown Ideal",
http://aynrandlexicon.com/lexicon/rationality.html http://aynrandlexicon.com/lexicon/rationality.html,
http://aynrandlexicon.com/lexicon/capitalism.html http://aynrandlexicon.com/lexicon/capitalism.html, http://aynrandlexicon.com/lexicon/socialism.html http://aynrandlexicon.com/lexicon/socialism.html,
http://aynrandlexicon.com/lexicon/self-interest.html http://aynrandlexicon.com/lexicon/self-interest.html.
http://aynrandlexicon.com/lexicon/selfishness.html http://aynrandlexicon.com/lexicon/selfishness.html.
The first definition isn't very good. You say


  Ethics refer to society idea of what is right and wrong 


which is fine, but then you add


  ... and we do it because society says it is the right thing to do.


Many people would disagree with this. There is some extra stuff here that is not part of the definition of ethics. Here is the http://www.oed.com/ OED definition (first one)


  Ethics Moral principles, or a system of these.
  ...
  b) The branch of knowledge or study dealing with moral principles.
  c) Moral principles; maxims, precepts, or observations concerning these.


Ethics is concerned with what is right and what is wrong, for whatever reason that might be: it might be something to do with society, it might not. Ethics can be interpreted using social contracts, but they need not be. https://en.wikipedia.org/wiki/Deontological_ethics Kant's ethics are the counter-example that comes to mind.
There could be something wrong with such behavior but there are several features that are necessary to make sense of it.　First, we are going to need some manner of moral theory. Without that, we are just grasping at straws. There are three main moral frameworks considered by contemporary ethicists in the West (whether this is good or bad we leave aside):

Consequentialist frameworks evaluate actions on the basis of their outcomes. They seek either to maximize or minimize some quantity ("happiness", "justice"). Such an account will actually have the hardest time identifying what is wrong with a person who is inconsistent in this regard. This is not to say consequentialists cannot give a reason why such inconsistency would be wrong -- just that it would be complex.

Deontological accounts emphasize rules. Here, you could have a view that contains a rule of consistency or self-disclosure. (Truth-telling may not suffice if the person claims their views are genuinely changed). But it seems sensible to be responsible for what one says and does.

Virue ethics accounts look at the development of character. Clearly, being inconsistent is an inverterate form of behavior that isn't going to develop the character of the individual involved. Here, the moral fault would be in having the sort of personality that can flip between such strong emotions rather than being more mellowed on an issue where one is uncertain.

Returning to the consequentialists, they can argue that there is something awry about the consequences insofar some sort of inconsistency leads to a reduction in goods (e.g. if I can trust my business partners only 50% of the time, then this makes it impossible to work to maximize profits).

But the word "wrong" can also be used in a legal context. It's hard to see how this is illegal unless the speaker is under oath in both cases.



Of course all of this is more complex if the agents have a duty to perform in certain ways that appear inconsistent but are not. E.g., lawyers and advocates. In such a context, it is understood that there is a responsibility to argue passionately for the position one is entrusted to defend. (N.b., there are still cases where advocacy represents a moral failing).
The theory you described is a part of the http://en.wikipedia.org/wiki/Economic_calculation_problem#Coherent_planning "Economic Calculation Problem" and would be affected by technology's ability to handle big data. 

How?

By making the point irrelevant to the debate over central planning & decentralized planning. In theory a centralized group could make informed decision about production that would be backed by irrefutable (in theory) numbers and facts.

The important implication of technology is that it is a tool. It can come up with the perfect utilization of resources to meet peoples needs or it can come up with the perfect utilization of resources to increase profits for my company. Technology is just as biased as those who create it.
There are really two interrelated questions at stake here. First, there is a question about the "sanity" of an individual relative to a culture. Second, there is a question about the "sanity" of the views of that culture. Much is hidden in the term "sanity" here however.

The first question is easily resolved. An individual is "sane" relative to their culture if they express views that are within certain norms for their culture and its expectations. What this does not make clear at all is whether this is something we should grant is "sane" or something we should say is "sane in the views of that culture." To know which, we will need to take a stand on the relativity of value.

This is where the second issue comes up. Here, I would maintain that merely because something is well-adjusted to a culture does not mean we should call it "sane." But this is because I believe there are some objective features of human flourishing (See for instance http://fordham.bepress.com/dissertations/AAI3543390/ here). We needn't think the list is exhaustive (e.g., we are not losing out if we have a different musical scale), but if we think there are some objective facts about human flourishing, then it stands to reason that we can consider a culture that works directly against those to represent a failure of sanity -- and adjustment to that culture to also be a failure of sanity.

One sentence in the quotation strikes me as odd: "This was not forced by social norms but a result of the strong shame." I don't know what this is supposed to mean -- as social norms are often enforced through shame. Legal norms perhaps would be enforced through laws and punishments meted out in the name of justice.

Thus, the problem presents several points of difficulty:


Are we relativists about value or do we think there are at least some objective features?
If we are relativists, are we to frame our evaluations in our own terms or in the terms of the culture we want to evaluate?
When we say "sanity" do we mean a merely cultural notion of adaptation or do we mean to refer to our modern ideas about psychology or do we mean healthy relative to some ideal of flourishing?

First, stand-your-ground laws actually protect the right to not retreat; they're codifying that a person need not attempt retreat from a situation in order to have a valid self-defense argument.  Basically, they're deliberately throwing out any argument based on an individual having an obligation to retreat.

Second, the necessary statistical work to make an argument from utility for these laws or against them is hard to get.  We can catalog deaths in locales with the various rulings, but any attempt to say "X deaths would have been prevented" solely from statistics is an assumption.  This is because you would have to know the consequences of a different choice in a complex situation with multiple actors in order to make a factual statement that the death could have been prevented.

Third, if you replace "obligation to retreat" with "obligation to attempt de-escalation", you have the basis of civil society.  The precise extent of de-escalation one is obligated to do is the crux of the question - the "retreat" phrasing implies that one is obligated to attempt all possible means of de-escalation before using lethal force in self-defense.

Last, assuming these laws are discriminatory is a tricky position to hold.  The law, as defined, does not specifically bar anyone from anything outside of a conflict.  I could still literally walk around with an arsenal regardless of whether this law is on the books, legality unchanged.  The law only applies to measures taken to avoid the use of lethal force - it does not affect the means by which I might use lethal force.
Both democracy (in its purer forms) as well as populism are ideologies involving a rule of the people by the people or representatives chosen/ selected by the people.


Democracy focuses more on the assignment of governance to the people regardless of class
Populism focuses more on the struggle between the common class and the elite class


Populism also lends its name to the practice of formulating policies and political (and other) maneuvers around the nurturing and harvesting of support among the most populous sections of a given population. This may be exercised in a democratic country.

"Vote-bank politics" involves populist maneuvers to gain the favor of a given sizable portion of the population to assure election over an entire population.

As such - populism can be exercised as a means to gaining or retaining power over minorities through gaining the favor of majority stakeholders. This may lead to 'Tyranny by majority' - a situation where the wishes of the many outweigh the wishes and needs of the minority.

One last comment on the matter is that while a system of governance or rule may be popular - it may not necessarily be right - and can often fall a long way short of perfection.

A lot more could be said on the subject - but would fall outside the scope of your question.
There are several reasons why quantum suicide is typically raised. The first is that the other interpretations are typically extremely unclear about how the world works and what exists in reality, see

http://www.daviddeutsch.org.uk/many-minds-interpretations-of-quantum-mechanics/ http://www.daviddeutsch.org.uk/many-minds-interpretations-of-quantum-mechanics/

As a consequence, they are unclear about how whether the terms that appear in equations correspond to reality, so it is difficult to say anything about what implications the theory has for anything.

The second is that in the MWI everything that can happen does happen. People sometimes naively think this implies that literally anything can happen. But this need not be true. It may be the case that if you put a gun to your head and pull the trigger there may not be any universe in which you survive.
A few approaches immediately come to mind:

1) A free-market approach would say that Country B shouldn't try to "tackle" immigration or restrict it, Country B should freely allow for immigration until wages (prices of labor) stabilize in both countries.

2) A utilitarian approach would look not just for the wages to stabilize, but the marginal utility gained by making life better "overall" vs. the (likely greater) utility lost by citizens of country B whose wages were reduced, and try to set caps on immigration and floors on wages accordingly.

3) Some religious traditions would ignore the potential economic benefit to country B and look only at immigration as an opportunity to do something good for someone. Independent of whether members of country B did not cause country A's misfortune, these traditions would say we still owe portions of what we have to the less fortunate.
You are right that the analytic/synthetic distinction is key to positivism, but you also need to make some important qualifications and the important difficulties for that distinction don't arise from speech act cases, but from other issues in the philosophy of language. First the qualifications: Not everybody who believes in an analytic/synthetic split is a logical positivist, but every logical positivist believes in the analytic/synthetic distinction. Showing that the distinction isn't exhaustive, or that it doesn't exist at all is one way to undermine logical positivism, but it isn't the only way. 

The more important point is that the positivists assimilate two other distinctions to the analytic/synthetic distinction. These are the distinctions between necessity/contingency and aprioricity/aposterioricity. 

Briefly: a statement is necessary if it is "true in all possible worlds" (i.e. if it just has to be true), otherwise it is contingent. Likewise, a statement is "a priori" if and only if it is capable of being known without experience. Positivists think that analyticity, necessity and aprioricity turn out to all be the same thing. The only necessary truths are the trivial truths of definitions. Everything else is a contingent fact knowable only by empirical observation.  

This is just wrong, however. Considerations of space forbid a full defense of the claim here, but the seminal book about why analyticity, necessity and aprioricity are all distinct notions is Kripke's Naming and Necessity. For an accessible introduction to it, see the relevant chapters of Scott Soames's book "Philosophical Analysis in the 20th century"
According to the standard theory, it is not true that there was nothing before the big bang, because there simply was no "before the big bang". The universe exists since the beginning of time; it's just that this beginning of time was about 13 billion years ago. Since there is no "before the big bang", the statement "there was nothing before the big bang" becomes meaningless because it presupposes that there was a "before the big bang" when there could have been either something or nothing.

Note however that this is not evidence-backed knowledge. We can get evidence for things happening only fractions of seconds after big bang, but we simply do not have knowledge about what happened at big bang, or even whether such an event actually happened (note, however, that often when people speak about the big bang, they don't actually mean that point, but the processes after; in that sense big bang theory is scientifically confirmed). Indeed, there are some theories which suggest that the "big bang" may actually have been a "big bounce". In which case there certainly would have been a time before the big bang, and there clearly was not nothing back then, but a complete universe that collapsed.

As of death, it is obvious that in this universe you don't exist any more after the death (both the matter and the information which happened to make up you do still exist, but they are no longer organized in a way that would be you). Even if somewhere in the universe there should by chance happen to be an exact copy of you at the time of your death, it would still be a copy of you, it would not be you. Whether you continue to exist outside the universe is something which we cannot find out as long as we live inside the universe, and therefore we can only believe or disbelieve, at least until we actually die.
Monads are individuals, in a metaphysical sense; thus, they are not like atoms of modern physics.

From http://en.wikipedia.org/wiki/Gottfried_Wilhelm_Leibniz Gottfried Wilhelm Leibniz :


  Leibniz's best known contribution to metaphysics is his theory of monads, as exposited in Monadologie. According to Leibniz, monads are elementary particles with blurred perceptions of one another. Monads can also be compared to the corpuscles of the Mechanical Philosophy of René Descartes and others. 
  
  Monads are the ultimate elements of the universe. 
  
  The monads are "substantial forms of being" with the following properties: they are eternal, indecomposable, individual, subject to their own laws, un-interacting, and each reflecting the entire universe in a pre-established harmony (a historically important example of panpsychism). Monads are centers of force; substance is force, while space, matter, and motion are merely phenomenal.
  
  The ontological essence of a monad is its irreducible simplicity. Unlike atoms, monads possess no material or spatial character. They also differ from atoms by their complete mutual independence, so that interactions among monads are only apparent. Instead, by virtue of the principle of pre-established harmony, each monad follows a preprogrammed set of "instructions" peculiar to itself, so that a monad "knows" what to do at each moment. (These "instructions" may be seen as analogs of the scientific laws governing subatomic particles.) By virtue of these intrinsic instructions, each monad is like a little mirror of the universe. Monads need not be "small"; e.g., each human being constitutes a monad, in which case free will is problematic.

It depends upon you definition of "hole". Like your definition of a hole (deduced from your question statement) is:


A cavity or gap in a generally plain surface  OR
A filled cavity or gap in a generally plain surface which is filled with a material different than used in the surface



It also depends on how much the container's material is different than the filled material (Highly subjective).
For example, More people will call a hole filled with snakes and snake-lings a hole than the people calling a hole filled with sand a hole. 
It also depends upon context of the statment.For example, 



Two friends are going in a jungle and one says
"Holy sh*t! Did you see that hole?" (He is most probably talking about a empty hole)
"Hey how did you get that hole in your shoe? your toe is sticking out" (He is certainly talking about a filled hole)



It is all up to you to decide, IF you are asking philosophically, otherwise there must be clear definitions for a hole exist, linguistically (that would be out of this forum's scope, I guess). 
In light of Robert's comment, I'm extending this answer in one respect. The thought experiment can be understood in two ways.

Way 1: Linguistic opposites: We are merely changing the meaning of words. "2" means -2. "-2" means 2. The utterance "true" now means false and vice versa. This particular thought experiment is completely uninteresting, because it merely means that the language of this opposite world is somehow coincidentally reversed for all descriptive terms.

Way 2: Metaphysical opposites: Here, the claim is that what is true in our world is false in this world, and what is false in this world is true in that world. In other words, it's not that they say "2" when they mean -2, it's that they use language in the same way but the objects that populate their world are opposite. 

But this is a self-defeating thought experiment, because when we move past trivial elements (like positional functions), not everything can admit of opposites in the way you're describing and that's what basically kills the thought experiment.  Sure 2 and -2 can be opposites, but true and false differ not just as poles but as functions that relate to reality. E.g., I am bunny and I am a tarantula are both false. If you reverse the meanings of true and false, then they both become true which is self-contradictory.

What this does show us, however, is that the units for position functions are arbitrary. (We can put the origin (0,0,0,...) wherever we want and just move everything from there). Mass functions and many other types of evaluative functions are not. To give an example:

Is is true that I am wearing a shirt and it is white?
Is is true that I am wearing a shirt?
Is is true that I am wearing a white shirt?


Consider if I am wearing a blue shirt under normal and opposite evaluation. Under normal, evaluation: false, true, false. Under opposite evaluation? it's not at all clear. Do statements containing and become true if either term is true in opposite evaluation? Do we apply the opposite afterward fully evaluating? (i.e. if we fully evaluate and then opposite: we get true, false, true. If we evaluate each piece and then opposite without changing the logical operators: false, false, true)
Because effects that are less immediate are less likely. I'm trying to calculate expected utility, but that calculation gets more and more uncertain the farther out I try to forecast, so if A and B both, hypothetically would produce ten utils of pleasure, but if those good consequences are an immediate consequence of A, but only a remote consequence of B, then I should prefer A to B, because all other things being equal, i'm more likely to actually get those 10 utils with A.
I'm not sure if the example is conclusive, but it sounds like Harasanyi is highlighting a general problem with maximization/minimization theories. These problems generally apply to utilitarian and consequentialist theories which suffer from two initial weaknesses:

(1) The problem of perceived benefits versus realized benefits in the calculation. This seems to be the highlighted issues in your description of the quote. This type of problem is one where we need to know whether we evaluate based on outcomes or good-faith predictions. Thus, the sample case -- do we need to recalculate based on the agent dying  while traveling to that mutatis mutandis better job or not? Or more softly, do we need to factor that into the calculation? Or to put it another way, is the morality of our actions contingent on the future?  The more general problem this creates is that the very fact we ask the question makes it problematic to view maximization or minimization as the core of morality (this last piece is Bernard Williams' objection).

(2) The second problem for calculation-based strategies is that they seem to leave no action unprohibited if it is a net benefit to the goal. Most compensate by having a harm principle, but then we return to Williams' objection -- if we need a second non-maximization / minimization principle to check the calculation, then surely the calculation does not capture all of morality.

Having not read the text in question, I'm a little limited in what I'm willing to say on it specifically but I would guess he's suggesting that insofar as Rawls is about constructing this sort of ideal society behind the veil of ignorance that Rawls' theory is sufficiently calculative to succumb to the first problem.

  When used as a plea for pity, this http://en.wikipedia.org/wiki/Appeal_to_emotion appeal to emotion can constitute a potential logical fallacy, while when used as an appeal for sympathy for weaker members of society, or the social good of the long-term health and viability of a society, it can constitute an argument for social justice generally accepted as appropriate.
  
  [...]
  
  Claiming to do something for the benefit of children is not a fallacy of itself, but if used to avoid logical debate, it is a http://en.wikipedia.org/wiki/Thought_Reform_and_the_Psychology_of_Totalism#Thought-terminating_clich.C3.A9 thought-terminating cliché. Ethicist http://proethics.com/about/ Jack Marshall described "think of the children!" as a "tried-and-true debate-stopper" used by "misty-eyed crusaders" to promote government policies and societal actions intended to alleviate the suffering or promote the well-being of children of improvident, impoverished, or otherwise flawed parents, which harms society by removing an important incentive for prudent behavior and by encouraging non-eugenic breeding by the type of person who has "fathered more kids than he can possibly support":


http://en.wikipedia.org/wiki/Think_of_the_children Wikipedia: "Think of the children"


  "Think of the children!" is a tried-and-true debate-stopper, but more often than not one that succeeds because of its ability to inhibit rational thought. Children routinely have to suffer the consequences of adult incompetence, recklessness, stupidity, dishonesty and irresponsibility, and if preventing that biologically-dictated result is humanity's priority, then society needs to abolish the enforcement of laws, the obligation to support one's own family, and common sense. [...] [U]nless society sticks to principles that require adults to be responsible regarding the welfare of children in their charge, the "Think of the children!" reflex will suffocate order and justice.


http://www.ethicsscoreboard.com/list/children.html Jack Marshall, "Think of the Children!": An Ethics Fallacy

I don't have access to it, but this may be interesting (and perhaps more scholarly) reading: http://www.tandfonline.com/doi/full/10.1080/00048400802340667 Paul M. Pietroski, "Think of the Children", Australasian Journal of Philosophy, Volume 86, Issue 4, 2008.
Probably the major (non-ancient) innovator of logic pre-Boole/Frege was http://plato.stanford.edu/entries/peter-spain/ Peter of Spain. He lived during the 13th century and wrote the Tractatus. This was not only a rehashing of ancient logics, but was also the basis of a tradition that would last almost 600 years. Since the Tractatus was ubiquitously used as a logic textbook for centuries, I would say that Peter of Spain probably made a more lasting impact on the logical landscape than Aristotle himself.

To find out more, I strongly suggest reading http://global.oup.com/academic/product/peter-of-spain-summaries-of-logic-9780199669585?cc=us&lang=en& Peter of Spain: Summaries of Logic. It is written by three heavy-weights of Medieval philosophy: Calvin Normore, Brian Copenhaver, and Terrence Parsons. For what it's worth, I've had the honor of taking classes with all three of them and saying they are brilliant is an understatement.
"Post-apocalyptic" is just a definition of a situation occurs after apocalypse and it would be anything; utopia, distopia, anarchy, capitalism, a re-ancient age etc. Sorry for my confusion and thanks for help.
I'm not sure the first question --whether there can be moral motivations "independent of human feeling" is answerable --but the second question, about the psychopath, is the entire subject of Plato's Republic.  

His answer, if you don't mind spoilers, is that it's always in your best interests, properly understood, to act morally, even in the case where the opposite seems true. 

It's well worth reading --the arguments are actually very good, even if you're not a Platonist.  Ultimately, however, it all comes back to his central contention that morality is deeply fundamental to the structure of the universe.
Aristotle in his Poetics examines three kinds of poetry - comedy, the epic & tragedy; he says


  Since the objects of imitation (mimesis) are men in action, and these men must be either of a higher or a lower type (for moral character mainly answers to these divisions, goodness and badness being the distinguishing marks of moral differences), it follows that we must represent men either as better than in real life, or as worse, or as they are 


By imitation, he does not mean mere imitation, but a condensation; that in comedy is caricature; and in tragedy and the epic aims at elevation and grandeur.

He notes that:


  It is the same in painting.


For example the caricatures of a Cruickshank, or Grosz; grandeur in the history paintings of Caravaggio or De La Tour.

He adds:


  for Comedy aims at representing men as worse, Tragedy as better than in actual life.


For life itself being the measure or the standard; art aims either above or below; pure imitation will be as a mirror and adds nothing new; thus again imitation as condensation.

and he repeats:


  The graver spirits imitated noble actions, and the actions of good men. The more trivial sort imitated the actions of meaner persons, at first composing satires, as the former did hymns to the gods and the praises of famous men.


Hence the laughter provoked by comedy, according to Aristotle, is not that which provokes our pity; but where we recognise meanness or lowness of spirit. 


  As for Comedy, it is an imitation of men worse than the average; worse, however, not as regards any and every sort of fault, but only as regards one particular kind, the Ridiculous, which is a species of the Ugly.


This sets it of from Nobleness which is a species of Beauty. He expands:


  It consists in some defect or ugliness which is not painful or destructive. To take an obvious example, the comic mask is ugly and distorted, but does not imply pain.


If it was painful or destructive, in a proper sense, then it would be a species of Horror; and this would jar against the good humour of comedy.

That which provokes our pity (one could say here, the pitiful; but one shouldn't because its sense is no longer consonant with that which provokes pity; but like the word pathetic, which was originally meant that which had pathos; now both are close to meaning that which deserves our contempt; this movement in emotional value is of significance itself; but I digress) according to Aristotle, is the emotion proper to tragedy; hence he says:


  For the plot ought to be so constructed that, even without the aid of the eye, he who hears the tale told will thrill with horror and melt to pity at what takes place. 


Thus Oedipus Rex, Iphigenia in Aulis or Medea.

It is the arousal of pity and horror, and then their cleansing or purification that Aristotle calls Cartharsis. To now go onto the points that you raise:


  In a crude sense, I suppose he means we laugh at the pitiful;


He means that it reveals the crudeness of both the audience and the comedian; but there are degrees of comedy; he notes for example comedy in Homer - whichis an epic.


  I've been guilty of it and this kind of humour is "sick," but I think Aristotle is right. Should one feel bad because they laugh at the misfortunes of others?


Theres an implicit value judgement in Aristotles Poetics which judges the higher better or nobler than the lower; he wouldn't say that it is'sick' but that it is 'low'; and thus the appropriate response is laughter; but to laugh at misfortune proper; the kind of mis-fortune that dogged Oedipus is, according to Aristotle, the wrong response; and thus 'sick'.
Not my area of expertise, but the basic problem hinges on a confusion in the use of "noise."  This could be a result of language or just word choice. 

but what you need is three terms: "music", "noise", and "sound." Sound would refer to the medium. Music to a "logical" arrangement of sounds. Noise would then be the "illogical" arrangement of sounds. By logical, I mean ordered according to some system -- which need not be identical for all listeners and composers but still needs to be some system.

Thus, on these definitions, music is no more sound than yes is no or true is false. Both involve sound in the same way speaking a language and grinding your fingernails against a chalkboard involve sound, but they differ precisely.
As usual, as I was typing the answer, Mauro ruined it by giving it in simpler terms in the comments!

In Categories, Chapter 2, Aristotle divides things into two types: things that are said, and things that are. For these things that are, he gives a fourfold classification schema based on a fundamental distinction between two types of predicative relation:


said of predication;
present in predication.


This second type of predication has no straightforward analog in modern logic, so let's skip to (1), which is what your question is about. Said of predication is quite easy to understand. Here is a definition:


  Definition. F is said of x    =df    F(x).


Purely extensionally, F is said of x just in case x is among the elements of the set F, i.e. iff x ∈ ext(F). 

Let's apply this definition to your example. 


The predicate F is "is an Athenian whose name is 'Socrates'";
the individual s of which you're predicating it is Socrates, the teacher of Plato and others.


To say F of Socrates, that is, to say that Socrates is an Athenian whose name is 'Socrates', you simply predicate F of him. F, extensionally speaking, is the set of all those Athenians whose name is 'Socrates'.  Presumably Socrates was an Athenian named 'Socrates', so Socrates is a member of that set. That makes the predication F(s) true. 

The further claim, namely that Socrates is the Athenian whose name is 'Socrates', requires  that there be no other person who was an Athenian and was named 'Socrates'. I personally know a Greek person, now alive, who is called 'Socrates', so the set F contains at least two individuals: the famous philosopher and this Greek guy I know. That means that "Socrates is the Athenian whose name is 'Socrates'" is not a good definition; it's similar to defining 1 as the closest integer to 0 (not including 0).

The lesson is the following. Even if F was a singleton set containing Socrates the philosopher:


  Lesson: there is a difference, between Socrates the individual and {Socrates} the set.


This resolves the worry you have about Aristotle's claim that particulars (e.g. Socrates) cannot be said of anything. Since {Socrates} is not a particular but a general term, it doesn't contradict Aristotle to say {Socrates} of Socrates (after all, Socrates ∈ {Socrates}).

                                                                     References

Ackrill, J.L. (1963) Aristotle's Categories and De Interpretatione, Clarendon, Oxford.
From a http://en.wikipedia.org/wiki/Rational_actor Rational Actor perspective, every decision a person makes is to maximize his own utility, meaning that less ambiguous acts of kindness, like giving away the food for free, are still done for personal reasons, namely that the person receives joy out of being kind, and therefore is really serving himself. So if one defines an act of kindness only as something that is not in your own interest, then kindness may not actually exist. A couple solutions to this:


There are no acts of kindness. Humans, as more or less rational actors, only do things that increase our own utility, therefore everything is self-serving and there is no kindness.
Humans can act with no self gain at all. Humans are not rational actors, so acts of kindness can occur when someone truly gains nothing from the act, not even joy or sense of fulfillment or social acceptance.
Kindness is derivative of purpose. In this case, an act of kindness could be defined as when the motivation for an action is to make another happy, even if that makes you happy in the process.
Kindness as non-material. This is probably the most common view. If somebody doesn't benefit materially, then the non-material benefits are not considered selfish. This view does run into problems like the one you put forth, where clearly something besides material gain is needed to truly decide the answer.


Personally, I favor #3.
I would argue that there is an implicit trade-off between bias and decisiveness, similar to trade-offs between accuracy and precision.  I take the following to be an unbiased statement.


  There may or, equivalently, may not be some manner of supreme being or beings.


I would hold this statement to be relatively non-biased.  It is certainly less biased than:


  There is a supreme being.


And also less accurate.

I would argue that by not believing in anything you are protected from bias.

Whether it is possible to have no opinions or beliefs is perhaps another question, but I believe by having no opinions to be biased about, it would necessarily result in no bias.


  Can you take a position on something as secularly uncertain as say religion or as controversial as social policy and NOT be influenced by feelings or "other limitations" that may make you biased in some form?


Oh no, you asked an extension question about taking a position.  Well that makes this harder.

I would argue that being sufficiently informed to produce a position on any issue will necessarily introduce non-zero bias.  However, we may be able to further refine our answers.  For example, perhaps we can eliminate one of the above cases with the following definition:


  A supreme being is taken to be supreme iff superior to all other beings in every way.


then


  There may or, equivalently, may not be some manner of supreme being or beings.


reduces to


  There may or, equivalently, may not be some manner of supreme being.


Now, should the added definition be considered as a bias?  Perhaps, perhaps a clarification, perhaps it is meaningless.  These are open questions.  However, to me this suggests it is possible to refine statements beyond no position, to some position, to a certain extent, without introduction bias.  I would argue that this continues only up to a certain point.  To me, religion is beyond that point, but I do believe it may be possible to get down to a single a religion.

I'm tempted to say only time will tell, but I'm worried that may carry with it my internal bias for optimism and progress.
In the fictitious universe of Star Trek, the Prime Directive is the guiding principle of the United Federation of Planets. The Prime Directive, prohibits Starfleet personnel from interfering with the internal development of alien civilizations. This conceptual law applies particularly to civilizations which are below a certain threshold of development, preventing starship crews from using their superior technology to impose their own values or ideals on them.

The idea is that if civilizations have developed to the point of Warp drive capabillity, then they would have equivalently mature ethical theories, which loosely translates to; the smarter they are, the more ethical they should be, therefore first contact is warranted. That is, a barbaric civilization wouldnt possess the intellectual sophistication to have a whole wealth of technology and resources we could benefit from, these are generally the fruits of refinement.
For Kant, we can only see objects that take up space because space is an a priori condition of all human experience of the external world[1]. For Kant, space does not exist independently of human experience. We might say space is the medium of our experience of the external world. Experience of the external world without space is as sound in a vacuum, for Kant it doesn't exist.

Kant's point is metaphysical (and perhaps psychological).

Wittgenstein's point is about language. Pictures have borders, we are only confusing ourselves and perhaps others if we talk about the part of a picture that isn't on the canvas.

[1] For Kant, time is the a priori condition of all inner experience.
From a purely logical point of view :

The first statement is equivalent to a conditional statement (Material implication), namely : If we require women to wear the veil, then we will protect woman from rape.  In order to show this statement is false, we need to show that it can be the case that the antecedent (the "if" part) is true while the consequent (the "then" part) is false.  With this in mind, statement one is false for the simple reason that many women have been raped while wearing a veil. 

Similarily, the second statement is equivalent to the conditional statment : If women cover their hair, then it cannot emit the rays that excite men.  The consequent here is plainly false since although a woman's hair may excite a man, it is not because it is emitting mysterious rays.  However, in order to look at the statement logically, we need to assume that a woman's hair does indeed emit these mysterious rays.  In this case, the statement is false for the same reasons that statement 1 is false, namely that many women have been raped while wearing a hair covering.

The third statement is more opaque. It is stated as a conditional statement, so in order to show it is false, we need to show that it is possible for the antecedent to be true while the consequent is false.  This follows from the fact that rape is commonplace in the animal kingdom.  However, it is not clear that civility is a term applicable to members of the animal kingdom.

Edit:  Let's face it.  We live in a world where men will turn to fresh fruit for satisfaction.  Can anything prevent rape?
Having been a companion to dogs throughout my life, I don't generally suppose that dogs are not self aware.

Unless one defines 'thinking' as only encompassing the way in which humans think. In that case, the answer is "No dog's cannot think." Another consequence of that would be that computer's cannot think. A third would be that if there is a god, it does not think.

Otherwise, there is no more basis for believing that a dog can think than that another person can think. Likewise, there is no more basis for believing that a dog cannot think than for believing that a person cannot think.
"Distrust your senses" is a very long tradition. Recall Plato's "cave" analogy from the early dawn of philosophy, ~400 BC.

Plato postulated that there is a reality outside of what humans experience. He compared the human "experience through the senses" to the experience of a caveman looking at a shadow play on the cave wall: The caveman can only see the shadows on the wall, he/she has never experienced anything else, and believes that those shadows are all that there is of reality.

Plato says that there's a reality outside of that: For the caveman, there's someone or something outside the cave that's projecting those shadows. There's a sun (or something) that provides the light, there's a whole world out there, but the caveman looking at flickering shadows on the wall may not even realize that the outside world exists. The caveman's senses (and of course, by analogy, any human's senses) can only tell that "this thing here is a shadow" and "that thing over there isn't". Plato's argument is that it doesn't matter how accurately our senses can detect shadows on the wall: The argument is that there is a reality outside of what our senses can perceive.

It's not hard to make modern analogies:

Suppose someone were brought up from birth exclusively on Hollywood movies, and had never seen the outside world. Would that someone be aware of the existence of scripts, cameramen, film sets, directors, actors, people whose only job is to do makeup or lighting? Or would they assume that whatever Hollywood showed them was reality?

Suppose that I'm watching a movie, and my senses tell me that this guy that they call the Joker is a very bad guy, while this other guy called the Batman is a nice guy and is going to save us all. What exactly does that sensory input (and my interpretations of it) tell me about reality?

Suppose that we all live in the Matrix, and that all our sensory inputs are created by somebody else, for some unfathomable (to us) purpose. Is there any way that an "I" living inside the Matrix could tell that my sensory experiences differ from an "I" living outside the Matrix?

That's the basic argument of Descartes: How can my senses tell me whether I even exist, given that there might be a god somewhere dreaming the entire world, and we're all living inside the god's dream?

Descartes famously answered "I think, therefore I am": If I have more or less control over my own thoughts, nothing else matters, and I have an existence independent of whether it's inside a dream. (I'm not arguing whether this is a good or bad argument, I'm just saying that historical philosophers have struggled with these questions. I can point out that Neo in The Matrix might very well ask himself the same. The external agent has changed, from God to hostile AI, but the question remains the same.)

Suppose that you people live in the real world, but I personally happen to have a very fond relationship to drugs and magic mushrooms, and every bit of my senses are telling me that there's an extremely lifelike dragon attacking me right now. Should I trust my senses?

There are lots of reasons to distrust our senses in general, even primary sense experiences like color and lightness. Illusionists, optical illusions, and drugs, show that even immediate senses can be fooled. Physics shows that there's a lot of the world that humans were hardly even aware of: Ultraviolet, X-rays, radio waves, radioactivity; none of which is directly perceptible by humans, but it's still real. So our direct sensory impressions can't be trusted too far either. That's perhaps a more modern argument, and these days it's not hard to find examples where our immediate sensory impressions are simply wrong.

Kant says that there's the "world as it is", which humans can never actually know, and there's the "world as it appears to me". We try the best we can to extrapolate our "here and now" limited sensory experience to rules applying to the world in general, but we'll never be able to know the "real" reality, we'll only know the simplified model we've built up inside our head based on our limited experience. And there is no amount of experience that will let us know everything.

The original philosophy argument, from Plato 2500 years ago, didn't doubt sense impressions as such: The cave analogy assumes that the sense impressions of the caveman accurately reflected the shadow play on the cave wall. The philosophical objection is that there might be an entire world outside of what humans happen to perceive, for some reason that the observer simply doesn't know about.
There is a fascinating paper by Marc Shell http://www.people.fas.harvard.edu/~mshell/Shell.%20EconLit.%20Chapter%201.pdf The Ring of Gyges (The Economy of Literature/1978, ch.1 p.11-62). Herodotus' and Plato's  versions of Gyges are interwoven with emphasis on power, vison and wealth.


  Tales of Gyges associate him with founding a tyranny in Lydia and with a power
  of being able to transform visibles into invisibles and invisibles into visibles. This power
  . .. is associated with new economic and political forms that shattered the previous
  world and its culture.


David Graeber in his writings on value often refers to cultural-anthropological findings that connect hidden wealth and visibile signs for it . Today Plato's version seems to be more popular but Hebbel's drama Gyges and his Ring (1865) relies more on Herodotus, with modern commentators noting the links as  done by Albrecht Koschorke in his https://kops.uni-konstanz.de/bitstream/handle/123456789/16740/Koschorke_167403.pdf?sequence=2  Phantasmagorias of Power: Hebbel's Drama Gyges und sein Ring
The evidence for a genuine relatedness is that the most influential inheritor of Russell's theory of types is Martin-Löf Type Theory which is also intrinsically connected to intuitionistic and constructive logic and mathematics.
The question is not the division of labour; after all it takes several kinds of skills to build a house - say an architect and a builder; but the atomisation of labour in factories where very small and discrete tasks are set. Thus reducing 'artisans' to mere tools. 

This is just one dimension to the notion of alienation. 

This idea, which may seem 'Marxist' in orientation was actually introduced by Adam Smith who wrote in his Wealth of Nations:


  "The man whose whole life is spent in performing a few simple operations, of which the effects are perhaps always the same, or very nearly the same, has no occasion to exert his understanding or to exercise his invention in finding out expedients for removing difficulties which never occur. He naturally loses, therefore, the habit of such exertion, and generally becomes as stupid and ignorant as it is possible for a human creature to become. 
  
  The torpor of his mind renders him not only incapable of relishing or bearing a part in any rational conversation, but of conceiving any generous, noble, or tender sentiment, and consequently of forming any just judgement concerning many even of the ordinary duties of private life... 
  
  But in every improved and civilized society this is the state into which the labouring poor, that is, the great body of the people, must necessarily fall, unless government takes some pains to prevent it."


and echoed by Marx:


  Marx argued that increasing the specialization may also lead to workers with poorer overall skills and a lack of enthusiasm for their work. He described the process as alienation: workers become more and more specialized and work becomes repetitive, eventually leading to complete alienation from the process of production. The worker then becomes "depressed spiritually and physically to the condition of a machine".


It was here that the concept of alienation was isolated; but this is only one dimension of said concept.

For Simone Weil, a Marxist intrellectuel who worked in a French factory for three months she discovered that a factory creates:


  "une docilité de bête de somme résignée" 
  
  ("the docility of a resigned beast of burden").


and she had also written in her http://simoneweil.net/home.htm#factory notebooks that the speed itself of factory work deadens the soul, whilst orders that one have to accept deaden consciousness.
If I understand your question correctly, you ask about the difference between an atom and a monad - is that right?

If it is, then the major difference between them is probably the fact, that a monad is not pure matter as atom (Epicureans were matterialists: even souls they considered to be build of atoms, just of some different type). I don't want to make a mistake, but I am quite confident that a substance in Christian philosophy is a term generally not associated with matter.

But there are other differences: I assume that Epicurus concept of atom is familiar to you and the Leibniz's monads just cause confusion. (Indeed, monadology is rather odd metaphysical system.) I would recommend those articles about monads, they describe the idea of a monad quite well:

http://en.wikisource.org/wiki/Catholic_Encyclopedia_(1913)/Monad http://en.wikisource.org/wiki/Catholic_Encyclopedia_(1913)/Monad

http://www.iep.utm.edu/leib-met/#H4 http://www.iep.utm.edu/leib-met/#H4

Note the biggest differences:


Atoms are pure matter, whereas monads are not.
Atoms are connected with other atoms, whereas monads can not interact
between them (monad "has no doors or windows")
Atoms are what is the world build of in a sense as bricks are what a
wall is build of, whereas monads are complete beings - they all are
the parts of the universe and in every of them there is a
reflection of the universe in which it appears as if they could interact.


And some differences according to particularly Epicurus' concept of atoms:


Atoms are not the smalles parts of the universe - they are made of minima, whereas monads are indivisible.
Atoms are not fully determined - there are some random movements of atoms called clinamen (greek: parenklisis), whereas monads... well, they are determined in a specific way and it is all quite complicated (the articles I recommended should put some light onto it).




On the other hand, there is no reason why couldn't the matter (not substance) be made of atoms in Leibniz's concept: I cannot though provide evidence that he has actually thought so and I am sure that it is not an important part of his philosophy (the atom concept in Epicurus' philosophy was developed for his ethics - and a similar role in Leibniz's philosophy certainly play monads).
It is a question of perspective.


What is the alternative? Assume there is a black guy living in a village with strong KKK-ties. They don't tolerate black people. Tolerance has a component of implying inferiority but what would be the better ending for this story? a) Strange, white people thinking of themselves as superior or b) Strange, white people thinking of themselves as superior standing around a burning cross? I like a) better. Tolerance can be an improvement.
What are you tolerating? Assume there is a black guy living in a village with strong KKK-ties. That's where you live too. If you are tolerant you might say: "Those stupid bedsheets-guys! But I tolerate them. Let them have their primitive human sacrifice rituals!" If you are intolerant you might want to call the police. Now intolerance would be the better choice. 


So should you be tolerant? No, never! One has to have a code of values. Everything that goes against this code should not be tolerated. Everything in accordance with that code is not in need for tolerance: There is nothing to be done about it, so it is not depended on one's graceful superiority to don't do something about it.

That should take care of all ethical questions. But what about differences where no moral is involved? Say, your neighbor likes to cook astonishing gross smelling food. Should you tolerate it? Depends: Are you a confrontative person? Or are you always trying to please everyone? Do you think bringing it up could endanger the otherwise good neighborliness? However it does not depend on religion or ethnicity. So what's the use of tolerance here? Can you live with the smell or not [btw: did I mention the neighbor is an English exchange-student]?
There is no fallacy here. I fallacy is an argument that uses poor reasoning. The reasoning is correct: If you accuse him of doing X, and a nice person wouldn't do X, then from his claim that he is a nice person  it follows that he didn't do X and your accusation is wrong. The reasoning is entirely correct, there is no fallacy. 

Of course it is possible that the person's claim is wrong. That the person is an evil, scheming, manipulative so-and-so and lies when they claim to be nice. That could be self-delusion, or a plain lie, but it is no fallacy. 

Facts + reasoning give conclusions. Correct facts and correct reasoning give correct conclusions. Correct facts + incorrect reasoning (fallacies) often give incorrect conclusions. But incorrect facts + correct reasoning also often give incorrect conclusions. 

(You might claim a fallacy like "appeal to authority": Since that person is the best authority to make claims about their state of mind, they expect you to believe the claim "I'm a nice person" without any further proof. )
Modernity is a complex phenomenon coming out of the Enlightment and earlier roots; one aspect of it is the Industrial Revolution and the increasing pace of technology as identified by Heidegger.

The division of labour in contemporary classical economics is an outcome of the self-regulation of the market-place (and thus society) by the invisible hand.

This metaphor was developed by Adam Smith to explain the self-regulation of market-place; in all his works he uses the metaphor three times; first in his Theory of Moral Sentiments:


  The rich … consume little more than the poor, and in spite of their natural selfishness and rapacity, though they mean only their own conveniency, though the sole end which they propose from the labours of all the thousands whom they employ, be the gratification of their own vain and insatiable desires, they divide with the poor the produce of all their improvements. 
  
  They are led by an invisible hand [emphasis added] ... and thus without intending it, without knowing it, advance the interest of the society, and afford means to the multiplication of the species. 


That is the invisible hand is the best of all possible invisible hands; and it is thus through their natural and therefore rational selfishness. 

The impact of this Smithian idea didn't become apparent until Arrow & Debreu turned it a mathematically model; where notions of commodity, price, rational selfishness and competitiveness were made precise. Like all models it had short-comings, which were easy enough to note, but it proved its strength by taking centre-stage in Economics. Its the thrust of these complex of ideas that form the core of the neo-classical synthesis. 

Recent work has focused on a critical examination of its assumptions. For example, Stiglitz on imperfect information. 
In the first situation she knows the name by remembering it. A person's name cannot be calculated, only memorized.

In the second situation she knows the result by calculating it. She has memorized (i.e. knows) how to multiply, how to calculate a result, but does not know what the result is until after doing the calculation.

The premise (that #1 is "know" and #2 is "not know") is true if "I know" is a synonym for "I have memorized" (but not a synonym for, "I am able to calculate").

Sometimes we can, colloquially, say that #2 is an example of "knowing the product of two numbers": we might say that the subject is "numerate" or "knows her numbers", or (more formally) "knows multiplication".
I would not say that the Dialectics and Logical positivism have the same enemy Metaphysics, because they do not use this term in the same meaning. The Marxists - I guess they are the most prominent proponents of the opposition Dialectics/Metaphysics - consider them as two methods or two ways to treat the movement, the change and the evolution in the Universe. The Dialectics can catch the change in the Nature and in the Society and correctly represent it, while the Metaphysics cannot. 

The Positivism advances criteria that can help attain some solid knowledge and supposes that if Metaphysics cannot meet these criteria, it is useless "...its members styled themselves as conceptual revolutionaries who cleared the stables of academic philosophy by showing metaphysics not simply to be false, but to be cognitively empty and meaningless." (SEP)

I guess that those who advance the Dialectics consider the Logical positivism as a regress. Probably, for positivists all dialectical methods luck a rigor.    
Most Physicists doesn't accept infinities, due to very obvious reason, such infinite physical objects are not quantifiable! that is we can't measure them, or maybe even really prove that they infinite!

Anyway, thorugh the history of physics, infinities was always raising between formulas, and usually in this case formulas was thrown away, considered as incomplete, or they kept searching for mathematical tricks to avoid them (that is, considered to be mathematical artifacts), and those approaches till now was very successful.

Maybe as a first exmaple, is when physists tried to apply Maxwell equations of Electromagnitism on electron self-energy, infinites raised, and was actually a huge problem, because in other areas, those equations was extreamly succesful in describing reality, later on, we understood that they was incomplete, and Quantum Electrodynamics solved those infinites.

Latter on they also raised in general relativity (singularity of black hole), and depending on our previous practice, physicists considering it to be incomplete, because till now we was not able to successfully unify Gravity with Quantum physics, which we hope that will put "limit" on the kind of singularity that can exist, and "patch" the law's break down as you mentioned.

There is other examples can be mentioned, but they are maybe harder to understand to none physicists, but as I remember, currently there is two left major problems with infinity in physics: Gravitational singularities, and Vacuum energy.

P.S. (1)

The example mentioned by "Niel de Beaudrap", is totally misleading in my opinion, because there is actually no infinite temperature, due to relativity, and http://en.wikipedia.org/wiki/Absolute_hot Plank Temperature is most what we can get. And the negative infinite temperature he mentioned, is an mathematical artifact, because in this case, the physical meaning of temperature breaks down and it became just some abstract mathematical parameter that holds no physical meaning by it self, even so it takes same place in formulas like the usual temperature, so it is just an analogy...

P.S. (2)

Some modern theories of cosmology, admits existence of an infinite amount of different universes, that is it admits infinites, anyway, those are just theories, and it seems (till now) that there is no way to prove them.

Edit(1)

In responce to "shane" answer, I would like to emphasize that in physics it basically possible (at least theoreticaly) to move from point A to B in 0 time, and that not only due to entanglement in quantum physics as mentioned in comments (and which is really depends on the interpretation you use), but even due to a more "classical" reasons, which is general relativity, because it has ability to bend space-time sheet to connect two points on different sides of it, anyway, it should be mentioned that time here is a relative thing, so we should be really cautious about "relative to which observer" it will be 0 time, anyway there is no "infinite" speeds here.
Alasdair MacIntyre's http://en.wikipedia.org/wiki/After_Virtue After Virtue is a must-read on this issue. He surveys what he calls the "Enlightenment Project"—the attempt to derive morality from Reason—and argues that it failed, miserably. He advocates a return to virtue ethics, which originally he thought could be done without teleology, but later came to see as essential. He argues for building upon Thomas Aquinas' update to Aristotle. For an analysis of the attempt to build a social life on secularism (which I would say is quite similar to said "Enlightenment Project"), I suggest UCSD law prof Steven D. Smith's http://rads.stackoverflow.com/amzn/click/0674050878 The Disenchantment of Secular Discourse, which contains this snippet:


  No one expects that anything called "reason" will dispel such pluralism by leading people to converge on a unified truth—certainly not about ultimate or cosmic matters such as "the nature of the universe" or "the end and the object of life." Indeed, unity on such matters could be achieved only by state coercion: Rawls calls this the "fact of oppression."[36] So a central function of "public reason" today is precisely to keep such matters out of public deliberation (subject to various qualifications and exceptions that Rawls conceded as his thinking developed). And citizens practice Rawlsian http://en.wikipedia.org/wiki/Public_reason public reason when they refrain from invoking or acting on their "comprehensive doctrines"—that is, their deepest convictions about what is really true—and consent to work only with a scaled-down set of beliefs or methods that claim the support of an ostensible "http://en.wikipedia.org/wiki/Overlapping_consensus overlapping consensus".[http://en.wikipedia.org/wiki/Political_Liberalism Political Liberalism, 133-172, 223-227] (14–15)


An introduction to his book can be found at the NYT op ed http://opinionator.blogs.nytimes.com/2010/02/22/are-there-secular-reasons/ Are There Secular Reasons?.
Apart from "slavery in a mental form" the description (and for what I remember, also the back cover) of Kevin Bales' book http://rads.stackoverflow.com/amzn/click/0520272919 Disposable People: New Slavery in the Global Economy argues that "more than twenty-seven million people are still trapped in [slavery]" (mostly) of the conventional kind.


  Slavery is illegal throughout the world, yet more than twenty-seven
  million people are still trapped in one of history's oldest social
  institutions. Kevin Bales's disturbing story of slavery today reaches
  from brick kilns in Pakistan and brothels in Thailand to the offices
  of multinational corporations. His investigation of conditions in
  Mauritania, Brazil, Thailand, Pakistan, and India reveals the tragic
  emergence of a "new slavery," one intricately linked to the global
  economy. The new slaves are not a long-term investment as was true
  with older forms of slavery, explains Bales. Instead, they are cheap,
  require little care, and are disposable.

Most arguments are kind-of semantic. We use moral talk everyday, we often discuss moral issues and try hard to convince each others of our moral judgements by appeal to reason and facts, just as if something objective is at stake. 
A good meta-ethical theory must account for this aspect of common language. 
Perhaps the best and simplest explanation is moral realism: there are indeed moral facts, there are things one objectively ought to do in such or such situation, and that's exactly what we are talking about when discussing moral issues.

Alternative explanations to moral talk, such as expressivism (the idea that moral judgments are mere expression of feelings, emotions or desires rather than fact statements) can be demanding.
One challenge for expressivism is the Frege-Geach problem.
If really moral judgements are only expressions, or attitudes, how is it that they can be combined into logical structures such as "If it is wrong to tell lies, then it is wrong to get your little brother to lie" ?

Another alternative to moral realism is an error-theory: we do pretend to make objective, factual judgments about what one ought to do in some situations, but we are actually plain wrong, there is no such fact of the matter. This position is against our common-sense intuitions, and does not by itself provide an explanation to why we keep on having moral discussions.

Note that moral realism does not mean that moral facts (things one ought to do) do not reduce to something else, such as social acceptability or utility. It only means that moral judgments express potentially true facts rather than nonsense or mere attitudes.
Academic philosophy is open to discussing all sorts of questions. Your own question is of the form if X, is Y acceptable. However, this X seems relevant only for theologians, generally not for philosophers. I'll cite just two of the many cases that discuss the Y.

http://www.law2.byu.edu/lawreview/archives/1991/1/bar.pdf Should the baby live?: the problem of handicapped infants


  There is a limit to the burden of dependence which any community can carry. The amount needed to care for one patient... could be used save the lives of scores of children suffering from malnutrition in less well-developed countries.


http://jme.bmj.com/content/early/2012/04/12/medethics-2011-100411.short After-birth abortion: why should the baby live?


  Abortion is largely accepted even for reasons that do not have anything to do with the fetus' health. By showing that (1) both fetuses and newborns do not have the same moral status as actual persons, (2) the fact that both are potential persons is morally irrelevant and (3) adoption is not always in the best interest of actual people, the authors argue that what we call ‘after-birth abortion’ (killing a newborn) should be permissible in all the cases where abortion is, including cases where the newborn is not disabled.

If you allow that e.g. "Come!" has the same connotation as "You must come!" (and allow linguistic jargon as well), you could call it a directive or http://en.wikipedia.org/wiki/Deontic_modality deontic modality:


  Deontic modality is a linguistic modality that indicates how the world
  ought to be, according to certain norms, expectations, speaker desire,
  etc.
  
  This category includes the following subcategories:
  
  
  Directive modality (commands, requests, etc.): "Come!", "Let's go!",
  "You've got to taste this curry!"
  


EDIT (following OP's EDIT :)


  A http://en.wikipedia.org/wiki/Dichotomy dichotomy is any splitting of a whole into exactly two
  non-overlapping parts, meaning it is a procedure in which a whole is
  divided into two parts. It is a partition of a whole (or a set) into
  two parts (subsets) that are:
  
  
  jointly exhaustive: everything must belong to one part or the other, and
  mutually exclusive: nothing can belong simultaneously to both parts.
  
  
  Such a partition is also frequently called a bipartition [...]
  
  The above applies directly when the term is used in mathematics, philosophy, literature, or linguistics. For example, if there is a concept A, and it is split into parts B and not-B, then the parts form a dichotomy.

Kant actually has such a feature in his moral system: it's called imperfect duty.

Kant specifies two: Your Own improvement and Helping Others.

In both cases, the means of fulfillment are not specified but that you have a duty to help others is specified. Similarly, you as a rational being in need of the developed aid of others have an obligation to improve your abilities. But this does not decided which abilities you must improve.

There may be one inadequacy to what you are looking for insofar as the obligation is not attached to a specific person.



In moral theory, I would guess the closest we come to what you are describing would be promises with implicit alternate fulfillment conditions. E.g., 


  I promised I would bring apples but I brought pears instead.


Depending on the conditions surrounding the promise, this would count as fulfillment because my obligation was to bring a healthy snack -- not specifically apples which served as a stand-in. 

The same sort of thing also happens in second-order rights talk.
There are some kinds of harms that result just from knowing something. When somebody finds out that they were adopted, say, this can cause harm that wouldn't have existed otherwise. The person could begin to question how they were raised, and wonder about their real parents, etc. In the same way, perhaps a widow who discovers long after her husband has died that he used to be unfaithful might retroactively feel harmed by discovering the betrayal. Perhaps in both cases, it would be better for the person not to know. 

However, clearly there are other harms that are harmful even if I don't know about them. If I am owed an inheritance from my long lost uncle, and my cousins I don't know about conspire to cheat me out of it and I never become aware, they have still harmed me. I would have had more money if they hadn't cheated me! It seems to me that most kinds of harms are this kind — you don't necessarily have to know that you are being harmed in order to be harmed. 

You could argue cases of entrenched discrimination are like this. Suppose that education, freedom, and autonomy are all good and that denying those things to someone is to harm that person in some way. The fact that I convince somebody that they don't really want education, freedom and autonomy any way doesn't mean that somehow I am not harming that person by denying him or her those things: it just means that I've managed to brainwash that person in some way that prevents him from recognizing the harm I've done to him. 

I don't want to enter into contentious political debates, but it doesn't seem to me that wearing the headscarf per se is discrimination in the sense I've outlined above. In other words, I don't think it's obvious that the only reason a Muslim woman might want to wear a headscarf is because she's been brainwashed into accepting a lack of autonomy, education, freedom, etc. I don't want to put words into anyone's mouth, but I could imagine a Muslim woman viewing her choice to wear a headscarf as an expression of autonomy. Of course, I can also imagine a Muslim woman being forced to wear a headscarf, and in such a case clearly there would be a kind of harm being done. 
It's not necessary to have any intrinsic sense of space in order to perceive it, though you do need something that learns correlations.  Space emerges from the correlational structure of perceptions.  You can help things out a bit by preparing the substrate for noticing correlations in a space-filling way (e.g. self-organizing map), but it's really not necessary mathematically.  For instance, a classic example of machine learning is http://www.cis.hut.fi/projects/compneuro/background.html taking images and producing what look like visual cell receptive fields (which are spatially extended).  Practically every machine learning method that isn't simply categorical can do this.

On the other hand, parts of our brain are laid out to receive spatial information without any input.  For example, visual cortex is laid out in a regular fashion to receive and process a 2D visual scene (from each eye), and http://www.ncbi.nlm.nih.gov/pubmed/16337917 correlations are sharpened pre-birth by spontaneous retinal activity.

The brain structure which handles most spatial recognition tasks is the hippocampus.  Part of it is designed to handle spatial orientation as evidenced by http://en.wikipedia.org/wiki/Grid_cell grid cells (which only make sense for spatial environments), though it's not clear whether that's completely pre-developed or whether it could be a consequence of learning.  However, the hippocampus is involved in all sorts of memory tasks including episodic non-spatial memory, so it's not clear what to take home from this.

Anyway, bottom line is--how humans actually do it is complicated.  But if you want to know what is necessary, you only need to be able to observe correlations, and space falls out.
I would not rule out 'waking up from death', in two senses.

First -- once there are few enough quasi-immortal beings, they would likely duplicate themselves.  So, even though many copies might die, one continuous memory going back arbitrarily far may be available as far as one wishes into the future.

Second -- I think bidirectional time (a la the Feynman universe where every current proton is the same one at a different age...) is a more likely quantum-mechanical model than Copenhagen or Many-Worlds.  In such a model, it would be possible to preserve information by 'threading' causality through a unique event which forces certain initial conditions on an earlier period.  If there is still matter in motion and the only way for time to continue were for certain particles to have been in a certain configuration at some point far earlier, then that will have to have happened.

The last few intelligent beings in existence might consider continued experience more valuable than continued existence, and would be willing to take that risk.  So it would, with immense effort, be possible for copies of that memory to be instantiated much earlier in the universe's history and imparted to multiple bodies as soon as that is possible.  This would allow streams of consciousness to exist in an infinite loop, continually recopying the wisdom of the version near the end of time to an earlier body.

These people would then be subjectively immortal.
Then is not really a logical connective at all. It is used in conditionals, but it is just an adverbial discourse marker. What it seems to show is that the speaker is prepared to assert what's coming next on the basis of whatever it is that then is referring back to being the case. It doesn't only occur in conditionals:


A: I'm leaving on Tuesday. B: Then you're going to miss the party.
You shouldn't do that, because you'll then be open to accusations of plagiarism.


Notice that there are no sentences in which the inclusion or omission of the adverb then makes any difference whatsoever to the truth conditions of the sentence. In the Original Poster's question the sentence has exactly the same truth conditions whether or not then is omitted:


  If a football is more demanding than tennis and John is good football player, then John can become a good tennis player. But if football is not more demanding than tennis, that does not hold. Hence, John is a good player.
  
  If a football is more demanding than tennis and John is good football player, then John can become a good tennis player. But if football is not more demanding than tennis, that does not hold. Hence, John is a good player.


The fact that adding or omitting then can never change our assessment of whether a conditional sentence was true, is evidence of the fact that it is not actually a logical connective. It merely adds a higher order comment about the speaker's attitude to a proposition.

Probably the best known work on the use of then in conditionals is by Sabine Iatridou http://web.mit.edu/linguistics/people/faculty/iatridou/natural_language_semantics.pdf On the contribution of conditional THEN. However, readers beware, the grammaticality judgements therein are, in my opinion, rather suspect in several places.

Iatridou pointed out that we are most likely to use then when we would not expect Q to happen unless P. There are many other interesting observations as well.

In short, however, then is not a logical connective at all.
This is an extract from The Philosophy of Language by A.P. Martinich, page 3


  For the most part, words are used in such a way that the word
  itself is not the primary object of interest [...] philosophers sometimes use single quotation marks to indicate that a word or phrase is being mentioned [...] 'Cicero' is a word with six letters.


Cicero is a word with six letters, might also work. 'Cicero' or Cicero is then used to mention the word itself. 

Cicero is a word with six letters, implies that Cicero the person and not the word 'Cicero' is a word of six letters. So one can say that a word is mentioned if it talks about itself. And is not mentioned if it does not talk about itself. But a word is always used. The sentence "I like Heroin " do not talk about the word 'Heroin', but about the song by the Velvet Underground. Because you told us you used it to talk about the Velvet Underground. 

I like Cicero and I like 'Cicero' mean different things, by the convention already stated. "I like Cicero" would imply, that I like the person Cicero, and "I like 'Cicero'" would imply I like the word 'Cicero'. 
Engels clearly sees the gender divide as the first class divide and the prototype for all later classism.  This is where we get Marxist-feminism originally.  From an earlier analysis of Celtic culture, he saw the initial ownership of land as having been restricted to women.  As a continuation of hunter-gathering culture, after agriculture, men owned cattle, which needed to continually move, so as not to overgraze land, and women owned houses, which controlled access to safe fields.  As agriculture intensified, it was more necessary to hold land, and it was held by women.

The original Welsh tradition of passing land titles to sisters'-son's (maternal nephews), and not biological heirs, is the oldest recorded form of male-to-male inheritance.  So women were the holders of lineage, and the ruling class.  Then, as war became a larger and larger scale of endeavor, men formed a sort of protection racket that eroded women's hold on land, in what then becomes the prototype of the first economic revolution.

(Sources "We are All Part of One Another" by Barbra Deming, and "Truth or Dare" by Starhawk.)

I don't agree, or at least, if there is such a divide it has more than two classes.  A more basic arrangement of human culture from a more modern perspective is the sort of polygynous collective agriculture still practiced in parts of Africa.  In such an arrangement, men who hold land have multiple wives, and the remaining majority of men and minority of women are consigned to the role of workers on other's land.

This is supported by evidence that several times more neolithic women than men have descendents.  http://io9.com/how-female-dna-came-to-dominate-the-human-gene-pool-1638621568 http://io9.com/how-female-dna-came-to-dominate-the-human-gene-pool-1638621568

It is a majority of men and a minority of women because if married men all take two or three wives, then there are between equally many and twice as many men who never get married, while almost all women will become wives.  If married men do not take two or three wives in this model, they cannot feed their cattle in hostile seasons, because there is a hard division of labor: men do not farm, they only clear land and manage animals. Tropical land where we originated in Africa is often single-use, once you grow on it one season, you have to let it grow over before it will bear decently again. So clearing land is a constant occupation that used up the labor of all those unmarried men. 

In such a system, one cannot make the argument that men as a whole or women as a whole are the more powerful economic class, since the majority of men hold nothing, and the majority of women jointly hold land, even if they cannot hold land in their own right.  Men in such societies also do not hold land until married.  The inheritor is the son first chosen by a woman's family for an arranged marriage.

Eventually two things happened to break this down. We developed a more plant-based diet, so the women's work became much more important than the men's, and the men started doing it with them. And we learned that we can improve soil by putting waste back into it, so land recovers fast enough that it does not have to be cleared of trees each time. I don't imagine it took too much pressure to break down universal polygyny once that was possible -- humans do not seem predisposed to sharing mates well.

From such a direction, you can only see this as a class system if you see it as a three-tiered one where women form the middle class and men form both the upper and the lower class.  But that keeps gender itself from being a class divide per se.

To my mind, this accords better with reality than the Marxist-Feminist notion that since that first deposition from power women have always been the losers.  In modern society we have difficulty with women leading, but 90% of the people we kill, lock up or just let die are male.  The fate of women is thus intermediate between the fates of these two classes of men.
To avoid answering several of these, I am going to lay out my favorite theory behind them.  Not everything in this answer is fully relevant to this question, but to the others like it here.  This is officially redundant.  Someone here already said this, but they used heavy notation that seems unnecessarily precise, and off-putting.

Because of the way English usually works, we are tempted to hear the subjunctive statement as an indicative one -- saying that for any time before six, if I started then, I would most likely arrive before noon.

But since this is a subjunctive implication, it means there is an untested or disputed theory behind it, which I currently accept and am basing my deduction upon.  That theory is incomplete and might be wrong.  If I find some aspect that shoots down my theory, then the statement loses its meaning.

In this case, the idea that I drive equally well at all times seems to be part of the theory.  If it fails, the implication does not have any force.

So in this case, if I left at 5:50, I might  arrive before noon, because that time before six is one where my theory still holds.  But if I left at 5, I would navigate worse.  So I have made an even more drastic correction than I would have needed to (strengthening the antecedent), and the correction itself has caused other unstated preconditions to break down.  

So this seems to disprove the statement, but in fact it only disproves the unstated theory behind it.  In a more complex case, more than one thing might have gone wrong to make the assertion fail.  If I have misidentified what antecedent is too weak, strengthening a different one can make it seem like I am being careful, when, in fact, I am just hiding productive lines of inquiry.

When working with subjunctive statements, in order to make real deductions, one needs to know the 'range of all possible worlds under consideration' basically, to know the theory under which the implication would be valid.  The problem is that folks usually cannot articulate those theories.

One formal way of dealing with this is to consider 'modal' statements made in the subjunctive as always true.  There is bound to be some bizarre theory on the basis of which they would be true.  But then we need to require removing (or 'relativizing') the subjunctive before we make deductions from it.

We can truly remove the subjunctive only by turning it into a complex implication "my theory => S", and since we cannot articulate "my theory" by mind-reading the source of the statement, we have to guess as to the completeness of our understanding of the theory.

This approach is called "modal suspension", because proceeding forward from the proposition is "suspended" by the "mood", awaiting further analysis.  And it implies that your confidence in the statement depends on whether the theory has been largely identified and satisfied, or whether you believe the source based on relevant experience.
As a man, who is six feet tall, I can say "Were I a woman, I would be less than six feet tall" and have it mean something.  It is a prediction of how the world might be different if we changed one thing.  This is what logicians mean when they discuss an alternate world.

I can say "If I am a woman, I am less than six feet tall", and it is nominally true, because the condition is false, but not in a way that means anything.  I could equally say "If I am a woman, the moon is made of green cheese." and it would be equally meaningful.

However, saying "Were I a woman, the moon would be made of green cheese" indicates the two things are related for some reason and that my gender would be likely to actually affect the substance of the moon, which is nonsense.

So changing the mood here changes the statement from true but pointless, to theoretical and highly unlikely.

[This is hard to keep track of because the English subjunctive looks too much like the past tense, and people just fail to use it most of the time, or they use the past tense instead.  Folks often say "If I was there", when they mean "If I were there".]

Also, there are variants of "would" that slightly alter the kind of alternate world the sentence is indicating.  For instance, if I said 'might' instead of 'would', it indicates that the difference would not be logically implied by the change in the world, but would make it possible.  "If I were a woman, I might wear pink more." means that my being a woman would make that possible.  It would not necessarily happen, but it surely is not going to happen as long as I am a man.   If I said 'should' it would imply things would be disapproved of in the alternate world because of social convention or moral sense, rather than actually being different.  "If I were a woman, I should wear a shirt."  But as I am a man, and it is 80F, I have mine off and no one cares.

[Again, hard to learn, because these are rules many people just fail to follow.  The precise intentions behind words like might, may, should, etc. drift around a lot because they are poorly used.  It is so bad that when it is necessary to insist they be used correctly, for instance in military specifications, people insist that their definitions be stated, even though they are being used with their traditional meanings.]

Logic models this as if all of the possible worlds created by changes to this one already exist, and we are just picking which ones to imagine working in.  This is in principle silly, but it gives a clear way of discussing alternatives.

You can imagine that every counterfactual conditional is true in some alternate world.  In impossible worlds, everything is true and false at the same time, and in possible worlds, the counterfactuals that are really true have true conclusions.

The problem is that the subjunctive places the statement in the realm of alternate realities and not actual reality, and without specifying which possible world you are talking about, the statement becomes true but useless.  We need to know what kinds of rules we imagine are not changing when we change the thing we have chosen to change.  Only the worlds that change what we intended and do not change the rest are allowed in an interpretation.

In situations where the range of allowed possible worlds is well-defined, either explicitly or by context, the subjunctive form becomes declarative when combined with that definition included in the premises.  So suppose we said "Because of basic biology, if my other genes did not change, if I were a woman, I would be less than six feet tall."  Then I can tell what is to be kept fixed when I consider the alternate gender, and only padded out in this way does the sentence have real and definite logical content.

Generally, we can guess what the omitted premises are, and we just don't say them.  But if we cannot guess them, we just have to act as if the false conditions are meant to be taken literally, and the statement is just true but meaningless.

With this interpretation, the same is true of statements of obligation, those 'should' constructions, and of statements of mere potential, those 'might' constructions.  They are all true all the time, but only meaningful with additional premises.
I think your attempt to divide the argument into stages is not bad, but your wording for 1 and 2 causes you to run into some problems from the get-go:


  
  defined as the actualization of a potential attribute of an object 
  


and


  
  The actualization of a potential attribute of an object cannot be caused by that attribute, because only what is actual can cause what is potential to become actual.
  


First, note that Aquinas does not speak of objects. This is because "object" at the time he's writing would have a very different meaning than to our post-Kantian conception. This also helps the "attribute" talk to be a little bit misplaced here. It's not potential attributes of the object, it's actual potentials in the thing. Or to put it another way, these are already in the thing as sight is already a potential of the eye even when my eyes are closed.

The second claim is also a little bit weirder as a translation for what Aquinas is saying. That might be closer to what's going on in the second way (efficient cause) than in the motion argument. The motion argument doesn't need such a complex notion of objects. It just needs (a) a definition of motion, (b) the fact of its occurrence, (c) an exploration of how the conditions that match the existence of motion and its definition can be met.

I generally formulate the argument roughly in this way:


There is motion
Motion is by definition A causes B to move.
A and B cannot be identical because actuality of motion must be brought about by actuality of motion -- not potential to move in the object itself. 


This is easy to misunderstand because we also speak of "potential energies" in physics, but even a ball at the top of a half-pipe is either (a) already in motion or (b) waiting for something to make it move.

If this set is right, we get two choices either an infinite series of prior As which Aquinas rejects as a vicious negative regress or some object that defies the normal definition of motion and causes its own motion.



There are several ways people can object to this argument, including (a) accepting the infinite regress as okay, (b) denying the validity of the definition of  motion [often made under a somewhat suspect claim that QM has defeated this concept of motion -- made by people who still function under it when managing to type and submit things], or (c) suggesting multiple unmoved movers.

Also, a semi-unrelated move is to reject the identification of this unmoved mover with God.
Well, there's pure theoretical reason, pure practical reason, and even pure judgment, but it sounds like this is not the thrust of your question.

I definitely disagree with the commenter you cited who claimed that God and human beings have the same faculties of reason. Possibly the single most important distinction for Kant is the distinction between intuitus originarius and intuitus derivativus. Only God has an intuitus originarius, a faculty of intuition which is capable of generating the object it intuits. The human being is a fundamentally finite creature and thus has a passive intuition. The object we intuit must already exist, by virtue of some power beyond us (as a thing-in-itself) in order for us to be able to receive or intuit it. Kant calls God’s faculty an intellectual intuition, because like our intellect it is spontaneous and discursive. We are capable of taking two concepts and combining them in thought spontaneously, but not so in intuition (keeping in mind that imagination is a separate faculty).

As I said, this distinction underlies everything essential to Kantian thought. Because of the finitude of our faculties, we are incapable of immediate intuition of the thing-in-itself. Thus all the questions posed by metaphysics cannot be answered by our pure theoretical reason, and rather represent the confusions of a finite mind grappling with the idea of infinity. The impossibility of answering these basic metaphysical questions, including the question of the existence of God, is the theme of the Critique of Pure Reason. Because our reason does have immediate access to the moral law, and is capable of spontaneously affecting itself in this domain, practical reason is our only access to ourselves as a thing-in-itself, which forms the theme of the second critique.

This point is also essential to the distinction between Kantian thought and the German Idealism of Fichte, Schelling, and Hegel. Fichte says that every self-consciousness is infinite and an intellectual intuition by virtue of the fact that the ‘I think’ creates itself by thinking of itself. Thus, like God, we create our own self-consciousness by thinking of it. This infinitude is the fundamental break with Kant, though these thinkers claim to be carrying on aspects of his work. 

Here is a relevant quotation from the first critique:


  It is, moreover, not necessary that we should limit the mode of intuition in space and time to the sensuous faculty of man. It may well be that all finite thinking beings must necessarily in this respect agree with man (though as to this we cannot decide), but sensibility does not on account of this universality cease to be sensibility, for this very reason, that it is deduced (intuitus derivativus), and not an original (intuitus originarius), consequently not an intellectual intuition, and this intuition, as such, for reasons above mentioned, seems to belong solely to the Supreme Being, but never to a being dependent, quoad its existence, as well as its intuition (which its existence determines and limits relatively to given objects). This latter remark, however, must be taken only as an illustration, and not as any proof of the truth of our aesthetical theory.


Kant, Critique of Pure Reason, First Part: Transcendental Aesthetic Section II SS 9

With regard to angels, I don't believe that Kant thinks any beings other than God could have an intuitus originarius, but there are still some interesting differences between their faculties and human faculties. Kant views all rational beings as possessing absolute moral principles. However, human beings often stray from what is right because they also have a faculty in conflict with practical reason, their sensuous appetites. Kant thinks that angels would never err from God's will or from practical reason, because they do not have the appetites to lead them astray. They need not eat and so feel no hunger; they are eternal and so have no need for sexual reproduction. 

He does not envy them, nonetheless. The conflict human beings face between appetite and practical reason is what ultimately makes us free agents. Angels, because they cannot err, have deterministic behavior. The possibility for error that is the constant struggle of the ethical life justifies for Kant heaven and its reward of eternal happiness. 
Much of what you seek is in Doctrine of Right which is the first half of Metaphysics of Morals (not Groundwork of the Metaphysics of Morals). E.g., Kant believes in capital punishment for murderers.

There, Kant stated: 


  Even if a civil society were to be dissolved by the consent of all its mem- bers (e.g., if a people inhabiting an island decided to separate and disperse throughout the world), the last murderer remaining in the prison would first have to be executed, so that each has done to him what his deeds de- serve and blood guilt does not cling to the people for not having insisted upon this punishment; for otherwise the people can be regarded as collaborators in this public violation of justice. (6.333)


I think Kant sees it as consistent with treating people as ends, because murderers have committed the moral evil of wiping one of those ends from the earth. Moreover, it is that you must treat them as ends and not mere means. And capital punishment is not a means to a different end, but an embodiment of justice for Kant.

Moreover, it's an implementation of http://www.utm.edu/staff/jfieser/class/160/7-cap-pun.htm the universalizability schema. i.e., the murderer wills that human life should be snuffed out. Ergo, his maxim is made universal for him.
We can make only some comment about your question, that do not form a real answer ...

Consider for simplicity only the "sub-clause" :


  Capital murder is the premeditated or non-premeditated murder of any individual in the commission of abduction, robbery, rape, attempted rape, forcible sodomy or attempted forcible sodomy 


First of all, it is a definition and the logical structure of definitions is not analyzed by propositional logic.

With proposistional logic, we can "formalize" it as :


Capital murder is the premeditated murder of any individual in the commission of abduction


or


Capital murder is the non-premeditated murder of any individual in the commission of abduction


or 


Capital murder is the premeditated murder of any individual in the commission of robbery


or


Capital murder is the non-premeditated murder of any individual in the commission of robbery


or ...

As you said in your comment, it is of the form :


  p1∨p2∨...∨pn, with n=12.


What we can say about it ? Basically, we have that pi ⊢ p1∨p2∨...∨pn, that is, establishing that we have a "case" in the list, we can conclude that "capital murder" applies to it.

If we move to predicate logic we are defining a predicate :


  Capital(x) iff Premeditated murder of any individual in the commission of Abduction(x) or Non-Premeditated murder of any individual in the commission of Abduction(x) or ...


What we have to note is the "role" of the negation : the "predicate" NPA(x) is not the negation of PA(x).

When we say that P(x) ∨ not-P(x) we are saying that, for every "object" x in the universe of discourse, P holds of it or not, which is true.

But in our case, we cannot assume that, for any "action" x, it is the case that it is a "Premeditated murder of any individual in the commission of Abduction" or it is a "Non-Premeditated murder of any individual in the commission of Abduction", because it is not true that every "action" which is not a case of "Premeditated murder ..." is a case of "Non-Premeditated murder..."
Your question is a bit border-line, as it essentially asks for our opinions. Instead I will discuss what some philosophers have said about this.

Let's, for the sake of the argument, assume that wealth indeed implies well-being, and that therefore wealth is desirable. I will also use rich and wealthy as synonyms, as I will use poor and unwealthy.
Then, we could say that the maxim we want to know if it's an element of or derivable from the ethics of certain philosophers, would be:


  Act such that your actions will converge inequalities in wealth between all people as much as possible.


Now, let's see what Kant would say about this.

Kant

Kant's famous categorial imperative says, in the first formulation:


  Always act according to that maxim whose universality as a law you can at the same time will.
  – Immanuel Kant, Foundations, p. 436


There are two other formulations, but here, especially the first formulation is interesting. On http://en.wikipedia.org/wiki/Immanuel_Kant#The_first_formulation wikipedia, the universability test, a method to determine whether or not your maxim can be one of those of the categorial imperative, is described.

Let's imagine a world in which the maxim as we defined it is a universal law. Therefore, everyone necessarily acts according to this maxim. Do we find a problem here? No, rather the opposite: 

If everyone would act according to a maxim as "act such that your actions will diverge inequalities in wealth between all people as much as possible", the world becomes terribly unstable.
First of all, this will create one group very rich people and one group very poor people. However, in the end the rich people will need the poor to be richer in order to get richer themselves (to sell things, to produce more, etc.) So in the end we can't diverge anymore. Any action to make the rich richer will need the poor to be richer as well; any action to make the poor poorer will need the rich to be poorer as well. We're running into a state in which the maxim doesn't really have any effect anymore: a final state, in which people can't really follow the maxim to its end anymore. The question is whether this final state is desirable.
Secondly, imagine some kind of natural disaster, somewhere along the way to this final state. A big flood comes, destroys everything of the rich, making them poorer than the poor. Following the maxim, the 'old poor' will now become richer and richer, while the 'old rich' become poorer and poorer. This demands adaptability from us, the humans in the world, an adaptability that we don't really have. 

So a world in which the opposite of our maxim is a universal law, isn't desirable. Kant doesn't say that from this follows that the maxim itself should be a universal law, but it could be argued with his help.

Nietzsche

The immoralist. He didn't really formulate his own ethics, but had lots of criticism on others. He argued that 'God is dead', meaning that we don't have any moral framework anymore, and that we're "plunging continually":


  Whither are we moving? Away from all suns? Are we not plunging continually? Backward, sideward, forward, in all directions? Is there still any up or down? Are we not straying, as through an infinite nothing? Do we not feel the breath of empty space?
  
  – Friedrich Nietzsche, The Gay Science (1882, 1887) para. 125; Walter Kaufmann ed. (New York: Vintage, 1974), pp.181-82.]  (http://www.fordham.edu/halsall/mod/nietzsche-madman.asp http://www.fordham.edu/halsall/mod/nietzsche-madman.asp)


It's a madman talking here, but it's assumed that Nietzsche referred to himself. In the end, the madman leaves:


  Here the madman fell silent and looked again at his listeners; and they, too, were silent and stared at him in astonishment. At last he threw his lantern on the ground, and it broke into pieces and went out. "I have come too early," he said then; "my time is not yet. This tremendous event is still on its way, still wandering; it has not yet reached the ears of men."
  
  (idem)


Nietzsche of course doesn't really tell you what to do in this case, but he complains about there not being a (universal) moral framework. With Nietzsche, you could therefore argue that making any maxims yourself is pointless as long as they're not universal. Furthermore, you could say that making a maxim universal would require more than a human, and therefore that making maxims yourself is always pointless (because you can never make them universal). It would require a God, to make maxims, but unfortunately, God is dead, and we can't get him back.

Žižek

I find the explanation of his thought on the http://nl.wikipedia.org/wiki/Slavoj_%C5%BDi%C5%BEek#Sociaal_en_politiek_denken Dutch wikipedia much clearer than on the English one, so I will translate a bit:


  The postmodern society lost its belief in bigger contexts such as explicit ideologies, authority, religion, tradition, etc. In Lacan's terminology, the http://en.wikipedia.org/wiki/Jacques_Lacan#Other.2Fother Other (with a capital), is the overarching justification of social order, death. This leads to social disintegration. As progressive thinker he doesn't regret every break with tradition, but he sees a couple of interesting, or alarming, effects of the lack of a social framework.


This last sentence is of course very much in line with Nietzsche, and Žižek also says, like Nietzsche, that this lack of a social framework is alarming. 

Maybe now it's the time for Nietzsche's madman, in the form of Slavoj Žižek. 

A way out from this lack of framework is not discussed as far as I know.

One could argue that you asking this question in the end is an effect of the lack of a moral framework, and that therefore your question "Am I immoral for ever spending money on the movie ticket?" doesn't make sense - there is no such thing as immorality.

Some loose thoughts


Spending money on a movie ticket may make you much happier so you become more productive, earn more money, and can give more money to charity in the end. It's not as black & white as it may seem.
How much of your donation will end up with the people needing help?
What is good help in this case? Should we try to help people get wealthier [knowing that this will take several generations, if we're ever going to succeed], or should we try to help the people that are alive now by social support, "being there for the other", etc.?
How much effect is your money really going to make? For $1000 you can feed x hungry people, who will then die one day later. For $1000 you can also travel there yourself (for example), talk to the people, give them this experience (being listened to) that they will never forget, and give them strength for the rest of their lives, albeit short.
You state that you think you're morally obliged to help less wealthy people. Yes, this does mean spending money on things you're not morally obliged to spend money on, would be considered immoral. However, I've been trying to point out that this moral obligation is not so clear-cut.

Decisions are not made at a quantum level. Human brains are classical, not quantum, computers. Quantum computers require very precise control over the components, whereas human brains are hot and wet.

Human decisions are things you can control and could be aware of. People aren't always aware of their decisions and motivations, but they could be more aware with better introspection, it's not just out of their hands.
For me, the problem with going the speed of light is that you are then outside of time.  Once you were How would you slow back down when you got where you wanted?

So there is the other side if the speed of light, but you can't get there from here, you have to go somewhere else first.  And the 'somewhere else' is a place where all material objects have infinite mass.

Theoretically, the only way to get 'superluminal' would be to translate yourself via CPT reversal, or some other translation mechanism that turns "you" into something that is already "tachyonic antimatter".  But that is in some sense not travel, just some sort of information re-encoding.

And it may happen spontaneously all the time.  (Yes, I am going where I always go with time questions, time either equals or does not equal entropy accumulation.)  If entropy does not accumulate in a perfectly linear way throughout the universe, then you occasionally go back in time, but you cannot experience the reversal, because memory is an exothermic process.

So to my mind, the way to get somewhere yesterday is not to pass the speed of light, it is to manipulate entangled states.  If time really is subjective, and flows both ways, you should be able to make something in the past that must match a given state in the future or keep some other event un-collapsed forever.  Since the odds of the latter are so low, a copy of you would have appeared there, then, to prevent it from needing to happen.

We think we know how to set up quantum states that can only be observed if they meet a given set of conditions.  This is the basis of quantum computing: if the bizarre combination of factors that formulate the problem were not met, the computer would have to stay 'uncollapsed' for way too long.  So just stating the question forces a solution to happen relatively quickly.

If time really has no implicit direction (or if any kind of paradox prevention is automatic) then creating a causal loop constitutes causation.  So stating a problem, and locking it away unobserved means that its outcome is not determined until we observe it.  I think we could, if fiendishly clever, force the conditions to be that the solution should match what we are going to put into a given box later.

Then we put you in the box.  And you are the unobserved solution, which means you exist shortly after the problem was stated.  But now you are observed.  The causal loop constitutes a cause of you being instantly duplicated at an earlier time at a given location, which may be arbitrarily far away from where you are now.

Of course, we cannot do that kind of thing now, and you may only ever be able to manage that state at the level of an individual quantum particle.  But if you could do it to something with history, it would prove that time's arrow is not a physical reality, but an aspect of perception.
The nesting is important.  This is easiest to see with the following scenarios:


  Joe will let Peter take his guitar anytime he asks and has good reason.
  One day, Peter takes the guitar without asking and Joe is furious--until
  he finds out he needed it to play for his girlfriend's younger sister's
  birthday party.


vs.


  Joe will let Peter take his guitar anytime he asks and has good reason.
  One day, Peter takes the guitar without asking.  Joe is furious, as Peter
  knew he would be, but Peter "just didn't feel like asking" this time.


The nested-if definition calls the second one dishonest appropriation, but not the first.  It's

(other knows circumstances => other approves) => not-dishonest


In particular, you can't just look at Joe's reaction before he knows what Peter's reasons are.

Edit: had the outer implication backwards.
I was able to get the answer.   It is a cognitive bias: the https://en.wikipedia.org/wiki/Normalcy_bias Normalcy Bias.  Quoting from the Wiki article,


  The assumption that is made in the case of the normalcy bias is that since a disaster never has occurred then it never will occur. It can result in the inability of people to cope with a disaster once it occurs. People with a normalcy bias have difficulties reacting to something they have not experienced before. People also tend to interpret warnings in the most optimistic way possible, seizing on any ambiguities to infer a less serious situation.

It is surely not clear that your initial statement is correct.  We can generally divide phenomena into three categories: the effective (information), the reflective (internal constructions) and the affective (emotional responses).  These distinctions have a lot of different names, but they seem to be a stable set of distinctions made in language (Germanic modal verbs), religion (Gnostic theories of the trinity, the three Pillars of the tree of life) and a variety of forms of pre-scientific magic (astrology and alchemy in their late Western forms).

In an absolute logical sense, only the first two are 'real', one can be either a realist or an idealist.  But the third category always gets thrown in there, and is generally the main ground of the topic of ethics, the most specifically human part of philosophy.

From one perspective, the reason that the third category is generally not seen as part of either of the other two is that it is partially interior reprocessing and partially mental experience of the body (according to the James-Lange theory of emotions and its more modern improvements).  It feeds back into itself so quickly that it becomes, to a certain degree, completely mixed, neither interior nor exterior and so independent of both of the preceding forms.  

If that is true, then to the degree that they are partially observations of your body's internal condition, its contents are not really learned from other people.  What we tend to learn instead is how to name and channel them, not how to feel them.  According to studies of adoption, (Not the greatest reference, but -- http://adoptionvoicesmagazine.com/my-second-mama/do-adoptees-have-more-problems/#.VIXXBTHF-So http://adoptionvoicesmagazine.com/my-second-mama/do-adoptees-have-more-problems/#.VIXXBTHF-So) if your own emotional landscape is significantly different from your parents' you tend to grow up undercontrolled, rather than underemotional.  So your theory seems backward.

To my mind, this is still not a good basis for judging someone else's level of emotional expression - they are either physiological or environmental, not moral.  Lower levels of physiological feedback are not a crime, they do not make you less human.  Nor are very effective strategies for predicting and restraining responses a commentary on your humanity.

However, emotional empathy is a basic form of communication.  According to attachment theory, the ability for one's own emotions to inspire empathy in others is their basic purpose.  To the degree others do not feel your emotions, they may find you to not be communicating clearly.  Communication is a basic way of being human, so they have some justification in the feeling you are withholding your humanity from them.

It is also true that overrestrained emotion is often an overadaptation to strong emotion enforced by a family with a history of trouble keeping normal levels of emotion from growing excessive and violent.  So a lot of people are frightened by a lack of emotional display, as it can be a predictor that someone will overreact in some later situation, perhaps violently, or perhaps simply at the worst possible time.  If what your emotional presentation causes in most other people is fear, then you may be unconsciously manipulating them, and they are then justified in looking at you as something of a criminal, who gets his way by using their negative emotions against them, and so 'less human' than someone with a more average presentation.

So basically, they could be wrong, or you could be wrong, or there could be nothing wrong.  But this is not as simple as you would like it to be, primarily because you have  an interpretation of the source of emotion that most of us do not find compelling.
A few quick points and arrows, since there's something very interesting around you're hitting upon here. (I wonder if you can't draw it out a little bit further?) 

There might be a line of inquiry here around the world-historical role of philosophy: perhaps to what extent it actually arrests thinking/abstraction (or did historically, this is something Deleuze for instance is curious about); and on the other hand how philosophy "hides" its real-structure-in-the-world behind the indeterminable complexity of the concepts it proliferates.

The accelerating complexity of the world is (more or less plainly) coupled with a reduction of the scope of ethics to theories of (efficient, agile) administration. This is something Baudrillard is particularly concerned with; and framed as a more general historical phase-shift is something Foucault is also curious about (his concept of governmentality would be something to investigate around this.)

Too quickly -- perhaps Virilio's ideas about the weaponization of speed could be relevant here as well. Negative Horizon seems maybe an interesting text to investigate in this direction, in terms of how to "live" an ethics after the Disaster, after the traumatic disappearance of distance. He seems particularly urgent in terms of crafting life-practices, though again there is a sterility here, a strangely "terminal" or administrative ethics, even though it is operating in a chaotic milieu of continuously exponentiating powers. Whereas of course there are other kinds of ethical visions entirely (which it might be worth returning to Spinoza to see clearly.)
I don't know about the first case, but the "nothing is contingent" case is excluded by Aquinas's own assumption that we simply encounter contingent things in our everyday experience. http://en.wikipedia.org/wiki/Quinque_viae%23The_Argument_of_Contingency Here is a translation of an excerpt from the original text:


  The third way is taken from possibility and necessity, and runs thus. We find in nature things that are possible to be and not to be, since they are found to be generated, and to corrupt, and consequently, they are possible to be and not to be.

I would suggest that to the degree cowardice expresses fear, courage mostly expresses anger.

It is the habit of properly outwardly acting upon passion, and 'passion' or 'joie de vivre' really are anger by another name.  We have a horrible tendency to label emotions very prejudicially, and make most of the simplest default labels negative.

I lament this here -- https://philosophy.stackexchange.com/questions/15541/what-is-it-like-to-be-happy/18324#18324 What is it like to be happy?  and should not repeat myself.

In Catholic folk-culture there is a traditional quadrature of virtues assigned to the Evangelists and linked to the "cross-quarters" of one configuration the elements: Courage, Prudence, Justice, and Temperance.   The main line of those traditions indirectly encode Aristotle, so I think it is OK to argue from that viewpoint here.

From that perspective, fear leads one to Temperance by stalling potentially inappropriate action in the present and to Prudence by motivation from the fear of want in the long run.  So it is not cowardice, if that lies exactly opposite Courage.

Anger in the same way, motivates both Courage and Justice, motivating both direct and indirect action against what is wrong and toward what is pleasing in an appropriate manner.

The two other "quarters" get mapped to "Sorrow" which motivates Justice and Temperance and "Shame" which motivates Courage and Prudence.
This is not about philosophy as much as psychology.  (But our systematically skewed perception of social assessments is something philosophers should keep in mind.  If someone could formulate it well, there is an interesting question in why the stuff most important to us is most often perceived least objectively.)

Estimated via serotonin and cortisol levels, confidence actually predicts competence pretty well in the median-earning range of the population.  One theory is that this is the range for whom feedback about competence, especially from our education system, actually bears on self-image, while people of privilege and people of either very low or high ability are often more immune to feedback.  And those latter are the people we think of when informally estimating social effects.

A history of success predicts future success, and also predicts confidence, up to a point.  One theory (from studies of gifted children) is that this is the point where more acute awareness in general may prime more acute self-awareness and lead one to over-correct, (and the curse noted by Bertrand Russel quoted in the comments begins.)

For instance on the up-side, IQ predicts both confidence and future income, up to a certain level, and then it stops.  Most people's IQ's lie well below that cutoff, but the most noticeable people's lie above it.  Something similar happens on the down-side -- e.g. the dumb and the arrogant are both more likely to go to jail, but the bet is on the dumb over the arrogant, while the arrogant are the more attention-grabbing cases.  So our perception of how much these traits are correlated is skewed.

We overestimate the limitations on this effect, because we are focused on extreme cases, but the mode lies near the median, so we are misled by that impulse.  Given that flaw in our perception, we should look at a couple more concrete observations that apply more directly:

The feeling of being powerful has been experimentally verified to allow one to think more abstractly more often, which pays off in modern life.  When mixed with raw competence, the tendency to frame things abstractly allows for future improvement, in a way that simple hard work does not.  So, to some degree, especially in the young, confidence predicts potential future increases in competence.

Also, risk taking is necessary to leverage competence for actual achievement.  Confident people, especially males, are valued for their ability to face risk productively in situations where more competent people might be put off.  In a group setting, you often need one party's confidence in order to adequately deploy another's competence -- we are all familiar with the strange stereotype of the underling who actually does all of the work that creates the reputation of 'the great man'.
From http://en.wikipedia.org/wiki/The_finger#Origin http://en.wikipedia.org/wiki/The_finger#Origin:

According to anthropologist http://en.wikipedia.org/wiki/Desmond_Morris Desmond Morris, the gesture probably came to the United States via Italian immigrants. The first documented appearance of the finger in the United States was in 1886 when Old Hoss Radbourn, a baseball pitcher for the Boston Beaneaters, was photographed giving it to a member of the rival New York Giants.

Thaat being said, there is a long history of using gestures as obscenities. In the late Renaissance and in the http://en.wikipedia.org/wiki/Early_modern_Europe Early Modern Period of Europe, biting your thumb towards someone was considered similar to giving them "the finger" in modern times. Knights of the Medieval period would raise their lances in what seems to be a similar fashion.

Short answer: We don't quite know, but the history of using physical gestures as insults is quite long.
I don't think you can address this from a Utilitarian point of view, because utility is not well-defined for those who do not have a consistent view of the world, and whose world-view is continually evolving or shifting to a great degree, such as children and the mentally ill.  You need to be more conservative, and work from the 'limited interchangeability of individuals'.  The clearest picture of that is from Kant.

From a Kantian perspective, the deduction that renders the "Universalizable" version of the Categorical Imperative to the "Ends-Means" version gives us the clue.  We cannot universally approved of manipulation based on superior agency, or we all become puppets of the brightest sociopath among us.  Instead, we need to find a way of negotiating that respects limited agency.

To me, that question has to come down to the possibility of reciprocal agreement.  An unconscious person cannot agree, and someone like a child cannot agree with an adult in a way that is truly reciprocal: the child's stated agreement is based on a more limited understanding of consequences, and what they are agreeing to is not really the same thing as what the adult is agreeing to.

So the question is whether continuous working memory is necessary for reciprocal agreement, or whether agreement can be based on the facts present in the moment.  I think that the answer is not the same in all cases, but I would argue it depends upon the nature of the facts, and that the unimpaired individual is competent to know the difference.

We are ourselves both in the moment, in that we have a personality of a certain character, and across time, in that our decisions reflect what is going on around us, and we can change that character.  Agreement that is in accord with the stable character of the person making it can be accepted even when that person is impaired.

If you do not have adequate exposure to the individual when lucid, you may not be competent to make that judgement, and in that case, expecting consent is out of order.  Also, if recent changes which the disabled person may not have processed correctly have taken place that might have real bearing to bias the decision in a negative direction, it is not safe to accept positive consent that has significant potential negative consequences that the individual has not already faced.

But uniformly deciding that someone who was once competent to initiate sexual relationships no longer is, is insulting and punitive.  So it is not a good idea to assume the impossibility of consent without analysis of the individual case.
I think a lot of directions in our society are converging on the notion that the world runs on feedback loops, and this notion of reducing things to an absolute basis is a fool's errand.

We are enthralled by the power of deductions that simplify complexity into something we can state compactly because we are beasts of language and limited memory.  But these powerful generalizations, just because they are powerful, are not 'more basic' than others.  There is no absolute 'downward' in the complexity of thought, only a relative one.

What is most basic depends strongly on the circumstance and the goal.  That one needs food is an immensely complex statement, given the incredible biological and chemical mechanisms behind food, and the even more staggering complexity behind the mechanisms we interpret as needing.  At the same time, the idea that matter curves time is comparably quite simple, though almost incomprehensible to almost everyone, almost always.

You need a more pointed goal and context to get any meaningful dialog on this kind of thing.
This looks like an http://en.wikipedia.org/wiki/Appeal_to_ridicule appeal to ridicule because it does not involve attacking you personally (so its not ad hominem). http://en.wikipedia.org/wiki/Begging_the_question#Definition Begging the question is another possibly-relevant type of circular argument.
The ancient Greeks called pure and robust peace of mind Ataraxia.  It is when one discovers no belief can ever be justified, at least according to the Pyrrhonists. As such, one suspends judgement upon anything non-evident.  The condition was said to have fallen upon the master painter Apelles; while trying to paint the foam of a horse, likely frothy saliva near its mouth.  He was so unsuccessful, in despair, he threw the sponge he was using to clean his brushes with at the medium and created the effect of the horse's foam.  Both the Stoics and the Epicureans also made use of the term.
Regarding the thing itself...

I'm sure there are people who hold the thing in itself is the fundamental construct of matter (probably subatomic particles rather than atoms). In fact, isn't this the materialist position?

However, if we look at the cup-in-itself as http://en.wikipedia.org/wiki/Noumenon noumenon in the Kantian sense, then I don't think atoms qualify.  Noumenon are unknowable.  Atoms on the other hand are (at best) indirectly knowable through their effects, or are (at worst) mathematical abstractions that help us model certain things.

Regarding whether the thing itself is mathematical...

Are things fundamentally mathematical?  Well, cosmologist http://en.wikipedia.org/wiki/Max_Tegmark Max Tegmark has put forward his http://en.wikipedia.org/wiki/Mathematical_universe_hypothesis Mathematical Universe Hypothesis and wrote a book called http://en.wikipedia.org/wiki/Our_Mathematical_Universe Our Mathematical Universe in which those views are presented.  

I also recall an author who made a half-facetious argument that we all believe mathematics is the fundamental ontology.  His argument went something like this.  Put 1 ball into an empty box.  Now put another ball into that same box.  Now count the number of balls in the box.  If there aren't 2 balls, then we'd look for holes in the box, physical anomalies, and even doubt our very senses.  Yet the one thing we would not do is doubt that 1+1 = 2.

This raises all sorts of fascinating questions. What does it mean for something to be a mathematical structure, as opposed to a physical one?  Are mathematical structures things that lack any properties and consist purely of relations?  If we go down to the smallest substrate of matter, would there be any properties?  Would properties imply parts?  At that point, would we be in an area where our traditional thinking simply cannot cope (although some would argue that we got there with Quantum Mechanics)?
Without a natural phenomenon to pin it on, humans would create arbitrary periods for measuring stretches of time. 

This has been done, for example, with the 'week', which has no basis in nature. The same goes for the 24 hours in a day etc. In many calendar systems the 'year' is inspired by the solar year but is not exactly defined by the position of Earth in the orbit. 

On a larger scale, humans have created arbitrary points of significance based on the decimal system, such as centuries and millennia, which are celebrated as special events. 

So if Earth would have had a billion year orbit, humans would have likely developed an arbitrary period similar to year. Probably not exactly 365 days, but it could have been something like 50 weeks or 100 weeks or any other arbitrary conveneant period. 
Although I respect Ken Wilbur and other transpersonalists, they seem to see evolution as a single track, whereas it is clearly something more complex.  It presents to us at least a tree of evolutions, and perhaps a network where traits emerge and fold back together into more complex systems than could evolve in a simple, goal-seeking manner.

We have certainly evolved adaptations that allow us to populate a broader range of the planet.  So while you may or may not want to consider local adaptations as evolution of the whole species, we do see the process of evolution at work selecting humans for different environments.  It is not clear this somehow advances the species as a whole, because they seem to take the form of selective trade-offs.  But they allowed us to migrate to areas we could not previously occupy.

In the north, for instance, light skin allowed us to move north of the tropics and still have enough Vitamin D (while letting us get more skin cancer).  And we evolved the various blood types to resist diseases more often caused by the cold (while creating certain difficulties in childbearing).  To the far north, we think we evolved fat accumulation for warmth, and when those same people moved through the far north to the middle of the Americas, we see the same mechanism adapt for long-term survival in regions with extreme seasonal variation (while making for a lot of diabetes currently).  So, this presents at least a 'tree' of adaptations that increases our flexibility.

Also, there is a good reason not to consider technology as 'evolution', since it can be lost and regained far more easily than atavism can allow physical traits to be lost and regained.  Merging these two concepts seems unwise, since they pursue different goals, an currently, to my mind, seem to be trending in opposite directions.  We are trying very hard to suspend physical evolution, and to make humans' ability to reproduce more equal, and we see the middle classes worldwide in particular producing fewer copies of their genes in the interest of promulgating more emphasis upon their 'memes'.

As they do so they mix those memes in new and interesting ways that change the memes themselves far faster than simply recombining would seem to.  I think our framing from most of advanced science, for instance, combines tropes from religions that arose far, far apart (in particle theories alone: spontaneous generation, irreducible indeterminacy, 'vibrations' as forms of matter, 'light' as the basic substance, the horror of emptiness), and generates new concepts entirely from among those combinations, that could not be foreseen in the originals or gotten by simple combination (like quantization of energy as well as matter).  I would claim this means our psychical 'evolution' is more of a network where tropes split and merge and present a less atomic model than 'memes as mental genes' allows for.

This non-linearity complicates the notion of 'levels' or 'degrees' of evolution beyond a very basic, broad scale.  And I think it is best done away with to begin with, since, wherever I have seen it as a trope, it is generally racist or classist in origin.
Ethics and morality are often used by philosophers as synonyms. Some philosophers have suggested that we use the words in slightly different ways, where “ethics” would refer to a system describing right and wrong action in  particular contexts like a profession or a role (like “business ethics"); “morality” would describe rightness and wrongness in the more general role of being a human being (like “it's wrong to harm other human beings”). But, there may not be a sharp distinction between these contexts, and the words are often used interchangeably.

Principles are rules. In ethics, they are rules about right and wrong action, like “If a person is innocent, it is wrong to punish him/her (or, you should not punish him/her).”

Virtues are properties of people or other beings who act rightly in a habitual way. A habit of acting rightly is a virtue. A person who acts rightly may or may not be following a moral rule or principle. In fact, it may be possible to have a virtue without believing in any moral principles, or if no principles exist.

Etiquette is a set of rules for how to behave rightly or wrongly that are generally understood to be less serious than moral rules. One way of making the distinction is to say that societies can differ radically in their rules of etiquette without any negative consequences, but not in their rules of morality; on this approach, rules of etiquette are defined as the rules that can vary among societies without any bad moral consequences. So, no society that allows punishing the innocent is a moral society, and such a society may not be a stable society. But one society can make it right to pass people on the right, and another society can make it right to pass people on the left, as a matter of etiquette, and it doesn't matter — all that matters is that there is some standard for how passing should work.
See http://plato.stanford.edu/entries/abduction/ Abduction : 


  The term “abduction” was coined by http://plato.stanford.edu/entries/peirce/ Charles Sanders Peirce in his work on the logic of science. He introduced it to denote a type of non-deductive inference that was different from the already familiar inductive type.


[...]


  the best way to distinguish between induction and abduction is this: both are ampliative, meaning that the conclusion goes beyond what is (logically) contained in the premises (which is why they are non-necessary inferences), but in abduction there is an implicit or explicit appeal to explanatory considerations, whereas in induction there is not; in induction, there is only an appeal to observed frequencies or statistics. 

Yes, in propositional logic you could use ¬ A ∧ ¬ B where A = 'no cats are black' and B = 'all cats are black', but that's hardly an improvement from C = 'some cats are black', which is an atomic sentence (can't be split up) as well.

For things like this it's much easier to use first-order predicate logic and quantifiers: ∃c ∈ C [B(c)], where C is the set of all cats and B(c) ⇔ c is black. You can read this as 'there exists at least one element c in the set C of all cats for which B(c), that is, the cat being black, holds'.

Abstract: ∃v∈V [P(v)] ⇔ there exists an (at least one) v in V such that P(v).

Other quantifiers are ∃! (there exists exactly one ... such that ...) and ∀ (for all ... holds ...).
Occam's razor is a principle to choose between two theories which have the same explanatory power. According to the razor, you ought to choose the one that postulates lesser entities. 

This explanation, while often helpful, is still rather unclear and the details are still debated among philosophers. For example, it is left open whether a theory with less types of entities or with less tokens should be preferred (Examples for types: Numbers, Sets, Functions..., and for tokens: 1, 2, {{45}}...).

To conclude: You don't apply it correctly, because the principle is about theory choice.
from http://www.morec.com/nature.htm William A. Wallace, O.P., “https://www.thomist.org/jourl/1997/973AWall.htm Thomism and the Quantum Enigma,” https://www.thomist.org/jourl/explore.htm The Thomist 61 (1997): 455–468. (my emphasis):


  Aquinas, having been taught by Albert the Great, had an excellent grasp of Aristotle's science of nature. He upgraded the knowledge this gave him to organize, as it were, a science of supernature (that of revealed theology), making use of analogy and the Aristotelian concept of a "https://books.google.com/books?id=ohxdaJqcRf8C&pg=PA295 mixed science [scientia media]," combining propositions established by reason with propositions assented to by faith. My project would be to do something similar: to take knowledge we possess from ordinary experience of nature to organize the special type of knowing we call modern science, making use of analogy or modeling techniques and the "mixed science" of mathematical physics, which combines propositions established through the observation of nature with those of mathematics. Here I rely on a teaching that is distinctive of http://www.newadvent.org/cathen/14698b.htm Thomism, in contrast to other http://www.newadvent.org/cathen/13548a.htm Scholastic systems of thought, namely, that analogical middle terms are sufficient for a valid demonstration, no less in mathematical physics than in the science of sacred theology. Such terms, and the models they frequently employ, can provide us with insights into the microworld and the megacosm that are not unlike those Aquinas offered his contemporaries into the spirit world of the immaterial and the incorporeal. 


Although he mentioned the "mixed science" of modern mathematical physics, I don't see why there couldn't be other mixed sciences, like your example: (social observation) + (psychological introspection and inspection).
There is no universal agreed upon answer, but personally I would be a little Aristotelian about it: art has a purpose, and the degree that it is fit to that purpose is the degree that it is great art.

Some would say that the purpose of art is to make our surrounding more pleasant. It is to make us feel good, so we may judge whether lots of people "like" something to determine its value. Under this definition, even a soothing wallpaper pattern or elevator music could count as art, and professional opinion is valueless compared to the aggregation of millions of "likes". Personally I think art is different from decoration, or at least great art is much more than decoration.

I have heard a definition of art that art is what the art community says is art. There are various traditions of creating art, and the artists in those traditions can evaluate what is actually part of those traditions. Professional opinion is all that counts as it takes training to evaluate how well a piece of art maintains a particular tradition or not. This is probably a gross simplification of the position, but I'm not happy with this definition, as I think there is a universal and timeless quality to art beyond what a community of artists may define it to be. 

Some would suggest that the purpose of art is to embody or communicate emotions and ideas, usually in a metaphorical way that gives it a general applicability that prose does not have. When we see great art, we feel something more, or understand something more than we did before, even if we can't say what it is. It is also accessible beyond a select group of people; it tries to communicate with everyone.

Art seems to have a language all of its own, and the way that we respond to art is largely determined by the associations we have with art from a similar tradition. These associations are like the language that we have picked up by previously being exposed to art. Some art languages we seem to pick up naturally, just as we pick up our native spoken language. Some languages take more work.

In this perspective, we can evaluate art by how well it uses the art languages from the traditions it draws on, and communicates to the desired audience. That audience may be people with only basic art language comprehension, or it may be people who have spent years learning artistic languages. 

Maybe great art needs to have something to offer both audiences. There is a certain genius to just communicating to people using a small vocabulary, as shown by Dr. Seuss. But it has nothing for highly literate people, so it may be a stretch to call "Fox in Socks" great literature. Likewise, I think a book that can only be read and understood by a select few is no good at communicating, so it can not be great art. 

Really great art seems to have a quality that allows it to transcend the art tradition in which it is based. It is almost like even if you don't fully understand it; you just know that it has a lot to say. So in that way, we can all judge great art to some extent, but it is good to have people around that can read it properly so we know whether it is really great or whether it is really just a babbling imposter.
The challenge is defining "immortality."

One must be careful in defining it as "something which does not change."  The only way to manage that is to (a) defeat thermodynamics and (b) remove onesself from everything governed by thermodynamics.  This basically means you have to discover something we believe to be impossible, and remove yourself from everything that isn't you.  That's a really high price to pay.  You literally could not have any influence in the world, or you would be subject to thermodynamics yourself.  Worse, if you judged wrong, and actually were still under the effects of thermodynamics, you would have removed yourself from anything else in the world that could potentially teach you how to truly overcome it!

If we shift viewpoints a bit, we could look at eigenvectors of the time function.  Eigenvectors are a neat thing that show up in linear algebra.  They are a value, call it X, which when thrown through a particular transformation, result in a value which is kX, where k is some real number.  For values which are not eigenvectors, the result is never so easy.  This provides an interesting potential definition of immortality, because the "flavor" of X remains, as long as k is nonzero.

If you could find the "time function" which governs the rules of the universe as time progresses, you could identify the eigenvectors of that transformation (assuming it is linear.  QM assumes it is linear, and there are equivalents for eigenvectors in non-linear systems, they're simply harder to talk about).  As long as you always ended up with a non-zero k constant, the "flavor" of your eigenvector could remain forever.

I wont go into the math, but if you make the huge assumption that the world is a linear system (which particle physics seems to suggest, but it's a big leap to go from particle physics to life philosophies), you find that these eigenvectors must actually reveal themselves as rotations.  The only non-rotation term allowed by this linear system is a vector of magnitude 1.  However, this acts more space-like, and less time like; less like "immortality" and more like "omnipresence."

Thus, if you see anything which holds fast, and tries not to rotate in any way, either it is not immortal, or the world is nonlinear.
They can usually be put into prepositional logic.  However, these phrases do demonstrate that typical conversational use of language does not adhere to first order logic.

For example, "I have stopped beating my wife" could be decomposed into a predicate "IHaveStoppedBeating(my wife)," where the predicate IHaveStoppedBeating can be applied to an individual. Following the formal (not casual) language patterns English relies on, this can be decomposed into "InThePastIBeat(my wife) AND NOT(InThePresentIBeat(my wife)),"  this clearly has a false value, for InThePastIBeat(my wife) is false.

The malformed question relies on a different meaning.  In conversational English context adds meaning which is not directly adherent to the first order logic.  saying "IHaveStoppedBeating(my wife) is false" may be a valid logical statement, but it is also a statement that is generally only made in a context.  Expanding to "InThePastIBeat(my wife) and not(InThePresentIBeat(my wife)) is false" in conversational English implies that there is a reason for one of these to be brought up.  If the person asking the question disconnects the answer (which is a predicate, whose truth or falsehood should be constant, regardless of whether it is associated to the question) from the question (which provided the context), it leaves an interesting situation.  The conversational English listener must determine why this phrase was worth uttering. 

As such, it is assumed either the value of InThePastIBeat(my wife) or InThePresentIBeat(my wife) is interesting enough to both speaking of.  In the case of both of these predicates, the "interesting" bit would be if one of these is true.  So we assume one or both of these must be true.  This assumption does not appear anywhere in the formal statement given by the suspect.

"InThePastIBeat(my wife) and not(InThePresentIBeat(my wife)) is false"  has two possible ways it can be accurate.  Either InThePastIBeat(my wife) and InThePresentIBeat(my wife) are both true, or both are false.  Formally this was not a problem, but in conversational English, the assumption that one or both of these statements must be true kicks in.  The only resolution of this predicate with this additional assumption is InThePastIBeat(my wife) is true and so is InThePresentIBeat(my wife).

Thus, the truth statement of "I have stopped beating my wife," given the full accurate context, is certainly false, because you never started beating your wife.  However, if the questioner can successfully remove the fact that the question was asked in that phrasing, conversational English adds an assumption which causes this phrase to imply you beat your wife.  However, this assumption is fallacious, because it is dependent on the conversational English rule that there must be something "interesting" about a statement, and translating that into "beating must have occurred."  This assumption is fallacious because it is not the only "interesting" thing about the statement.  It is also interesting that it is the answer to a poignant question.  Thus I believe it is reasonable to reject this line of thinking, leaving only the answer "'I have stopped beating my wife' is false." 
▻ THE TEXT

It is true that the text of Zeno's 'Politeia' has been lost but there is a large collection of surviving fragments to be found in Diogenes Laertius, Philodemus, Clement of Alexandria, St John Chryostom and others. The fragments are all cited in the first volume of Arnim's 'Stoicorum Veterum Fragmenta'. 'Politeia' is unlikely to have been the title Zeno himself gave to the work. 

▻ CLASS STRUCTURE

Plato's 'Republic' contains three classes or kinds of person, and not exclusively the wise. Wisdom resides exclusively in the philosopher-rulers. There is some indication by contrast that Zeno's ideal polis or state is a polity of the wise. 

▻ PAST OR FUTURE

The evidence is not clear but the majority view is that Zeno projects his ideal polis or state into the future; it is not a description of lost, past political conditions. The status of Plato's 'Republic' is, of course, a matter of great controversy. Did he think it achievable ? Unlikely, because it is perfect and humanity has no part in perfection. Is it a thought-experiment of what political perfection would be like. Not only do views vary but they are likely to continue to do so. 

▻ PLURALITY

Plato talks only of a single polis - the kallipolis - though he does not exclude the possibility of a plurality of perfect polities. He just says nothing about it. Some commentators have thought that Zeno just one ideal polis in mind, others that he envisaged a plurality of polities or even a 'world state' (to use an anarchonistic expression). Here I think we just have to say we don't know whether Zeno had one polity, several polities or all political communities in mind as possible embodiments of the ideal polis.

▻ HOW TO GET THERE

Plato talks of the possibility that rulers should become philosophers, and the kallipolis come into existence that way. Zeno gives no indication of how the ideal polis or state is to come about. 

▻ SEX 

Plato is much concerned in the 'Republic' that sexual relations should be controlled in the interest of not diluting the excellence of the class of philosopher-rulers. Zeno appears more relaxed and less inclined to control; this would make sense on the assumption that his ideal polis or state was a polity of the wise. If all are wise, there is no risk of generational decline through intercourse with the unwise. 

▻ CIVIC LIFE

The ideal polis or city is to have no temples, law courts or gymnasia. Gymnasia were perhaps excluded because the wise would not be concerned with such lowly things as the condition of own bodies; their minds were on higher things. Law courts were unnecessary presumably because the wise cold sort out disputes for themselves. Temples would be imperfect constructions, unworthy of the gods. There is every reason to suppose that Plato retained gymnasia (Rep., III, 403d ff.). And since he wants, not to eliminate a belief in the gods but only to purge religious beliefs of false elements, we may suppose that temples were not abolished in the kallipolis. Plato's view of the situational insight that the philosopher-rulers possess by virtue of their acquaintance with the Forms (eide) suggests that their wisdom would supersede the need for law courts. 

Other details, such as coinage, which Plato said nothing about in the 'Republic', could be explored but Zeno's ideal polis or state has been outlined here from the available evidence.

▻ SOURCES

H. von Arnim, 'Stoicorum Veterum Fragmenta', vol 1, Bibliotheca Teubneriana, 1903.

H. C. Baldry, 'Zeno's Ideal State', The Journal of Hellenic Studies, Vol. 79 (1959), pp. 3-15.
I think the issue mostly arises from having an insufficiently rich language to describe just-the-actual-world vs. model-worlds.  When you can't cleanly distinguish between the two, existence becomes problematic.  If there is no red apple, simply saying "a red apple" is already a problem since it does not refer to anything.

One solution is to use modal logic.  One can define a non-problematic (or at least less-problematic) type of existence http://plato.stanford.edu/entries/logic-modal/#QuaModLog like so:

E(t) := ∃x(x=t)


But one needn't necessarily go to such lengths: just say that existence is a property of your model objects (regarding how they correspond with reality) and don't make such a claim about actual objects, and you're pretty much free of problems.
Humans will go towards whatever motivates them (may not necessarily be money).Abraham maslow
has done a greate deal of research on what motivates humans ,and why different people may be motivated by differnt things. :http://www.simplypsychology.org/maslow.html http://www.simplypsychology.org/maslow.html
It might be helpful to look at some of Rawls' earlier works to grasp what he's meaning by the term. I take it to refer back to the basic meaning of deliberation in a Theory of Justice. 

The central image of a Theory of Justice is that we can arrive at a just society if we deliberate about how we would want society organized if we did not know where in society we would wind up. To put it another way, it's a game and we're making the rules before we play it. On the theory, we are going to produce something moderately fair because we would not in our rational deliberation pick a world where say we personally were tortured every day.

At least that's how I take him to be using the term. There's some evolution after the first work as we move towards the you're looking at. Reading the passage in question, I take Rawls to be saying the following things:


Premise: There is no need for external pre-existing rights in the case of the deliberator (we don't need to make him deny that these exist here). Instead rights are created as consequences of the deliberation process.
Parallel: Rational autonomy like the deliberation in question such that it produces its own list of desideratum without the need for prior commitments to other rights.


The idea in both cases is to avoid requiring sharp metaphysical commitments to get justice and modern liberal democracy respectively (with liberal here not referring to political leaning but to the idea of free rational agents).
While this is a broad question, I think the answer to your question is hidden in the question itself.

People who are fine with relativistic ethics either on a personal or cultural level think it is impossible, unnecessary, or undesirable to justify an ethical system.

In other words, such views take your seemingly rhetorical question:


  If a code of ethics is relative, then there is no absolute right and wrong and everything is only justifiable inside the appropriate point of reference.


And then they go for the other horn of the dilemma -- they accept that this means everything is only justifiable inside the appropriate point of reference. Generally, they then assert that this what has always been going on in ethics. Often this is done by taking either a sociological or biological explanation as the correct understanding of ethics.

As such, they accept that the only justifications you can usually get are internal (rather than external).

A philosopher (or maybe by the end of his career he would have preferred not to be called one) who accepted this view was Richard Rorty. At one point, he was president of the APA. He believes that when we imagine we're hitting bedrock with our thinking that we are just fooling ourselves.

More recently, Gil Harman and David Wong have both defended species of http://plato.stanford.edu/entries/moral-relativism/ moral relativism.

Several other prominent philosophers disagree. A classic attempt to rebut this in the modern era is offered by James Rachel in his chapter entitled "http://becomingserpentine.weebly.com/uploads/3/7/6/2/37623317/james_rachels_-_why_morality_is_not_relative.pdf Moral Relativism."

A rather generic argument follows the same course as your question but potentially amends it with questions about what moral relativism cannot condemn. Rachels for instance points out that the moral relativism cannot condemn slavery in either the past of his own culture or the present of any culture -- nor is it clear that he can condemn murders or holocausts in any of these.

Parts of your answer hint at a natural law approach to ethics. On such views, moral relativism fails because there is a natural moral order in the universe, and we are the sort of creatures endowed with the reason to see it.
I am a little bit puzzled, because you assume that Mill's liberty principle can encourage paternalistic intervention. However, Mill's liberty principle, as you quote, denies any kind of of intervention except to prevent harm dont to others.

Instances of paternalistic intervention would be, for example, the ban on drugs, because they are not good for yourself. The last sentence, i.e. "His own good, either physical or moral, is not a sufficient warrant.", states that taking drugs without harming other people is fine. 

The quoted SEP article seems to confuse his two consequentialist arguments against paternalistic intervention and his liberty principle.

His two arguments according to SEP are "First, state power is liable to abuse" and "Second, even well intentioned rulers will misidentify the good of citizens". But as I pointed out above, his liberty principle explicitly rules out any paternalistic intervention, good or bad, working or not working.

Also, it seems that Mill is after something like the following. Given that an institution has so much power to intervene with the people, even if they never abuse it, they could, and that is sufficient not to have such an institution. The possibility of abuse weighs more than the 'good' that could be done. 
I've seen a few approaches (not necessarily agreeing or disagreeing, just saying that they are there):


A deontological (duty-oriented) approach would say "using drugs and alcohol is bad for you, and so giving money to poor people for them to use drugs and alcohol makes you an enabler."
One reasoning is: well, we have homeless shelters, and giving the homeless money allows them to continue living on the streets instead of the shelters where they are better off (and where meals can be provided more cheaply). I've heard this both from a utilitarian point of view (it's better for everyone if they just go to the homeless shelter), but also from some who look it as a "tough-love" sort of approach (as if not giving to someone is doing that person a favor).

Kant's moral psychology (i.e. his ideas about how feelings, thoughts, morality, and actions) does not allow for sentiments and feelings to simultaneously motivate an action. His moral psychology is not especially clear on this point, but I would recommend looking at Marcia Barons' Kantian Ethics almost without Apology if you want a long version of how one Kantian tries to handle this.

The basic problem is that bifurcates "subjective" and "objective" grounds for action. (Before jumping ahead, be careful of the following definitions of terms).

For Kant, "objective" grounds means only those grounds that arise through the use of your capacity for pure reason. And these would be bases that are found in reason rather than in your animality.

Conversely, "subjective" for Kant refers to those grounds that arise through our feelings, desires, and wants.

It's not perfectly clear, but it looks like for Kant objective reasons arise noumenally whereas subjective reasons arise in phenomenologically with bases in the world of experience. This means the latter would be things that happen according to the known laws of physics whereas the former happen in according with our free wills.

Again, Kant does not make perfectly clear that overloading is impossible, but he does express that any action motivated by these subjective grounds cannot qualify as good even if it is the same action. In other words, Kant's moral theory is not just about our actions but also about our grounds for acting (we might say "motive" but this word is foreign to Kant's description and generally refers to those things that cause us to respond to them, cf. "emote")
I think what you're looking for is "comparative religion."  There are any number of texts out there --one I found particularly good is Karen Armstrong's History of God, although it strongly focuses on the so-called Abrahamic religions (Judaism, Christianity, Islam).

One thing I learned in my studies of comparative religion is that there are certain general forms which tend to be reflected in different religions around the world, even when they are not natural fits.  These include:


monotheistic  
polytheistic  
messianic  
mystical/philosophical  


Thus, even though Catholicism is monotheistic, the panoply of saints has echos of polytheism (similarly, there are strains of Buddhism that revere a canon of bodhisattvas).  Conversely, Hinduism is polytheistic, but there are Hindu sects that are essentially monotheistic.
Many young children are frightened of going to sleep.  I suspect that philosophers may continue to notice the similarities between sleep and death long after most people.  I myself was well into adulthood before I got used to losing consciousness.  

However, we all do it an average of at least 365 times every year we are alive, so it at last becomes a well-practiced activity.  It's also a restorative necessity.  

It does seem like there are at least some people who, by one route or another, eventually come to a place in their lives or their minds where the passage into death is as fearless for them as is the passage into sleep.
Demean basically means "humbling" yourself or "lowering yourself in rank".

In this context you are stating that you place yourself below the law so the rank of the law is greater than your rank if you have any,
basically stating that you accept the law and will follow it. 
that's at least how I understood it.
The point of Schroedinger's cat is not just that you have not checked, the point is that the cat is killed based on an indeterminate event.

Indeterminate events in normal physics have to be in one of the allowed states.  Indeterminate events as we observe them in quantum dynamics can be in multiple states at once, and only decide what state they were in during the past when some result affects something measured.

This ability to not have to write history until you hit another particle is the point.  It seems insane.  We like to believe history is written as time passes.  But on a microscopic scale where individual particles may be far enough apart that we can separate out each interaction and determine its state, this just is not true.  Past history is written when particles interact later.

Of course there are so many particles, on any normal scale, that this almost never matters.  Immediately after one interaction, there is another, and another.  Fairly quickly some of those contribute to some noticeable effect on our shared reality, and things are decided.

In a literal case of a dead cat in a box, that cat is going to rot or not, and you are or are not going to smell it rotting.  No need to open the box.  Even if you had an airtight box, you would have to isolate the cat so thoroughly that its body heat could not contribute to the temperature of the room around it, as on some subconscious level we all measure that.

So this is not a realistic idea, just a hypothetical to make the point of how strange time is on the tiny scales where complete accounting is theoretically possible.

This kind of leads one to accept a view of physics like Leibniz's, where the monads all 'commune' and 'decide' what happens, over a form of materialism where actions are independent and absolutely predictable.  You can consider the distinction a word-game, but it seems to really matter.

For instance, why should time run slower when there are a lot of particles present?  (We observe the gravitational time dilation proportional to mass from general relativity, and macroscopically, mass is basically a particle count.)  You can insist it is all about objectivity and relativity of measurements, but maybe that is just the effect.

It makes comparable sense to consider that those particles, being more numerous and more intimately interconnected really might have to 'commune' more in order to 'decide' how to move on -- so time really passes faster for more 'more independent' particles more isolated in space.
I would choose to rephrase the meaning of sacrifice to be "the exchange of a perceived greater good for a perceived lesser good."  Thus the idea of such an example might be expounded upon from the parent's point of view as, "We came across many situations where we perceived something like a vacation or car as a greater good than the child's education appeared at the day.  However, we had reason to believe that our perceptions were in err, and in fact the money was better spent on the child's education.  However, we as parents, recognize that there are probabilities involved.  Our child might not use that degree effectively.  However, we believe the Return on Investment (ROI) is good enough that we will sacrifice our guaranteed vacations and cars for a probability of improving our child's life in the future."
People are using "Buddha" to mean two different things.


The physical body of the guy identified as Buddha. 
The achievement of a mental state associated with that of a Buddha.


If they spent some time clarifying their terms, this argument would not arise.

The problem with natural language arguments is that often, people talk about different things, so it's a waste of time.  The greater part of such debates should be to clarify terms, and when that is done, often one realizes there was never any argument to begin with, just semantic confusion.

As for the moral of the story, whether it's irrelevant depends on how one treats Buddhism.  For those who take Buddhism to be a philosophy concerned with the relief of suffering, these details are irrelevant. For those who take Buddhism as a religion (and consider the person of the Buddha to be central), these details are of supreme importance.
Here is a possible set of criteria for understanding something:


You know the concepts and facts, and how they link together
You know the underlying assumptions
You know the implications of the thing you understand
You know the conditions under which your current understanding would be false


These may be either too stringent or too loose, and might be a possibly incomplete list. The term 'understanding' is fairly vague - and as will be clear from what follows, important in clarifying the question. 

It is important to note: Whether you judge yourself to have understood something is logically independent of whether you have actually understood it. Consider the following four cases:


You have not understood something, but judge yourself to have.


An example: You believe yourself to have understood why 13 is a prime number, even though you don't know the reasoning behind it, or have a mistaken notion of the reasoning behind it. 


You have not understood something, and judge yourself not to have.


You don't get what implicit differentiation is. And you judge yourself not to have gotten it. No problems here


You have understood something, but judge yourself not to have.


This case is probably most problematic. One path I could posit here is that you have met the criteria that others would reasonably expect you to have met - e.g. being able to teach it to someone else, replicate the steps in a proof, but you intuitively believe that there is more that you need to know in order for it to count as understanding, according to your self-imposed definition.


You have understood something, and judge yourself to have.


You understand the principle of addition - 2+3 = 5. And you know that you understand it.



This means you could be wrong about having judged yourself to have understood a thing. You are probably right that there is some complacency involved after we have judged that we have understood something. Maybe we should be more hesitant with our judgments of our own understanding.

On the issue of falsification, it is true that most of what we believe ourselves to understand could be falsified under some imaginable circumstance. And you are right that insisting on complete unfalsifiability is unrealistic. This suggests that in order for the concept of 'understanding' to have any practical import, we have to set a lower benchmark than "complete, unfalsifiable knowledge". Otherwise it would follow that we do not understand anything, in which case 'understanding' would be a useless notion. I think this stems from an unreasonably high benchmark for 'understanding', and that we could moderate it.

I think that criterion 4: "You know the conditions under which your current understanding would be false." - introduces a notion of defeasibility which could be useful in fine-tuning your understanding.

Hope this helps!
Is science part of the order of the Spectacle? To be answerable better, the question should grasp science and the spectacle as historically changing categories. You may find the following thoughts by Debord useful, not from Society of the Spectacle (1967), but from http://libcom.org.libcom.org/files/Comments%20on%20the%20Society%20of%20the%20Spectacle.pdf Comments on the Society of the Spectacle (1988):


  All experts serve the state and the media and only in that way do they achieve their status. Every expert follows his master, for all former possibilities for independence have been gradually reduced to nil by present society’s mode of organization (...)
  
  Nothing remains of the relatively independent judgment of those who once made up the world of learning; of those, for example, who used to base their self-respect on their ability to verify, to come close to an impartial history of facts, or at least to believe that such a history deserved to be known. There is no longer even any incontestable bibliographical truth, and the computerized catalogues of national libraries are well-equipped to remove any residual traces. It is disorienting to consider what it meant to be a judge, a doctor or a historian not so long ago, and to recall the obligations and imperatives they often accepted, within the limits of their competence: men resemble their times more than their fathers (...)
  
  It is sometimes said that science today is subservient to the imperatives 
  of profit, but that is nothing new. What is new is the way the economy 
  has now come to declare open war on humanity, attacking not only our possibilities for living, but our chances of survival. It is here that science -- renouncing the opposition to slavery that formed a significant part of its own history -- has chosen to put itself at the service of spectacular domination. Until it got to this point, science possessed a relative autonomy. It knew how to understand its own portion of reality and in this has made an immense contribution to increasing economic resources. When an all-powerful economy lost its reason -- and that is precisely what defines these spectacular times -- it suppressed the last vestiges of scientific autonomy, both in methodology and, by the same token, in the practical working conditions of its ‘researchers.’ No longer is science asked to understand the world, or to improve any part of it. It is asked instead to immediately justify everything that happens. As stupid in this field, which it exploits with the most ruinous disregard, as it is everywhere else, spectacular domination has cut down the vast tree of scientific knowledge in order to make itself a truncheon. To obey this ultimate social demand for a manifestly impossible justification, it is better not to be able to think at all, but rather to be well trained in the conveniences of spectacular language. And it is in such a career that the prostituted science of our despicable times has found its latest specialization, with goodwill and alacrity. The science of lying justifications naturally appeared with the first symptoms of bourgeois society’s decadence, with the cancerous proliferation of those pseudo-sciences known as ‘human’; yet modern medicine, for example, had once 
  been able to pass as useful, and those who eradicated smallpox or leprosy 
  were very different from those who contemptibly capitulated in the face of 
  nuclear radiation or chemical farming. It can readily be seen, of course, that medicine today no longer has the right to defend public health against a pathogenic environment, for that would be to challenge the state, or at least the pharmaceuticals industry. But it is not only by its obligation to keep quiet that contemporary science acknowledges what it has become. It is also by its frequent and artless outbursts. In November 1985, professors Even and Andrieu at Laennec hospital announced that they had perhaps found 
  an effective cure for AIDS, following an experiment on four patients which 
  had lasted a week. Two days later, the patients having died, several other 
  doctors, whose research was not so far advanced, or who were perhaps 
  jealous, expressed certain reservations as to the professors’ precipitate haste in broadcasting what was merely the misleading appearance of victory -- a few hours before the patients’ condition finally deteriorated. Even and 
  Andrieu defended themselves nonchalantly, arguing that “after all, false 
  hopes are better than no hope at all.” Their ignorance was too great for 
  them to recognize this argument as a precise and complete disavowal of 
  the spirit of science; as the one which had historically always served to 
  endorse the profitable daydreams of charlatans and sorcerers, long before 
  such people were put in charge of hospitals. [emphasis added]


I would also recommend a reading of Boy Igor [Phil Meyler]'s situationist-influenced book http://www.revoltagainstplenty.com/index.php/archive-global/30-and-yet-it-moves.html And Yet It Moves: The Realization and Suppression of Science and Technology, published in 1985.
However you generate light, the excited medium always 'warms up' before it can emit it and 'cools down' before it stops.  If the variation in the intensity of light is not a collection of short bursts that get spaced out further to give the illusion of dimming, it is driven by a current or heat which would have to vary continuously.

So in each case there is still a continuous process involved, unless you think of these effects at a quantum level.  Whatever varies continuously is subject to Zeno's paradox.

At a quantum level, Zeno's paradox does not apply, because there is no absolute rest, and so the attempt to declare the particle not to have already moved by the time you establish its position is a complete non-sequitur.  Heraclitus is right, and the Eleatics and all their successors are wrong, on this: The lack of motion is the thing that is essentially alien to all matter.
The argument is valid, but does leave room for some ambiguity.

The first is true just by virtue of the fact that 'illegal' and 'criminal' are interchangeable in the jurisprudential argot.

The second is doubtful given that some murders (if we define 'murder' as 'killing a person') are done from the self-defence and hence wouldn't be considered criminal. Otherwise, the premise is analytic, to wit, true by virtue of the terms it employs. For any act to be classified as 'murder', it is necessary that the act is classified as criminal.

If we reformulate your argument given the above distinctions, then we have:


All criminal actions are criminal.
All murders are criminal actions.


C: All murders are criminal

Thus, now the argument is sound and unequivocal. 
To Rousseau, human beings in the First Condition are like beasts. From his http://www.constitution.org/jjr/ineq_03.htm On The Origin And Foundation Of The Inequality Of Mankind:


  While the earth was left to its natural fertility and covered with immense forests, whose trees were never mutilated by the axe, 
  
  it would present on every side both sustenance and shelter for every species of animal. Men, dispersed up and down among the rest, would observe and imitate their industry, and thus attain even to the Instinct of the Beasts, with the advantage that, whereas every species of Brutes was confined to one particular instinct, man, who perhaps has not any one peculiar to himself,
  
  would appropriate them all, and live upon most of those different foods which other animals shared among themselves; and thus would find his subsistence much more easily than any of the rest.


Then, according to him, if human beings keep remaining in animal like conditions, naturally they will be divided into two species: Stronger and Weaker; he then calls upon the Passions. 

From his http://www.constitution.org/jjr/ineq_03.htm The Origin Of Foundation Of The Inequality Of Mankind:


  Whatever moralists may hold, the 
  
  human understanding is greatly indebted to the Passions, which, it is universally allowed, are also much indebted to the Understanding. It is by the activity of the Passions that our Reason is improved; for we desire Knowledge only because we wish to enjoy; 
  
  and it is impossible to conceive any reason why a person who has neither fears nor desires should give himself the trouble of reasoning. The passions, again, originate in our wants, and their progress depends on that of our knowledge; for we cannot desire or fear anything, except from the idea we have of it, or from the simple impulse of nature. Now savage man, being destitute of every species of intelligence, can have no passions save those of the latter kind: his desires never go beyond his physical wants. The only goods he recognises in the universe are food, a female, and sleep: the only evils he fears are pain and hunger. I say pain, and not death: for no animal can know what it is to die; the knowledge of death and its terrors being one of the first acquisitions made by man in departing from an animal state.
  
  It would be easy, were it necessary, to support this opinion by facts, and to show that, in all the nations of the world,
  
  the progress of the understanding has been exactly proportionate to the wants which the peoples had received from nature, or been subjected to by circumstances, and in consequence to the passions that induced them to provide for those necessities.
  
  I might instance the arts, rising up in Egypt and expanding with the inundation of the Nile. I might follow their progress into Greece, where they took root afresh, grew up and lowered to the skies, among the rocks and sands of Attica, without being able to germinate on the fertile banks of the Eurotas: I might observe that in general, the people of the North are more industrious than those of the South, because they cannot get on so well without being so: as if nature wanted to equalise matters by giving their understandings the fertility she had refused to their soil.
  
  Then he calls upon the General Will in order for human beings to form the Association for the purpose of the Common Goods amongst human beings (in another words, for the stronger not to be sole remaining in Nature but to protect weaker through the Passions), 


From his http://www.constitution.org/jjr/socon_01.htm Social Contract


  The social treaty has for its end the preservation of the contracting parties. He who wills the end wills the means also, and the means must involve some risks, and even some losses. He who wishes to preserve his life at others' expense should also, when it is necessary, be ready to give it up for their sake.
  
  Furthermore, the citizen is no longer the judge of the dangers to which the law-desires him to expose himself; and when the prince says to him: "It is expedient for the State that you should die," he ought to die, because it is only on that condition that he has been living in security up to the present, and because his life is no longer a mere bounty of nature, but a gift made conditionally by the State
  
  So that so, flying from the animal kind through the general will human beings will create laws=contracts, and thus Nations etc etc for the preservation of common good.


From his http://www.constitution.org/jjr/socon_02.htm Social Contract


  I have already said that there can be no general will directed to a particular object. Such an object must be either within or outside the State. If outside, a will which is alien to it cannot be, in relation to it, general; if within, it is part of the State, and in that case there arises a relation between whole and part which makes them two separate beings, of which the part is one, and the whole minus the part the other. But the whole minus a part cannot be the whole; and while this relation persists, there can be no whole, but only two unequal parts; and it follows that the will of one is no longer in any respect general in relation to the other.
  
  But when the whole people decrees for the whole people, it is considering only itself; and if a relation is then formed, it is between two aspects of the entire object, without there being any division of the whole. In that case the matter about which the decree is made is, like the decreeing will, general. This act is what I call a law


And also from http://www.constitution.org/jjr/socon_03.htm#001 Social Contract


  What then is government? An intermediate body set up between the subjects and the Sovereign, 
  
  to secure their mutual correspondence, charged with the execution of the laws and the maintenance of liberty, both civil and political.
  
  The members of this body are called magistrates or kings, that is to say governors, and the whole body bears the name prince.
  
  Thus those who hold that the act, by which a people puts itself under a prince, is not a contract, are certainly right. It is simply and solely a commission, an employment, in which the rulers, mere officials of the Sovereign, exercise in their own name the power
  
  of which it makes them depositaries. This power it can limit, modify or recover at pleasure; for the alienation of such a right is incompatible with the nature of the social body, and contrary to the end of association.


Have a good day.
Assume that (p → □q) is valid and "contrapose" it :


  (~□q  → ~p).


But ~□ is equivalent to ◊~, and thus we have :


  
    (◊~q  → ~p).
  


Consider now your example; under the above assumption, we have :


  "if it is possible for Smith to have not a sibling, then Smith is not a brother" 


that "sounds" wrong.
Answering your question properly would require a whole book. I have a shelf full of books on conditionals, having studied it a fair bit and the very short answer is: 


What elementary textbooks of logic tell you about conditionals is wrong. Real conditionals do not behave like material implication. Even if you try to rescue the account by restricting the assertability of conditionals using pragmatic maxims it still doesn't work. 
The distinction between so-called indicatives and so-called counterfactuals is greatly overstated. They are not as different as most accounts make them out to be. Very commonly, exactly the same thought can be expressed indicatively and counterfactually, just by shifting the epistemic perspective. 
The assertability of a conditional is (with a few odd exceptions) completely independent of whether the antecedent is true, false, unknown, undetermined, or even impossible. As such, the term "counterfactual" is misleading as a classification of conditionals unless it is used simply to mean a conditional with a false antecedent. 
The subjunctive mood in English is actually quite rare. Just because a conditional contains "would" and "were" does not make it subjunctive. As such, trying to classify conditionals by their grammar is also misleading. 
A lot of real conditionals are used to express relationships that are not certain - arguably there is little that is certain in real life. For a great many conditionals, 'if A then B'
is better understood as: 'it is highly probable that B, supposing A'. 
Conditionals can only be understood by reference to the pragmatics of their usage: trying to give an account of their semantics on its own is futile. In fact, some commentators have gone further and claimed that conditionals don't have a semantics at all and are just pragmatic devices (though I don't agree with this). 
The meaning of a conditional is not the same as the grounds one has for asserting it. Many conditionals are used to express a hypothetical inference from A to B. This inference could be causal, deductive, inductive, abductive, analogical, etc. These are different kinds of grounds, not different kinds of conditionals. 


  
  If A is true, then B is false.      3. If A were true, then B would be false.
  


If you reflect a bit on them you should find out that these two examples work very differently. If the premise is not true in (2), then according to classical logic the whole conditional is vacuously true, the inference scheme remains valid but its particular application in (2) is not sound. The interesting case occurs when A is true (or at least you think so), which then allows you to conclude that B is false if you accept the argument in the first place.

But this cannot be how (3) works, because the subjunctive conditional already presupposes (or conventionally or pragmatically implicates, in other theories) that A is actually false. If that would suffice to make the whole subjunctive conditional true, all subjunctive conditionals would be true by virtue of their own presupposition (viz., their conventional or pragmatic implicature). This can't be quite right. If they make sense at all, we'd like some subjunctive conditionals to be true, and others to be false given that their premises are actually false, and, of course, this should be so for a reason. The idea is that we sort of imagine what the world would be like if A were true, and then check on the basis of this 'knowledge' whether B or not B would hold in that world. And that is where the mess starts, for there is no universal agreement of how to spell this out in formal terms and there are many competing explanations for the meaning of conditionals like (3). These types of sentences and their mood are also expressed very differently in different languages, there is interplay with the tenses and a lot of cross-linguistic variation.



To give you an idea of how different a semantics for (3) might be in comparison to an ordinary conditional, here is an example inspired by Lewis's conditional logic, though probably not identical to it. Order all possible worlds (=models of the logic) by some reflexive and transitive relation of closeness centred around the actual world. If w0 is the actual world, and w1, w2, w3 are other worlds, and e.g. w0 < w1 ~ w2 < w3, this means that w1 and w2 are equally close to the actual world and closer to it than w3. The counterfactual (3) then roughly means:

In all worlds in which A is true that are closest to the actual world (in which A is false), B is false.

If that is the case, the counterfactual is true, otherwise it is false.

People tried to spell out this closeness in terms of minimal change, e.g. by counting the number of changes to the assignments of truth and falsity to propositional variables you need to get from the actual world (viz. "the right model", current state of the universe) to the respective other world (viz. model, possible state of the universe). But this approach is highly disputed, because sometimes a very small change at a time t can have incredibly huge effects at a time later than t.



A final caveat: In all of the above, I have silently presumed that the correct semantics for (2) is the ordinary standard conditional, which is only false when the antecedent is true and the subsequent is false. However, this is only a highly idealized and rough approximation to the meaning of English if-then clauses.
Dying is the most proximal efficient cause of death. In one sense it is a process, and someone can be dying, but get rescued before arriving at death.  So in some sense, there is a logical point here.

However, we do not tend to consider efficient cause a cause very often in modern thought, because we have a largely Alchemical/Newtonian view that conflates material and efficient cause -- the way things are is an immediate consequence of the way they just were in the preceding instant, and natural laws.

From so strong a notion of physics, someone is dead when they are dead, and to declare them to be dying before then is misreading their current state.  You can never rescue someone who is dying.

Intuitively that last sentence seems wrong.  So I am in favor of Aristotle's four causes (final, formal, efficient, and material) over Alchemy's three (correspondingly: mutable, cardinal, fixed (and even more fixed)).

I would assert that for philologic reasons, you are unlikely to care about this point if you are a modern English speaker.  We have sets of triads of modal verbs (will, shall, and can; would, should and could; might, may and must) borrowed from the Germanic family, that map nicely onto the three causes (mutable, cardinal, and fixed).  So we have trouble thinking of the fourth one, (the non-modal indicative?) as being separate from the third, and our traditional view of physics makes it redundant anyway.

(It may not be coincidental that Newton was an Alchemist, or that both he and Maxwell were English.)
One claims someone is using a slippery slope argument to prompt them to prove the actual likelihood of progression.

It does not challenge the argument per se but insists they detach predictions of the future from assertions about the past or present and analyze the prediction itself.

It is basically the most time-worn version of "correlation does not imply causation", where in order to acknowledge an explanation has merit, you must have a trend and a plausible mechanism, which establishes the possiblity of causation and its direction.
Capital punishment has seldom been seen as absolution.  When the Church burned Witches their choice not to be absolved was seen as part of their punishment.  But from a more objective position, as you point out, it is not necessarily punishment at all.

Still, it goes beyond that.  People commit suicide, and we try to stop them.  So this notion that emotional suffering is bad, and determines the value of life does not really hold water.  The notion that being defined by an emotion is part of your penance is both too Catholic and not Catholic enough.  It presumes those who commit crimes are more normal morally than we find to be the case, and it assumes our empathy applies or matters to them.

As Roy Royston has pointed out, it can be seen as vengeance.  But it is more often simply a matter of avoidance.  We want the bad man gone, and sometimes being held in a box is not gone enough.

Manson gets to torture his victims' families one last time by making the news with his marriage.  They thought he was gone, but, no, there is more suffering to be had.  He doesn't care, he is God, remember?  He is just dying as fast as the law will let him so he can be omnipotent again.

It is surely not guilt that would define Tsarnaev, he considered what he did activism, and his willingness to suffer for it makes him a great man in his own mind.  If he dwelt on this issue, he might suffer most from failure, not guilt.  So if you let him out fifty years from now, he might be just that much more likely to do it again.

This is one of the reasons remorse is usually one of the criteria for judgement.  The remorseful man will suffer more by living, and that suffering may make for actual absolution.  The remorseless man simply remains a pointless risk, and can feed on his pride indefinitely to defend himself from his guilt.  

If you know you have the right guy, and he is clearly not going to care, what is the purpose of just making him uncomfortable indefnitely?
This is the fallacy of converse accident.

We have a set C of people to whom we can give charity. The set B of young healthy beggars is a subset of C: B ⊂ C. Now, the argument goes as follows:


∀ b:B, givingIsWrong(b)      — for all b in B, giving to b is wrong
B ⊂ C                                  — all B's are C's
∴ ∀ c:C, givingIsWrong(c)   — therefore, for all c in C, giving to c is wrong


In other words: Because some property (giving is wrong) holds for all elements of a subset (B), it must hold for all elements of the superset (C). This is a case of http://en.wikipedia.org/wiki/Faulty_generalization faulty generalisation:


  A faulty generalization is a conclusion about all or many instances of a phenomenon that has been reached on the basis of just one or just a few instances of that phenomenon. It is an example of jumping to conclusions.


The fallacy of composition occurs in a context of "whole" and "parts of the whole". In this case, "young healthy beggars", i.e. B, is a part of "people to whom we can give charity", C. In this case, the argument would go like this:


givingIsWrong(B)     — giving to the set B is wrong
B ⊂ C                       — all B's are C's
∴ givingIsWrong(C)  — therefore, giving to the set C is wrong


The argument would have been committing the fallacy of composition if it would have discussed giving charity to the whole group, i.e., if we were discussing giving charity to all young healthy beggars together. This is not the case: we're talking about giving to young healthy beggars as individuals. 


  For example: "This fragment of metal cannot be fractured with a hammer, therefore the machine of which it is a part cannot be fractured with a hammer."


Don't worry, it is common to confuse the two.


  [The fallacy of composition] is often confused with the fallacy of hasty generalization, in which an unwarranted inference is made from a statement about a sample to a statement about the population from which it is drawn.


(Last two quotes http://en.wikipedia.org/wiki/Fallacy_of_composition from Wikipedia)
Freud's http://www.panarchy.org/freud/war.1915.html "Thoughts for the Times on War and Death" might be a relevant read - it's fairly short and deals with contemporary political struggles. Freud saw psychoanalysis as a way to improve life for individuals, and applied its methods on a societal scale as well, for example in Civilization and Its Discontents. His view of the relationship between politics and desire might be somewhat like the Marquis De Sade's take on the French revolution - how can we become free from oppression if we are imprisoned in ourselves?

As for Derrida, all of his work has a political and ethical element, and his life also involved engagement with more immediate political struggles. As for the question of whether he wants to make society a better place, I would say sure, who doesn't? Even Hitler thought he was going to make society a better place. And as we know, many social interventions inspired by Marxism have not been successful, so understanding the relationship between a thinker and politics requires more than attributing good intentions to him or her.

The best source for Derrida's political views (that is, his immediate engagement with contemporary political struggles) is Negotiations. He wrote against apartheid and in support of Nelson Mandela, he wrote on behalf of Mumia Abu-Jamal, he organized teachers and others in Paris to oppose the elimination of philosophical education at (their equivalent of) the high school level, taught seminars favoring the abolition of the death penalty, was imprisoned in Czechloslovakia after visiting dissident intellectuals there, and had many other political engagements and views expressed throughout his writings and interviews.

Just as important are the political implications of his thought. Given your interest in Marx, Specters of Marx would probably be the best place to start regarding his engagement with political theory. As Derrida said from the beginning of his work, there can be no transcendental signified or master discourse which encompasses all others and gives them stability. Marxism attempts to treat economics as such a transcendental signified, claiming that all history is in essence the history of class struggle, and that all alienation will be eliminated once economic alienation is eliminated. The subsequent history of Marxism belies this claim. For example, in the seventies, groups such as feminists pointed out that their interests and the social changes necessary to eliminate their unique forms of social oppression and alienation were being opposed by the Marxists who said such issues would either take care of themselves after communism was in place or that they were of secondary importance. This is only one example of a form of difference other than economic difference (although intertwined with it - gender is also not absolute or a transcendental signified) which can produce what Marx called alienation. 

Derrida's intervention in Marxism is to attempt to open the space for these different forms of alienation to speak. He does not think that challenging economic structures is unimportant - but rather that it is not the only form of oppression or alienation we face, and that only relative, not absolute, improvements are possible. Specters of Marx also elaborates a list of projects which he feels would further the goals of Marxism within our vastly changed economic and political world.
oh my, that's a good one. Firstly i should mention that i wanted this to be a comment, but anticipate it being longer than a comment can handle.

ok, well, if i am understanding what you are getting at, which i am only partially doing. Not, one hundred percent sure what the platonic solids are. (unless thats the stuff sagan was trying to tell me about euclid... what with the triangles and dodecahedrons n'such?)

I am not sure that they are mutually exclusive. As i think it is a question of context. Think of the metric system vs... the uhhh... not metric system. (forgive my foolish public education) So you have two systems that use the same numeric system in differing and non equal ways. (non equal, like its not a perfect substitution. Like ten "units" from a metric system wont match up with ten of any of the other systems units.)

I think of it in terms of minecraft. Growing up in america i tended to see the world in a decimal sort of system. Which is to say, in orders of tens. Well now after hours of minecrafting, i can now break things down in its terms. where 8 becomes the new 10, as it were. The "stack" becoming the new "100" if ya dig.

In that way of thinking, i haven't changed so much what the numbers mean, so '2','+', and all that still apply, but in a modified sort of way. Firstly, everything was a 10 or was building to a 10, so that i could get it to 100, and so on. But then, the new master unit is 8, with the goal being 64, and then 128, and so on.

Mayhaps, some distant future will reveal some "groundbreaking" new understanding about math and the way we perceive it, but the part about geometry... Everything i understand about math as a "language" or a framework for understanding, quantifying, and measuring things. As taught to me by the movie "mean girls" (gah, i know right... ::shudders::) Was basically that it is "universal." A triangle is always a triangle as there are "three" "angles". The vocabulary may be different. or perhaps the contextual understanding of it. (metric vs m'rican) May differ from our present view of it. But that geometric principle remains.

All of this talk of perception and math gets me thinking of the novella "Flatland." http://en.wikipedia.org/wiki/Flatland wiki article It's about a 2 dimensional world inhabited by different polygonal forms. Well Square one day meets sphere, and since square had existed entirely in a world of only 2 dimensions. He could not see the sphere for what it is. It originally appears to square as a circle sort of dot thing. that grew to be a larger circle then shrunk back down to the size it started. It was not until square had visited "spaceland" where sphere was from, a land with 3 dimensions. That he could "accurately" perceive the spherical form.

Just because we squares cannot see the spherical forms... doesn't entirely mean that they are not out there. But even in that sense. It would seem to be that it would only be expanding on the mathematics, as we know them. So perhaps some(astrological)day out in the distant future. The most complex and "freaky deaky" maths we know of. Will just be covered in a small section of math textbook nueral data interface whatevers... under the heading "All the ape descendents figured out about math." 
The concept "right" implies choice, so the right to free speech is not an obligation to speak, the right to own property is not an obligation to own property. The right to live, thus, is the right to live if you so chose, from which it follows that you also have the right to chose not to live. You don't have a generic right to die only in the manner and time that you chose (which would imply that you have a right to whatever is necessary in order to keep you alive until the time that you've selected for your demise), but, if you chose to end your life at some time in some manner, and you can stay alive until that time and can implement your death, they you have the right to act on that choice. (Modulo respect for other people's rights, i.e. you don't have a right to die in a nuclear holocaust that wipes out NYC).

However, in a State Boon theory of rights, where rights are all and only the actions deemed by the sovereign to be "rightful", then the only way to answer the question is see whether a right to die has been granted.
Yes, this is a form of the naturalistic fallacy; no, you're not completely off. From http://en.wikipedia.org/wiki/Naturalistic_fallacy wikipedia:


  The naturalistic fallacy is the idea that what is found in nature is good.


Your examples do claim that something (eating meat; eating animals) is good because it's found in nature (history of the human race; other animals).
This sort of thought experiment boils down to an empirical matter: whether all choices made by people are motivated by the same reward system that delivers feelings of happiness and/or regret, and whether there are any rational actions aside from going along (on some time scale) with whatever this reward system decides is right.

Looking at it this way, there are two obvious ways that an X could act as you've described.

(1) X is not motivated entirely by that reward system.  Thus, depending on the weighting between that system and whatever other motivational state he has, it could be "rational" to go along with the other motivation.

Note that this is not entirely implausible given the relative commonality of extraordinary actions like self-sacrifice for offspring, both among humans and other animals.

(2) X has reason to believe (maybe he's read Hume) that just because said reward system is pulling his strings, it doesn't follow that he should blindly go along with it.  Instead, he reasons that for whatever reason his emotional state is not aligned with what is rational, and somehow manages to opt for the latter.  (You may need touches of (1) for this to even be possible.)

This is also not entirely implausible: one might understand that murder is wrong and even when losing one's sanity and feeling desires to murder (with no sense that you'd feel remorse) nonetheless reason that it is rational to resist these urges.
This question is essentially duplicated from English SE https://english.stackexchange.com/questions/252446/does-this-essay-question-make-sense Does this essay question make sense?, where the phrasing "The process of changing into new social contexts requires compromise between sacrifice and opportunity" provides additional context. Reference to social contexts implies that the "experience of transition" refers to change in social behavior of a person.

There is a difference between making sense and being true, "2 is an odd number" makes perfect sense, even if it is false. "Negotiation between sacrifice and opportunity" may or may not be necessary for engaging in social transition, but asserting that it is is a sensible statement. What is off about the OP sentence is the use of a passive noun "experience" (or "process") instead of an active one like "engagement", whereas "negotiation" implies conscious or at least willful participation. Presumably, one could experience a transition just happening to them without any active engagement, negotiated or otherwise. But even such interpretation would make the sentence false, not non-sensical. Of course, "negotiation between sacrifice and opportunity", read literally, involves communication between two abstract concepts, which is non-sensical, but such metaphorical frivolities are commonly used, e.g. in "struggles between passions and reason".

Summarizing, https://en.wikipedia.org/wiki/Principle_of_charity the principle of charity directs us to construe a text in a way most favorable to its being sensible and true, before doubting and criticizing. We could then assume that in the author's view even passively stated "experience of transition" requires person's action to occur, e.g. sacrificing old habits to new social opportunities, and imagine a psychological theory under which such action involves either conscious or subconscious "negotiation" of trade-offs between the two.
These are all important topics but I'm not sure that they're hanging together in a coherent manner; not am I sure that this, as a question, belongs to logic though of course it involves the use of reason: philosophically logic and reason are two different things; it appears to be a question that belongs to rhetoric (classically speaking) and by the nature of its subject also to political philosophy.

Warfare, at least classically, takes between state-actors; there are concepts such as asymmetrical warfare which accounts for a state against non-state actors which would include guerrilla warfare and policing; and in a sense, the war on terror is more akin to policing than it is to warfare. 

Civil warfare generally describes the implosion of a polity through actual sustained violence (and not occasional acts), which is different from the feuding of various factions in a balance of power; such as the civil war that tore Rwanda apart, or the American Civil War.

Polities have institutions - disciplinary institutions in Foucauldian terms; this is not the same as warfare in general terms.

The word 'Terror' like the word 'Free' means many different things; and you're comparing, I think two unlike, but associated meanings of the same word; which is a subtle tactic of rhetoric (under which comes things like advertising, public relations and propaganda); the question is whether it is being adopted towards a political end that can be justified. 

However, when one considers the following:

For example, https://en.m.wikipedia.org/wiki/Statistics_of_incarcerated_African-American_males incarceration rates in America for the black demographic make:


  40% of the total prison population...and exceed the average in twenty states.


But https://en.m.wikipedia.org/wiki/United_States_incarceration_rate#US_racial_demographics also


  Whilst the States houses 4.4% of the worlds population, it houses 22% of its prisoners.


This whilst not classical civil warfare is certainly not social cohesion; and I find disturbing.

One might ask what is the proper concept to describe this.
Take looking at a house.

One could think that house is beautiful and desire to have it, but this is not the beauty Kant is talking about.

Beauty in the Kantian sense is when someone observes the house and just stands there contemplating its beauty.

Perhaps you can imagine this beauty occurring in an exposition, in looking at nature, looking at your spouse (which sometimes but certainly not necessarily induces desire), etc.
I would say yes, as they are either universally beneficial or universally harmful or both. For example the ability to split the atom was universally beneficial as it lead to many advances in all aspects of life from energy to medicine.

Also it could be considered universally harmful as the improper containment of the by-products / use of atomic material has impacted the globe!
Confucianism and Daoism are a classic pair of opposites in Chinese philosophy. They are also both terms that are notoriously different to pin down.

The Problem of Definitions

I know of at least three definitions of Confucianism: (a) the works attributed to Confucius and Mencius, (b) the works in (a) plus 24 centuries or so of commentaries on them, and (c) the political implementation of something vaguely related to (a) or (b) often in China but also in Joseon Korea and Tokugawa Japan. Also, there are different streams of interpretation in (b) with the most prominent being the neo-Confucians, such as Zhu Xi and the Chang brothers, and the New Confucians in the 20th Century including such thinkers as Kwongloi Shun, Chengyang Li, and Tu Weiming. (For a treatment of just the different forms this takes in China, see Xinzhong YAO, An introduction to Confucianism "Confucianism, Confucius, and Confucian Classics" Cambridge University Press, 2000).

For Daoism, there are also multiple definitions which I'm less competent to comment on, but we can see Zhuangzi and Laozi as two separate traditions and then we can also look at Sun Tzu's Art of War as a Taoist text (at least Roger Ames does). We can ask what its relationship is to itself and what its relationship is to Buddhism (some forms of Taoism copied Buddhist practices). We can also see it as either the philosophy of protest against the "Confucian" state or an esoteric religion about finding immortality potions. Philosophical Taoism has been having a recent resurgence with prominent defenders.

The History of Dissimilarity

As rival views, the two are often presented as opposites when teaching Chinese philosophy. But this is a useful teaching device rather than proof that we should see the two as truly opposite. There are surely differences.

The Claim of Similarity

Many of the prominent defenders of Taoism in contemporary philosophy see Confucianism as having much in common with Taoism. I have heard Karyn Lai say things to that effect as well as Roger Ames.

Why do they make this claim?

First, Confucianism and Taoism as philosophical positions both are about Dao (道). Second, even though they are rival views about what Dao is, they share some of the same ideas as they are rivals in context. Comparing Descartes and Confucius is harder than comparing Confucius and Lao Tzu. Most of the other similarities build on the second one. But a third similarity is that as the Confucian commentaries advanced, they had to adapt to beliefs that their target audience found plausible. Thus, Zhu Xi spends a lot of time talking about li (not 禮 but 理) which means order because they had to develop a cosmology to respond to the Buddhist missionaries.

Fourth, a key reason why the West likes Confucianism but not Taoism has to do with a controversial set of translations. The Jesuit translations rendered many of the ideas of Confucianism into familiar Western terms:


"Rightenousness" for 義 yi / now sometimes rendered "appropriateness"
"Heaven" for 天 tian / now sometimes left untranslated
"Virtue" for 徳 de / now sometimes rendered "power"
"Benevolence" for ren 仁 / now sometimes rendered "humanity"


(I could expand the list if necessary). But a recent challenge is how accurate these renderings really are to the Chinese context. This is an open area of debate, but part of why it matters is that the Jesuits saw the potential for a synthesis between Christianity and Confucianism but not Christianity and Taoism.

Moreover, the definition of all of these terms is something where Taoists make interesting claims. For instance, they deny that ren should be understood in pedestrian terms. Similarly, they see de as referring to the power to influence. Interestingly, you can reread Confucian texts with these definitions and they still make sense.



To give a parallel, Descartes and Locke take opposite views in epistemology, but it does not mean they have nothing in common. Both write as Christians. Both try to solve problems of perception. Both reject certain features of the classic medieval philosophies while implicitly accepting others.
What do Montaigne, Paine, and Wittgenstein have in common?
About Paine, I do not know.
On LW and MM, the connection is obvious: both recognize that no general system can encompass all of philosophy. But they address the point from different stands: MM goes easy: his essays show that we can not grasp any law about anything (especially things ethical). LW goes the hard way: he states the same but, as he can not find an answer, he suffers.
But the basic aim remains: how to live a good life?
The answer lies not in words, not in discourse, but in acting, in seeing trough.
There is an inherent back pressure that comes from a doctrine like this.  "That which is necessary is legal" presses back on the definition of "necessary."  If there is a strong desire for an act to be illegal, there will be a corresponding strong desire for that act to be deemed "not necessary."

If you apply this logic to "That which is necessary is moral," the backpressure gets extraordinarily strong.  Questions of "is it necessary to remain alive?" arise.  Consider the Samurai: in many situations the only moral course of action they had available was to commit ritualistic suicide.  They did not have the luxury of assuming that living was a necessity.

However, it does lead in an interesting direction to explore.  This quote feels very similar to the attitude that there is always a moral path to follow, no matter how far one has strayed.  Doctrines can be slippery at times.
Act-utilitarianism has a lot of problems with mapping well onto our innate moral sense.  This is hardly the only example.  There are oodles of other examples, such as when it is okay to eat all the cake at a birthday party.  (Hint: it is not okay to eat it all yourself even if, in the absence of social responses, it would increase your happiness more than everyone else's combined.)  With many of these, you end up tempted to then start lying and hiding your behavior as then "nobody is hurt", except the reaction to discovery of lying and hiding is even worse, and you end up with a cascading mess.

Unfortunately, this mismatch is so bad that the act-utilitarian would probably end up deciding that nobody should strictly follow act-utilitarianism, even themselves.

So I think the meta-answer (neglecting concerns about disease, desire to kill or at least not prevent the deaths of sexually desirable people, etc.) is that to some extent you simply must acknowledge that people have a powerful innate moral sense and you have to work with it.

That is, it is immoral (as a utilitarian) to do things that disgust and horrify people, even if that reaction is moral in character, and even if they only know in the abstract that such things happen, not about individual instances.  At least given present societal norms, Necrophilia would be in that category.

Note that this is also true, as a utilitarian, for e.g. gay marriage or sex out of wedlock.  You might envision a society where it is accepted, but your calculation could well be that it's immoral solely by virtue of (some) people's distaste for it, or that you must fix the distaste first.
This is the basic goal of the project of Greek philosophy that arises from Cynicism and Stoicism -- to be either so deeply authentic personally, or so in tune with Nature that nothing really disrupts ones equanimity.

It got farthest in the Late Academics and Pyrrhic Skepticism.

These two combine in Sextus Empiricus, who elaborated Pyrrhonism in a context cultivated by Academicism, and pretty much preaches a Western analog of Buddhist detachment (which in other ways, given its Greek roots falls far from Buddhism itself -- for instance something like civic peace, a group analog of individual peace and balance, seems to be valued over compassion, although a moderate obligation toward compassion is a corollary).

Sextus' approach to not being bothered was basically to 'bracket' all decisions until they can be authentically accepted.

One must act, but one need not to be committed to the action simply because one has taken it.  It is impossible to search its causes for any essential truth, only to tally its probability.  And even that fails to a large degree.  Nature being inconstant, there is no demand for consistency to be achieved by a person, much less his actions.  Beyond that, we can, and therefore should, bracket all considerations where commitment is unnecessary, indefinitely, not forming any opinion, but simply letting them go.  That way that there are no pointless considerations complicating the choices beyond the likelihood of choosing the right action based on experience.

Personally, this is a mode I can pursue, and have come close to it at certain points, for years at a time.  It becomes a kind of endogenous depression without negative emotional content.  One even physically slows down, without that slowness affecting one's effectiveness or ability to act in crisis.  It is very strange.  And it contradicts a lot of our cultural assumptions about emotion, comfort and effectiveness.

But I have given it up.  I feel that we as humans have an obligation to be bothered, and to live with a certain level of tension between our more accepting nature and our drive to think.
Arguing such cases is not trivial.  Both sides are often vehemently opposed to the other side of the logic.  I don't think I can get away with holding such a debate in this format (SE is not designed for it).

However, I would like to draw an analogy.  The analogy is from electrical engineering and amplifiers.  I leave it to you to decide whether such an analogy is well drawn from your question, but I find it has several ways in which it could be connected, so I find it useful to offer.

Amplifiers are usually built with transistors.  Here is a graph of the behavior of a transistor (technically a FET, but this is analogy so that detail will not matter).  I will ignore many details, but the details which will matter are:


Vds is the input to the system
Id is the output from the system
There is an "Ohmic region" which is rather linear looking
There is a "Saturation Region" which is mostly flat
There is a "Breakdown Region" which is very abrupt.


https://i.stack.imgur.com/0WCkH.png 

Now, if I want to amplify a signal using a transistor, I will build an amplifier which provides inputs within the Ohmic Region.  The audio amplifier in your phone or stereo does this.  The goal of such an amplifier is to maintain the characteristic of the sound by simply making it "bigger."  Now, let us say there is a "lesser" input (which would correspond to the pornography of your original question) which fits nicely within this region.  We actually have well understood rules for dealing with the consequences of this.  By the analogy, the effect on both the viewer of pornography and the associated degradation of women fit within a region which can be simply thought of as "amplification."  This tends to be thought of as a grey area in ethics: where something is considered bad, but can be balanced against other effects to determine if it is worth pushing back against, or if there are bigger fish to fry.

However, a larger signal can cross a line.  A "greater" input (corresponding to prostitution) may leave this region and enter the saturation region.  Any signal which enters this region behaves using different rules.  Because the curve is so flat in that region, it is hard to tell the difference between 5V (perhaps a minor sex-for-cash enterprise) and 15V (a major heroin-abusing prostitution cartel) simply by observing the output.  In electrical engineering, this change is from amplification to "switching."  In a more social analogy, it means the system must treat this as a switch - you are either on the wrong side, or the right side.  The grey areas which showed up in the Ohmic region is made invisible by the logic of this saturation region.  This is where we see arguments like "you are with us, or you are against us."

Push further, and we reach breakdown points.  At this point, all useful models start to fall apart.  This is the point where you see people "putting their foot down," and allowing all sorts of atrocities in order to combat something.  Most societies avoid this with prejudice, but we can see this in comic books, when a hero is told to "do whatever it takes" and goes very far down "the dark path" before arising victorious.

All of this is just a model.  It does not have to be a valid challenge to your position.  However, it is based on a very simple model:


At low "stress," the system has grey areas as it can create proportional control loops to keep things in check.
At mid level "stress," the system has to rely on clear right/wrong true/false boundaries as the saturation effects wipe out the grey areas.
At high level "stress," the system breaks down completely, allowing virtually any response to occur.


If you can consider such a model valid for the topic of this discussion, then there two very clear points on the model where it is natural that a mere increase in signal may cause the system's handling of the response to change dramatically.  The open question for debate would be where those points are (and whether there are additional points to consider).  However, from a historical perspective, our society has decided on where the line most likely is, and it is between pornography and prostitution.  Perhaps it is not the best place to put the line, but it is the best decision society has been able to make to date.
In http://www.unc.edu/~ujanel/DesireBackground.pdf INTENTIONALITY AND PROPOSITIONAL ATTITUDES Lycan gives a nice summary of Fodor's representational theory and Dennett's objection, with rebuttals and follow-ups. 

"What the representational says is only this: Propositional attitudes are like sentences in that (i) they have conceptual parts, (ii) they have semantical properties such as truth values and entailments, (iii) they have a grammar or syntax by which their conceptual parts are compounded into whole propositional contents, and (iv) they are physically realized in the brain...  

Objections

(Dennett): Tacit propositional attitudes (the belief that New York is not 
on the moon, the desire that one not be beaten to death by angry insurance 
adjusters from northern Tibet, the hope that one will be alive 30 seconds from 
now). 

Reply: The Representational theory applies only to “occurrent” states, not to tacit attitudes.  

Rejoinder:  But what about the tacit attitudes?  You said the Representational theory is a theory of the propositional attitudes, but now you’re saying it’s a theory of only a few of them. 

Standard move:  Tacit attitudes are only dispositions to be in the corresponding occurrent states (...you tacitly believe that I am less than 35 feet tall because you have judged that I am only about so tall, which entails that I am less than 35 feet  tall)...

Suggestion:  We may not need to solve that problem.  How about letting the tacit beliefs be those logical consequences of “occurrent beliefs” or judgements, that are not themselves occurrent? ...The tacit beliefs are implicit in judgements, by being logically contained in them.    

That’s as good an idea as I know. But it faces two difficulties. 

First difficulty: Intuitively, we don’t want to count every proposition that’s entailed by one of my judgements as a tacit belief of mine. To take the most extreme case, every logical tautology is entailed by every judgement I make, but it seems wrong to say that I even tacitly believe that blah-blah, where blah-blah abbreviates a gigantic tautology... 

Second difficulty: It would not always be easy to identify the particular judgement of which a given tacit belief is supposed to be a logical consequence..."
I have seen a valid argument for "belonging," but it was explicitly opposed to the concept of being a thing which was actually "achievable."

The pattern worked like this.  If we start from the presumption that we are imperfect, and that we must interpret any ethics using our imperfect self, then it is potentially possible to misinterpret any perfect ethical rule.  Many systems claim the result of this misinterpretation to be suffering.  However, if we work as a group, we can be collectively more sensitive to the minute details around us, and are more likely to collectively achieve an interpretation of the ethics which is better than an individual can.

If one finds a group which seeks such goals, then "belonging" has ethical merit.  If one belongs, by your words, the group understands you.  That means that there is a group (which you presumably trust) which can turn that sensitivity onto you, and help you identify that within yourself which you "should" change.

I have also found there to be a biological defense of such an argument: the way our cells grow and are taught how to behave follows a similar pattern.  The whole of the body knows more about the world than, say, an individual muscle fiber, and can work with that fiber to teach it the exact best way to contract and relax to accomplish goals that are much too subtle for a mere muscle fiber to have fully comprehended on its own.
Sorry to rant, but I am going to.  

What is wrong here is not in the logic of the statements you question, it is in the weird attachment a modernity shaped by Christianity has made to feminine morality in the name of a patriarchal God.

God is not against killing.  Even in the most rigid version of Evangelical Christianity, there is no logic to be had in this notion.  Jesus condemns wealth a dozen times as often as violence, and never mentions killing.

The early Church thought the world was ending soon, and we might all have fairly few years of life left anyway.  They were much more concerned about being kind to people while they were alive.

The Old Testament is even clearer on this issue -- the first few murderers in the Bible go free, while the first rape mentioned is avenged with the deaths of an entire village.

Killing for stupid reasons is out, but the word we translate as 'kill' in the ten commandments does not apply to war, or punishment, or appropriate vengeance, and may actually not cover most of the reasons people kill.  Otherwise the recounted killings to avenge rape, or punish adultery, or take Jericho would not be demanded and praised in the same text.

OK, enough ranting at people who cannot read their own books.

Our morality in the meantime has slid into what Nietzsche calls 'slave' morals, where the preservation of life is always paramount, discarding a whole host of traditional male values in the process.

Morality is obviously about more than preserving life, it is about allowing people to actually live.  If that means that sometimes they only live for a shorter period, because they choose to stand up for the freedoms of people in another country; then that can constitute "really living", more than staying home and fretting about it.

We like to give people choices in modern societies, but one of those choices can be whether or not to become part of a military force and risk having to balance the value of individual lives against all the rest of the values of one's own and one's society.
This doesn't really answer your question in the terms given; but it might throw some light on it.

Arendt wrote in her short book, The political theory of Kant:


  How serious Kant was about the enlargenment of his own mentality is indicated by the fact that he introduced and taught a course on physical geography at the university.


then she goes on to say:


  he was an eager reader of all kinds of travel reports; and he - who never left konigsberg - knew his way around both London and Italy; he said he had no time to travel precisely because he wanted to know so much about every single country.

Despite your question is a bit ambiguous, to me it seems to imply that you mean by "war" an armed conflict between sovereign countries. Then of course the person who declares war, does it only in its role as a e.g. leader of that country. Therefor not the person qua person declares war, but the the country declares war through the voice of its leader.
The B series:


  From a second point of view, one can order events according to a different series of temporal positions by way of two-term relations which are asymmetric, irreflexive and transitive: "comes before" (or precedes) and "comes after" (or follows)
  [https://en.wikipedia.org/wiki/A-series_and_B-series#McTaggart.27s_use_of_the_A-series_and_B-series 1]


So, the B series is more concerned with placing events into a causal chain of events, not based on pastness, presentness, or futureness. whereas the A series assigns these qualities to events based on the 'present' as a point reference. the difference between the two is similar in many respect to the difference between a subjective and objective view.

In terms of your question, the B series is tenseless. so the statement, 'It's 1AM here' is not a B series statement, since it contains (implicitly) a reference to 'now'. I think its perfectly possible to construct a B series of someone, provided that it is objective, so instead of 'Bill is here now' you'd say 'on September 9th at 10PM bill was here' (obviously it would be much better if you could say it in some kind language that didn't rely on tenses to be coherent.)

not sure that theres a specific word for that kind of series though... 

For the second question, 

I believe Blanchot is referring to the idea that death and consciousness cannot coincide. Thus, Death cannot happen 'to' us, we cannot experience it https://en.wikipedia.org/wiki/Maurice_Blanchot#Themes [2], because experience is, by its nature, life. 

Similarly, for death to 'arrive' it must present itself to our awareness (we must become conscious of it), but this is impossible. Imagine your mind is a pitch black room with a torch suspended from a string in the centre. The beam of the torch is your awareness and the dark parts are your unconscious. The torch is free to point wherever it likes,but no matter which way it points, from its perspective the entire room is brightly lit. So we cannot become conscious of what we are unconscious of, and death is by its nature a severe kind of unconsciousness. Just as the blackness of the room cannot become present to the beam of the torch, so death cannot become present to your awareness.

Hence, the arrival of death which never arrives and never happens to me.
This is another question I have never seen considered by a philosopher, but it is important in psychoanalysis.

Death may not meaningfully deprive the dead person of anything, but it clearly deprives those most closely involved with that person of whatever they are getting out of those relationships.  So considering death is averse to the person being killed because that person holds obligation personally, including the ambiguous obligation that simply being in relationship entails.

Existential forms of psychoanalysis that focus on identity and motivation therefore always impute loss to death, because one affect how the requirements of your identity play out or pursue your motivations after that point.  To the point that your social identity is you, it is threatened by death, and, therefore, you are.  Your bodily instincts know that, even if your conscious mind has decided otherwise.

From that point of view, killing is an assault not so much on the dying person in his current state, but upon his past ability to plan his life (the ways in which that has been rendered flawed, incomplete or damaging when it could have been more rational) and upon those who might have been affected by those plans.

This includes (if you believe in them) the transpersonal and transferential motives that might have been advanced by his existence.  (For Kleinians, e.g. the loss of a parent removes a symbolic real-object link, and any attachments to the images of that parent then must be resolved by less efficient means.)

Only a person free from the conflation of self and identity, outside the constraints of real obligation, and neutral toward all transpersonal goals, can, then choose death without loss.  Those are relatively few.  And even toward them, forcing the issue, by threatening death, or otherwise causing it to be considered as an option, is always destructive, even when the actual killing might not be.  The very detachment and equanimity that would make them able to accept death is threatened by instinctive reactions over which no human truly has control.

Besides that, the transpersonal and transferential goals are still not those of the person making the decision, so they cannot be ruled out by his choice.

Thus, from a broad enough psychodynamic perspective, death is a comparative harm both to the individual and to others, in imaginal and symbolic terms, even though there is not necessarily anything in the real world that is being taken away from the deceased.
Referring to http://plato.stanford.edu/entries/aristotle-logic/ Aristotelian Syllogism, a "typical" case of fallacy is the https://en.wikipedia.org/wiki/Fallacy_of_the_undistributed_middle Fallacy of the undistributed middle.

The argument :


  All z is B
  
  All y is B
  
  
    Therefore, all y is z
  


is not valid.

In your case you have an individual term in place of a general one, but the fallacy is basically the same; the argument :


  All Chinese are men
  
  Socrates is a man
  
  
    Therefore, Socrates is a Chinese
  


is not valid.
You don't think robbing a bank is ethical? What you think bankers do?

Yours is probably posed as a narrower question of legal ethics. But from a broader perspective it may be considered as a philosophical question of the ethics of civil disobediance. How to act if you are caught between two incommensurable ethical commands, usually between "state" and some "higher" authority. 

According to Hegel, this is the very crux of dramatic tragedy and a dialectical engine of history. He cites the case of Antigone, who is literally destroyed by the historical shift from ancient family duty to the laws of the state.Thoreau's or Mandela's "civil disobedience" are other example, though our modern fates are usually somewhat softer than Antigone's. 

So, when could one ethically defy a law or violate a legal contract? When one can and must act rationally in the expectation of a "more just" future, however determined.

I would argue that the entire financialization and credit-debt structure of the U.S. is overtly unethical, concentrating wealth based ultimately on bonded taxation in the hands of small rentier class. It is "unethical" not only in being inequitable and unsustainable, but in the Hegelian sense of forcing the mass violation of other, incommensurable social duties.

The prime example is student debt. Capital value demands ever-increasing levels of education, which becomes an implicit duty of the young citizen. Yet the costs are transferred to other countries (hiring educated workers abroad) or into mass indebtedness of an entire generation. An indebtedness that increasingly outpaces the capacity of future earnings to liquidate it. Hence ethically and socially "irrational."

By this standard, I actually believe students today may have a "higher obligation" to not repay debt, one that outweighs the personal or even immediate social consequences. The case of housing may not be so immediately apparent, but there is a good historical argument that mortgages should be valued against average earnings in some way, not according to shareholder value or remote derivative transactions that affect the debt levels yet are entirely unaccountable to the social necessity of housing. 

Bit of a stretch from home mortgages to Antigone, but hope this helps you consider the issue afresh.                
The passage in question is Plato: Phaedo 77,e8-9. 

The Greek text speaks about ἐπᾴδειν, which means to exorcize by singing. The Greek sentence has no explicit subject, it says one has to exorcize. The translator has added the voice of the charmer. 
Remember that Aristotle lived two thousand years before Hobbes, and therefore, his criticism of the latter can only be made in retrospect by us. 

Aristotle starts off in the Nicomachean Ethics by defining happiness as "living well and doing well" (Book 1 Chapter 4). He then mentions the different ways in which people define a happy life - life of enjoyment, life of politics, life of contemplation, and life of making money (Book1 Chapter 5). He calls the first a "life fit only for cattle." He criticizes the last because people are forced into it, and because money is only a means to happiness and never the end. About the life of politics where people seek honor, he argues that honor is secondary to virtue. And he reserves the life of contemplation to a later chapter. 

Happiness, or eudaimonia, for Aristotle is a complex state of being of an individual that comes out of a lifetime of cultivation and practice of the virtues at all levels and in all spheres of human activity, and where the individual is recognized, admired, honored, and rewarded for these virtues. This is what fulfills his definition of doing well and living well, that is, a virtuous life and a prosperous life must go hand in hand for a person to be happy. But Aristotle goes one step further - a prosperous life must only be the consequence of the virtuous life. A person seeking prosperity for its own sake is doomed to not attain happiness. 

Hobbes seems to define happiness as being found in the continual success in the acquisition of things. That, for Aristotle, is a form of false happiness. 
I'm assuming this is a follow-up to https://philosophy.stackexchange.com/questions/28636/was-socrates-a-monotheist/28640#28640 this question.  As discussed there, while Socrates was arguably monotheistic, he probably wasn't openly so, as a member of a firmly polytheistic society.  He certainly occasionally used common idioms and stock phrases that referred to plural gods.

Nevertheless, I believe that in that particular passage, it is Cebes, one of the chief interlocutors of the dialog, who makes the reply referencing "the gods."

Of course, in reality it was Plato who wrote both the question and the answer, but he would presumably have sought to create a reasonable facsimile of how the original person would have spoken.
You seem to be confusing a very fundamental distinction. 

And mixing up symbols and whatever they are about. To "explain" something is not to "reproduced" or "expand" it. It is to "reduce" it in some functional way. A way general enough to be accessible to others.

The classic example is a map. The map is useful only because it removes information. A map as large and detailed as the territory is not very useful. Indeed it leaves you right back where you started. 

Your idea that a "fundamental theory" should be more complex than what it explains is, I will say, original and entertaining. And it can often seem that philosophy strives to do just that. 

And yes, there is nothing that proves Occam's razor is "more true" than adding in all sorts of layers. Fairies, martians, spells, invisible rays, your own theory of gravity. You can add in all sorts of layers to "explanations." 

But strange as it sounds at first, "knowledge" is actually a reduction of possibilities. Possibilities are infinite. Knowledge reduces them.    
No.

Biological data is merely data.  It requires an interpretation.  To show A's superiority over B, we must define a metric upon which we can declare A=B, or A>B.  It is trivial to define metrics which show humans to be superior to animals, and it is trivial to define metrics which show humans to not be superior to animals.  The choice of metric is not founded in biology, it's founded in what we believe being superior means.  It should not be surprising to find that humans find "superior" means doing human like things!

As an example of where such metrics get complicated, a human is typically viewed as superior to an ant.  However, it is less clear if humankind is superior to antkind, because suddenly the ants get to leverage their massive numbers to make the metrics look bigger.

If we're not careful, "inanimate" things can score higher than us.  For possibilities of actions, the great ball of fusion in the sky above us is capable of more possible actions than all of us (measured using potential quantum states as a metric for possibilities), thanks to its enormous mass!  It just goes to show how tricky defining such metrics can be.
For his own interest and benefit and security [see https://en.wikipedia.org/wiki/Adam_Smith Adam Smith, from https://en.wikipedia.org/wiki/The_Wealth_of_Nations The Wealth of Nations (1776)] :


  As every individual, therefore, endeavours as much as he can both to employ his capital in the support of domestic industry, and so to direct that industry that its produce may be of the greatest value; every individual necessarily labours to render the annual revenue of the society as great as he can. He generally, indeed, neither intends to promote the public interest, nor knows how much he is promoting it. By preferring the support of domestic to that of foreign industry, he intends only his own security; and by directing that industry in such a manner as its produce may be of the greatest value, he intends only his own gain, and he is in this, as in many other cases, led by an invisible hand to promote an end which was no part of his intention. Nor is it always the worse for the society that it was no part of it. By pursuing his own interest he frequently promotes that of the society more effectually than when he really intends to promote it. I have never known much good done by those who affected to trade for the public good. It is an affectation, indeed, not very common among merchants, and very few words need be employed in dissuading them from it.


See also http://plato.stanford.edu/entries/smith-moral-political/#AdvSmiMor Adam Smith's Moral and Political Philosophy :


  Smith is further from utilitarianism than Hume. [...] He believes that our faculties of moral evaluation are always directed toward the motivations and well-being of particular individuals in particular situations, not to goods that might be possessed jointly by groups of human beings, and he rejects the idea that our assessments or decisions should aim at the greatest happiness for the greatest number of people.

The question is a little bit difficult to follow, but the reference really helps in understanding where this is coming from.

Fideism is a term usually applied to speak negatively of a view though a few have owned up to the term. (Note how the article describes it as an "ascription").

To call someone a fideist is to maintain that they believe faith is opposed to reason rather than somehow compatible with it.

The Pascal quote (which I don't have the broader context within Pascal  off the top of my head) is referring to 1 Corinthians "foolishness to the Greeks" description of the Gospel. The idea is that some claim is inscrutable to reason.

We might reword it as :


X is true such that no argument can be given for X.
Ergo, to ask for an argument for X is to misunderstand the nature of X.


Or alternately:


X is true BECAUSE X is foolishness (i.e., cannot be known through argument). = if foolish, then true
There is an argument that concludes X is true
Then X is false (per modus tollens 1,2 )


presumably you're going to need some qualifiers to make such an argument work (it would be a terrible position to hold that anything that is foolish is true).

We can find somewhat similar arguments in Kant about the nature of things-in-themselves and understanding them (namely, that understanding does not apply to things in themselves but rather things as objects).
I think you are correct, but a little more specifics are valuable, because there's many ways one can "neglect the percentages."  Base rate fallacy can cause people to believe that a medical test is more valuable for determining whether one has a disease than it actually is.

One example of this is heart attack symptoms.  One of the great frustrations for groups trying to help with the detection of heart attacks and saving lives is just how hard it is to identify a definitive symptom.  Most healthy individuals will experience at least one heart attack symptom at least once in their life.  Chest pain presents quite reliably in male heart attacks, but because it occurs in healthy individuals, and there are so many more healthy individuals than individuals suffering a heart attack, chest pains are not actually as good of a sign as one would like.  If you are a hypochondriac suffering from extreme anxiety about heart related issues, you may commit a base rate fallacy by presuming your chest pain is an indication that you are likely having a heart attack, when in fact many chest pains are not heart attacks.

Disclaimer: Seek medical attention if you think you need it!  I am not a doctor!  That being said, if you are a hypochondriac suffering from anxiety due to this base rate fallacy, know that there are other signs of a heart attack which have a lower false-positive rate (such as those which consider the rate of onset of pain) which your doctor may be able to go over with you to form an educated opinion on the matter.

The best example of base rate fallacy I have seen is one regarding TSA and terrorism, and its easy enough to turn into a medical version of the same story.  In this version, you develop a test to detect terrorists.  It has a false positive rate of 0.1% (a non-terrorist is flagged as a terrorist), and a false negative rate of 0% (no terrorist ever fails to be flagged).  If a person is flagged by the system, what's the probability they are a terrorist?  If you commit the base rate fallacy, you will believe it is highly likely that a flagged person is a terrorist, because it catches all terrorists and only a fraction of non-terrorists.  This fallacy occurs because you underestimate just how few terrorists there are compared to normal people.

Roughly 3 million people fly per day, so 0.1% is about 3 thousand people.  That means there will be 3 thousand people flagged every single day that are innocent -- false positives.  Now we don't know for sure how many terrorists fly, but given that there are 0 documented cases of a terrorist being stopped by TSA (unknown whether there are any classified instances or not), we can presume its quite low... almost certainly well below 1 terrorist per day.  So, when you do not commit the base rate fallacy, you see that the probability of someone who is flagged actually being a terrorist is quite low, well under 1%, and likely a thousandth of a percent or less (depending on the actual number of terrorists thwarted by TSA, which is not public knowledge).
You have to go back to http://plato.stanford.edu/entries/medieval-philosophy/ Medieval Philosophy.

Under the influence of ancient Greek philosophy, European medieval theologicians developed the "project" of applying the tools of rational inquiry to the "science of God".

To be able to rationally prove the existence of God was aimed to have a very powerful argument to contrast atheist and other people "lacking of the true faith" (Jews, Muslims,...). 
First off, it's important to note that as understood in philosophy, fallacies apply to arguments. Thus, we have to make sure someone is making an argument. In the case of "Kitchen Nightmares", this seems at least somewhat justifiable.

Also, it's important to note that "name the fallacy" is normally not the most important thing to do with fallacious arguments. 
The argument is something like this:


If the food were bad, then people would complain.
No one complains.
Therefore, the food is not bad.


This argument is formally valid. But the argument is not sound. Validity means if the premises were true, the conclusion must also be true. Soundness means validity and true premises.

Where then does this argument go wrong? I would say that it goes wrong in that we have no reason to believe the first premise is true. Thus, the argument appears to engage in what we can call a "false cause fallacy." But again, it's not important to name the fallacy so much as it is to show that an argument is wrong.
I'd first question your use of the term "moral" in this context. Most modern concepts of "morality" entail universality. They attempt to transcend the historical and conditional. The "nation-state," democratic or not, is not, by definition, a universal state... and thus not in itself a source of "moral" obligation. 

I would also question your rationale, "considering that democracy was achieved through substantial efforts." One could say the same of almost any government. Even hereditary kings must sit uneasily upon the "throne of bayonets." The Third Reich was an outcome of considerable effort, much of it "democratic" in form. 

In addition, the term "democracy" has many different definitions and electoral forms. In the newly founded United States, for example, perhaps only one-fifth of the adult working population was legally enfranchised. In certain phases of Marxism, on the other hand, national elections and parliamentary systems were abolished as too centralized, abstract, and prone to corruption, while workers in the early "soviets" could vote on work and management issues directly affecting them. In ancient Athens, ideally, "representative" government was not thought "democratic," and citizens voted on everything and rotated through public offices.

Finally, I think it is easy to imagine scenarios in which not voting could be a "moral" act of civil dissent. Most nation-states, no matter how despotic, have official elections of some sort. A referendum legitimizes power and, in advanced countries, gives a compensatory sense of popular participation, somewhat like "commenting" on internet sites. There may be cases where assent to narrow, false, or corrupted choices may not be the highest moral act.     
Soundness is a logical concept, and when a philosopher uses the word, she means the logical concept. Soundness is not a property of a statement, but that of an argument. An argument is defined as a set of statements where one statement is a conclusion and all the others are viewed as premises (or assumptions). An argument is staged to show that the conclusion of the argument has to be true. For this, an argument must be both valid and sound. By definition, an argument is valid when it is logically impossible for all its premises to be true and its conclusion to be false, In other words, if all its premises are true, in a valid argument, the conclusion must be true. Also by definition, an argument is sound when it is valid and all its premises are true. Ergo, the conclusion of a sound argument is always true. For this reason, soundness is the ultimate virtue of an argument. (By the way, the above material is taught in any intro to logic course). 

Philosophers draw different, sometimes, conflicting, conclusions, because they disagree on the truth of some of the premises. For example, pro-choice theorists tend to assume that a fetus is not a person, and more like a tumor. To establish this assumption, they might appeal to the fact that both tumor and fetus share similar aspects (unwanted, growing big, hindering the ability of moving around freely). Once establishing the identity (fetus = tumor), they can conclude that it is morally permissible to remove a fetus from one's body. Pro-life theorists, by contrast, assume that a fetus is a person. To prove this assumption, they will try to appeal to the fact that human society and culture have treated a fetus like a baby. Once establishing the identity (fetus = baby), they can conclude that it is morally impermissible to kill a fetus since it is morally impermissible to kill a baby.      

As issues surrounding abortion arguments show, all the heavy lifting is done through establishing the truth of the premises (no philosopher proper would make a mistake in the validity of an argument), which is nothing but showing the soundness of the argument.    
My advice to you is to assume the worst, and then decide how you will handle it.  Picture the interview, see yourself "coming up empty" on question after question, and the interviewers scolding you for wasting their time, and having their security people escort you out, somewhat roughly, and only halfway through the scheduled interview time.  Is that close to the worst outcome you fear?  (In any case, picture the worst possible outcome you fear, and play it in your head like a movie, feeling the feelings.)  

Now, imagine that you are sitting on the curb, right after that, right where security has dumped you. Feel the feelings of rejection, of worthlessness, panic about not having enough money to buy food, knowing you cannot pay rent and will be evicted and will have to live in a homeless shelter, and your former friends will shun you.  Feel whatever it is you are worried you will feel.  After a few minutes, release those feelings, and start thinking about what you will do, now that the worst has occurred.  What can you take action on to improve the situation from here?  Each day at the homeless shelter, will there be something you can go do to improve your situation?  Will there be a food bank you can visit to obtain food?  Think through and make peace with what you will need to do, when the absolute worst happens.

Now, the next step is to breathe a sigh of relief, that none of this has actually happened yet.  Find someone who knows most of the truth of your situation, and describe the worst case scenario you pictured to them.  Their reaction is unimportant; what's important is how do YOU feel about how all this sounds when you describe it?  Does it sound ludicrous, totally unrealistic?  Do you feel capable of improving upon this situation?

For movie therapy, I suggest watching "The Pursuit of Happyness" with Will Smith, which covers some of the homeless shelter ground.  Could you do at least some of what he did in the movie?

Now, when you start obsessing over the situation, think of the worst case and be amused at how ridiculous it seems.  Feel the emotion of amusement, at entertaining something so ridiculous.  Substitute amusement for the emotion of fear.  Then turn your mind to constructive steps you can take to create an outcome better than the ridiculous worst case you have pictured.  Repeatedly ask yourself a focusing question, such as, "what can I do today that would improve my chances for a better outcome?"  even, "what can I do RIGHT NOW that would improve my chances for a better outcome?"

To limit obsessing over this ONE interview circumstance, I also suggest asking yourself the question, "regardless of what happens with the recruiter and the interview in the coming days, what can I do today to improve my prospects for other situations that could be even better?"

You would do well to actually WRITE THESE QUESTIONS DOWN EACH DAY on a piece of paper, and then spend some time thinking, and write down whatever ideas come to you as answers.  This physical act of writing the question can be an important trigger for ideas. The physical act of writing down actions also helps your brain to believe in, and go to work on the ideas you come up with.

Make peace with your fear.  Make peace with the potential of very undesirable outcomes.  Accept them, as if they had already happened.  Accept the work you would need to do to recover from that worst outcome. Then change channels, think of how amusing and unlikely that worst outcome is, and then ask your mind to answering your focusing question about what you can do to improve your chances for a better outcome.

P.S. Some would suggest that you make your movie of the worst outcome even worse, to the point of being utterly ridiculous, and downright laughable.  I would encourage you to experiment with that as well, and see if that also helps.
Rand's view of morality is something like the following. Human beings have the capacity to create knowledge about the world, and to make choices in the light of that knowledge. The world is not set up in such a way that you and other people are bound to be victims dependent on alms. It is possible, and good, for you to gain from trading with other people. That is, you can deal with another person entirely on the basis of offering him some value he wants in return for him offering you some value you want: the http://aynrandlexicon.com/lexicon/trader_principle.html trader principle.

You wrote:


  Or, did Rand believe that there was some extrinsic moral force that gave her ideas a real property of being "good" beyond the maximization of individual happiness?


Rand though you should pursue happiness in the sense that you ought to pursue values that will lead to you being http://aynrandlexicon.com/lexicon/happiness.html happy:


  Happiness is that state of consciousness which proceeds from the achievement of one’s values. If a man values productive work, his happiness is the measure of his success in the service of his life. But if a man values destruction, like a sadist—or self-torture, like a masochist—or life beyond the grave, like a mystic—or mindless “kicks,” like the driver of a hotrod car—his alleged happiness is the measure of his success in the service of his own destruction. It must be added that the emotional state of all those irrationalists cannot be properly designated as happiness or even as pleasure: it is merely a moment’s relief from their chronic state of terror.


What about government? Rand's position was that individual rights are necessary to secure the conditions under which people could act on the trader principle. Securing those rights requires that people should act according to an objective set of rules: don't steal, commit fraud, commit murder etc. She argued that http://aynrandlexicon.com/lexicon/government.html government was necessary to secure those rights, and that securing those rights is government's only proper function.

If you want to know more, "The Virtue of Selfishness" is a short introduction to Objectivist moral philosophy, You may also be interested in "Philosophy: who needs it" and her novels especially "Atlas Shrugged" and "The Fountainhead". If you want to discuss these issues further, you might be interested in the https://groups.yahoo.com/neo/groups/fallible-ideas/info Fallible Ideas discussion group.
The second premise is false unless "heinous crime" and "insane" are defined to make it true by definition, in which case the definitions are question begging. But because people committing heinous crimes are convicted despite the insanity defense, premise 2 fails at least on the legal definition of "insanity".

The third premise is also false; otherwise clinical psychiatrists are either insane or engaged in a futile endeavor.

This would make the argument unsound, but not a fallacy since a fallacy is supposed to derive an erroneous conclusion even if a fallacy's premises are granted. 

But there is still a genuine fallacy in this argument because of the substitution of cause for motivation in the conclusion. Even if a maniac is so insane as to be impenetrable even to a most insightful psychiatrist, the maniac's behavior can still be analyzed rationally, by treating the maniac as a black box. It doesn't take understanding of motivations to surmise that poking one with a sharp stick would provoke an outburst, in which case poking would be the cause no matter how convoluted the motivation that mediates between that and the outburst. 

Finally, even without the substitution the core of the intended reasoning seems to be that "sane people just can not understand what motivates the insane". This is http://utminers.utep.edu/omwilliamson/ENGL1311/fallacies.htm They're Not Like Us type of ad hominem fallacy, and involves https://en.wikipedia.org/wiki/Equivocation equivocation on "understand". It is also related to what Pasnau calls the http://spot.colorado.edu/~pasnau/inprint/content.pdf content fallacy, the content of mind is conflated with the state of mind to conclude that only insane minds can grasp their likes. We can understand arguments (externally) without sharing their premises or logic but adopting them for the sake, but we can not 'understand' them (internally) since that would involve the sharing. The same goes for the workings of insane mind, even if we can not share them we may still be able to explain them rationally by external reasoning.
First off, obligated is a very powerful word, which almost certainly will yield a "no" answer to your question.  That being said, the question of what is ethical is one whose answers are not fully agreed upon.  At the very least, some suggest it is the "good you do" while some suggest it is "how you do it."  That debate alone will limit the ethical obligations.

Also, there is no reason to believe on is ethically obliged to do any particular charity.  At best, if one chooses their base set of assumptions carefully, they may be obliged to offer charity in general, but the particular means is not essential.

You could argue that "you should be ethically obliged to make it easier to do ethical things," but that argument is really hard.  All you're doing is removing the activation energy of the charity process -- your ethical obligation for charity is unchanged by the presence of a tool to make it easier, except by a tiny fraction (most of the process of donating to charity is the hours of work it takes to make the money.  The 30 seconds it takes to donate is a trivial part of the charity process from an effort perspective)
I assume that the society in question has a constitution and a set of laws. These are necessary requisites to determine the rights of citizens.

The first decision has to determine, which criterion is to be choosen to measure historical injustice. There are several alternatives:


The human rights as specified by the UNO
Constitution and law of the former society
Constitution and law of the present society.


In Germany one has focused on criterion 1 to asses the historical injustice of Third Reich and of the firing order of DDR, the former society. In addition, one has used the present constitution and the present laws concerning the rights of the present citizens.

A second set of questions asks to which degree society wants to asses historical injustice without dividing society.

To come back to your original question - and interpreting the intended context:

Yes, a society can condemn doers of historical injustice while upholding the rights of the doers who are now citizens of the new society.
Both of the answers are correct to point out that the dialogues are fictitious. It is extremely unlikely any section of any length is a transcript of an actual conversation between Socrates and anyone.

This, however, should not surprise us because the idea of making dialogue in a written work a transcript is a modern concept. No one would have done so until well past 1500 CE.

So in a sense, we need to split this into two parts. Cicero's speeches are also fictitious in that what we have recorded is not what he said but rather a highly polished version written well after the fact expressing what Cicero wanted to express. Same with Julius Caesar and most other classical Greek and Roman writing. But the ideas and thoughts contained therein are Cicero's or Julius Caesar's. (Aristotle's works by and large appear to be an exception -- not as in they are transcripts but as in they seem to be unpolished lecture notes). Let's call this feature "polishing your memoirs." This was and is common and doesn't strike me as wrong (Heidegger's later essays for instance are often papers he delivered several times and revised and improved along the way -- but that doesn't mean we would not say he was delivering the same paper).

A more important thing to notice is that Plato's Socrates often functions as a mouthpiece for Plato and does not necessarily say what Socrates would have said. In Plato scholarship, dialogues are divided into http://plato.stanford.edu/entries/plato/#HisSocEarMidLatDia early, middle, and late. Early dialogues. My AOS is not Plato, but the basic gist is that early may be closer to what we believe Socrates himself might have done and said (i.e., someone who didn't claim to know things, asked questions about knowledge claims) and that late dialogues present Plato's elaborate theory of knowledge in sophisticated terms (probably not a view that Socrates held). (this is a key focus of Kierkegaard's dissertation The Concept of Irony and a returning theme in his writing). Let's call this "the Plato-washing of Socrates."
The concept to classify by division has already been used - and possibly invented - by Plato under the name dihairesis (= division). Plato starts his work Sophistes with several examples of this method. 

One can describe dihairesis as creating a decision tree. In the most simple case it is a binary tree. The tree has several layers and the leaves of the tree are the terms to be classified. Of course the inner nodes of a tree are not distinguished by any structural property, only root and leaves are. Hence the principle of "definition = species plus genus proximum" seems a simplication. It takes into account only two neighbouring layers of the decision tree. 

It has been discussed whether such decision trees are uniquely determined: To which degree are the properties used in the classification predetermined by the things to be classified and to which degree do they just mirror our point of view?

Apparently Aristotle uses the method for his biological classifications. In biology, the lowest category of species has an objective meaning - capable of interbreeding and producing fertile offspring. But the higher categories are much more arbitrary. 
There's several ways in which you're using potential and actual strikes me as very non-Aristotelian fashion in your question. To grasp what Aristotle is talking about with potential and actual, you need to recognize he's working on the problem of change and his resolution is to work from a concept of substance with properties and accidents. This contrasts to his representations of http://plato.stanford.edu/entries/aristotle-noncontradiction/#9 Heraclitus and (http://plato.stanford.edu/entries/parmenides/ what sort of kind of seems like) Parmenides who advocate either that nothing has fixity or nothing changes respectively.

Aristotle's solution is to speak of substances and to divide potential and actual in two ways. Something that has no form and only matter is pure potential (or potential potential if you like). Something that has form and matter has actuality and potential. Something that is pure act has no potential (this is also Aristotle's definition of God).

Now, we can turn to the SEP article and try to make sense of what it is saying. First off, it's not the article on potential and actual in Aristotle -- there are two better suited articles: http://plato.stanford.edu/entries/aristotle-metaphysics/#ActPot Aristotle's metaphysics and http://plato.stanford.edu/entries/aristotle-natphil/ Aristotle's natural philosophy. You may wish to look at those to better understand the issue.

If we want to speak of the potentials of the statue, then these are going to be the changes the statue could undergo and still be the same substance.

If we speak of potentials simpliciter, this could be anything.

Thus, when you ask whether becoming a disk is a potential of bronze statue, the answer is no. Sure, that could be done, but it is done precisely by eliminating the bronze statue -- so it's not a potential of the bronze. 

Conversely, if you paint it green or blue or any other color, then it remains a bronze statue (there's a complex problem in this particular example in that Aristotle has a special treatment for artifact [human made things] -- so leaving aside any necessity of the color we see being bronze, it can work).

Thus, generically, there is a potential to change what a thing is, but that is not a potential of that thing.

This account is best suited to natural kinds. So for instance, a human being does not have the potential to have no heart -- because that would kill the person and change the sort of thing we are talking about. But a human being does have the potential to have any hair color they want (with the use of some chemicals)
I would like to suggest that the notions of past, present, and future are all nonsensical.

A complete scientific description of the universe would not tell us when now is.  The "present moment" cannot be endowed with any special property that is not shared by any other temporal moment.

Thus, the concept of now appears to be a purely subjective one, and one which is obviously very important to the way the world appears to us.  Yet we never seem to experience the present moment.  The processing of sensory information takes time.  Our perception of the world around us must travel through our nervous system along the various neural pathways to our brain.  There it must be integrated with the disparate sensory information we are receiving to form a coherent representation of our experience.  Yet once this process is complete and the information is cognitively available to us the moment has passed.
Aside from the psychological question of whether to act or remain static with respect to harm and blame, such as in the Trolley Problem (or Train Track Dilemma), it seems that many folks like to think in black and white. Recent discussions pertaining to terrorists comprise one set of examples relating to whether to kill or let be, ban or accept.

Another important consideration, which is probably more related to ego, beliefs of soul and causality, and ideas of punishment, is that it feels better (emotionally) to put others into categories of good and bad, good and evil, without considering the finer details and factors of causality involved. A person is often seen as either "civil" or "criminal", as if these words really describe a person's value or virtue whatsoever. Hence, if people see Hitler as being "evil", they may feel that any showing of sympathy is intolerable and unbearable. After all, why would you help a "bad" person?

Society would need to accept the idea that causality is complex and that most ill events cannot rightly be blamed wholly on one entity. Furthermore, it is absurd to think that Hitler alone "did" what they say "he did". Leaders are often spoken of as if they personally enacted all the events that unfolded under them. Also, what is to say that there were no pressures on Hitler from others? Could there, for example, have been friends, family, or lovers peer-pressuring him to act in certain ways? Maybe some person or entity was even manipulating him in one way or another. I doubt he was magically impervious to influence from others.

Moreover, Hitler is by no means whatsoever more evil in nature than a relatively large portion of society both then and now. A great many people exist today, who if they were in a position of great power, would do great harm to a great many other people and or things. Taking it even further, imagine putting a 4-year-old child into a position of great power. That would be quite scary indeed. Are 4-year-old children then evil monsters? Well, perhaps, but the point is that society involves interaction, and almost all large-scale events could not have happened without a great many hands at play -- both physically and in terms of social and or financial influence.
The Greek word for virtue is arete. 

It is already used by Homer and applied to the heroes figthing at Troia but also to women. 

Socrates in Plato's early dialogues asks experts for a definition of special virtues, e.g., he asked Laches, a general, what courage ist.

Plato who was the teacher of Aristotle explicitely named the four virtues Prudence, Justice, Fortitude = Courage, and Temperance as fundamental virtues.

But Aristotle was the first who gave a definition of virtue in general. His definition from Nicomachean Ethics (EN II,1f) of virtue as a mean between negative extemes fits to many virtues, but not to all, e.g. not to justice. Indeed, Aristotle himself gave two different defitions for justice, see EN V, 3-5.

Later philosophers abanoned the search for a general definition and returned to focus on separate virtues. Christian faith complemented the four cardinal virtues named above by the three maior Christian virtues: Faith, hope and love. Thomas Aquinas made a systematic study of all 7 virtues.

Hence many philosophers do not consider virtues according to the general definition of Aristotle. For an introduction to the whole subject see
http://plato.stanford.edu/entries/ethics-virtue/ http://plato.stanford.edu/entries/ethics-virtue/
This is answer is inspired by  Philip Klöcking's comment, and part of it was originally posted https://philosophy.stackexchange.com/a/30575/13808 here

Yes subjective experience can be described, even though the description of the experience and the experience itself are different. The difference between the subjective experience itself and it's description is illustrated by the concept of https://en.wikipedia.org/wiki/Qualia qualia.


  Here's an example that demonstrates what qualia is: Imagine a dentist called Mary, who is the top dentist in her field, she has aced every dentistry topic there ever was, can cure any patient who comes to her with a dental problem no matter how bad that patient's condition, and has studied every biological, neurological and chemical aspect of what a toothache is. In short she knows everything there is to know about toothaches. However, she has been an avid tooth brusher ever since she was a kid, and she has never ever had a tooth ache in her life. A dualist will argue that because of this, she doesn't really know what a toothache is at all. She knows all there is to know about toothaches, but she doesn't know what a toothache is, having never had the feeling herself. This difference between knowing everything about something, and knowing what something is or how it feels, is what philosophers of mind call qualia. Dualists think that qualia is proof that there is a non-physical dimension to the mind, since if the mind were purely physical, there would be no difference between the "about" and the "is".


So there can be an informal or ordinary language description of pain. And there can be a more detailed and comprehensive scientific description of pain as well. Dentists and Doctors use the description of of pain to diagnose patients all the time "Can you describe your pain?" "I feel a constant throbbing in my left jaw" "Oh - I think you have  cavity in your left tooth.". And yet pain remains the ultimate subjective experience.



Note that qualia are used mainly as an argument for dualism. Regardless of whether the concept does indeed prove dualism (I personally don't believe this), the explanation above does show how something can be both subjective and describable, and what the difference between the experience and the description is. 
Peter Singer identifies wrong with pain. It's not particularly clear that plants experience anything similar to what we normally call "pain" whereas it seems like many types of animals do experience identifiably similar pain.

So it's discriminating based on what he takes to be the relevant feature of morality: pain. And this is linked to some form of sentience and consciousness.
Insisting upon a human interpretation to put a result in context does not reduce the value of the result itself.  Interpretations of statistics are still interpretations, after all.  The objection is to considering the result as a meaningful object on its own, outside a context that integrates all of the social constructions that might have pushed the results in one direction or another.  The supposedly objective method is not irrelevant, it is just no more relevant than other sources of ideas.  All sources can be integrated into an overall explanation.

Clinical psychologists of a basically interpretivist bent often use scored instruments, especially projective media, to derive a starting point for searching out ideas about their clients that might be hidden by the client during communication.  They just do not accept that the instrument necessarily 'says' something definite.  After all, the fact that someone gives answers similar to those given by people with a given way of interpreting the world can mean you are one of those people.  But it can also mean that you have been trained or influenced to consider those kinds of answers more acceptable, or that you have an unusual way of taking the world it that produces the same patterns for some other reason.

Various kinds of ascetics, for instance can come across as depressive if you look at how many hours a night one sleeps, what one's food intake is, how one moves through space, and other objective measures, and even opinions about the world and other people.  An instrument will score them that way.  But you can just look at them and see that a different frame of reference is in order.  That does not mean that an objective measure might never tell you someone is depressed when they pretend otherwise in public because they have been trained to not get others down.  It means that you have to actually think, despite the fact you have a supposedly objective instrument.
Left-nested conditionals, like your (A → B) → C are actually quite rare and sound strange in natural language. Right-nested conditionals A → (B → C) on the other hand are commonplace and are typically considered equivalent to (A ˄ B) → C by the import-export rule. For example, "if I bet on this horse then if it wins I'll buy you an ice cream" is the same as "if I bet on this horse and it wins I'll buy you an ice cream". 
The straight forward answer to your question is http://www.iep.utm.edu/ethics/ Ethics: that is the field of philosophy which directly studies ethical dilemmas such as the one you mention. There are also higher level ways of studying the questions you mention: 


An ethics question would be: Should I kill the man or the tiger? 
A http://www.iep.utm.edu/ethics/#H1 meta-ethics question would be: Is killing the tiger instead the man a subjective or relative truth (that changes from individual to individual, culture to culture, etc...) or is it a universal truth that can always be applied? 
A question of https://en.wikipedia.org/wiki/Axiology axiology or https://en.wikipedia.org/wiki/Value_theory value theory: What are the ways of measuring the value of human life against the value of animal life? Plant life? etc...




In the example you gave: 


Saying that "save the man because he is human" is following https://en.wikipedia.org/wiki/Deontological_ethics deontological ethics: You're choosing what is good and what is bad based on rules which classify some behavior as good and some behavior as bad - with the rule being followed in this case "We should always try to save a human life". 
Saying "save the human because he could save you in another dangerous situation, and the tiger will never save you (except by chance)?" is following a https://en.wikipedia.org/wiki/Consequentialism consequentialist ethics where the outcome of the behavior is what makes good or bad, not the fact that there rules that should be followed. 

An expression such as Fx, with no quantifier, contains an unbound variable, and so it is not a sentence and does not have a determinate meaning. The following are sentences: "everyone is happy", "someone is happy", "Fred is happy", but this one is not "___ is happy". That is a just a fragment of a sentence with a place-holder where something ought to be. So if H is the predicate "happy", (x)Hx is a sentence, as is (Ex)Hx and Hfred, but not Hx. 

So the question is, why does your logic book bother to introduce statement functions at all? What is the point of writing expressions with unbound variables? The answer is that if you are going to use the rules of inference that you have learned within the propositional calculus, the presence of quantifiers within a sentence gets in the way and obstructs the straightforward operation of the rules. I'm not familiar with Hurley's book, but a common method for eliminating the quantifiers is this: 1. Convert the sentence to prenex normal form; 2. Eliminate the existential quantifiers by skolemisation; 3. Eliminate the universal quantifiers and operate with the assumption that any unbound variable is universally quantified. 

It is with reference to this third rule, that Fx may be described as a surrogate for (x)Fx "with certain liberties". 
This is a very amusing question, and I like the considerations you articulate.

To place it in a less solitary context, you are asking about moral standards that are not "utilitarian" or "consequentialist." In other words, approaches to morality that are not based on harm to others or the consequences of actions.These are sometimes called "deontic." 

So your habit of sneaking around behind your own back is wrong only through a "deontic" approach to morality... of which Kant is the best-known "modern" representative. The "wrong" lies in the rational contradiction between the "universalization" of the act and the very assumptions that make the act possible...in this case the rules of solitaire.

Kant's example of lying is similar. If we "universalize" that act, the act of lying itself would not be possible or rational, since it depends in itself on the reliability of language. Ditto for solitaire. In fact, your example plays into the transition from Kant to Wittgenstein, for whom our practices are enabled by their rules or "language games." 

Your sense that you wouldn't want your "mother" to know is a good Kantian example of the way "universalizing" such idealism is also grounded in our human social being and "common sense." I believe your question has a funny stance that also invites more nuanced answers. But I'll leave it at this. 

I tend to admire Kant. But personally, I cheat at computer chess, by taking back moves. (On the other hand, my chess application cheats too. It often crashes just when I have a nice position set up.)   
I do not see any similarity between Kant's noumenon, taken as the thing-in-itself, and the empty set from mathematical set theory.

Kant and science in accordance with him hypothesize the existence of objects in the real world. According to a constructivist epistomology we do not have direct access to these objects (thing-in-itself). But that hypothesis is the easiest hypothesis explaining why we make sensual experiences. And why the latter serve as input to our cognitive information processing. 

In the field of mathematics you are free to create arbitrary ideas as long as they are free from inconsistencies. Mathematical concepts do not necessarily relate to the real world. The empty set is a clever concept from set theory. E.g., it is useful to define the intersection of two sets even when they are disjunct. Moreover the empty set is the basis of von Neumann's construction of natural numbers. Different from the things-in-themselves there exists only one empty set.

I would not name neither the concept of the thing-in-itself nor the empty set a principle. Possibly you can explain a bit why you consider both similar regulative principles.
In our legal system Judges, who are assigned that role by others, can levee fines.  If I go out and levee fines, it is extortion.  That is not a conflict with universality, it is just another precondition in the maxim.

The proposed maxim is naturally universal, once you state it completely, specifying the role of the person who is to do the action.  If you do it the other way, you are simply doing a different dance.  In fact simple definitions and logical tautologies are always immediately universal.  So they basically lack ethical content.

The only way a simple fact can have moral content is to be culturally embedded, and for the user of the fact to have a contingent duty to be truthful about its use.  If the hokey pokey (that is the name, at least in the US) had moral content, the duty would rely upon the idea that it communicates a specific message in a specific context.  If that message is important enough, by doing it wrong, you are lying, and in a way that is unethical.  (If you do the statistical 'dance' of computing a p-value in a science paper wrong, it is unethical.)

But the hokey pokey is not the Medieval Mass, (or even the modern peer-review process) or some similar cornerstone of deep cultural definition.  So its definition is not going to be 'categorical' enough to be a general truth or rule of conduct, at least not up to Kant's standard for generality.
I am currently reading Ian Hacking’s Why is there  Philosophy of Mathematics at all, and it is mostly about the contemporary dabate platonism/ nominalism, so I would recommend it as a good place to look for an answer to this question. A crude copy-paste is given below. 

Hacking asserts that it is Paul Bernays who introduced the modern idea of 'platonism' and further he writes about the different brands of platonism, noting (p228):


  After Bernays had introduced the word 'platonism' to the philosophy of
  mathematics, it should have been clear that one should not speak of platonism,
  but about platonism restricted to some domain of objects, such as the
  class of integers, or any Zermelo-Fraenkel set, or whatever. Perhaps even
  something like the class of all ZF sets, in a van Neumann-Godel-Bernays
  set theory.


So a good point to remember is that according to him 


  Absolute platonism, asserting the existence of all definable mathematical
  objects and relations, is untenable. What remain are relative platonisms.
  The weakest interesting platonism described by Bernays asserts the existence
  of a totality of whole numbers...a twenty-first-century platonist will say: positive integers are abstract objects. Kronecker said that God created them. So he thought they exist. That's a species of platonism - about numbers. He was a very modest platonist, but a platonist all the same.


Hacking comments:


  Such a thought seems never to have crossed Bernays' mind, for he thought of platonism in terms of totalities rather than 'abstract objects'.


So Bernays turns out to be a restricted platonist while 


  In Bernays' vocabulary Boolos is a cautious platonist. He has no problem
  about the totality of whole numbers, but he has many qualms about sets
  whose existence is proven within Zermelo-Fraenkel set theory with the axiom
  of choice.(229)


Perhaps there is something of an answer in this  heap of quotes
Our perceptual mechanisms are not passive.  To a large degree, they work like sciences do, generating and confirming guesses, rather than assembling causal relations out of individual pieces: https://www.braindecoder.com/up-to-90-of-your-perception-could-be-made-up-purely-by-the-brain-1104633927.html https://www.braindecoder.com/up-to-90-of-your-perception-could-be-made-up-purely-by-the-brain-1104633927.html

Association, then, is the norm, and distinctions are discovered by an active process.  This goes along with William James notion that the infant mind is a "blooming, buzzing confusion", that our ideas originate con-fused (bound together, associated), not distinct, and that we grow into clear perceptions and ideas by separating, not combining.  At a different extreme, it is a common aspect of Buddhist practice to assume All is One and distinctions are created by attachment.  

So there are a range of mutually reinforcing psychological approaches, ranging from sheer introspection to observations of learning to neurological timing data, that credibly controvert the more physical, atomistic notion that perceived events are individual things in need of correlation, which Hume struggled with.

The idea that our actual ideas are 'pastiche' and not 'sculpture' is a side effect of our need to adapt them into stories, which have a succession of plot points and are told in assembled structures like sentences and words.  We are such social beasts that the social reconstruction of our actual thoughts seems more realistic to us than their real, underlying forms.

So "There is no 'there', there."  There is no need to explain how we put ideas together, however basically unrelated they may seem, only why we distinguish them.
It probably depends more on the specific example, but you are probably referring to https://en.wikipedia.org/wiki/Faulty_generalization faulty generalization: 


  A faulty generalization is a conclusion about all or many instances of a phenomenon that has been reached on the basis of just one or just a few instances of that phenomenon. It is an example of jumping to conclusions. For example, we may generalize about all people, or all members of a group, based on what we know about just one or just a few people.


However, in your specific example, I cannot imagine someone who had only ever met men who used language like that to refer to women, so the specific example you gave just seems to be someone ignoring facts.
There are two possible answers to your question: 


Without the existence of humans, the question dissolves. The very concepts of "good", "bad", "blessing", and "curse" are based on human thoughts, language and self-awareness, and don't have any meaning independent of the human subject. So the statement "the earth would be better off if humans didn't exist" doesn't have any meaning.   
The second answer is similar to one I gave https://philosophy.stackexchange.com/a/31357/13808 to another question: It is possible that humans are the only self-aware agents on earth, and as such are the only way by which the earth itself becomes self-aware. This would imbue humans with a special value that other living organisms don't have. From this point of view, one could answer that the earth is better off because of human existence, no matter what other damage and tragedies humans cause. This assumes a self-aware earth is better than a non-self aware one. See https://philosophy.stackexchange.com/q/29016/13808 this question as well for a related discussion. 

Wittgenstein is not denying that the correlation is useful, he acknowledges as much in the first passage. What he means is that the correlation is established in a way that does not allow for any independent checking of sensation's authenticity. What way is there to tell if the sensation is the 'right one' other than observing the rising blood pressure? We only have sensation's 'word' for it. With a truly private diary the "S" is all there is, by construction there is no possibility of mistakes. And if there is no rising blood pressure then it is not that we identified the sensations 'wrong', there is simply an imperfect correlation or no correlation at all. We can have "S" publicly linked to rising blood pressure with a possibility of mistakes, or we can have private "S" without it, but we can not have the same "S" be both.

This is why in the end he says:"And that alone shows that the hypothesis that I make a mistake is mere show. (We as it were turned a knob which looked as if it could be used to turn on some part of the machine; but it was a mere ornament, not connected with the mechanism at all)". Another example he gives, in PI 266, is that it is pointless to buy several copies of the same newspaper to assure yourself that what it says is true, "justification consists in appealing to something independent" (PI 265).

See https://books.google.com/books?id=6I9oetOUdQ8C&pg=PA74&lpg=PA74&dq=And+now+it+seems+quite+indifferent+whether+I+have+recognized+the+sensation+right+or+not.+Let+us+suppose+I+regularly+identify+it+wrong,+it+does+not+matter+in+the+least.+And+that+alone+shews+that+the+hypothesis+that+I+make+a+mistake+is+mere+show.&source=bl&ots=B7sZU8l6om&sig=gbfMKUm_M3_EM15arq8hP4vGNhA&hl=en&sa=X&ved=0ahUKEwjzzouzsPDKAhXGJiYKHUTxBoQQ6AEIITAB#v=onepage&q=And%20now%20it%20seems%20quite%20indifferent%20whether%20I%20have%20recognized%20the%20sensation%20right%20or%20not.%20Let%20us%20suppose%20I%20regularly%20identify%20it%20wrong%2C%20it%20does%20not%20matter%20in%20the%20least.%20And%20that%20alone%20shews%20that%20the%20hypothesis%20that%20I%20make%20a%20mistake%20is%20mere%20show.&f=false Fogelin's commentary in Taking Wittgenstein at His Word and https://philosophy.stackexchange.com/questions/34111/did-wittgenstein-consider-the-possibility-of-a-language-that-was-token-private/34157#34157 Did Wittgenstein consider the possibility of a private language with public content? for more on the private language argument.
You can make any claim you want, since no particular ethical system is mentioned in the problem.

I think one could at least have good fun arguing that the knowledge that one chose NOT to buy a ticket is a thing in and of itself, which can cause pain or pleasure.  The raising of the prize increases the ability of that knowledge to hurt you.

One could also argue that, if a particular government has sold itself as having certain properties such as fairness or equality, the raising of the lottery could be seen as eroding key "rights" guaranteed to individuals.

Of course, we know nothing of why the government made their choice.  It's entirely possible that, if you look at a bigger picture view, you get to see a way non-lottery-ticket-holders benefited from the decision which isn't immediately apparent with the ultra-narrow focus we are given in the problem statement.  There's simply not enough information to make a definite answer one way or another, but it at least appears that there are points of view from which one could argue damages.  Whether those positions are valid depends entirely on what system of ethics is actually in play.
Your translation is potentially correct but ambiguous in English, it could mean that every football club has at least one player.  A better translation is "there exists a footballer who plays for every club."
The problems you see are typical issues that one arrives at when using too simple of a utility function.  For example, it is well recognized that life-years is a terrible metric for utilitarians, with all sorts of known pitfalls (such as quality of life arguments).  A utilitarian would argue that ones metric must be more developed than that.

The problem of future people is actually a subset of the real challenge utilitarians face: the problem of the unknown.  At the moment a utilitarian makes a judgement call, such as to save one fertile woman, he or she does not know what will happen in the future.  For all he or she knows, this woman will become the mother of a very evil dictator who will bring the world into war.

This problem is tackled in many ways, but the one which is most accessible is probability.  Potential outcomes are assigned a probability, or at least a likelihood, and then are weighed accordingly.  Different utilitarians may assign value in different ways.  One utilitarian may value the fertile woman's ability to bear potential children more.  One might value the lives of the other 100, because they know that all of these people can be saved, but the woman's bearing of children is just a probability.  Others might even be more nuanced, and start comparing the ability to produce offspring against the sum wisdom of the 100, and their ability to enlighten other offspring (presuming this 1 woman is not the only fertile woman in the world).
The "standard" translation of the https://en.wikipedia.org/wiki/Categorical_proposition categorical proposition:


  "No S are P"


is:


  "do not exist x that are S and P.


In symbols: ¬(∃x)(Sx ∧ Px).

This is equivalent to: (x)(Sx → ¬Px).

With reference to your example, we have:


  (x)(Human(x) → not-Immortal(x)),


i.e. "every human being is mortal".

As you correctly say, the above conditional is true whne there are no human being at all.

This was not Aristotle point of view; see http://plato.stanford.edu/entries/aristotle-logic/ Aristotle's Logic and he problem of https://en.wikipedia.org/wiki/Syllogism#Existential_import existential import.
A more common (but still debatable claim) is that with Christianity, religion and philosophy fused in a way that had not before in the west (that is, Greek and Roman pagan theology did not dialog with philosophy in the way that, eg Augustine or Aquinas seemed to work in the overlap between the two).

To answer your question, consider these three examples:


http://plato.stanford.edu/entries/aquinas/#ThoAri Aquinas's work in general was greatly influenced by Aristotle, as well as his http://www.iep.utm.edu/aq-moral/ moral philosophy. Obviously, being Christian, he would say something like Aristotle lacked the full knowledge of morality, but his work certainly indicates Aquinas's approval of large parts of Aristotle's morality and ethics.
Augustine discusses the influence of Pagan philosophy on his https://en.wikipedia.org/wiki/Confessions_(Augustine) conversion to Christianity, but in particular looked to Cicero as a hero, frequently citing https://en.wikipedia.org/wiki/Hortensius_(Cicero) Hortensius (a work that is unfortunately lost).
It was a particularly thorny problem in Medieval theology and philosophy what to do about Socrates - he was a pagan, but he was also acknowledged as one of the wisest, and therefore one of the most moral, people in history. http://historyofphilosophy.net/albert-aquinas-ethics Aquinas and his teacher, Albert, grappled with this issue.


So, just based on these three data points, it's safe to say that morality is not a Christian invention, given that three prominent Christian philosophers (Augustine, Albert and Aquinas) think otherwise.
Saving the lives of one's children at the cost of one's own is not uncommon.  So there is a dimension missing from your question.

Broadened to include the continuation of one's family and culture or the larger systems of which those are a part as a form of 'indirect selfishness', however, the answer has to be 'no'.  From the point of view of Dawkins' theory of selfish genes, we are made by genes for their own selfish purposes.

If we were not somewhat selfish, to a degree that we gathered power to ourselves in the service of our genes, they would stop bothering to make us and make something else.

This is the second-hand, scientific version of the basic notion of Nietzsche's philosophy.  The goal of an organism is to serve its will to power.  Fortunately, power takes so many forms that the lives of organisms can be complex, artistic, and fulfilling.
Just because you don't know what gravity is doesn't mean you can't experience it. So yes, people can definitely feel a feeling even if they haven't conceptualized it yet.
This is the question posed by Duchamps infamous/famous http://www.tate.org.uk/art/artworks/duchamp-fountain-t07573 object d'art that he submitted to an open exhibition that then refused to exhibit it; which under some readings, asked the question - what is art?

Before one is a writer, or an artist; one is a human being with an aspiration to write or paint. 

The two situations that you suggest are not mutually exclusive or disjoint but touch each other at many points; and in fact, are grounded in each other.
Confusing a term's definition with its connotation is a linguistic error of mistaking the strict meaning of a term with its associations. Someone who is told, "I love you with all my heart," and responds, "Hearts are just lumps of meat," has clearly failed to understand the intended connotation and is guilty of that confusion. It does not imply any fallacious reasoning as such. 

I don't see how the two examples you give relate to this. To say, "Terrorists are cowards because they are evil," is simply a non-sequitur, since not all cowards are evil and not all evil people are cowards. To say, "The minimum wage is not socialist because without it, many people will go into poverty," seems to be a claim that supporting the minimum wage does not make one a socialist, because one might agree with a minimum wage without taking on board all the rest of the socialist ideology. As such, this is a reasonable claim. 

More generally, arguments of the form "B because A" (or "A therefore B") are not all about explicating definitions, so it is not correct to dismiss an argument just because B is not part of the definition of A. Plenty of arguments involve more complex lines of reasoning that go beyond merely understanding definitions, and plenty more are not deductive at all, but abductive or inductive: this does not make them fallacious. 

Speaking of fallacies, the modern obsession of looking for fallacies everywhere, and asking, "What fallacy is this?" is misleading and unhelpful. Soon I will have to start a petition to get the word fallacy banned; it is one of the most overused and misused words around. Logic is not about fallacy hunting. If you think a particular argument is bad, just say what is wrong with that argument, without trying to compare it with others. Referring to fallacies is only helpful if it is unclear exactly what defect in an argument you are trying to draw attention to. 
The idea that quantum mechanics fundamentally challenges the rules of logic was popular for a while, but has fallen out of favor in recent years. 

While intuitively it might seem that quantum superposition (i.e something being in more than one base state at the same time) is what challenges the rules of logic, by invalidating the law of non-contradiction, this is not the case. An electron in a superposition of spin |+> and spin |-> might seem like a contradiction, but it can simply be treated as being in a distinct third state of being "either |+> or |->". 

The real challenge to classical logic from Quantum Mechanics comes from the uncertainty principle, which leads to situations where: 

[(p and x) or (p and y)] is different from [p and (x or y)].     

Birkhoff and Von Neuman proposed in the 1930s that the paradoxes of Quantum Mechanics can be explained if we abandoned classical logic and used some form of https://en.wikipedia.org/wiki/Quantum_logic Quantum logic instead (Birkhoff, Garrett; von Neumann, John. "The Logic of Quantum Mechanics". Ann. Math. 37 (4): 823–843.). Such a Quantum logic would change or abandon all together some of the rules of classical logic, and would be a perfect case of logical axioms arrived at by observation. 

Hilary Putnam discussed this in depth in his paper http://www.socsci.uci.edu/~dmalamen/courses/prob-determ/Putnam.pdf "Is Logic Empirical?",  later republished as "The Logic of Quantum Mechanics." ("The Logic of Quantum Mechanics" in Mathematics, Matter and Method (1975), pp. 174-197). In it, he argued that, just as empirical physical results - relativity - forced us to abandon Euclidean geometry, so it is possible that the results of quantum mechanics will force us to abandon classical logic. 

Von Neumann, Birkhoff and Putnam all seemed to have moved away from this position in later years. Quantum logic didn't really solve any physics problems or provide any new insights into the epistemic challenges posed by Quantum Mechanics. 

Although Quantum logic is still an active field of study up to the present day, it does not get much attention from most philosophers and has been abandoned completely by physicists. The only people who are paying attention to it are pure mathematicians who study different types of logic as mathematical structures (Quantum Logic's relation to Orthomodular Lattices and its relation to Fuzzy Sets), without paying any attention to the semantic or epistemic value of such non-classical logics. See for example http://arxiv.org/abs/quant-ph/0101028 "Quantum Logic, M.L. Dalla Chiara, R. Giuntini, arXiv:quant-ph/0101028". 

You will occasionally come across the term "Quantum Logic" used in the https://en.wikipedia.org/wiki/Quantum_computing quantum computing literature, but by that they do not mean the QL of Von Neumann and Birkhoff. Instead, what is meant by that is classical Boolean logic applied to quantum states and quantum bits.   
As far as I understand the whole passage chapter 2, 19ff. speaks about Atman, not about matter. E.g. see the parallel between Katha Upanishad 2,19 and Gita 2,19. 

The Gita says: 


  How can a person who properly understands this [Atman] as indestructible, eternal, unborn, and without decay cause the death of anyone or kill anyone? (2, 21)


On the other hand, the following passage is generally interpreted as speaking about the dead of the body, a kind of matter: 


  This eternal embodied soul is indestructible and beyond comprehension. The bodies it inhabits, however, are said to be finite. [...] (2,18)


In which passage of the Gita do you read that matter does not exist without being perceived?
I think I have an idea about what you're asking, but please correct me if I am wrong. I feel as if you are asking a question about subjectivity related to a final state of being, which in this case is death. Let me list off some questions I think you may be asking or need to ask. 

-Can someone rationalize ending their life via physician assisted suicide, assuming that the person knows everything about their pain and that pain is absolutely subjective?

If this is your question, then the answer must be looked at in the first way which Aristotle views suicide. Since it would end human life before its natural end was reached, then you cannot rationalize engaging in physician-assisted suicide. However, you must also ask another question. 

-If someone cannot positively contribute to society, can they then rationalize going through physician assisted suicide?

This question is based in relativism. That is, what positively contributing to society is different across cultures without many absolute specifics that certain philosophers (i.e. the utilitarians) have attempted to define, but even have holes there. Just because something is for the common good does not mean that it is good for everyone. We must then look for another way to define the worth of life. This question leads us into another question then:

-Does human life have inherent value to it?

This question is heavily debated, but let us take a view at it via Aristotle. Aristotle believed that the goal of life is happiness. From this point of perspective, we must also take into account the fact that we cannot experience happiness in death. Because we cannot experience happiness in death and only in life, we should not die when we can avoid death so that we can continue to experience happiness, which is our overall goal. Therefore, we should not go through physican assisted suicide so that happiness can still be felt.

-A question I ignored from above- Is pain subjective?

Yes, as you've almost explained above. Since pain cannot be shared with others, it is a feeling that is purely internal. Thus, pain is subjective?

-How can we master something that is subjective, like pain?

It is clear to me that if something that is not objective, we cannot develop a mastery of, since we cannot be sure of the extremity of our pain. I could stub my toe and not cry, and let's also say that this is the worst pain I've felt. Then, I can say that I have mastered my pain to the best of my degree when it is clear that there are so many other things so much more painful that I could experience. Since we cannot experience objective value in pain, we cannot master it because there is no true way to comparing our ability to master our pain compared to someone else's. Without the certainty of the mastery of our pain, we cannot rationalize ending our life over a given amount of pain.

Your other option is to view this question through the base questions of medical ethics (i.e. autonomy, beneficence, non-maleficence, justice, etc.), which may give you more of an answer you are looking for.
Kant deals with these kinds of social relations primarily in the Critique of Judgment. If you look at sections 39 and 40 entitled "Of the Communicability of a Sensation," for example, you will see there he introduces the notion of the sensus communis, which is the basis of universal communicability. The is the relational power which makes all human cultural communication and interpretation possible. This grounds the community of persons, according to Kant, and serves as a required condition for the kingdom of ends imperative. I interpret this to mean that we feel each other through an equal or mutual relatedness prior to establishing rational or cognitive connectedness (i.e. social contracts). It is also how we feel, Kant states at the end of section 83, a "hidden aptitude within us for higher purposes." 

He then picks this theme up and develops it in the sections 82 and 83 on the methodology of teleological judgments when he discusses the difference between "technique and skill" and "culture. The latter is concerned with the culture of our moral powers to facilitate social communication, for the sake fulfilling our ultimate purposes beyond the order of nature. For us to establish a "collective self" we need to cultivate the self-discipline and skill to develop both are inclinations and interpretations, which are vital to the maintenance and growth of cultures. Kant refers to this as the cosmopolitan whole that gives humans a "predisposition for community." Community means a community of nations or states, for Kant, in the sense of international relations. A political and cultural order that mirrors the natural order. It is only when all the nations and peoples of the earth put their talents and skills together that culture may thrive! I suggest the following passage as a beginning point for what you may find interesting in Kant's lively thoughts on this very interesting topic that you raise:


  Without such a whole--and given how the very possibility of such a scheme is hindered by people's ambition, lust for power, and greed [dangers to social cohesion], especially on the part of those in authority--there will inevitably be war (in which some states dissolve and split up into smaller ones, while other states unite with smaller ones and try to form a larger whole). Though war is an unintentional human endeavor (incited by our unbridled passions), yet it is also a deeply hidden and perhaps intentional endeavor of the supreme wisdom, if not to establish, then at least to prepare the way for lawfulness along with the freedom of states and thereby for a unified system of them with a moral basis.

Your question recalls the arrow paradox of Zenon, see http://faculty.washington.edu/smcohen/320/ZenoArrow.html http://faculty.washington.edu/smcohen/320/ZenoArrow.html

The paradox shows that one cannot argue with infinite sets analogous to finite sets. 

The correct method to deal with the paradox is calculus. Accordingly one has to integrate the velocity at each each point of time to obtain the distance covered by the arrow 


  s(t) - s(t_o) = Integral from to_0 to t [ d_tau v(tau) ], v = velocity, s = distance, t = time.

I. doesn't follow. All cats are lions, but not necessarily all lions are cats. So there may be lions that are not cats. Some lions are mice, but they could be those lions that are not cats.

II. follows. If all mice are giraffes, and some lions are mice, then those lions that are mice must necessarily be giraffes, because all mice are giraffes.

III. doesn't follow. While, from II, some giraffes must be lions, they could perfectly be those lions that are not cats.

IV. follows. If all mice are giraffes, then some giraffes must be mice.

(all this supposes that there are any lions, cats, mice, and giraffes; if some or all of these sets are empty, then we would have a problem with equally empty referents, which would make the truth value of these statements more complicated.)

So, only II and IV follow. As this doesn't match any of a/b/c/d options, then the correct option is (e), none of the above.
A single electron in empty space evolves as predicted by the Schrodinger equation or some similar equation of motion. And even if the electron doesn't interact with anything else in a given region, to understand what happens in a given region, you have to know its state throughout that region. For  example, an electron whose wave function is a Gaussian wave packet will spread out differently in space than an electron whose wave function is a plane wave. The resulting difference is physically measurable and the only available explanation is that the electron actually has the relevant evolving state.
When a child has been fathered and is on its way to become a self-conscious being, it can feel pain (in particular when being killed) and can have hope for happieness. Therefore it is evil to kill it in a society where murder is considered evil.

When a child is not created, there is no child, no pain no hope. In addition, the billions of possibe configurations of children that could develop by chance after one sexual act but obviously cannot all be realized, makes it irrelevant whether one more remains unrealized.
The schema is from:


https://en.wikipedia.org/wiki/Charles_Kay_Ogden Charles Key Ogden & https://en.wikipedia.org/wiki/I._A._Richards Ivor Armstrong Richards, https://books.google.it/books?id=EPeVQgAACAAJ The Meaning of Meaning : A Study of the Influence of Language upon Thought and of the Science of Symbolism (1923), page 11 (modified from http://plato.stanford.edu/entries/peirce-semiotics/ C.S.Peirce).


See page 9:


  Symbols direct and organize, record and communicate. In stating what they direct and organize, record and communicate we have to distinguish as always between Thoughts and Things.It is Thought (or reference) which is directed and organized, and it is also Thought which is recorded and communicated.


See page 11:


  Between a thought and a symbol causal relations holds. When we speak, the symbols we employ is caused partly by the reference we are making [...].
  
  Between the Thought and the Referent there is also a relation; nore or less direct [...].
  
  Between the symbol and the referent there is no relevant relation other than the indirect one, which consists in its being used by someone to stand for a referent.


And see page 102:


  It will be convenient here to define a true Symbol as distinghuised from a true Reference: A true symbol [set of words: proposition or sentence] = one which correctly records an adequate reference. [...] It correctly records an adequate reference when it will cause a similar reference to occur in a suitable interpreter [the subject partecipating to the process of communication]. It is false when it records an inadequate reference.
  
  It is often of great importance to distinguish between false anc incorrect propositions. An incorrect symbol is one which in a given universe of discourse causes in a suitable interpreter a refernce different from that symbolized in the speaker.

One option is that under https://philosophynow.org/issues/10/A_Gentle_Introduction_to_Structuralism_Postmodernism_And_All_That structuralism, where language is reducible to a formal system, language is 'dead'. From Paul Ricoeur:


      1. I wish to show that the type of intelligibility that is expressed in structuralism prevails in every case in which one can: (a) work on a corpus already constituted, finished, closed, and in that sense, dead; (b) establish inventories of elements and units; (c) place these elements or units in relations of opposition, preferably binary opposition; and (d) establish an algebra or combinatory system of these elements and opposed pairs.
       The aspect of language which lends itself to this inventory I will designate a language [langue]; the inventories and combinations which this language yields I will term taxonomies; and the model which governs the investigation I will call semiotics.
       2. I next wish to show that the very success of this undertaking entails (as a counterpart) an elimination from structural thinking of any understanding of the acts, operations, and processes that constitute discourse. Structuralism leads to thinking in an antinomic way about the relation between language and speech. I will make the sentence or utterance [énoncé] the pivot of this second investigation. I will call semantics the model which governs our understanding of the sentence. (http://rads.stackoverflow.com/amzn/click/0810104423 Conflict of Interpretations, 79)


Here are the two forms of 'deadness':


Language does not 'grow' to encompass anything new. It is fixed, unchanging as a rock.
Language no longer connects to reality in any systematic way.


The paradox is that 1. was supposed to lead to the opposite of 2. It was supposed to be possible to bring everything under a single system in an objective way which would remove all needs for that messy thing called 'interpretation'.

For a very different perspective of what language could be, I turn to David Braine:


       A right account of language is, I believe, the key to a right account of the nature of human understanding and thought, and thereby the key to a right understanding of human nature as a whole. Yet the whole theory of language is in considerable disorder, and my aim must therefore be first to seek to remedy this. This will take me into the heart of current debates in linguistics, philosophy, and psychology, and lead me to undertake an extended study of grammar. The effect will be to show how language exhibits the ultimate freedom of the human intellect and will from conformity with mechanically applicable rules and from limitations set by neurology. The brain plays a key role in the normal functioning of the human mind, but does not determine or shape linguistic understanding and thinking in the medium of words, as this develops through adaptation to and learning within a social external environment of other speakers and hearers, all within a setting of natural things. Such understanding and thinking have to be considered the activities not of brains or minds, but of human beings as such. In effect, the animal becomes intellectual in nature as soon as the brain reaches such malleability in its modes of functioning as to set no restrictions on the animal or person's intellectual operation, allowing autonomy to the mind's operation. (http://rads.stackoverflow.com/amzn/click/0813221749 Language and Human Understanding, 1)


That crucial step of malleability and freedom is antithetical to structuralism.
Yes, there are similarity relations where one can describe a common factor that underlies the similarity. A mother and a daughter may share same eye colors, nose shapes, etc (the analogy in the bbc episode seems indeed faulty). Wittgenstein's claim seems to be  that the picturing relation is just not one of these relations. There is no describable common factor to that similarly.

To inquire why this might be, let's take an example. How does the sentence 


  the cat is on the mat


picture the state of affairs where the cat is on the mat? What is the common structure between the sentence and the SOA? Well, it seems to be something like: 


  In both instances there are two objects, and a certain relation between them


The two objects being, on the one hand the cat and the mat, on the other hand the expressions "the cat" and "the mat".

We have just described the common structure, haven't we? It is just that the terms that we had to use in the description are "senseless" by Wittgenstein's lights.


  6.54 My propositions are elucidatory in this way: he who understands me finally recognizes them as senseless, when he has climbed out through them, on them, over them. (He must so to speak throw away the ladder, after he has climbed up on it.)


We have spoken about two objects. But there is really no way to count objects, as such. "Object" is not a true concept, but a pseudo-concept, a place-holder. As Bertrand Russell put it, in his Introduction to the Tractatus:


  Wittgenstein contends and, I think, rightly . . . that “object” is a pseudo-concept. To say “x is an object” is to say nothing. It follows from this that we cannot make such statements as “there are more than three objects in the world,” or “there are an infinite number of objects in the world.” Objects can only be mentioned in connexion with some definite property.


So the only way to describe the picturing relation is in "Tractarian" senseless terms; using pseudo-concepts such as "object". This does not amount, by Wittgenstein's lights,  to saying anything. It only amounts to showing something.
Just because something is false doesn't mean it's a "fallacy" - at least in a philosophical context. That word is reserved in philosophy, generally speaking, to mean logic that by virtue of it's deceptiveness is tragically bad.

In this case - the "Stupid Assumption" - I would say this could possibly be a fallacy called "begging the question," which essentially means:


  "...begging the question (petitio principii) can occur in a number of
  ways. One of them is when the proposition one is trying to establish
  is unwittingly assumed."


http://plato.stanford.edu/entries/logic-informal/ Stanford Reference

Or, simply by saying "This company is stupid" you are actually just stating the thing that you are trying to prove. It's circular reasoning. The other propositions stated (such as "they blocked my TV show") don't lead to a conclusion of "this company is stupid" - that has to be assumed for the conversation to even make sense. 

People are casually, conversationally, question-beggers all the time and it's not normally a problem. They're either not saying what they actually mean (I hate this company for preventing me from watching what i want) or the remaining logic is assumed / implied / obvious.

So, that's why I include the caveat of tragically bad in the context of philosophy. There's enough implied in the comment for you to assume it's someone's opinion. It's under circumstances where things are stated as facts that it becomes truly messy and where I feel like the term "fallacy" is appropriate. 

An example:


  "I should get paid for answering your question on philosophy stack
   exchange because people who are helpful deserve compensation."


Sneaky, factually stated, circular reasoning - completely inaccurate. Fallacies in philosophy are wolves in sheep's clothing. If we're too liberal with the term's usage we run the risk of conversationally pedantic.

Hope this helps. Happy hunting.

  Are there many ethical issues specific to forced psychiatric treatment of the schizophrenic?
  
  I'm guessing there's a few, if the failure of psychoanalysis to treat the schizophrenic represents a failure of the universality of its claims.


It's a bit unclear whether you realise that there is a distinction between psychiatry and psychoanalysis. Some people who called themselves psychoanalysts refused to coerce their patients, see e.g. - 'The Ethics of Psychoanalysis' by Thomas Szasz.

There are ethical issues in the treatment of schizophrenia. One issue is that lots of psychiatrists say it is an illness like diabetes, but they are wrong and are giving bad advice. There is no way to detect schizophrenia by finding a chemical structural abnormality in the body. A pathologist could never look at a corpse and conclude the person had schizophrenia. Schizophrenia is always diagnosed by looking at person's behaviour and complaints made about him by himself or by others. Schizophrenia is a label to legitimise coercing a person in the light of such complaints. Claiming schizophrenia is an illness is similar to counterfeiting:

http://www.independent.org/publications/tir/article.asp?a=557 http://www.independent.org/publications/tir/article.asp?a=557.

This obscures moral and personal problems and so makes it more difficult to solve such problems.

Another ethical problem with coercive treatment is that it amounts to imprisonment or forced drugging without trial. As such, coerced treatment is a grave threat to the rule of law. See "Law, Liberty and Psychiatry" by Thomas Szasz.
In a sense, that's right, but I think not in the way you propose.

The Golden Rule was proposed within https://www.biblegateway.com/passage/?search=Matthew%207&version=ESV a particular context. 


  "Judge not, that you be not judged. For with the judgment you pronounce you will be judged, and with the measure you use it will be measured to you. Why do you see the speck that is in your brother's eye, but do not notice the log that is in your own eye? Or how can you say to your brother, ‘Let me take the speck out of your eye,’ when there is the log in your own eye? You hypocrite, first take the log out of your own eye, and then you will see clearly to take the speck out of your brother's eye.
  
  "Do not give dogs what is holy, and do not throw your pearls before pigs, lest they trample them underfoot and turn to attack you.
  
  "Ask, and it will be given to you; seek, and you will find; knock, and it will be opened to you. For everyone who asks receives, and the one who seeks finds, and to the one who knocks it will be opened. Or which one of you, if his son asks him for bread, will give him a stone? Or if he asks for a fish, will give him a serpent? If you then, who are evil, know how to give good gifts to your children, how much more will your Father who is in heaven give good things to those who ask him!
  
  "So whatever you wish that others would do to you, do also to them, for this is the Law and the Prophets."


The word "So" implies that the Golden Rule is conditioned on those paragraphs above it. From that we see, the point is not that you're doing "unto others" specific things out of inflexibility. Read in its context, the point is that one should be patient and merciful, should help those in need, and give the help that is needed ("bread," "fish") and not spite ("stone", "serpent"). 

As often happens with common Bible phrases, they are pulled out of their context as they are echoed. One may agree or disagree with the overall point, but it's important to understand the context of the saying before the point can be understood in order to accept or reject it.
I am actually an anarchist but to answer your question I would say yes. A government is an artifice that is maintained to protect its citizens and enforce its borders. That is its role/job. There are some political campaigns somewhat based on foreign aid but they tend to be more private as the general public would not like their tax money leaving the country they feel they are part of. Plus, most foreign aid is usually allocated in advance. Any sudden emergency aid is discretionary.  
This is actually quite a simple argument.

A - The preferences of the citizens with regards to the severity of the sentence and the severity of the crime are neglected.

B - Vigilantism by the citizens occurs.

C - The law and order of the state are threatened.

D - Anarchy and injustice ensues.

Premise 1 isn't actually necessary in your argument. Let's see:

Argument


Whatever
A → B
B → C
C → D


Therefore: A → D

Formulated like this, the argument is valid. 



Note that it doesn't matter what or who neglects the citizens' preferences (with regards to ...), the the conclusion still follows, specifically because of premise 2.

  if something is a social construction, then this thing is not too legitimate.


While people no doubt make this type of argument, I don't think this is a good argument. A better argument about social constructions says that people make certain category errors when they treat social constructions as objectively true or concrete entities. For example, it has been established that race is a social construction and not a natural kind of biology (i.e., races are not actually genetic groups). Social constructions are perfectly legitimate when recognized as such, and indeed they have proved extremely useful. There's a reason why social constructions exist and are so prevalent. Some social constructions are useless or even harmful of course; as an overall category they are neutral with respect to utility and morality (not unlike the category of "ideas"). I think the key point from philosophers who are critical of constructions like race and gender is that we are free to do away with them because they are constructions; so if they are harmful, we should do away with them. "Employed person" is also a construction, but it's more useful than it is harmful so no one is terribly concerned with it (except perhaps for some Marxists).

Is social construction a social construction? I think the answer is yes. That means we should be careful to recognize it as such, but it doesn't render the concept illegitimate or useless per se. There may be bad arguments that rely on social construction not being a social construction. A claim that all constructions are bad or useless (similar to your example argument) would be undermining itself. However, consider this not-so-bad argument:


  
  Race is a social construction.
  Social constructions are not necessary; they can be done away with.
  The concept of race can be done away with. (1,2)
  The concept of race is harmful to people.
  Harmful ideas should be done away with if they can be.
  We should do away with race if we can. (4,5)
  
  
  We should do away with the concept of race. (3,6)
  


This argument is perfectly valid and does not depend on social construction not being a social construction. 
Social contracts are negotiated. They can be wrong, but they still require something other than simple contradiction before you can expect them to go away. Forego all the benefits of a government, including the right to live safey within the borders defended by it, or address its rules with something resembling respect.

I support the Rainbow Family, who accept that they can be free of taxes as long as they accept no benefits, including safety, promised by our government. So they cannot presume the government will not violently attack them.  (And I admire them greatly for only 'attacking' back in respectful ways.)

I also support people like Priests and radical Quakers, who own nothing, and therefore are not taxed.

But I don't support pretentious Libertarians who want everything both ways.

This argument is parallel to the notion that rent is a crime.  You don't get ownership, and you are expected to pay money, so rent is theft?  No.  You get limited protection and rights for your money.  If you don't pay your rent, you are expected to leave.

(The right to ownership that is conveyed upon the lessor is itself a government function secured through taxation.  Without government action, the landlord would have to hold his property by force.  We are so used to the trappings of government-established peace that this is not obvious.  But it is proven by the existence of popular revolutions that simply reassign property rights.  So the existence of this fiction of 'ownership' does not make any difference between the two arguments.  If rent is not theft only because it is secured through government theft, then rent is theft.)

Yes, it is somehow insane that people own land or buildings at all, given that everyone must live somewhere and not everyone has access to ownership.  But it is a negotiated part of our social contract, and your parents signed on.  Some part of all social contracts will fail to make sense.  And obviously, we can change them, but only in one of two ways 1) with the leverage you can get from others subject to the same system, or 2) by imposing a different social contract wholesale from outside.
This problem is broader than just the issue of organ donation. You might do anything that helps someone, or saves their life, and it might happen that they go on to do wicked things. The story goes that when Hitler was a boy he fell into a river and was drowning and a man rescued him. Is the man responsible for the bad things that followed? Clearly not. 

Explaining why not will involve different answers depending on your preferred moral theory. A consequentialist might answer that while you are potentially responsible for the actions of others, this can only extend to what is reasonably forseeable. The man who rescued Hitler could hardly be in a position to forsee what would follow. To an absolutist, you are responsible only for your own actions, not those of other people. You may calculate the likely consequences of your actions, but when those consequences involve the will of other people, your moral responsibility does not extend to them. In the Kantian version of deontology, what makes the donation morally right is your motive for doing it, irrespective of the consequences. If you act out of duty to help others, then this is what makes it good.  A proponent of virtue ethics might say that helping people and donating your organs for use after your death is good because it exemplifies the virtue of compassion, and in the absence of any contrary or competing virtue, it is therefore a good thing to do. 
A compound phrase like married bachelors is not a logical contradiction](https://en.wikipedia.org/wiki/Contradiction https://en.wikipedia.org/wiki/Contradiction), it is a https://www.oxforddictionaries.com/definition/english[/contradiction-in-terms contradiction-in-terms.

In this case married and bachelors are sub-terms. Combining the sub-terms gives a contradiction-in-terms. By the definition of the sub-terms -- that is to say, going by what it means for something to be a married thing and going by what it means for something to be a bachelor -- one can't have a married bachelor. If the meanings of the sub-terms of a compound term contradict each other then it is fair to say that the compound term is meaningless, one can talk about such things using natural language but one can never actually construct or instantiate such things in the “real world”. Another example would be spherical cubes.

A logical contradiction a "logical incompatibility between two or more propositions". This is the given definition of a logical contradiction. None of the examples you gave are logical contradictions. But if you had given any examples of a logical contradiction we could be assured that they have one and only representation†, that of ⊥ (falsum).

I think it is fair to say that a contradiction-in-terms is semantically meaningless. Let's analyse your assertions A)..F) and your questions G)..L) in light of this.

Assuming we are not talking about fictional scenarios -- I could imagine some kind of Woody Allen surreal comedy playing with these kinds of entities :) -- none of A) through F) could articulate actual states of affairs.

Something like G) Do any married bachelors exist? does make a kind of sense. It could be answered with no. And this hinges on a fine distinction. Not because there happen to be no married bachelors at this point in time and/or in this particular place and/or in a certain way -- that is to say, that there could be but there happen in this instance not to be -- but because there could never be by virtue of the semantic meaninglessness of the compound term.

Something like H) Do 95% of married bachelors live in Maryland? is more nonsensical because we're not merely speaking about existence now but sort of assuming the existence and asking for additional information. Valid response are both no and the question is meaningless. It could be answered like, “The answer is no but not because of anything to do with quirky facts about percentages and Maryland but because it doesn't even make sense to talk about married bachelors so the whole question is a quite meaningless.” I) and J) amount to the same kind of thing as H).

For K) answer is yes for the reasons given for G).

The answer to L) Is it necessarily false that any married bachelors exist? is a simple yes because that necessarily acknowledges the contradiction-in-terms.

†


  In classical logic, particularly in propositional and first-order logic, a proposition φ is a contradiction if and only if φ ⊢ ⊥ . Since for contradictory φ it is true that ⊢ φ → ψ for all ψ (because ⊥ → ψ ), one may prove any proposition from a set of axioms which contains contradictions. This is called the "principle of explosion" or "ex falso quodlibet" ("from falsity, whatever you like").
  
  In a complete logic, a formula is contradictory if and only if it is unsatisfiable.

Looking at the Nicomachean Ethics the unity of virtues appears to relate directly to the individual based on the repeated phrasing of who's virtue or duty is being described:

(Below emphasis is mine)


  Givers, too, are called liberal; but those who do not take are not
  praised for liberality but rather for justice; while those who take
  are hardly praised at all. And the liberal are the most of the most
  loved of all virtuous characters, since they are useful; and this
  depends on their giving.  Now virtuous actions are noble and done for
  the sake of the noble. Therefore the liberal man, like other virtuous
  men, will give for the sake of the noble, and rightly; for he will
  give to the people the right amounts...(See Book 0, 0, p. 54 of Ref)


... and then


  If, then as is asserted, the virtues are voluntary (for we are
  ourselves somehow partly responsible for our own states of character,
  and it is by being persons of a certain kind that we assume the end be
  so), the vices also will be voluntary; for the same is true of them. (See Book 4, 1, p. 43 of Ref)


.. and the duty of virtue in society


  ... but think it their duty 'to give no pain to the people they meet';
  while those who on the contrary, oppose everything and do not care a
  whit about giving pain are called churlish and contentious. (Book 4, 6
  p. 66 of Ref)


.. on friendship


  What is evil neither nor should be loved; for it is not the one's duty
  to be a lover of evil, nor to become like what is bad; and we have
  said that like is dear like. (Book 9, 3, p. 149 of Ref)


Reference


https://socserv2.socsci.mcmaster.ca/econ/ugcm/3ll3/aristotle/Ethics.pdf Text 

If saving the species is to be done at all costs, then the scenario you present does't really present a dilemma. Even after the last male Gorilla is killed, certain steps can be taken to preserve the genetic material of the Gorilla. His sperm can be collected and used to impregnate the remaining females, and his DNA can be preserved for cloning purposes. 

The real dilemmas arise at the meso- and macro-ecological levels: Consider a lost tribe on a Pacific Island whose entire culture and way of life centers around hunting a single species of whale, and that whale species is about to go extinct. The tribe has only a few 100 individuals left and their whole culture and language are in danger of being forgotten. Who do we save? The tribe or the whale species? Keep in mind that the individual members of the tribe are safe. At any moment they can simply be integrated into another society. It is there culture and way of life, not the individual members who are in danger. 
The term syllogism is due to Aristotle (originally sullogismos).

Aristotle defined it as:


  an argument (logos) in which, certain things having been laid down, something different from what has been laid down follows of necessity because these things are so. (http://plato.stanford.edu/entries/logic-ancient/ Source)


So by Aristotle's definition, all syllogisms are valid.

But consider this. In Aristotle's logic there is a valid form of syllogism called Darapti:

All C's are A's
All C's are B's
∴ Some A's are B's


But this is invalid in modern logic. (When there are no A's, B's or C's, the premises are true and the conclusion false.)

So modern usage might differ. The http://plato.stanford.edu/entries/aristotle-logic/#AriDedModValArg SEP entry on Aristotle's Logic says this:


  In modern usage, ‘syllogism’ means an argument of a very specific form. Moreover, modern usage distinguishes between valid syllogisms and invalid syllogisms. The second of these is inconsistent with Aristotle’s use: since he defines a sullogismos as an argument in which the conclusion results of necessity from the premise.

For Kant, if the action has no moral worth, then it is morally neutral.

In your example, as James Kingsbery pointed out, there may be hidden duties like that of honoring your parents, which would make it so you have an obvious right thing to do. But if we take the example at face value, in such a case you are merely acting on your desires, and your action therefore has no moral dimension.

In this connection it may be worth remembering that actions that are performed merely from desire are not actually free actions for Kant. So in choosing your ice cream you are just like a rock rolling down a hill - totally causally determined and without any moral features. 

Final addendum points: Evil actions are also conditioned by desire, but in those cases one's desires actually contradict the moral law, as opposed to the ice cream case, where they do not impact it at all. Also, I should say that this answer is focused on Kant, and may not generalize to other deontologists. Thank you.
It is not a logical argument or "syllogism" whatever; it is a piece of rethoric, that runs as follows:

i) if some one, on a similar occasion, prayed and entreated the judges with many tears, and produces his children in court, ... he is using the "appeal to pity";

ii) myself (Socrates) "I am a man, and like other men, a creature of flesh and blood, and I have a family, yes, and sons, O Athenians, three in number, one almost a man, and two others who are still young; and yet I will not bring any of them hither in order to petition you for an acquittal."

In denying to having recourse to the "appeal to pity", Socrates is doing exactly this.
As you can see in the post https://philosophy.stackexchange.com/questions/35866/what-is-the-origin-of-the-truth-table-in-logic What is the origin of the truth table in logic, the truth-functional definition of material implication (or conditional) was codified by ancient Greek http://plato.stanford.edu/entries/logic-ancient/#SynSemComPro Stoic logicians with the so-called philonian conditional.

In modern time, it was "re-discovered" by Gottlob Frege in his https://en.wikipedia.org/wiki/Begriffsschrift Begriffsschrift (1879).

It corresponds to point (1) of your analysis:


  a conditional with true antecedent and false consequent is false; in all other cases it is true.   


Your case (2) is correctly analyzed as:


  ∀x (Perf_Sq(x) → ¬ Prime(x));


it is a generalization of a conditional (in the past called "formal implication"; see the post: https://philosophy.stackexchange.com/questions/22376/%e2%86%92-is-the-symbol-for-material-implication-is-there-such-a-thing-as-immaterial is there an "immaterial" implication ?).



For (3), you have to consider that the relation of http://plato.stanford.edu/entries/logical-consequence/ logical consequnce (or entailment) is what defines:


  A good argument is one whose conclusions follow from its premises; its conclusions are consequences of its premises.


It is a relation that holds between a set Γ of sentences (the premises) and a sentence A (the conclusion) symbolized with:


  Γ ⊨ A


and "discovered by http://plato.stanford.edu/entries/aristotle-logic/ Aristotle with his notion of deduction (sullogismos):


  
    A deduction is speech (logos) in which, certain things having been supposed, something different from those supposed results of necessity because of their being so. (Prior Analytics I.2, 24b18–20)
  
  
  Each of the “things supposed” is a premise of the argument, and what “results of necessity” is the conclusion.


The relation between logical consequence and conditional is:


  A ⊨ B iff ⊨ A → B,


but we have to consider that the two are not the same: for example, the definition of logical consequence does not change also when the language has no conditional (→) connective.
You did not specify a formal meaning for your terminology, so we have to do some guesswork.  Believe it or not, the precise meaning of the word "in" matters here.  When one starts exploring the largest (or smallest) limits of a way of thinking, as you are, the funniest words become surprisingly important.

One valid meaning is to refer to space in either a Newtonian sense, or a relativistic sense such as spacetime.  In these cases, space is not actually a "thing."  Rather it is the domain upon which the equations of physics are applied.  If one did not desire to calculate the trajectory of objects, one would not need a spacetime to make sense of the universe.

Another valid meaning is mathematical.  For me, your question immediately conjures up visions of set theory.  I can treat all of your propositions as set theoretic phrases.  "We are in a house" could be we ∈ house, or "'we' is an element of 'house'".  These can continue forth with house ∈ earth* and house ∈ space, but what is space an element of?  Can we write space ∈ ??? if we fill in the question marks with the right thing?

Potentially, or potentially not.  We can always construct a new set which contains space, and everything in it.  We could even give it a name, like "superspace" or "megaspace" or "user17325's thing bigger than space itself"  However, this almost certainly not the direction your question was going.  You want to know what is bigger than "everything."

Well, that gets to be complicated.  In set theory, there is the concept of U, the "universe of all sets."  However, some complications arise here.  U is not actually a set, itself.  It is a category.  There is nothing that "contains" U because U cannot be an element in any set.  Sets can only contain other sets (in most set theories, anyways).  Since U is a category, it cannot be contained in a set.

This sort of distinction becomes important because there's all sorts of paradoxes that arise if one tries to presume U is a set.  The most famous is https://en.wikipedia.org/wiki/Russell%27s_paradox Russel's Paradox.  Russel's paradox arises when one seeks to use a naive assumption that all definable collections are sets.  If this were true, U would be a set, because it is defined to be "a collection of everything."  However, this definition creates a paradox:

Let's define two types of sets: tail chasing sets and normal sets.  A tail chasing as a set which contains itself, like a dog chasing its tail.  Clearly U would have to be such a tail chasing set, if it is indeed a set itself.  Normal sets are those which do not contain themselves.  These are the well behaved sets we're used to, such as {1, 2, 3} {{12}, {3, 4}}.

Since we defined these two types, we can define two sets by them: the set of tail chasing sets, and the set of normal sets.  But which set is the set of normal sets part of?  Is it tail chasing, or normal?  If it is normal, then it must contain itself, thus it is tail chasing.  If it is tail chasing, then it does not contain itself, thus it must be normal.  This is a contradiction, and it is this contradiction that shows that our assumptions are invalid.  As it is, the "collection of all normal sets," which is clearly either U or a subset of U, is not a set at all.  They coined the word "class" to describe these concepts.  U is a class, not a set.

So this is where your question can lead, using modern science and math.  One can take other approaches as well, if desired.  The Chinese have the concept of the Dao.  Everything is part of the dao.  However, if one asks what the dao is a part of, the question falls apart.  It becomes hard to construct phrases related to your question.  The most common response I have seen is "the dao is," and it is left at that.

It also could be https://en.wikipedia.org/wiki/Turtles_all_the_way_down turtles all the way down.

* I'm handwaving away some technicalities in set theory.  In the strictest sense, if a ∈ b and b ∈ c does not entail a ∈ c in all cases.  However it is possible to construct my sets for "we" "house" "earth" and "space" such that this relationship does in fact hold.  For the exact construction, see https://en.wikipedia.org/wiki/Ordinal_number#Von_Neumann_definition_of_ordinals von Neuman ordinals as a well respected formal construction that uses this approach.  This distinction will only be important if you continue exploring set theory, and is not really required to understand the rest of the answer.
From at least one resource's interpretation of Cosmism, it sounds like that just might be what you are looking for:


  Cosmism is an existential philosophical orientation that sees the survival of mankind and of the individual as part of humanity's "common task". The migration of humans into space is seen as inevitable, since it is essential for humanity's long-term survival.


-- https://sites.google.com/a/cosmism.info/cosmism/what-is-cosmism What is Cosmism?

So, that is your answer from a philosophical perspective. There could be one or more new religious movements that espouse these views, but I haven't found any in a cursory search.



It is also interesting to note that https://en.wikipedia.org/wiki/Survivalism survivalism in general anticipates a coming disaster that will affect humanity, and survivalists usually try to ready themselves to survive whatever event(s) may come. In this sense, cosmism could be seen as a variation of survivalism. To some extent, https://en.wikipedia.org/wiki/Survivalism#Other_groups_related_to_survivalism anarcho-primitivism could be seen as being at the opposing end of a spectrum of survivalist views, where anarcho-primitivism says we need to return to a more primitive* state to survive and cosmism says we need to advance.

* "Primitive" here is not meant as a value judgment, just in the literal sense of a more original state.
For Kant, morality only applies to rational beings. At some points, he will use the word "humanity" as a synonym. Thus, if we look at the second group of formulations of the categorical imperative we find:


  Act in such a way that you treat humanity, whether in your own person or in the person of any other, never merely as a means to an end, but always at the same time as an end. (Groundwork of the Metaphysics of Morals)


For Kant, the only known rational beings are humans. God is posited as a rational beings but not known. Angels occupy a weird position as "pure limited rational being" which is absolutely never fleshed out.

The background behind this is that Kant believes the material world and animals are largely deterministic in terms of their behavior. They are limited to their natures. In contrast, human nature involves having rational and a free will that can choose based on reason. As a consequence, our nature is precisely to not be wholly determined by our biology (a point shared by Aristotle).

So, yes, spiders are not for Kant moral creatures and thus not bound to the categorical imperative. They can do no wrong (this isn't to say that they can't be involved in wrong -- like using spiders to torture people, but that they themselves are not moral agents there).
To quote Bill Clinton, it depends on what the definition of "is" is.  If "is" is meant to express strict equivalence, the statement is false.  If the race is between major party candidates A and B and minor party candidates x and y, a vote for x is not exactly the same thing as a vote for B, for at least the following reasons:  


The standing and legitimacy (and potentially funding) of x's party
may depend on total number of votes, even if x does not win. 
A vote for x may influence A or B to adopt elements of x's platform. 
A vote for x may feel like a more morally correct choice to voter v, which
in turn might change v's relationship to the political system. 
Being associated with x's party might have a personal impact on v (it might give v a different perspective).
Even if it is unlikely, x might actually win if enough voters make the choice to vote for x.


If "is" is meant only to express the direct results of the election in terms of the eventual winner, however, it might be true that the impact of switching from candidate A to x is similar to switching from A to B.  In the aggregate, when x draws largely or exclusively from candidate A, voting for x accrues to the benefit of B.  Of course, however, the numerical impact is only half as large, because a vote is subtracted from A but not added to B, so even here, the accuracy of the statement is questionable.  

Considered as an argument, if the intent is to convince the audience that voting for x is exactly the same as voting for B, this commits the logical fallacy called "https://en.wikipedia.org/wiki/False_equivalence false equivalence."  More charitably, however, it might be considered as an exaggeration of a valid underlying point for rhetorical effect (in the case that the strict truth of the claim is not implied).  In any case, the more honest (and more sound) claim would be "voting for x instead of A increases B's chances of winning."
The "quick" answer is no. However, because you used the word "every feeling", it will take only one feeling without thought, to negate the premise.
Typically, thinking is a capability associated with "higher intelligence" forms.  However, "lower intelligence" forms do have feelings, even though they are incapable of thinking, therefore, there is "feeling without thinking."      
I don't think that the following extract from Whiteheads Science & The Modern World constitutes an answer, but it might help; he quotes Lecky, History of European Morals


  The Roman legislation was in a twofold manner the child of philosophy. It was in the first place formed upon the philosophical model, for instead of being a mere empirical system adjusted to the existing requirements of society, it laid down abstract principles of right to which it endeavoured to conform, and in the next place these principles were directly borrowed from Stoicism.


This suggests to me, at least that a system of Right  are principles of Right; that is they have been formed with a specific philosophical end in sight; a modern example, would be freedom of speech whose motivating philosophical principle, amongst others, would be freedom.

I take this is what Foucault is possibly describing as a 'legitimacy to be established'; but his perspective is not legitimacy - even when it is legitimate - but 'subjugation'.
yes there is a dichotomy.  But a dichotomy is not a contradiction.  My cat is  a unique individual; she is also a member of a species. Being a member of that species is precisely what makes her a cat. But she remains a unique individual.  Similarly if prosociality is what makes us human, that does not mean we are not unique individuals.
A name that aptly describes the person or thing it refers to is an https://en.wikipedia.org/wiki/Aptronym aptronym; one which describes the opposite attributes is an https://en.wikipedia.org/wiki/Aptronym#Inaptronyms inaptronym.

There is a hypothesis that when a person is named a certain way, they tend to behave in a way that fits their name; this is called https://en.wikipedia.org/wiki/Nominative_determinism nominative determinism. There does not seem to be a term describing the opposite idea; if you like, you might call it “nominative indeterminism”.
Merleau-Ponty is a phenomenologist so his received view is "essence" in a Husserlian sense, as the ideal core of an intentional object, see https://philosophy.stackexchange.com/questions/33463/what-does-husserl-mean-by-essences/33470#33470 What does Husserl mean by essences? However, he is also an existentialist, so Husserl's essentialism is revised along the lines of "existence precedes essence", see Bauer's http://transmissiononline.org/issue/awareness-as-existingness/article/phenomenology-of-the-essence-and-appearance-in-merleau-ponty Phenomenology of the Essence and Appearance in Merleau Ponty.

"The classical essence as Husserl’s description as being constituted by the mind in noetic noematic frame is replaced by contemplative awareness and non conceptual thought... Essence can be the very presence of the thing appearing. There can be the representation of the thing and there can be the appearing of thing. Representation is only one form of knowing. Representation is not experiencing the essence of what is." 

So in the OP quoted passage the essence of time is seen as its direct presence to self, non-interpretive self-awareness, the "archetype of the relationship of self to self". In this view Merleau-Ponty is indebted to Bergson's analysis of time as qualitative duration opposed to the mechanical time of physics, which he saw as contaminated by spatial notions. In this regard Bergson's conception is quite close to Husserl's "time-consciousness", see https://www.academia.edu/604832/Husserl_and_bergson_on_time_and_consciousness Husserl and Bergson on Time and Consciousness. However, Husserl interpreted such self-aware presence as a "noetic" (experiential) component of the intentional act rather than as a part of its "noema" (purified object, essence), unlike Merleau-Ponty.
You seem to mix up something here. Auto-nomos could be translated to self-ruling, i.e. giving oneself laws of one's own. And Kant refers to autonomy of the will.

Autonomy has therefore nothing to do with the source of ends, as all ends but the highest good are heteronomous (see Critique of Practical Reason, Ak. 5:109-110). Autonomy is to give the will a law (rule) that is not determined by external factors or ends:


  Autonomy of the will is the characteristic of the will by which it is a law
  to itself (independently of any characteristic of the objects of willing). (Groundwork for the Metaphysics of Morals, Ak. 4:440)


The law the will imposes on itself, the categorical imperative, is a law that determines how to choose maxims (not ends!), which is made quite clear in the definition of heteronomy, that also endorses the points i made just before:


  If it is in anything other than the ﬁtness of its maxims for its own universal
  legislation, hence if - as it goes beyond itself - it is in a characteristic of
  any of its objects that the will seeks the law that is to determine it, the
  outcome is always heteronomy. (Groundwork for the Metaphysics of Morals, Ak. 4:441)


Therefore, and that's the point in all of the Groundwork, if autonomy can be found, it can only be found as a case of acting where the maxim of the act is chosen by the categorical imperative as the law the will, because the categorical imperative as such is that abstract that it cannot contain empirical contents that makes it heteronomous (see Groundwork, Ak. 4:402 and 4:419-21). It is pure and can therefore only be imposed by reason alone.

Or to put it in another way: Every act is autonomous that is done only because its maxim did pass the test by the categorical imperative. That has to be the primary reason behind acting, not achieving an end. Because then, the law of the will is a law that is given to the will by itself (in the form of pure practical reason).

Edit because of the edit of the question

For Kant, you should destinguish between ethics and morals. Moral choices are indeed only those in which you stand back and at least ask yourself what the right thing to do would be (see e.g. Groundwork, Ak. 4:421-3). And all Kant is saying is that if you ask this by the means of applying the categorical imperative and act accordingly because of the positive outcome, this is autonomy and morals proper (everything else is pretty much selfishness/ethics in disguise). Doing this in every  moment a decision is needed would be totally impractical, wouldn't it?

Now, of course not every act is autonomous or even morally relevant. Happiness is a necessary end of all humans (see Groundwork, Ak. 4:415), but nevertheless the pursuit of happiness and the operations of reason at work there are clearly destinguished from morals (see Ak. 4:415-7). This is ethics in the narrower sense.

To make it even more complicated: In the Critique of Practical Reason ethics are in some sense reintegrated into morals by the concept of the highest good, i.e. persuit of moral perfection is necessarily linked to the fulfillment of happiness. But not within mortal life. That's where Kant's take on religion is founded in, but this would take us too far from the original question by now.

Regarding the feeling that is linked to morality as something different from pleasure, see for an extended answer https://philosophy.stackexchange.com/questions/3499/does-a-moral-act-have-to-be-necessarily-beautiful/35835#35835 this answer of mine to another question. In short: For moral acts, we feel respect, a feeling that is self-imposed (by our practical reason).
Marx was reacting to the taking of Vienna by "Croats" subservient to the monarchy. What followed was rape, murder, looting and arson to extraordinary proportions while the representatives of the bourgeoisie, the "German National Assembly" looked on. Marx advocated a revolutionary terror aimed at the monarchist fascists who had taken Austria to impose German rule on it and who were massacring anyone who opposed them. The terrible repression began in Nov 1848, following a working class and peasant uprising the month before. Whereas in France the bourgeoisie had opposed and overthrown the monarchy, in Germany the bourgeoisie timidly accepted the repression of the people and would not lift a finger. That is why Marx said the workers should not be passive to this but should fight back with all means at their disposal. He was advocating an armed uprising of Austrian patriots, workers and peasants against the German/Croat annexation and massacre. one can call him a 'terrorist' and indulge in moral outrage if it suits one. Words, after all, are only words. 
https://www.marxists.org/archive/marx/works/1848/11/06.htm Here is a link to the whole Marx article
First, we need to distinguish between the https://en.wikipedia.org/wiki/Proximate_and_ultimate_causation proximate and mediate causes, because we would call throwing a ball to be the (mediate) cause of a broken window even though the throwing hand never came into contact with the window. But it is indeed common to assume that proximate causes, the elementary links of causal chains, do involve spatio-temporal proximity. Cognitive psychologists believe that the concept of cause initially forms in infants from observing that making something move requires them to contact it, and then is reinforced by observing motion transfer through contact. So the "intuition" behind causation does presuppose the spatio-temporal nearness, a.k.a. contiguity, and even a material connection, contact. Here is http://www.tandfonline.com/doi/pdf/10.1080/09515080801980179 Mandler in On the Birth and Growth of Concepts:


  We know that infants perceptually differentiate self-starting from starting with contact at least by 6 months of age (Leslie, 1982)... Pace Hume’s belief that we cannot see causality, there is evidence that we can, or at least we see one of the major components of a commonly experienced causal relation, namely, the transfer of motion from one moving object to another. White (1988) proposed that the powerful sense of causality perceived when, say, a billiard ball strikes another, comes from the short duration of iconic storage (about 250 msec).
  
  The iconic store is a large-capacity sensory store that holds visual information prior to attentive processing; it is continuously refreshed, enabling the temporal integration, which makes us see motion as continuous... We see a causal relation when a conflict exists between two types of continuity cues. Spatial discontinuity between two objects says there are two objects, whereas continuous motion suggests there is only one. The conflict is resolved by perceiving the causal sequence as the transfer of motion from one object to the other.


However, the transfer of motion example rather confirms http://www.iep.utm.edu/hume-cau Hume's view that what we directly observe is not the "causing", but one type of event following another with regularity. Thus, causation plays a role similar to theoretical entities, that of unifying experiences, except it formed even before scientific practice made introduction of such entities systematic and deliberate. Hume thought that it is formed through a subconscious psychological mechanism, "the law of association". Once this "theoretization" is accomplished the presence or absence of causation is determined based on holistic considerations of forming a coherent picture of environment rather than just induction on individual observations. Some previously admitted regularities, such as thunder following lightning, are reclassified as having common cause rather than directly causative, others are dismissed as coincidences or superstitions. At this stage proximity still remains a requirement for proximate causes.

In science, however, the notion of causation is further subsumed under  instantiation of a general predictive law, or a combination thereof, classically expressed by various differential equations. For example, we can say that one ball causes another one to move upon contact because that is what the laws of mechanics predict. At this point it becomes logically possible to have proximate causes without spatio-temporal contact, and indeed Newton introduced https://en.wikipedia.org/wiki/Action_at_a_distance#Newton gravitational action at a distance that ostensibly did not require it. This was however in sharp conflict with the pre-existent everyday notion of proximate cause, and the following passage from his letter to Bentley shows just how conflicted he was about it before acquiescing to https://en.wikipedia.org/wiki/Hypotheses_non_fingo "hypotheses non fingo" [I feign no hypotheses] stance:


  It is inconceivable that inanimate Matter should, without the Mediation of something else, which is not material, operate upon, and affect other matter without mutual Contact... That Gravity should be innate, inherent and essential to Matter... is to me so great an Absurdity that I believe no Man who has in philosophical Matters a competent Faculty of thinking can ever fall into it.Gravity must be caused by an Agent acting constantly according to certain laws; but whether this Agent be material or immaterial, I have left to the Consideration of my readers.

This is a potentially interesting question. There's two things that need to be cleared up before we can get to it thought.

First, there's a good deal of fuzziness about what "utilitarian" means. Its classical referent is the views of Jeremy Bentham and John Stuart Mill, but clearly you can't be meaning that here. On their views, it's only human pleasure and suffering that matters. But even within their views, there is variation. Bentham took the view that all types of pleasure are equal in kind (just differing in degree). Mill took the position that there we baser / nobler pleasures and that somehow discerning people who prefer the latter and that they have a superior character.

This confusion can be unwrapped though in that sometimes "utilitarianism" is used in contemporary philosophy as a synonym for the entire family of values that think the good can be calculated and then maximized. So I'll assume that's what you mean by the term.

This leads to the second issue. Given the broader meaning, it is not by any means clear that utilitarians must be committed to the belief that all life (of any sort had value). This seems to be an implication in the views of several contemporary utilitarians -- for instance Peter Singer. Singer, in fact, specifically argues that we should treat problems in Burma as on par with problems that are local in "Famine Affluence and Morality."

But pushing that aside, it's completely plausible to come up with a consequentialist theory that accepts that proximity matters (i.e. that makes it okay to care about things that are near me more than things that are further away). What seems like it might be lost here is that now your ethical theory has a type of "egoistic" component in that proximity to you becomes a condition for considering what's going on. This need not be a robust egoism, however, because you could (though this would not be required) maintain that each person should incorporate proximity into their valuations of happiness.

A non-utilitarian variation of this serves as an objection to both "utilitarian" and deontological accounts. Here, I'm thinking specifically Bernard Williams who in the volume Moral Luck has an essay on the problem with any moral theory that can't present a moral difference between saving someone you know versus a random person. (He also has a reductio against utilitarianism in that it doesn't seem like we get our moral values from it so much as we try to produce a utilitarian calculus that produces our moral values).

Also you can find semi-calculative moral theories that value other things that can incorporate distance. 
The argument is deductively valid (though it rests on an assumption about the future).

You can reformulate it as a basic modus ponens as follows:


If I water the plants assiduously, my garden will be healthy
I will water the plants assiduously


Therefore,
3. My garden will be healthy
Truth tables are used to define what validity is. For that reason, they cannot fail to be reliable indicators of validity.

There may be cases in which what seems intuitively invalid comes out as valid according to the truth table. In these cases, you will likely want to examine your intuitions or make some clarifications as to your language.

For example, this might happen if you treat a counterfactual conditional in the same way as a normal conditional. In this case, people considered the matter and decided that the intuitive problem was due to the counterfactual nature of the conditional, not to a problem with the truth table. 

Philosophers are very reluctant to conclude that truth tables are not guides to validity because they are definitional of what validity is, so challenging them would require a wholly new conception of validity (not that there aren't such conceptions around, but I assume that's outside the scope of this question).
Contrary to Jobermark, I believe Kant provides a very straightforward answer to your dilemma. Kant's based https://en.wikipedia.org/wiki/Categorical_imperative his categorical imperative on one question https://en.wikipedia.org/wiki/Universalizability "Is it universalizable?", and in your case the clear answer is vote for what you think is right for everybody (presumably in your case that is party B, since it would help more people than party A). Here's why:


Based on your reasoning on why to vote for party A, a white person, even though he is not personally racist or bigoted in any way whatsoever, is still justified in voting for a white supremacist party, since after all they would advance his own interests and those of his family at the expense of those of others. This reason for voting is not acceptable, and so per Kant, any voting for special interests at the expense of general interests should be avoided, since if it were universalized, the above described voting for white supremacists scenario would be acceptable. 


But then, you might ask, what about situations where there is an inevitable conflict of interest between different interests in one society? In particular, what about situations where the interests of a small group are inherently in conflict of those of the majority. How does one take into account such situations, yet still allow for a universal ethics of voting? 

An answer was provided by Harvard political philosopher https://en.wikipedia.org/wiki/John_Rawls John Rawls, with his concept https://en.wikipedia.org/wiki/Veil_of_ignorance of the veil of ignorance, also explained in his idea of the https://en.wikipedia.org/wiki/Original_position original position. As described http://radio-weblogs.com/0104634/stories/2002/07/18/theVeilOfIgnorance.html in this blog: 


  Imagine that you have set for yourself the task of developing a totally new social contract for today's society. How could you do so fairly? Although you could never actually eliminate all of your personal biases and prejudices, you would need to take steps at least to minimize them. Rawls suggests that you imagine yourself in an original position behind a veil of ignorance . Behind this veil, you know nothing of yourself and your natural abilities, or your position in society. You know nothing of your sex, race, nationality, or individual tastes. Behind such a veil of ignorance all individuals are simply specified as rational, free, and morally equal beings. You do know that in the "real world", however, there will be a wide variety in the natural distribution of natural assets and abilities, and that there will be differences of sex, race, and culture that will distinguish groups of people from each other.


John Rawls, basically asks to vote as if we didn't know anything about our place in society, i.e to vote from behind a veil of ignorance, when deciding what the fairest political system or distribution of resources should be. 

Note that John Rawls is not a defender of income redistribution or total equality in a given society. Rawls finds inequality in a society perfectly acceptable, as long as it benefits everyone, including those least advantaged. http://plato.stanford.edu/entries/rawls/#TwoPriJusFai See Rawls's second principle of justice as fairness. For example, it is acceptable, and maybe even preferable for doctors to be paid more money than most other professions, since this would guarantee that talented people would choose to be doctors, therefore increasing the well being and health of society as whole.  



In response to the comment on utilitarianism

The standard text book definition of http://plato.stanford.edu/entries/consequentialism/#ClaUti Utilitarianism is typically given as maximizing the happiness or the good over all people. For example John Stuart Mill states in his book Utilitarianism: 


  The only proof capable of being given that an object is visible, is that people actually see it. The only proof that a sound is audible, is that people hear it... In like manner, I apprehend, the sole evidence it is possible to produce that anything is desirable, is that people do actually desire it… No reason can be given why the general happiness is desirable, except that each person, so far as he believes it to be attainable, desires his own happiness… we have not only all the proof which the case admits of, but all which it is possible to require, that happiness is a good: that each person's happiness is a good to that person, and the general happiness, therefore, a good to the aggregate of all persons.


So it is pretty straightforward for a utilitarian as well, that you should vote for the greater good, not the special good. I don't know enough about utilitarianism to see how to it develop further or how to put it in to practice (How to measure the good? What about inherent conflicts? etc...). 

In response to the comment about Marx:

Marx wouldn't have much to say about this dilemma, as his thought concerns economy more so than political theory qua politics (although there is inevitable overlap). To put it another way, Marx's ideas would be the end result of the voting process, not guidelines on how to vote.   
Self contradictory is something that contradicts its own self. 
A set of all sets is self contradictory because a set cannot contain its own self normally, so it cannot contain "all sets". Its a self reference paradox. 

https://en.wikipedia.org/wiki/Self-reference https://en.wikipedia.org/wiki/Self-reference

https://en.wikipedia.org/wiki/Category:Self-referential_paradoxes https://en.wikipedia.org/wiki/Category:Self-referential_paradoxes

https://en.wikipedia.org/wiki/Russell%27s_paradox https://en.wikipedia.org/wiki/Russell%27s_paradox

https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory https://en.wikipedia.org/wiki/Zermelo%E2%80%93Fraenkel_set_theory

EDIT:
To locate the self reference element check this example:

Suppose that every public library has to compile a catalogue of all its books. Since the catalogue is itself one of the library's books, a librarian should include it in the catalogue for completeness. But what would happen if there were a restriction that when the book is a catalogue it should include a list of its contents? 
This will lead to an infinite repetition of the catalogue name with its contents.

Book list
--------
a,
b,
c,
book list (a, b, c, book list( a,b,c, book list( -> inf ) ) )


So it is not pure self reference that makes the paradox (it only sets the basis for the paradox to appear) but the self-negating possibility of a self referential statement. I can say "I am alive" but i cannot say "I am not alive". 

A snake does not have a problem eating snakes or tails but if it eats its own tail it is self consumed. This leads to the Russel's paradox , where a universal set can contain itself but it would lead to another set of sets that "do not include themselves" which is paradoxical for a universal set.   
In the socio-political arena, I often view interactions between people as math equations. If women are paid $10 an hour for doing a job that men get paid $15 for doing, then we have an unbalanced equation. If they get paid the same, then we have a balanced equation (called justice).

Similarly, a military attack on another country for no just cause, resulting in the deaths of 1,000 people, is bad math. But if that country has weapons of mass destruction and a reputation for using them on civilians, then attacking that country and killing 1,000 people in order to save millions is good math. Most people would probably consider it just.

This is an obvious example of doing a bad thing (killing people) for a just (good) cause.


  On the other hand can something be so overwhelmingly good/ perfect,
  but implemented or enacted in such a way that it is evil?


I just read that Mao Tse Tung ordered peasants to kill sparrows, which eat edible seeds. His intentions were good; he wanted more food available for people. However, after millions of sparrows were killed, the insects the sparrows ate exploded in population, ravaging crops. Millions of people starved.

This illustrates the well known fact that things don't always go according to plan.

Or are you asking about a good "thing" that's intentionally used to do evil? One example might be vaccines.

Vaccines were originally invented as a way to promote human health. However, there are concerns about vaccines being used to sterilize people, and some worry they could even become an instrument of eugenics.

You said you want to stick to "philosophical viewpoints," so I'm not sure if the practical examples I offered are what you're looking for.
The two are interwined.

http://plato.stanford.edu/entries/culture-cogsci/#WhaCul Culture consists of the beliefs, behaviors, objects, and other characteristics common to the members of a particular group or society. 

And https://en.wikipedia.org/wiki/Society society can be defined as the people who interact in such a way as to share a common culture. 

Thus, it is hard to define a one-way only "causal" relation between them.
This question really boils down to the notion of innateness, which essentially is the idea that we are born with certain innate ideas and concepts which we would know regardless of experience. 

If it were found to be true that humans are born with certain innate concepts (like mathematical necessities, e.g. 2 + 2 = 4) then this senseless person you are talking about could arguably think about things. You could go further and argue that from the a priori truths this senseless person is innately born with, they could employ deductive reasoning to come to even more complex conclusions, leaving themselves with a lot of think about!

However the other side of the argument claims that all our thoughts, habits, and ideas are the result of our experience with the world, more specifically, our sensory experience with the world, this is a viewpoint known as empiricism. If this viewpoint stands to be true, then this senseless person would not have any thoughts whatsoever, it would be impossible! Without initial sensory experience to give him ideas to think about, he would be left alone in his own head.

Anyway thats just my opinion of it, I'll leave it up to you to make your mind up about what you think is true.

Hope that helps!
This is a topic of current controversy among social theorists.  One prominent stance can be traced to activist educators Pat Bidol and Judith Katz, to the effect that "racism is power plus prejudice."  In other words, any one can have a prejudice, which is to say, a preconception about another person based on characteristics such as race and gender.  But only when the prejudicial person is in a position of power over the prejudged person can it be an occasion of discrimination.  To use your example, the Jews may well have been prejudiced against the Nazis during the WWII era.  But it defies common sense to suggest that they were in a position to discriminate against them.

There is, however, an ample backlash against this position, with a common criticism being the fact that the assessments of who has the power are too simplistic and absolutist --there can be situations where the nominally weaker person is actually in the stronger position.  This is one way of considering your objection.  While men are typically favored by society, we might say, perhaps this is not universally true.  (The waters are muddied, however, by the fact that privilege and power are often more invisible to those who possess them than those who do not.)

On the other hand, we can also take your position as representing a different, less confrontational strain of thought in the social justice movement, to the effect that things like racism and sexism hurt us all, even the putative beneficiaries.  This was a perspective promoted with great success by civil rights figures such as Martin Luther King, Jr.  In this case, however, we would be less likely to say that "discrimination is bidirectional" and more that "discrimination also hurts those who discriminate."
I think you have dissolved your own dilemma ethically with the caveat "...not necessarily do serious injury or harm". I no longer see a problem with universalizing the rule here. If others were to attempt to enforce their versions of what is moral on me, but in a way that did not cause any serious injury or harm, I think that would be an imposition that I'd be prepared to live with in order to allow me the later autonomy to be able to take small actions to restore what I consider natural justice without having to appeal to the state.

If you removed the caveat about doing harm to the person, the categorical imperative would be breached as you will have harmed a person motivated by your desire for property, but without the harm (or even without the intention to harm) all you are doing is acting to retrieve property which the law says is yours to take. I don't see how he fact that it is in someone else's hand at the time, or the fact that you may incidentally harm them as a result of your actions makes any difference. From a Kantian perspective so long as you do not intend to harm them the action is moral.
What is at issue, it seems to me, is the scope of the qualifier "necessarily" in Socrates's sentences. In other words, what does the qualifier "necessarily" apply to?

Socrates is saying things like:


  He who does just and holy things necessarily is just and holy


And the OP interprets this as if "necessarily" applies to "is just and holy".


  he who does just and holy things [necessarily (is just and holy)] 


As if there were, on the one hand, people who are (merely) just and holy, and, on the other hand, people who are necessarily just and holy! These would be the perfect people, angels-like, unable to err...

Socrates's actual intention seems to me, however, more mundane. I think that he uses the term "necessarily" merely to qualify an implicit, trivial implication. As if it was written


  If someone does just and holy things, (then, necessarily) he is just and holy


Under this mundane interpretation, the term "necessarily" qualifies the implicit implication ("if...then"), not the attribution of properties ("is just and holy"). And Socrates refers always only to (ordinary) people, who are contingently just and holy, not to (magical) people who are necessarily just and holy...

The mundane interpretation will sit well with what we already know about Plato, that he is used to emphasize the contrast between the perfect immaterial Forms and the imperfect material particulars. Just as no drawn circle is ever a perfect, mathematical circle, only an approximation, so no living person is expected to be perfect - ever - in Plato's view.
Let's see if I can give you a hint for a strategy, rather than a complete proof. Let's take your second proposition as the candidate for proof: 

◇(p ∧ q) → (◇p ∧ ◇q)


Start by proving ◇(p ∧ q) → ◇p  and also  ◇(p ∧ q) → ◇q. 
It is easy to prove the ◻ counterparts of these:  (p ∧ q) → p is a tautology of PC, so you can derive ◻((p ∧ q) → p) and hence by the K-axiom ◻(p ∧ q) → ◻p. The proof of the ◇ counterparts involves extra steps, because you need to use the equivalence of ◇φ and ¬◻¬φ and prove the contrapositive. In general, it is always the case that if φ → ψ then ◇φ → ◇ψ, though this isn't needed as a separate axiom within K, because it can be derived from the others. 
Now that we have ◇p and ◇q we need to get to ◇p ∧ ◇q. If we were allowed to help ourselves to the rules of PC, this would be easy, because this is simply the introduction rule for ∧. But your problem specifies that we may only use the tautologies of PC, not its rules. A useful tip is that a rule of PC corresponds to a conditional tautology. So in general, if in PC we can prove ψ from φ then the material conditional φ → ψ is a tautology. So the introduction rule for ∧ implies the following is a tautology: φ → (ψ → (φ ∧ ψ)) and hence ◇p → (◇q → (◇p ∧ ◇q)). 
You should now be able to show that ◇(p ∧ q) implies each of ◇p and ◇q and hence by transitivity that it implies ◇p ∧ ◇q. 

This is a paradox (solvable, but rather undecidable at this stage), but not an aporia (logical impossibility).

I would consider your model as incomplete: there is no definition of the object "wish". In particular what is the scope of the effects it can have? May a wish (effect) influence back the source (genie) or not? If yes, what are the rules and limitations?

In mathematics, this comes under the heading of "feedback". In order to decide on an answer to this question, you would have to define the feedback function and see what happens to your system. It might stabilize to a definite answer (which is preferable), flip-flop to two or more answers, become wildly chaotic, go to infinity, etc..

The simplest way to solve the issue is, of course, to forbid feedback: a wish is invalid if it concerns the genie in any way, shape, or form, as well as someone else's wishes. But that would also remove the challenge in your question.

The answer of gnasher 729 is not that off the mark, even though it is formulated, as it were, as a reductio ad absurdum: his implicit assumption being that the genie is omnipotent: hence not only the wish cannot affect him, but he can break it (a value of humor is that is shows absurdity). His is an acceptable axiom, though perhaps not what you intended either.

But since you invented the problem, is up to you (not the genie) to give a more substance to your problem, so that it becomes decidable.
Discussions of philosophical issues can often benefit from taking into account what we know about physics. In this case this may not be as pronounced because the teleportation paradox is more about semantics and folk intuitions in psychology, than about physics. Since semantic conventions and folk intuitions are mostly based on "intuitive physics", which is at best Newtonian, and in some aspects pre-Newtonian (think of intuitions about inertia), relativity and quantum mechanics are rarely brought up. Also behind it is http://www.davidyerle.com/tag/derek-parfit anxiety over "teleporting souls", which is even less about physics. https://books.google.com/books?id=y2R9DAAAQBAJ Gallois' Metaphysics of Identity gives a comprehensive discussion of the related philosophical issues.

Basically, the question is if the "continuity of self" or "self-sameness" is preserved across the teleportation events, and the problem, as I see it, comes from mixing the first and the third person perspectives in interpreting the said continuity or sameness. See https://philosophy.stackexchange.com/questions/38333/how-does-one-bring-mind-and-matter-into-a-single-ontology-that-accounts-for-subc/38335#38335 How does one bring mind and matter into a single ontology that accounts for subconscious mind? on other troubles with such mixing. Will the teleported body(ies) "feel" continuous with the original (first person)? Under what circumstances and for what reasons should we treat the teleported body/person as "the same" (third person)? We have no way of answering the former without actually doing the deed and asking, and the latter is a question of semantic conventions, which require publicly, and hence empirically, accessible criteria. It is there that the spatiotemporal contiguity, the ship of Theseus, etc., come in.

Does relativistic perspective change things? Let us take the timelike case as the most plausible. The import of relativity seems to be that there must be a time leg between de- and re- materialization. But that can be done already in the Newtonian context simply by making the re-materialization machine wait before it does the re-materialization. This suggests that contiguity might not always be a good criterion for self-sameness, but it was always seen as pragmatic and empirically opportunistic symptom, rather than the "essence" of it, for a similar issue with causation see discussion in https://philosophy.stackexchange.com/questions/37513/when-trying-to-identify-causality-do-we-assume-nearness-between-cause-and-eff/37538 When trying to identify causality, do we assume "nearness" between cause and effect? And it tells us nothing about the "feelings". So we still have nothing concerning the first person perspective, and are still open to suggestions on the third person one.

Perhaps, quantum mechanics could contribute more. Kane, who is a philosopher well versed in it, made an interesting point about a related question of "acting differently in the same circumstances" as a definition of "free action". Assuming that the brain can channel micro-quantum indeterminacy into macroscopic events, as say the Geiger counter does, this "definition" becomes dubious (not that we could create "the same circumstances" any more than teleport bodies). In http://www.informationphilosopher.com/solutions/philosophers/kane Responsibility, Luck, and Chance Kane writes:


  "Suppose two agents had exactly the same pasts up to the point where they were faced with a choice between distorting the truth for selfish gain or telling the truth at great personal cost. One agent lies and the other tells the truth... Where events are indeterminate, as are the efforts they were making, there is no such thing as exact sameness or difference of events in different possible worlds. Their efforts were not exactly the same, nor were they exactly different, because they were not exact."


  These two underlying premises seem quite far-stretched


To me, it's the attribution to Socrates of these two premises that seems a bit far-stretched. Let's examine them in order.


  First, it presupposes that the virtues [in this case, justice] are analogous to the crafts in all salient aspects.


This seems wrong, even nonsensical. I don't see what kind of analogy could there be between a virtue and a craft. The relation seems rather to be something like the following:

a. A good ruler is one who works for the benefit of the ruled, not for one's own benefit.


  There is no one in any rule who, in so far as he is a ruler, considers or enjoins what is for his own interest, but always what is for the interest of his subject..


b. Justice is a good quality of action.


  Then the just has turned out to be wise and good and the unjust evil and ignorant.


c. Therefore, justice cannot be the benefit of the ruler, pace Thrasymachus.

Next,


  The second assumption is that all crafts have the same structure and are analogous to one another.


Not quite. Socrates does make this general assertion, but it is not an assumption in his argument. The argument does not depend on this generalization, but on a more direct analogy between specified particular cases. Just as the good doctor works for the benefit of the patients, just as the good pilot works for the benefit of his crew and passengers, so any good ruler works for the benefit of the ruled, and not for the benefit of oneself.


  Then, I continued, no physician, in so far as he is a physician, considers his own good in what he prescribes, but the good of his patient ...
  And the pilot likewise ... will provide and prescribe for the interest of the sailor who is under him, and not for his own or the ruler's interest ...
  Then ... there is no one in any rule who, in so far as he is a ruler, considers or enjoins what is for his own interest, but always what is for the interest of his subject or suitable to his art; to that he looks, and that alone he considers in everything which he says and does.

It's not a fallacy per say, but more of a definitional paradox or lack of clarity; it's called Sorite's Paradox and it stems from poorly defined predicates. The paradox basically asks "At what point is a heap of sand not a heap anymore if a grain of sand is removed at a time?" There are several proposed solutions to the paradox such as setting a defining limit or gathering a group consensus. 

I think this vagueness is more of a point of dispute rather than a fallacy however. Here's a wikipedia article if you want to know a bit more:

https://en.wikipedia.org/wiki/Sorites_paradox https://en.wikipedia.org/wiki/Sorites_paradox
There may be a distinction you could draw between the use of the phrase 'social construction' that Berger and Luckmann (1966) popularized in the https://en.wikipedia.org/wiki/The_Social_Construction_of_Reality Social Construction of Reality, and more reflexive ideas about the constitution of social rules or 'methods' such as https://monoskop.org/images/0/0c/Garfinkel_Harold_Studies_in_Ethnomethodology.pdf Garfinkel's (1967) ethnomethodology.

Berger and Luckmann's understanding of how social knowledge is accumulated and retrieved in response to the always-current need to make sense of the world draws on Alfred Schütz' concept of 'interpretative relevance', which in turn draws on what Schütz' describes as Husserl's phenomenology of time in the "passive synthesis of recognition". When we encounter some phenomenon in time, we draw on our stock of social knowledge - retrospectively - to formulate anticipations and projections to make sense of it and respond in some way. Schütz suggests that this functions as a kind of reflexive normativity - a way to synthesize subjective experience with a stock of knowledge about how things are commonly done, used, or understood. This enables us to reflect on, rationalize and stabilize our otherwise fragmentary experiences of perceptions and thoughts over time. 

Garfinkel was also strongly influenced by Schütz' phenomenology, but equally strongly by Wittgenstein on rules and language games. Following Schütz', he  acknowledges that social knowledge and phenomenological experience are reflexively co-constructed, but for Garfinkel this also means that the materials and objects of study for the phenomenological sociologists like Schütz are inherently unstable. Therefore following Wittgenstein, he focused on the reflexive constitution of social norms through the language-game-like 'methods' used by a social group in a way that constitutes their group-ness. His studies in 'ethnomethodology' are literally that - studies of the relatively stable methods by which members of a group constitute the relatively unstable norms, rules and phenomena of their social world.

The implications of this distinction are that while you can theorize about the social construction of gender, rationality, nationality etc. these theoretical constructs are not necessarily involved in how these abstractions are socially constituted. Conversely, in ethnomethodological studies such as conversation analysis, to claim that gender, rationality or nationality are being constituted through social interaction, analysts must be able to show that those constructs are interpretatively relevant to participants in a particular setting. So one  distinction could be that describing something as 'socially constructed' presents the analyst as occupying an etic epistemological standpoint, whereas understanding how something is socially constituted might require a more emic description.
Most humans share a bias toward the value of empathy, which makes a Kantian notion of ethics natural to us.

On a worldwide scale, we find people who lack this bias to be ill, as the IDC-10 contains the diagnosis of https://en.wikipedia.org/wiki/Antisocial_personality_disorder Antisocial Personality Disorder.  And it seems obvious that murder contravenes even the most basic values of empathy.

So even if murder were not forbidden by taboos and laws, it is likely that an individual would still feel guilt for doing it.  Freud explains some possible reasons in https://en.wikipedia.org/wiki/Totem_and_Taboo Totem and Taboo.  It is unlikely that the same bias would arise pointlessly in so wide a sample space.

For those reasons, from many perspectives, the injunction against murder is more than just a social convention, it is an evolved or psychogenetic trait, and thus an anthropological universal.  From that point of view, yes, murder is wrong, in a fairly objective sense, and not just a socially constructed one.
If there is confusion, it is most likely the confusion of the ideas of "murder" and "killing a person." To say that murder = killing a person is http://www.merriam-webster.com/dictionary/oversimplify oversimplification.  While the definition would surely vary between societies, "murder" generally is defined as:


  the crime of unlawfully killing a person especially with malice aforethought


(source: http://www.merriam-webster.com/dictionary/murder Merriam-Webster. Note the use of the words "crime" and "unlawfully", which would have their own definitions that would vary by society.)

In other words, "murder" generally is not entirely synonymous with "killing a person" since it involves specific details, such as being unlawful. If capital punishment is lawful, then it would not make sense for it to also be murder, which is unlawful.

The following argument contains the fallacy of "affirming the consequent":

If there is murder, then a person killed another person.
A person killed another person.
Therefore, there was murder.


Using the above definition, it is entirely possible that there is "killing a person" without there being "murder." We could teach our children that murder is wrong without teaching them that killing another person is always wrong.

In fact, practically speaking, societies generally don't teach their children that killing another person is always wrong.  Most societies have some form of military, some type of law protecting killing in self-defense, and sometimes a legal distinction between "killing" and "premeditated killing". So, if we are already teaching our children that some killing is not wrong, such as serving in the military, then what would prevent us from teaching them that capital punishment is another instance in which killing another person is not wrong?

There could be confusion, but it is a kind of confusion that needs to be explained using the proper definition of "murder." However, to say that capital punishment for murder is a paradox is a false statement (using the generally accepted definitions).

A paradox (of sorts) would be if an opponent of capital punishment said, "Capital punishment is wrong, and the people who perform it should be executed for their crime."
See http://plato.stanford.edu/entries/epicurus/#4 Epicurus' Psychology and Ethics :


  For Epicurus, there are some fears that are perfectly legitimate; so too are some desires. Epicurus offers a classification of desires into three types: some are natural, others are empty; and[...] Natural and necessary are those that look to happiness, physical well-being, or life itself. [...] Empty desires are those that have as their objects things designated by empty sounds, such as immortality, which cannot exist for human beings and do not correspond to any genuine need. The same holds for the desire for great wealth or for marks of fame, such as statues: they cannot provide the security that is the genuine object of the desire. Such desires, accordingly, can never be satisfied, any more than the corresponding fears — e.g., the fear of death — can ever be alleviated, since neither has a genuine referent, i.e., death as something harmful (when it is present, we do not exist) or wealth and power as salves for anxiety. Such empty fears and desires, based on what Epicurus calls kenodoxia or empty belief, are themselves the main source of perturbation and pain in civilized life, where more elementary dangers have been brought under control, since they are the reason why people are driven to strive for limitless wealth and power, subjecting themselves to the very dangers they imagine they are avoiding.


  the reason us humans do anything, is because of the biological reward system in our brains that gives us pleasure and pain.


...is a reasonable presumption in the absence of any evidence to the contrary. By biological reward, I presume you mean dopamine, endorphin, oxytocin etc. Experiments manipulating levels of dopamine in rats have demonstrated that their motivation to act can be modified beyond their ability to reason simply by the continued supply of dopamine. Ethical concerns prohibit such experiments on humans, but we have no good reason to presume we are any different.

The problem is in the common confusion that our emotions/brain chemistry just "tell" us to do something. The chemicals concerned can be triggered by different parts of the brain in response to different stimuli and are not mutually exclusive. So in response to, say, a strange cat, you could get a signal telling your cortex that stroking it might produce a boost of oxytocin (remembering that it did last time you stroked a cat), visual signals of the condition of its coat might result in the chemical signals of disgust telling your cortex not to stroke it. It is your cerebral cortex that has to make the decision as the action may cause pleasure or pain or both. Even 6 month old babies viewed under fRMI show this cerebral processing prior to the final determination of a brain state. So the first flaw is statements 1,2 and 3. Everything we do is done because of an estimation by our cortex using our past experience (and occasional genetic pre-dispositions) that the pleasure will outweigh the pain.

This leads to the main flaw at 11, you have presumed that because the temporariness of pleasure is painful, that in any action there will be an equivalence of pleasure and pain. Rather, the cortex will measure the expected pleasure from the action against the expected pain (whether from the subsequent loss of pleasure, or any other pain) and reach a conclusion about whether to follow the action or not.

In addition, we suffer from a bias known as hyperbolic discounting, where the processing of the cortex is pre-disposed to value more immediate rewards. This will (in most cases) favour the pleasure to be gained from an action over the later pain from that pleasure subsiding.

The systems the brain uses to imagine or recall an event are the same as those it uses to experience it (just with an additional layer of data telling you it is not real), so your cortex, when considering an action has good reason to believe that the pleasure can be re-lived, which, together with hyperbolic discounting, makes the statement at 5 ambiguous at best.
The brain also selectively recalls events that were pleasurable when it is in a normal state, so an equal quantity of pleasure and pain will be recalled as mainly pleasure.

Finally there is a "pleasure" neurotransmitter concerned entirely with the future, serotonin. This provides a reward when we are in an environment which is perceived as beneficial to us in the future, rewards which we are not currently taking advantage of, but we believe we could. Thus the mere existence of potential future pleasures provides a reward. People with low serotonin, have clinical depression and many do indeed end up taking their own life, so in it's most simple terms the answer to the question "why do we not just kill ourselves?" is serotonin.
Even in Quine's understanding, I wouldn't call them trivial or meaningless, but incomplete and therefore vacuously true.

Every 'should' statement, for instance, is true for someone crazy enough to believe it: 'You should kill anyone who owns a ShiTzu' is true for someone who worships the great ShiTzu-avenging godling of New Berlin.  If you develop an acute schizophrenia tomorrow, that person might be you.  (He might be you now, so let me inform you that I do not own a ShiTzu.)

Such a statement can only become false when a given moral principle is attached afterwards as a premise or tacitly presumed by the speaker; say, religious relativism, or obeying the law.  At the point the omitted condition is adequately grounded, the statement is rendered conditional and not modal.

This remains a position other than realism or fictionalism: since this clarification is ultimately intended, direct modal statements are statements about reality, this reality rather than some extended one, and they not fictions.  (That does not mean that modal statements about fictions are not about fictions.  But we make indicative statements about fictions, too.)

The same is true of the statement's inverse -- they are both true, and do not conflict.  There is no problem with 'You should and you should not do X.'  It simply indicates your moral premises are not clear enough yet.  So the mechanism of box and diamond operators in modal logic just tracks what is and is not grounded yet, relative to a single given missing premise, and therefore cannot lead to broader deduction.  This makes it a much less useful tool than it might seem.

The notion of moods captures the fact that humans never really intend to clean up after themselves in this regard.  We just rate the odds of guessing the condition wrong, and continue.  The mood marker (would, should, could, etc.) besides conveying this intention to remain elliptical, indicates the likely form of the condition.  But it does not really have meaning in and of itself, and does not even prescribe the condition's nature entirely.  One can clarify what initially appears to be a moral issue by providing facts, for instance, which displace the applicability of some given premise other than the missing one(s).

Grammatically this interpretation of modal statements as vacuously true due to missing premises is now called 'suspending control', and is one modern interpretation of the grammar of the subjunctive and related constructions.  In this model, when you use subjugation with a mood like the classical English subjunctive or the old Greek 'optative', you are making the statement for which a condition exists which controls the meaning of your statement, but you are admitting that you are not certain of exactly what that condition is.

This whole framing makes modality elliptical, rather than nonsense.
Rorty's response might be that you are presuming some universal notion of "truth" with autonomous "meaning" that extends across societies. And since meaning is established only through social engagement and common practice (he is a pragmatist) such extension is unworkable, and any questions it generates are as a result meaningless. He is quite explicit about his dissolution of epistemology in politics in https://philosophynow.org/issues/30/Truth_and_Progress_by_Richard_Rorty Truth and Progress:


  "...strategy for escaping the self-referential difficulties into which “the Relativist” keeps getting himself is to move everything over from epistemology and metaphysics into cultural politics, from claims to knowledge and appeals to self-evidence to suggestions about what we should try."


This is Rorty's "new pragmatism", see more under https://philosophy.stackexchange.com/questions/36489/how-should-we-choose-between-different-theories-according-to-rorty-based-on-kuh/36491#36491 How should we choose between different theories according to Rorty, based on Kuhn? and https://philosophy.stackexchange.com/questions/34251/have-any-philosophers-applied-the-concept-of-underdetermination-to-non-scienti/34255#34255 Have any philosophers applied the concept of "underdetermination" to non-scientific contexts? For Rorty even Quine's "old pragmatism" with naturalization of epistemology wasn't radical enough because it privileged science over the rest of culture. I agree with  Zammito's assessment in http://press.uchicago.edu/ucp/books/book/chicago/N/bo3634623.html The Nice Derangement of Epistemes:


  "In insisting that only moral stipulation animates any of the discriminations that traditionally appeared epistemic, Rorty wraps himself in the final dogma of positivism, the fact-value distinction. For him there are only arbitrary value judgments over against an "ontology" which he deliberately relegates to epistemic inaccessibility... What is left is language and the arbitrary "poetics" of conversation. Rorty dissolves too many distinctions; his new "pragmatism" entails a cavalier disdain for rational adjudication of dispute. "


But one does not need to go down the rabbit hole with Rorty and give up on rational adjudication of dispute, to believe that social norms lie at the root of rationality, because they lie at the root of imparting shared meaning. This position in various forms was held by semantic pragmatists like Quine, Wittgenstein, Sellars, Davidson, and now most explicitly Brandom, who writes in Articulating Reasons and Reason in Philosophy:


  "It is a rationalist pragmatism, in giving pride of place to practices of giving and asking for reasons, understanding them as conferring conceptual content on performances, expressions, and states suitably caught up in those practices.
  
  The game of giving and asking for reasons is not just one game among others
  one can play with language. It is the game in virtue of the playing of which what one has qualifies as language (or thought) at all. I am here disagreeing
  with Wittgenstein, when he claims that language has no downtown... This is a kind of linguistic rationalism. ‘Rationalism’ in this sense does not entail intellectualism, the doctrine that every implicit mastery of a propriety of practice is ultimately to be explained by appeal to a prior explicit grasp of a principle. It is entirely compatible with the sort of pragmatism that sees things the other way around."


I doubt that most semantic pragmatists would call social norms the "ultimate determinant of rationality", but there are ways of interpreting some of them as constitutive of rationality without falling into cultural relativism a la Rorty. Some arguments for and challenges to semantic pragmatism are surveyed  under https://philosophy.stackexchange.com/questions/38564/what-arguments-support-the-idea-that-rational-thinking-requires-language-use/38653#38653 What arguments support the idea that rational thinking requires language use?
The relationship between conceptual engineering and social change is based on tightening concepts along certain directions like amoebas, whereby it may be effected that the average citizen avoids certain unwanded aspects of concepts.

Consider that you are in a swimming bath with springboards, and that by analogy, concepts are the springboards of thinking. The bouncing of the springboards makes it possible to effect great leaping figures in the air, before landing in the water. A concept may also contain such resonance possibilities, in many directions simultaneously. 

If anybody has something against certain of these conceptual resonances, and has enough authority over mass media (Film industry, TV, newspapers, censorship in the internet), he may manipulate the everyday thinking of the thereof unconscious average citizen, by imbeding this concept in a kind of straightjacket, i.e. by contaminating the concept, or distorting it in a certain direction, like an amoeba.

If a citizen is in this sense continually provided with certain straightjacket-points-of-view, it appears him, as if it were a “thoughtcrime” (George Orwell) to think the opposite of this, i.e. the unconstrained concept. Hence for him, the concept’s resonance force in certain directions has got lost (--> “crimestop”). 
Society naturally divides into authoritarians and liberals.  The reason is, everyone starts out as authoritarian, following instruction from parents and teachers.  (Authoritarian does not mean being bossy or socially dominant (SD); it means following authority.)

Later in life, upon entering adulthood, some people self-actualise, individuate, start thinking more independently, become free-thinkers.  (The process is a difficult wrench because the automatic follow-the-leader instincts and peer pressure have to be wrestled with.)

People who are free-thinkers tend to let others get on with their own thing: a generally https://en.wikipedia.org/wiki/Liberalism liberal attitude.  Authoritarians generally stick to their chosen authority base.

https://en.wikipedia.org/wiki/Bob_Altemeyer Professor Bob Altemeyer devised a psychological test to measure right-wing authoritarianism (RWA),.  He managed to get US politicians to take the test, the results from which are shown below, (and http://members.shaw.ca/jeanaltemeyer/drbob/TheAuthoritarians.pdf here, page 201).  The scoring showed that the right-wing tended to cluster to their authority-bound attitudes while the left were more distributed across the spectrum, following their own ideas, however varied they were.

https://i.stack.imgur.com/f1s1T.jpg 

Autoritarian versus liberal is not the only division in politics.  There is also an economic dimension. https://www.politicalcompass.org/ The Political Compass is a good site for finding out about this.

https://www.politicalcompass.org/analysis2 https://www.politicalcompass.org/analysis2

https://i.stack.imgur.com/eqAwI.gif 

https://i.stack.imgur.com/FdTq2.png 

The Political Compass puts the terms left and right on the economic axis saying:


  Our essential point is that Left and Right, although far from obsolete, are essentially a measure of economics.


However, in its origin, and still to a large extent today, I would say, left and right relate to the social authoritarian-liberal spectrum.  In the French Revolution the right were the authority loyalists and the left were the ones with new ideas.  There was also a respective dimension of wealth preservation and wealth redistribution though.

https://en.wikipedia.org/wiki/Left%E2%80%93right_politics#History_of_the_terms Wikipedia: Left-right politics


  The terms "left" and "right" appeared during the French Revolution of
  1789 when members of the National Assembly divided into supporters of
  the king to the president's right and supporters of the revolution to
  his left.


Presumably the rebels were placed on the establishment's left side, the "sinister" (http://wordinfo.info/unit/3777/s:and sinistro) side, for nuanced effect, while the authoritarian loyalists were on the honoured right.

By contrast, in reference to the Tâo Te Ching, http://www.sacred-texts.com/tao/mt/mt34.htm Chapter 31, C. Spurgeon Medhurst (trans.) writes:


  The references to the right and the left will be understood when it is
  remembered that in China the left is the seat of honor, the right the
  lower and inferior seat.


So in China the esteem of left and right is opposite to that in the Roman and Western world, indicating no common source.
If you just want to use logic there is no counter argument as these are just propositions. As there's no argument there's nothing much to counter.

I can see two ways in which you could offer a counter. 

One would be if you are assuming that the argument is for one who is born, death is certain therefore for one who is dead, birth is certain. This is just poor logic as if X then Y does not imply if Y then X.

The other way is if you argue that the words have meaning. In which case there is plenty of evidence that would suggest that for one who is born, death is certain is likely to be true. There is currently no evidence that for one who is dead, birth is certain. Whether or not the lack of evidence bothers you is a separate matter.
We should also consider, depending on what tradition you follow, human flourishing or that which is simply good.

A big proponent of the first view, virtue ethics, would be https://plato.stanford.edu/entries/aristotle-ethics/ Aristotle, while the second, deontology, is, to me, best expressed by https://plato.stanford.edu/entries/kant-moral/ Kant.

Virtue ethics depends on an a posteriori understanding of what makes a good life, or eudaimonia. Kantian deontology is based on an a priori notion of the universality of the rational will, which forbids a rational actor from using another rational actor as a pure means, and not also as an end in themself. 

Additionally, religion could be a source of either virtue ethics or deontology.

These traditions criticize consequentialism (of which utilitarianism is a specific manifestation) of focusing on the wrong thing (human lives instead of human flourishing or the outcome of an act rather than its intent) or being ungrounded in, well, anything, and simply supposing that the greatest good for the greatest number of people is good.

Notably both of these answers reject the premises on which your question is based. In a robust utilitarian framework, the utilitarian thing to do is never morally wrong, although your belief that something is the utilitarian option could be mistaken. We must leave this framework to find criticisms of it.



A common "counterexample" to utilitarianism goes something like this:
Say you have a speck of dust in your eye. Well, that's a little annoying, right? Now say that everyone who ever has or will have lived will have a speck of dust in their eye at some point. That adds up to an awful lot of discomfort. You can stop this discomfort, and keep the dust out of everyone's eyes forever, but only by torturing an innocent for 50 years.

If you have enough dust in enough eyes this ought to balance out at some point. And we can substitute any relatively-trivial discomfort for dust.

A virtue ethicist might say that a flourishing human being would not torture an innocent person for 50 years; virtuous people just don't do things like that. And a deontologist could say that torture uses a person as a means and not an end, so it is inherently wrong.
Praising the defense is technically still argumentum ad hominem.

In your example the second statements are both "to" the "person" and neither is counter-argument, however, "Bob favors Y" and "Bob favors not-Y" are also not technically arguments in the first place, they are statements of an opinion. So in this instance, there is no logical fallacy of argumentum ad hominem, merely the exchange of sentiments.

If an argument is presented (either "for Y" or "against Y") and all that is offered in support of the argument is "the person arguing for (or against) Y is ABC..." then for either there is a false argument to the person.

If, however, the argument pertains the merits of the persons character, then ad hominem is apropos. For example:

Bill is Governor
Bill is a lazy shiftless loser
Therefore, Bill should not be re-elected Governor


and

Bill is not Governor
Bill is a hard worker and winner of the 2016 Gilded Nostril Award
Therefore, Bill should be elected Governor


Further thoughts: if praise of the arguer is misconstrued as positing the arguer as an authority (e.g. an expert, or an authorized judge) when they are not one, this would be an argument from false authority. There is also the argumentum ex cathedra which is "argument from the chair" or an appointed seat (of authority).
In S4 modal logic, nested modalities of the same type collapse, so □□A ↔ □A and ◇◇A ↔ ◇A. In S5 modal logic, all nested modalities collapse to the rightmost one, so □◇A ↔ ◇A, and ◇□A ↔ □A. If you want to be able to say that it is possible for something to be impossible, i.e. ◇□ ¬A, without this collapsing to □¬A, you need to use a modal logic weaker than S5. 
This is a variant of https://en.wikipedia.org/wiki/Nirvana_fallacy#Perfect_solution_fallacy appeal to perfection, which itself is a type of https://en.wikipedia.org/wiki/False_dilemma false dichotomy.  The general form of a false dichotomy is a forced choice between only two options, "it's either A or B," when it could in fact be C.  In this specific variant the choice presented is between a perfect solution or giving up entirely.

The way to defend against a false dichotomy is to identify the plausible third option, and in the case of the appeal to perfection, that option is to improve on the default, even without reaching perfection.

In the case given, the impossible "perfect" solution A kills no animals, but the "pretty good" solution C kills fewer than the default solution B (to continue as is).
When Satre says you cannot apply positive values to a role he does not mean positive in the sense of 'good' he means positive in the sense of affirming or concluding. He contrasts this with Negative values which one can assign to roles as negating; "I am not a chef" is allowed, "I am a chef" is not.

That being said, Satre is, as most existentialists, trying to make something mundane and obvious sound impressive by tapping into the laissez-faire Zeitgeist of his social group at the time of his writing. When one says "I'm  a chef" no-one is, like Satre's Waiter, saying "all that I am is now and forever contained within the description 'Chef'", they're expressing the result of their decision "I have chosen, for the time being, to carry out the functions society assigns to the role 'Chef'". Nothing in that statement says anything about their future intentions, the degree of freedom they felt inherent in making that choice, nor constrains their actions in any way. People using the term are referring implicitly to what they are now and what they intend to be, not what the actually will be.

Furthermore, even if we were to take someone who, by virtue of circumstance assigned themselves a role outside of what they consider their free will, there remain ways in which they could do this. Firstly, by definition - if you have just served people dinner for money, you are a waiter, whilst the action is being taken you can do nothing but accept that definition (presuming we are to take a pragmatic view of what constitutes 'now', or 'the time being'). Secondly Satre admits that choices can be constrained and so the number of choices is finite. He offers no explanation, however, for why that number might not, by virtue of contrived circumstance, ever reach one. If a person's life is relatively unconstrained they may have fifty choices, Satre admits that circumstance may remove one of them. If someone is so constrained that they have only two choices, what mechanism has now sprung into existence to prevent one of them being removed?

All that we're left with is the fairly mundane statement that people often overlook one or two of the choices that they actually have, or use the expression "I had no choice" when what they really mean is "I didn't like any of the other choices". Since any rational person knows what they mean anyway, making such a linguistic turn of phrase into an entire philosophy is sophistry.
Take a look at Consequentialism, or more specifically at Utilitarianism.  One branch of Consequentialism is act Consequentialism (Utilitarianism simply being hedonistic act Consequentialism), which is briefly summarized as follows by SEP.  "Act consequentialism is the claim that an act is morally right if and only if that act maximizes the good, that is, if and only if the total amount of good for all minus the total amount of bad for all is greater than this net amount for any incompatible act available to the agent on that occasion." (https://plato.stanford.edu/entries/consequentialism/#ClaUti https://plato.stanford.edu/entries/consequentialism/#ClaUti)  Basically consequentialism attempts to maximize utility, in other words general well-being, of all people.  I think this is more or less what you're looking for.
One reason is general philosophical assumptions (best dealt with on Philosophy SE). Broadly speaking, classical Aristotelian principles have largely been replaced with Kantian thinking, and legal positivism is one version of that way of seeing man in relation to society. Ultimately it comes down to the difference between the Aristotelian "what objectively exists" view versus the Kantian "how it appears" view, the latter view now being (apparently) more widely accepted. A related matter is that it has been difficult (not impossible) to avoid invocation of a divine source for law. Appeals to "the common good" run into analogous problems with objectively defining and measuring that standard for identifying law. There are vastly fewer problems with determining whether a city has enacted an ordinance. 

A second reason is what is known as the separability thesis (a feature of legal positivism), that legal validity is distinct from moral validity, whereas natural law holds that legal and moral validity are largely to entirely the same (the "overlap" thesis). If you believe that a man's right to his property is absolute, then it follows that a law allowing taxation is "bad law" from the moral perspective: there is no natural law to the effect that you must surrender 38% of your income to the sovereign. But it is absurd to think that there is no law requiring you to pay income tax. On the other hand, there really is no law requiring you to slaughter a goat during the Vernal Equinox. Legal positivism says, simply, that there does happen to be a law requiring payment of taxes (currently, in the US), and there is no Equinox Goat Slaughtering law (currently, in the US). Either fact could change if The Lawmakers say so, and that is obviously correct. 

https://plato.stanford.edu/entries/legal-positivism/ This is a reasonable article on legal positivism, as is http://www.iep.utm.edu/legalpos/ this, and for natural law, http://www.iep.utm.edu/natlaw this article and https://plato.stanford.edu/entries/natural-law-theories/ this are useful.
Yes, an argument can be true even if the premise(s) is false.  Validity is only about the structure of the argument, the form, which is why it's called formal logic.  Having a false premise is irrelevant to the formal validity, and is instead an informal error.

As Mauro pointed out in his comment on your question, all conclusions can be shown to follow from a contradiction, so the example you gave is indeed a valid argument, although I don't think the example you gave is really that relevant to your real question.  A more relevant example to your actual question would be an argument like this.  "All bachelors are married.  John is a bachelor.  Therefore John is married."  So just assume that john actually is a bachelor for this example.  This is a completely valid argument, at least in terms of formal logic.  Note, however, that even though it is a valid argument, it gives a false conclusion because it has a false premise (that all bachelors are married), and that premise will always be false.  

Note also that it COULD give a true conclusion, if we had also made the other premise (that john is a bachelor) false, meaning that john really isn't a bachelor.  Then the same argument would give the same conclusion, that john is married, but now the conclusion would be true, because the premise that john is a bachelor is false.

Basically it's just that validity determines whether that form of argument works and is completely consistent.  If you take a false premise, then your knowledge about whether you'll get a true conclusion goes out the window (depending on how many premises are false and how many are true, etc).
In the maximally formal sense logics are defined with only one connective and negation. Most often the connective is either "and" or "implies". The reason this is allowed is because all of the other connectives can be constructed out of a series of negations and the chosen connective, you can think of the other connectives as short hand. 


  A v B is shorthand for ¬(¬A ∧ ¬B) in a logic defined with negation and conjunction. 


It is entirely similar to functions, predicates, and variables. In the maximally formal sense variables are not called "x", "y", and "z". They are all called x with subscripts running from 1 to however many variables exist. Function letters and predicate letters are the same, they are not truly written "F", "G", or "H", but instead are written with a superscript which labels how many arguments it takes and a subscript also running from 1 to the total number of functions or predicates. The subscripts differentiate them in the same way "x" is differentiated from "y". 

In any advanced textbook on logic you will find that initially the authors present the idea in the most formal way and then relax the constraints so that we use different letters instead of numbered letters for the sake of convenience. It is similarly done for connectives as well. As an example, from https://en.wikipedia.org/wiki/Saul_Kripke#Unpublished_manuscripts_and_lectures Saul Kripke's Elementary Recursion Theory and Its Application To Formal Systems: 


  We have described our official notation; however, we shall often use an unofficial notation. For example, we shall often use 'x', 'y', 'z', etc. for variables, while officially we should use 'x1', 'x2', etc. A similar remark applies to predicates, constants, and function letters. We shall also adopt the following unofficial abbreviations:
  
  (A ∨ B) for (~A ⊃ B);
  
  (A ∧ B) for ~(A ⊃ ~B);
  
  (A ≡ B) for ((A ⊃ B) ∧ (B ⊃ A)); 
  
  (∃xi) A for ~(xi) ~A.


In this excerpt "⊃" is the symbol for implication which is the connective he chose to define the logic with, "≡" is the symbol for biconditional, "~" is the symbol for negation and (x) is the universal quantifier. The numbers next to the variables in the beginning as well as the i next to the universal quantifier are supposed to be subscripts, however the philosophy.SE sadly does not allow for mathjax typesetting. 

So in the most formal sense logics only have either "ors" or "ands" or another connective. However, that might seem confusing since colloquially we do have both. The answer as to why we do have the concept of both of these comes from the logical study of natural language. https://plato.stanford.edu/entries/aristotle-logic/ Aristotle's logic is really the progenitor to modern logic, although modern logic is quite different. From the SEP article on the same subject:


  In On Interpretation, Aristotle argues that a single assertion must always either affirm or deny a single predicate of a single subject. Thus, he does not recognize sentential compounds, such as conjunctions and disjunctions, as single assertions. This appears to be a deliberate choice on his part: he argues, for instance, that a conjunction is simply a collection of assertions, with no more intrinsic unity than the sequence of sentences in a lengthy account (e.g. the entire Iliad, to take Aristotle’s own example). Since he also treats denials as one of the two basic species of assertion, he does not view negations as sentential compounds. His treatment of conditional sentences and disjunctions is more difficult to appraise, but it is at any rate clear that Aristotle made no efforts to develop a sentential logic. Some of the consequences of this for his theory of demonstration are important.


Aristotle agrees with modern logic that conjunctions, for example, are not atomic formulae. They are compound formulas and as such they consist of subformulas. Really, the introduction of connectives comes from our natural use of words such as "and" and "or". We defined "and" as trying to represent what we logically mean when we say "It is raining AND it is cloudy". What does the "and" mean in that sentence? Well it means that both of those atomic formula, the assertions "it is raining" and "it is cloudy" are true at the same time. Therefore we structured the logical "and" to reflect a common connective we use when dealing with propositions, or assertions, in natural language. Due to the fact that we can construct all of our connectives out of just a negation and one other connective, the choice of which connectives we use is arbitrary. 
But this is not what the author says.

The https://books.google.it/books?id=ygVzJuFV6uMC&pg=PT154&dq=walton+Informal+Logic+invalid+arguments example rewrites the original argument form, from modus ponens into:


  the less specific form: A; B; therefore C.
  
  Instead of representing the first premise as a conditional, we could also represent it as a simple proposition, A. [...] And that form of argument is invalid [emphasis added]. Even if both A and B are true, it is quite possible that C could be false, for all logic tells us.


And the conclusion is :


  So we have to be careful here. Even if we know an argument has an invalid form, it need not automatically follow that the argument must be invalid.


The author is not asserting that: "A and B; therefore C" is "equal" (equivalent) to: "if A, then B, and A; therefore B".

He is asserting that an "incorrect" formalization of an argument can "hide" its validity.
Aristotle notes that the formal and final cause are often the same. Concerning uniqueness of the formal cause here is a relevant passage from https://books.google.com/books/about/Aquinas_on_Mind.html?id=5vjJfL4BTjQC Kenny's Aquinas on Mind (p.149):

"Whenever there is a true sentence on the pattern ‘A is F’, we can speak of the
form of F-ness; an accidental form or a substantial form as the case may be. If A is hot, there is such a thing as the hotness of A; if A is an animal, there is such a thing as the animality of A. Thus, the hotness, or heat, of a hot body is what makes it hot, and that is an example of an accidental form. The substantial form in a human being may likewise be introduced as being, truistically, that by which a man is a man, or that which makes a man a man. In each of these cases the ‘makes’ is the ‘makes’ of formal causality, as when we say that it is a
certain shape which makes a piece of metal a key..."

In other words, a form does not have to be substantial to serve as a formal cause, and accidental forms are certainly not unique for a thing. Even if we assume that only substantial forms can serve as formal/final causes it is not clear that Aristotle asserts unicity even of substantial forms. Apparently, there was a controversy about it in Aquinas's time, which suggests to me that Aristotle was none too clear on the issue:


  "Now human beings grow and take nourishment, just as vegetables do; they see and taste and run and sleep just as animals do. Does this mean that they have a vegetable and animal soul as well as a human soul? Many of Aquinas’ contemporaries answered this question in the affirmative. They held that in the human being there was not just a single form, the intellectual soul, but also animal and vegetable souls; and for good measure some of them added a further form, a form which made a human being a bodily being. This was a ‘form of
  corporeality’... Aquinas rejected this proliferation of substantial forms."


The "form of corporeality" comes from another well-known interpreter of Aristotle, Avicenna, see https://www.google.com/?gws_rd=ssl#q=AVERROES+FORM+OF+CORPOREALITY Pasnau's Form and Matter. But even if we accept Aquinas's position on unicity of the substantial form, assume (even against him) that only such a form can be a formal/final cause, and postulate some sort of analogous "ultimate" final cause in all cases, the final cause of final causes as it were, (which seems to be the idea in the OP), I do not see why Aristotle would want to restrict final causality to such a uber-cause. His https://plato.stanford.edu/entries/aristotle-causality/#FouCau four causes are devices of explanation, whenever a teleological explanation is called for in a particular context the thing will have a (context-dependent) final cause. 

Think of an architect building a castle, his various construction decisions aiming at making it defensible, imposing, pleasing to the eye, etc., all of this together wraps into a final cause he is working towards. But then think of the architect's client, the king, who sees it as a strategic piece in a grand plan for expanding the kingdom, and cares little for, or does not even know, the construction details, while the architect has not a clue about the grand plan. Which is the castle's final cause? The architect's and the king's designs overlap, but none contains the other, and while we could wrap them together artificially, the explanatory value of such a chimera is dubious. There might also be a problem with this "ultimate" cause itself because Aristotle rejects actual infinity. It is one thing to potentially have a final cause in any context, it is quite another to actualize them all into an "ultimate" cause.
With poetically similar language such as "destroyer of hope" and "bringer of hopelessness" or "taking hope" vs. "destroying hope", the semantics are typically not obvious.  One needs to look at the surrounding context to determine what those phrases mean.  However, there are general patterns which do occur in English speech that can be useful.

One key aspect is the concept of action.  "Destroyer of hope" is an active phrasing which typically indicates an individual who seeks out hope and actively tries to destroy it.  A "bringer of hopelessness" is more of a passive phrasing which typically indicates an individual who inspires hopelessness around them, but isn't necessarily actively trying to seek out hope to destroy.  If anything, one might say that the cultivate hopelessness, and permit that hopelessness to counteract what hope it finds.

Again, this is not a 100% semantic rule, but it is a trend that I find tends to lead an author to choose one phrasing or the other.  In the end, both may result in a loss of hope, but the way they cause that loss of hope is different and that is often conveyed by such phrasings.  When facing a "destroyer of X," one might rationalize that it's a good idea to hide your X, so that the destroyer cannot find it.  When facing a "bringer of not(X)," its usually encouraged to feed your X and make it shine brightly to overcome the miasma of not(X) that surrounds the bringer.

"Taking" vs. "destroying" on the other hand, has a more clearcut difference.  Taking is an action that obeys conservation.  If an individual takes X from you, you no longer have X and they now do have X.  The concept of "taking back X" becomes meaningful to talk about.  A destroyer does not try to conserve.  If an individual destroys your X, you no longer have X and they also do not have X.  There's no equivalent meaning of "destroying back X" which could be applied, although the idea of doing that starts to enter the realm of punitive measures as you find some equivalent of X that they have, perhaps X', so that you can say "you destroyed X, so I will destroy X' ".
Philosophy makes a distinction between the identity of material objects* (and immaterial objects if those are thought to exist) and what is called "personal identity." From the Stanford Encyclopedia of Philosophy's article Personal Identity: 


  https://plato.stanford.edu/entries/identity-personal/ Personal identity deals with philosophical questions that arise about ourselves by virtue of our being people (or, as lawyers and philosophers like to say, persons). This contrasts with questions about ourselves that arise by virtue of our being living things, conscious beings, material objects, or the like.


In terms of the identity of a material object, philosophers would argue that a transgender person has the same identity as before. From the SEP's article on Identity:


  https://plato.stanford.edu/entries/identity/#1 A distinction is customarily drawn between qualitative and numerical identity or sameness. Things with qualitative identity share properties, so things can be more or less qualitatively identical. 


A transgender person will have a massive amount of qualitative similarities with the person they were before coming out as transgender. These include:


  Causal history of physical interactions
  
  Genetic details and natal history
  
  Collection of memories and historical subjective experiences. 


The history of a person, the actions they've taken, who their parents are, the event of their birth, all of these and more are enough qualitative facts that are enough to say that someone has the same material identity as they did before. It is the same reason we would argue someone is the same person at one moment that they are one moment later. 

In terms of personal identity, there are reasons to argue that a transgender person may be a different person than before once they come out. From the World Health Organization:


  http://apps.who.int/gender/whatisgender/en/ "Sex" refers to the biological and physiological characteristics that define men and women.
  
  http://apps.who.int/gender/whatisgender/en/ "Gender" refers to the socially constructed roles, behaviours, activities, and attributes that a given society considers appropriate for men and women.


So, taking the definition of gender to be a collection of rules, behaviors, activities, and attributes of an individual, we can argue that a change in these would fall in line with the definition of personal identity. Someone's personal identity can be seen as the answers to the question "who am I?" that they would give. 

Consider that there is a transgender person named Alice, and Alice lived for twenty years as a man. At that point in her life she realized that she was transgender and wanted to be treated as the woman whom she felt she was. If we had asked her when she was 16 who she was, (maybe at that time it would have seemed appropriate to say "who he was" depending on the circumstances) she might have said something about her interests and her desired career and being someones son and so on. However, after she has had her revelation about her gender, her answers to that question would completely change. She would no longer identify herself as someones son, she would identify herself as a woman and a daughter. In this way, her personal identity has in fact changed with time. We could also consider the situation in which Alice as having known since she was 6 that she felt more like a girl and that was how she wanted to be identified. We would then need to look at her different behaviors and activities, as suggested by the WHO definition of gender, to evaluate whether or not she has had a change of personal identity. 

At any rate, personal identity is different than the general metaphysical idea of the identity of a material object. It would not be well supported to argue that a transgender person is a different material object before and after they come out. In terms of personal identity, philosophy treats these issues by asking about things like social and psychological identification, as well as others. A transgender person may be, and most likely are, a different person after their transition, in terms of personal identity. 

In regards to the three points made at the end of your question, a philosopher might make the arguments:


  a) They would agree that they're the same material object, with the same qualitative properties of identity outlined above.
  
  b) The metaphorical sense would be what was outlined above as personal identity, in some respects their personal identity has changed.
  
  c) This depends on the person, if the person has felt since they were very little that they knew they were transgender then maybe that aspect of their personal identity hasn't changed, but there might be other aspects of their identity such as their actions, how they dress, how they talk, etc. that have changed after they came out. 


*Additionally I'd like to say, of course I am not suggesting that we should treat people "like objects," I am just using a metaphysical distinction. Material objects are things that are created from matter and physically exist; the societal and cultural issues of referring or treating somebody as an "object" is a completely different matter. I'm not mixing the two.  
Possibly a https://www.logicallyfallacious.com/tools/lp/Bo/LogicalFallacies/150/Red-Herring red-herring, or https://en.wikipedia.org/wiki/Ignoratio_elenchi ignoratio elenchi (i.e. an ignoring of a refutation)?  


  Attempting to redirect the argument to another issue that to which the person doing the redirecting can better respond. While it is similar to the https://www.logicallyfallacious.com/tools/lp/Bo/LogicalFallacies/61/Avoiding-the-Issue avoiding the issue fallacy, the red herring is a deliberate diversion of attention with the intention of trying to abandon the original argument. 


In your case it sounds like https://en.wikipedia.org/wiki/Non_sequitur_(logic) non-sequitur (i.e. it does not follow) followed by opinion.
Linguistic relativity is a thesis that talks about people's perceptions of the world. It doesn't do anything to alter the actual structure of the world. The laws of causality are defined by Einstein's relativity and they tell us that nothing can travel faster than the speed of light, and superluminal messaging is what is required for time travel. Linguistic relativity is a thesis that talks about how language influences, or possibly defines people's thought, not how it influences the causal structure of the world. There's no part of the thesis that gives language the power to restructure physics. Information cannot travel at superluminal speeds so people cannot have cognition of events before they happen. If a language develops to a point where somebody perceives themselves as being able to tell the future, that is a completely different thing than that person actually being able to tell the future. 
First, you overestimate the agreement that incest is morally wrong. Yes, I would find it viscerally disgusting to “mate” with my sister, but many philosophers wouldn't accept this subjective feeling as an argument for the claim that incest between adult siblings is morally wrong (if it is consensual and no children are conceived).

Now to the main point: There are people who argue that because homosexuality is not found in nature (which is incorrect), it must be immoral. In this case the argument that it is found in nature shows a fatal flaw in the premise of this reasoning.

Of course, this alone does not mean that homosexuality (or more precisely, a homosexual act) is moral, it just refutes the argument before it even starts. You would need to supply another argument to justify your claim that homosexuality is immoral. Because in general, the person making the claim has the burden of proof.

In reality, something else is behind the meaning of “natural”, which is not found in the clumsy and wrong argument that homosexuality is not found in nature. It means that by looking at the genitals we can guess what their purpose must be – presumably heterosexual intercourse.

So the argument would be: Since a homosexual act frustrates the purpose of the genitals, it is immoral.

But this line of reasoning assumes a lot. Arguing on the basis of purposes (teleology) is not fashionable anymore. That's something from the Scholastics in the middle ages. It would be rejected by most philosophers today.

And even if we agree (contra Darwin) that there are purposes in nature and in ourselves, can we really be so sure that we can correctly detect those?

And even if we accept that homosexuality frustrates the purpose of the genitals, it still doesn't follow that homosexuality is immoral. Because why should the frustration of the purpose of something be morally wrong?
There are no counterexamples to Kant's "argument" because it is not an argument. It is a view of predication under which being/existence is not a "real" predicate discussed in http://www.phil.pku.edu.cn/resguide/Kant/CPR/18.html Transcendental Dialectic (Chapter III, Section 4):


  "Anything we please can be made to serve as a logical predicate; the subject can even be predicated of itself; for logic abstracts from all content. But a determining predicate is a predicate which is added to the concept of the subject and enlarges it. [...] "Being" is obviously not a real predicate; that is, it is not a concept of something which could be added to the concept of a thing... By whatever and by however many predicates we may think a thing - even if we completely determine it - we do not make the least addition to the thing when we further declare that this thing is. Otherwise it would not be exactly the same thing that exists, but something more than we had thought in the concept: and we could not, therefore, say that the exact object of my concept exists."


One can disagree with Kant's view, or one can even agree with it but still choose to treat existence as a predicate for technical convenience. It is unclear what path you take in your example. Of course, we can think of listing "characteristics" (Kant's "logical predicates") of an object in a database, and one of those characteristics could be "exists in nature" (like horse) or "fictional" (like unicorn). What Kant is talking about is not such technical convenience that "abstracts from all content", but rather descriptive ("real") predicates that constitute our conceptions of objects. Be it horse or unicorn we do not learn anything new about our conception of them by discovering that such an animal occurs in nature. In fact, we need to form a conception first, and then go out into the nature to find out if an object of this conception exists there. Kant used this view to https://en.wikipedia.org/wiki/Ontological_argument#Immanuel_Kant object to the ontological argument, which first declares existence to be part of the conception of "God", but then infers his real existence from it.

Kant's view was https://plato.stanford.edu/entries/existence/#FreRusExiNotProInd incorporated into the standard predicate logic by Frege and Russell, who treat existence not as a property of objects (predicate), but as a property of properties (quantifier). It applies to a list of properties ("conception") and returns true or false depending on whether there is an object in the domain of discourse which instantiates that list. Of course, existence can still be treated as a "characteristic" of objects with the help of identity, ∃x(x=a) expresses that a exists. But on a closer look what is being asserted is not that a exists but rather that the predicate is-a ("=a") is instantiated. There is no way to say that some a does not exist, ¬∃x(x=a) rather says that being-a is not a property of anything. One can object to such logicization of Kant's view (against his express words, apparently), but this is the prevailing interpretation of what he meant even among opponents of his view. Kant's notion of "conception" was developed into the descriptivist semantics by Russell, which came under heavy criticism, especially by Kripke in 1970s, see http://www3.nd.edu/~jspeaks/courses/mcgill/415/kripke-descriptivism.html Kripke’s Attack on Descriptivism by Speaks, who offered an alternative theory of reference. http://maverickphilosopher.typepad.com/maverick_philosopher/2014/10/a-quibble-with-kripke-over-existence.html Arguing with Kripke over Existence on Maverick Philosopher quotes him on existence specifically: 


  "To deny that it [existence] is a first-level concept is to deny that there is a meaningful existence predicate that can apply to objects or particulars.  One cannot, according to Frege and Russell, say of an object that it exists or not because, so they argued, everything exists: how can one then divide up the objects in the world into those which exist and those which don't?"


An alternative approach to existence is pursued in https://plato.stanford.edu/entries/nonexistent-objects logics of non-existent objects, which go back to Meinong with more recent versions developed by Parsons, Priest, and Zalta. There one does have the existence predicate, denoted E!, in addition to the existential quantifier ∃. Such logics distinguish between "there is" and "exists", where the former applies even to imaginary objects like unicorns and Pegasus, while the latter is something more real. In paticular,  ∃x(¬E!(x)) expresses that there are objects that do not exist. But even these logics may not be in a real disagreement with Kant, but only regiment the use of "predicate" differently. This is subtle, see https://books.google.com/books?id=UobUqRMyUawC&source=gbs_navlinks_s Berto's Existence as a Real Property: The Ontology of Meinongianism for a more radical view that traces Kantian view of existence all the way back to Parmenides:


  "What the standard Platonic-Aristotelian interpretation puts in Parmenides’ mouth may be phrased by claiming that, for him, any concept including or entailing nonbeing in any form applies to nothing... much contemporary philosophy is dominated by what one may call the Parmenidean Thesis: everything exists... The philosophers we are about to meet have reinforced the Parmenidean position in two moves: (1) they have expanded the slogan “Existence is not a predicate” into the thesis that existence is (nothing but) property-instantiation; (2) they have also explained existence in terms of the logical notion of quantification. Both moves were made mainly by Frege and, derivatively, by Russell.
  
  [...] Here comes a non-Parmenidean approach. To begin with, “exists” is a predicate of individuals just like the others – a predicate for real, not only from the point of view of our ordinary language’s surface grammar. It is a predicate in the same sense that “eats”, “flies”, and “is a man” are... The motto is Alexander’s famous one: “To be is to have causal powers”... The Parmenidean conception, once forced to admit that even from its viewpoint existence can be a property of individuals, still explains it away, defining it via the existential quantifier and identity, that is, via logical notions. But existence is not taken as a logical notion from now on: if to exist has to do with the enjoying of causal powers, whatever these actually are, they are not logical features."

You make reference to 'our current beliefs', but there is not a unity of belief on animal welfare.  Witness the various limitations and protections on kosher/halal slaughter in various parts of Europe.  If modern discourse had a single standard here, we could harmonize the views of 'a painless death' with the kosher/halal view of 'a quick and merciful release from a healthy state of being' by appealing to some clear, shared, central standard.

But both are operating, current standards for what is humane, held by actual people in Europe.  (Not the center of the universe, but hopefully among this 'us' of yours.)  From the pro-restriction point of view, the pain of the knife should be dulled by drugs or electricity to make the death painless.  From the point of view defending kosher or halal standards, that just means the animal is not healthy and happy for as much of its life as possible, right up as close as is practical to the moment it dies.  From another, moderately paranoid version of this same notion, the meat is no longer 'natural', as it has been handled unnaturally, and we do not know with certainty whether or not it may be less than healthy to consume.

Clearly brainless chickens would never be happy, and would always be painless.  But they might also never be healthy, and they would clearly not be natural.  So I would say theories about them bother us because we are in fact insecure about what those four things have to do with humaneness.  But we should not pretend there is already consensus.

Given just the four concerns raised here, there are a range of relevant questions.  Many of us in the U.S. used to regularly eat chickens that lived too close together to be healthy without a continuous infusion of antibiotics.  Was that a moral change? or just a protection of the continued power of our medication?  It felt moral to those who proposed it, but most of us really did not care until the latter issue became relevant.  If we don't want sad chickens, would we eat a chicken that was happy only because it was on the chicken equivalent of 'catnip' its entire life?  Do we care about naturalness in that way?  If we value naturalness, we clearly cannot make a chicken's entire lifetime painless, as that would be unnatural, so why do we focus on giving it a better death than nature would?

From a Kantian point of view, behaviors that restrict future extension of compassion should be avoided, since universal compassion is a necessary component of the ethical process.  So natural sentiments like this matter, even if they are not rational, but merely project human concerns onto other species.

The question becomes whether the kind 'weirding-out' and subsequent 'getting-accustomed-to' that brainless chickens would require would make us less sensitive to other animals, and ultimately to humans, or whether our current habits of interacting with actively behaving chickens already do this to a greater degree.
First thing I want to point out is that you seem to be pre-assuming your position, and then trying to come up with arguments that support it, so that you can "win". This is, I believe, something we all often do, being more interested in finding arguments that make others think we're right than actually in being right. What I suggest is to first think by yourself, without standing in any position or having any bias, from a neutral and ignorant point, and coming up with arguments and counterarguments for yourself and all sides. Then, as you grow in wisdom in that matter, you may start having a position, which will feel more natural and according to what is true (or what is there of any truth).

I'm sure that if you do that you will come up with a lot of reasoning and arguments of yourself, maybe even change your mind a little, and in the process become more wise.

EDIT: So, applying this to your question, I would first start by trying to understand what is the theme of the discussion: Morality, that is, the rightness or wrongness of an action, and how should it be accessed. Of course, this brings up another question: what does it mean for an action to be right or wrong? Well, first, we have to know what an action is - it is something caused by an agent. Any kind of agent, like even the wind?, no, a self-conscious and in control agent (yes, this can give rise to a lot of discussion). This excludes any action that is not pre-meditated, like reacting impulsively, or stepping on an ant unaware. Now, this lets us understand that morality only applies to actions where there is some kind of free will able to impact the decision (and yes, this brings another lot of discussion, so you can see everything is linked). Okay, so now that we're a bit clearer on what is morality and what are actions, how can we tell if an action is right or wrong? Wait, why do we need/want to know how to tell if an action is right or wrong, why is it important to us? Well, as I see it that's because we have the notion of value, something which can't be described physically, the feeling or understanding that there are things which have an intrinsic, absolute value or importance, and want to act according to those values, or, in case of conflict, the highest value (I imagine this is the line of thinking through which Aristotle got to the same idea). The conclusion of this reasoning is then that we say an action is right or wrong whether it is or isn't according to a certain value; in morality, according to the highest or most important value (I wrote "certain" because we do not only use "right" and "wrong" with morality, but also when analyzing any action in relation to any value - example: "you are doing it wrong, that's not how we dance Gangnam Style", someone is not acting according to the value of dancing gangnam style properly). So we're entering here the line of thinking of  Virtue Ethics. Now, some problems arise. What is then the highest value, what are the values we should follow? How do we know that a value is in fact the highest one? Of course, there's a question we need to ask first: What is a value? After some thinking, I've come up with the following: a value is anything of importance to a subject, and in accordance to which it acts; a moral value, however, is something which is intrinsically important, which is not important directly to the self but the subject values it because it sees in it an absolute, non-material importance. Now on to the question of what is the highest value, and how can we know so? For being such a subjective, hard to answer question, it gives rise to a lot of fear and consequent need for an answer. Thus, we often either look for an answer out there somewhere and take it as absolute or listen blindly (eheh) to what others say - and that's one possible explanation for what is called faith (the belief that someone or something other than us has the correct answer, in this case), and for many cases where people follow ideologies blindly (many times in religion, through which we tend to say that we've got the answers). The alternative is to try and investigate by ourselves the answer to those questions, and maybe that is the correct path because we, with our minds, are the only reliable source that we have for finding that out. Even if we can't yet reach for the ultimate answer, we are able to arrive to reasonable ideas and, with that, we create principles that we will to use to guide our actions the most accordingly possible to the highest value. In this way, we create imperatives - ideas or values that we should always follow in our actions to be the nearest possible to what is right. We share these imperatives with others, who may agree and follow them, or not. Both in this case, where we're following principles that we've come up or agree with, and when following ideas from outside, as when we say, for instance, that everything in a given religious book is the truth and depicts the highest values, we are following principles and believing that people should always act in accordance to a given imperative - it is our duty. That's how we get to deontology. Okay, so this is all very pretty, but what about this: what we want is to always act with the intention of promoting the highest value, or is it that our actions always be such that their consequences promote the highest value? Well obviously what we want is to impact reality and not just try to, so the answer is we want that the consequences of our actions always result in promoting the highest value, and not the scenario where we want to, but it doesn't happen. This is the main argument behind Consequentialism. Why don't we all follow consequentialism then? Well, in a perfect scenario where we are perfect beings and know everything and how everything will turn out and have complete control of everything, that ideal would be possible. However, we are not perfect beings, but human beings, and do not know everything neither control everything. Thus, I think we should focus on what we do control for now, meaning above all and absolutely our intentions, and work around that, while we try and evolve in two ways: try to discover each time more and get closer to the highest value, and try and become more wise so that we understand better our universe and get closer to the ideal situation of consequentialism, where we know how everything will turn out. Meanwhile, we can also enjoy the trip, and the ignorance we have and the unpredictableness of the universe.

I hope through this you agree with me that all these theories (Consequentialism, Deontology and Virtue Ethics) are not against each other but rather are variations around the theme of morality, being interconnected instead.

Taking your example of the murderer, I know I can't control all the consequences, even less when dealing with other human beings. So I would focus on what what I can control, which is my intention and actions. I don't want to kill, so I won't kill, and I can't tell if the murderer is actually going to kill the other two if I don't or change his mind, and I don't know if if I kill one he will keep to his word.

I hope I demonstrated how better it is to do what I suggested and think by ourselves, focusing on finding the truth.

I wrote my own thinking and it is probably full of mistakes and disagreements, and am hoping for the discussion on the comments :)
Unjust prayers

You correctly note that God never does injustice to anyone but still we pray to God for our interest, even when such a prayer entails injustice. This is however not a problem with prayer but rather our flawed application of it. Of course God will not grant a prayer for an unjust action; such a wish is, I think, probably to be counted as a sin.

Prayer generally

But of course these are not the only kinds of prayers. The more interesting question is, why pray at all? After all, God has perfect knowledge, and God's will is immutable, so if you are in the kind of trouble that God would help you with, God is already knows about it and is already either going to help you or not.

From Aquinas's Summa Theologica, Part 2.2, Question 83, On Prayer, Article 2, Whether it is becoming to pray?:


  Objection 1. It would seem that it is unbecoming to pray. Prayer seems to be necessary in order that we may make our needs known to the person to whom we pray. But according to Matthew 6:32, "Your Father knoweth that you have need of all these things." Therefore it is not becoming to pray to God.
  
  ...
  
  Reply to Objection 1. We need to pray to God, not in order to make known to Him our needs or desires but that we ourselves may be reminded of the necessity of having recourse to God's help in these matters.


There you have it. We pray for ourselves, to remember our place in the world. That's Catholic dogma, at least, but the logic holds true for any religion with a similar conception of God as all-powerful, all-knowing, and all-good.

Poverty and the Bible

You end asking why the Bible says that God helps the poor when people can achieve success through hard work. However it is not the case that our fortunes are entirely under our control; this is a basic result of living in a world with other people and changing conditions. Our success is highly determined by the place in and kind of society we are born in, general economic trends, natural disasters, etc.

Humans have free will, so I think we can safely assume that economic trends are, generally, not the kind of thing God would interfere with. Similarly society is shaped by humans to be unequal. That's on us, not God. That leaves natural disasters and the like. For that, see https://plato.stanford.edu/entries/evil/ this excellent summary, and, if you still have questions,---and it's a fair bet that you will---ask them here!
Answers vary, depending on what school of thought the one responding adheres to. 

For accuracy, my answer would be a combination of both philosophy and psychology.

It is one of the basic principles of psychology that the self is composed of the "conscious," "subconscious," and the "unconscious." As the name suggests, the conscious is the part of the mind that is, well, conscious. It is the "visible" part of the mind. The thoughts we think are flashed in the conscious part of the brain. It is the "captain" of the ship.

The unconscious, on the other hand, is like the engine room. The unconscious is the storehouse of all our memories, even memories made in infancy (even in fetal stage). Within the unconscious, one can find the deepest drives that, though unknown to the conscious mind, subtly manifest themselves. There are many drives in the unconscious, but to summarize it, there's the "life instinct" and the "death instinct."

The subconscious serve as a gatekeeper of the information from the conscious to the unconscious. It also serves as the gatekeeper of the unconscious drives. 

Now, our acts are actually sometimes dictated by the unconscious. Why? Some memories are too difficult, too painful to process for the conscious that the subconscious buries them to the unconscious. In the unconscious, they, too, join the battle of tugging the death instinct or the life instinct. It is the reason why we sometimes just feel a "natural connection" with things, animals, or food, or an irrational aversion to them. Now, the repressed memory would want to surface, but the conscious would not let them. One of the defense of the conscious (or the ego) is further repression, intellectualization, rationalization, sublimation, altruistic surrender, etc. 

Yet, we may not be fully aware that we are already doing sublimation or altruistic surrender or intellectualization. The ego does what it does to protect itself.

Still, however, we may become aware of them through deep self-introspection or through psychoanalysis. Nietzsche believes that even though the self is continually changing, constant introspection will still allow us a deeper insight into ourselves. 

Furthermore, Nietzche believes that our actions are sometimes motivated by things which we have already forgotten about.
In physics there is a differentiation made between "vacuum" and "void". From an article entitled https://www.quantamagazine.org/20160705-feynman-diagrams-nature-of-empty-space/ "How Feynman Diagrams Almost Saved Space" by theoretical physicist https://en.wikipedia.org/wiki/Frank_Wilczek Frank Wilczek: 


  Vacuum, in modern usage, is what you get when you remove everything that you can, whether practically or in principle. We say a region of space “realizes vacuum” if it is free of all the different kinds of particles and radiation we know about (including, for this purpose, dark matter — which we know about in a general way, though not in detail). Alternatively, vacuum is the state of minimum energy. 
  
  Intergalactic space is a good approximation to a vacuum.
  
  Void, on the other hand, is a theoretical idealization. It means nothingness: space without independent properties, whose only role, we might say, is to keep everything from happening in the same place. Void gives particles addresses, nothing more.
  
  Aristotle famously claimed that “Nature abhors a vacuum,” but I’m pretty sure a more correct translation would be “Nature abhors a void.” 


In this view, vacuum refers to an actual region of spacetime. Remember that quantum field theory tells us that there are fields that exist all throughout the universe. Even if you remove particles from a region of spacetime the fields themselves still exist in those points, they are just in a https://en.wikipedia.org/wiki/Vacuum_state minimum energy state so there is no actual particle or radiation in that space. If you were to put a field vacuum inside of a box (build a box around a particular region of vacuous spacetime?) then that region would still have a dimension of length, because it is still spacetime that is permeated with minimum energy field vacua. 

On the other hand, a void is the idea of a region of space and nothing else. It doesn't have minimum energy field vacua, it doesn't even have a time component. It is the abstract concept of just having a region of space. If you were to have a box that had a void inside of it it would still have a length because it is the definitional concept of space. From Wikipedia:


  https://en.wikipedia.org/wiki/Space Space is the boundless three-dimensional extent in which objects and events have relative position and direction.


Even in a void, a theoretical region of space that has nothing inside of it, it still has spatial dimensionality because it is, by definition, space. 

A third concept, the concept that you are most likely meaning to refer to, is the idea of an absolute nothingness. Nothingness, or emptiness, is what is called a universal, or an abstract concept. There isn't a region of the universe you could go to that would contain nothingness because all regions of the universe have spatio-temporal dimensionality. So you wouldn't ever be able to find a region of actual "nothingness" to enclose. However, that brings up the problem of universals vs particulars, or even just abstract objects in general. Here are two introductory resources that will help you get a better understanding of the subjects: https://plato.stanford.edu/entries/properties/ Properties and https://plato.stanford.edu/entries/abstract-objects/ Abstract Objects. It is not a closed debate about whether or not universals, such as "nothingness", or abstract objects in general exist. There are good arguments on both sides and this is a question that has been discussed at the forefront of western philosophy since Plato and Aristotle. Whether or not "nothingness' as a universal exists, you still would not be able to encapsulate it within a box. As for vacua and voids, however, both of those do have dimensionality because both of them are bounded by spatial dimensions. 

To state definitively and succinctly: a vacuum would have a size of one cubic meter in your example; a void would have a size of one cubic meter in your example; the abstract concept of "nothingness" would not have any sort of size and could not be put into a box because it refers to the abstract concept of nothingness. Hopefully now it makes more sense why the definitions you chose really matter and a question such as "If 'nothing' is encapsulated by 'something', does this imply it has size?" at face value is a little vague. 
I would suggest instead that in the case of rectilinear uniform motion, for a single body, we accept that it is not a change or movement in the Thomistic-Aristotelian sense. Let me be careful here. Of course, this directly contradicts Aristotelian physics itself, but we are trying to adapt the doctrine to the world that accepts Newtonian mechanics, and relativity with quantum physics on top of it. In this world we need to distinguish physical change from the pseudo-change that is entirely due to our descriptive devices. I submit that rectilinear uniform motion belongs to that latter category (and arguably so do quantum mechanical collapse, and hole transformations in relativistic gauge theories). Aristotle is unlikely to have accepted looking at an apple from a different angle as a change in the apple. If he learned of the nominal character of the rectilinear uniform motion he is more likely to have discarded his physics rather than his metaphysics, and adopted the same position.

A sign that rectilinear uniform motion is not a physical process is in the fact that it can be eliminated by choosing an appropriate reference frame, namely the one comoving with the body. This would not work with other types of motion because comoving frames introduce real physical effects, the so-called https://en.wikipedia.org/wiki/Fictitious_force fictitious forces like the centrifugal, which are real enough to kill you. But none of that happens under the rectilinear uniform motion, all inertial frames are physically equivalent in Newtonian physics as well as in relativity. If we analogize spacetime to space "moving" uniformly amounts to choosing coordinate axes. The only "change" is that of conventions and spatiotemporal location. To make it "real" we must take the spacetime per se to be real, and indications are that such spacetime substantivalism goes against the grain of modern physics. https://plato.stanford.edu/entries/spacetime-holearg Einstein's hole argument is directed against it:


  "If one has two distributions of metric and matter fields related by a hole transformation, manifold substantivalists must maintain that the two systems represent two distinct physical systems. This physical distinctness transcends both observation and the determining power of the theory since: The two distributions are observationally identical. The laws of the theory cannot pick between the two developments of the fields into the hole."


And here is Einstein's conclusion in his own words:


  "Formerly, people thought that if matter disappeared from the universe, space and time would remain. Relativity declares that space and time would disappear with matter." 


Fictitious change requires no mover. But while rectilinear uniform motion of a single body is physically indistinguishable from rest, rectilinear uniform motion of two bodies relative to each other is physical: there is no frame in which both are at rest. One way to deal with it is to "meta" the above reasoning. In Aristotelian physics rest is the only "baseline" state, everything else requires a mover, but this is not the case in Newtonian, let alone modern physics. They admit multiple "baseline" states known as https://en.wikipedia.org/wiki/Vacuum_state vacua. Being in a vacuum state requires no "cause", only exciting out of it does. Nonetheless the so-called zero-point energy of vacua is positive, which means that something is going on there (in quantum field theory it is sometimes identified with incessant https://en.wikipedia.org/wiki/Zero-point_energy creation/annihilation of virtual particles). In our case we can declare systems where all bodies move rectilinearly and uniformly relative to each other to be mechanical vacua, on equal footing with the Aristotelian total rest. It would be a stretch to call the steady-state change in vacua "fictitious", but in a way it is uninteresting, vacuous, the motto then becomes that vacuous change requires no cause. It is in line with the Newtonian physics where the cause, force, is responsible for only for accelerating bodies, not just moving them.

If this is deemed unsatisfactory, we should recall that Aquinas's understanding of causes is more subtle than of those in temporal chains of events. If we do only admit the total rest as baseline it is natural to inquire how the bodies acquired those uniform velocities they display. The only way to get them  starting from rest is to accelerate, which only forces ("movers") can do. Hence we still have a "mover", albeit a remote one. This still leaves the puzzle of the motion continuing after the mover stopped acting, but a solution to that was suggested as early as Philoponus, if not already Hipparchus, see https://www.jstor.org/stable/3654219 Avempace, Projectile Motion, and Impetus Theory by Franco. It was that the mover impresses a force upon the body, which continues to move it even after the end of direct contact. It was needed to fix Aristotle's "theory" of projectile motion, which Philoponus mocked by pointing out that on it one could make an arrow fly by waving hands behind it. The impressed force, later dubbed https://en.wikipedia.org/wiki/Theory_of_impetus impetus, was popular with Islamic Aristotelians, and with Buridan et al. in Europe soon after Aquinas. Mechanical momentum is the modern descendant of impetus, and the momentum conservation law can be interpreted as saying that changing momentum requires an intervention (external "mover"), while  momentum itself is the internal "mover", the faint trace of past impressions.
The question seems a bit like: I have a half-assed quasi-formalization of a cartoon version of science, why am I getting a weird conclusion from it?  Okay, that was a bit harsh (please forgive my rhetorical flourish), and the idea presented isn't totally wrong, it's just that it over simplifies things to the point where you've lost some important details.  

Since you've tried to formalize the problem and discuss it in terms logical proof, your question calls to mind the ideas of https://plato.stanford.edu/entries/logical-empiricism/ Logical Empiricism.  

Supposed you put on a http://www.iep.utm.edu/con-meta/#SH2b Logical Positivist hat and tried to formalize all of the evidence as well as TENS itself, in a manner along the lines of  http://www.iep.utm.edu/carnap/ Carnap. Could it work out that that formal version of TENS is a theorem derivable from E? Sure, it depends on what E is.
Indeed, sometimes the idea of evolution, when cast as "survival of the fittest" is viewed as a tautology, and thus necessarily true; "survival of the fittest" is not really a complete description of the theory,  but being a tautology it would enter the formalism as an axiom within this project, an axiom that gets you very close to proving TENS ... maybe.

This might seem like a stretch in biology, but it happens regularly in high energy physics (and probably other places in science that have a similar degree of mathematical formalization).  For a simple case, if you take conservation of energy, momentum and angular momentum as axioms, and observe neutron decay (and a bunch of axioms that allow for the interpretation of these observations), then the neutrino must exist.

In some (many?) cases this kind of formal description and analysis can be useful, if only to really clarify a representation of what you're talking about, but it can't be the end-all be-all of science for all of https://plato.stanford.edu/entries/logical-empiricism/#Iss the reasons that Logical Positivism/Empericism folded.

I'll leave off with what I think are a few additional salient points:


Scientific proof is not universally defined as formal logical proof amongst scientists nor especially in common parlance.  Different scientific communities have different methods and criteria for establishing "proof"; compare high energy physics to medical science.  This almost always involves some form of empirical verification, and is usually expressed in a statistical sense, which is another deviation from formal logical proof.
TENS, when looked at in detail, is not "one theory" it is a class of ideas. E.g. I'm under the impression that it is an open question whether group selection is a real (in some sense) phenomenon (people in the field could probably pick out other open questions); which ever way this question, or the other open questions, pan out, you're still dealing with TENS.
Observations can rule out entire classes of scientific ideas.  If you get enough observations rule out a bunch of other ideas, such that for everything that is left as a possibility is still in the class of TENS; then you've "proven it"; again, I've seen this more in physics (more mathematically rigorous) where experiments rule out enough different possibilities that what is left is some variation on one kind of idea. 
I get the impression that the term "demonstrated as a good explanation" is a better way of understanding what "scientific proof" means.  Demonstrated in the sense of having evidential support and good entailing some combination of empirical adequacy, generalizability, simplicity etc. 

By way of preable to answering I'll quote Robert Trumbull's PhD dissertation, online here:

http://escholarship.org/uc/item/9p43t6nf#page-4 Derrida, Freud, Lacan: Resistances


  The death drive ... is Freud’s attempt to envision a force present in
  the living, but antithetical to life, a drive opposed to the drives
  that sustain organic life. At the same time, Freud views this death or
  destruction drive as a type of aggressivity central to the formation
  culture. Tracking Derrida’s thinking on the death drive across his
  work, I demonstrate how this figure and the notion of “life death” it
  suggests come to be at the center of Derrida’s engagement with Freud.
  Through close readings of Derrida’s work, I trace how he reads Freud’s
  writing against itself, locating there something Freud himself does
  not entirely think through.


Freud wrote about the death drive, Thanatos, in Beyond the Pleasure Principle.  Prior to that he theorised that the pleasure principal motivated behaviour.

Also of relevance in the context is the http://www.terrapsych.com/freud.html Nirvana Principal:


  Nirvana Principle: Barbara Low's term for the psychological equivalent
  of homeostasis, the push for the least amount of tension. Different
  from the pleasure principle in that 1. pleasure sometimes increases
  with tension, and 2. the Nirvana Principle is primarily under the sway
  of the death drive, whereas the pleasure principle is powered by Eros.


Derrida analysed and developed Freud's theories in https://www.amazon.co.uk/d/cka/Postcard-Socrates-Freud-Beyond-Derrida/0226143228/ The Postcard, specifically in the essay To Speculate--on "Freud", an extended commentary on https://en.wikipedia.org/wiki/Beyond_the_Pleasure_Principle Beyond the Pleasure Principle.

Freud published Beyond the Pleasure Principal in 1920, shortly after World War I.  Mankind going to war contradicted his Pleasure Principle, so he proposed the Death Drive.  As it were, an opposing drive.

In fact, the answer lay deeper and connected with his other observations on the repetition compulsion.

As Derrida observes, the root of the repetition compulsion is a deep desire to understand problems.  (The relief/pleasure of solving them links to the Nirvana principle.)  People are drawn to repeat what they do not understand.  It is part of the mechanism that enables a person to master their environment - an essential for survival.

Tenacity and problem solving can spill over into OCD because the drive is deep and not primarily conscious.  E.g. the unputdownable crossword puzzle.

Mastery can make a person able to flourish and build, but then again it can lead to invasion and plunder.  Historically, and in uncivilised times, this is survival of the fittest.  The primary drive for Mastery aligns with the most basic evolutionary principle.

The pleasure principle is not opposed to mastery, however it is subservient to it, in the same way as the pleasure principal is subservient to the Reality Principle.  The Reality Principle differs from the drive for mastery in that is it more conscious and deliberative.

The death drive is one name for the drive for mastery.  Another would be the war drive, or contrastingly, the civilisation-building drive / culture-building drive.

It is only because it is more complex than the pleasure principle that is can appear to oppose it, for example when it provokes war, or simply when the unputdownable crossword puzzle drives you to distinctly unpleasureable distraction.
Philosophers like Tomas Aquinas and Agustin Hipona wrote that we have fear of death because we were not born for it. Agustin Hipona, http://www.gutenberg.org/files/45304/45304-h/45304-h.htm#Page_436 De Civitate Dei/City of God, Book XI, Ch. 11, 27


  What! do not even all irrational animals, to whom such calculations are unknown, from the huge dragons down to the least worms, all testify that they wish to exist, and therefore shun death by every movement in their power? Nay, the very plants and shrubs, which have no such life as enables them to shun destruction by movements we can see, do not they all seek, in their own fashion, to conserve their existence, by rooting themselves more and more deeply in the earth, that so they may draw nourishment, and throw out healthy branches towards the sky? In fine, even the lifeless bodies, which want not only sensation but seminal life, yet either seek the upper air or sink deep, or are balanced in an intermediate position, so that they may protect their existence in that situation where they can exist in most accordance with their nature.





  Quid? animalia omnia etiam irrationalia, quibus datum non est ista cogitare, ab immensis draconibus usque ad exiguos vermiculos nonne se esse velle atque ob hoc interitum fugere omnibus quibus possunt motibus indicant? Quid? arbusta omnesque frutices, quibus nullus est sensus ad vitandam manifesta motione perniciem, nonne ut in auras tutum cacuminis germen emittant, aliud terrae radicis affigunt, quo alimentum trahant atque ita suum quodam modo esse conservent? Ipsa postremo corpora, quibus non solum sensus, sed nec ulla saltem seminalis est vita, ita tamen vel exiliunt in superna vel in ima descendunt vel librantur in mediis, ut essentiam suam, ubi secundum naturam possunt esse, custodiant.


(I prefer that one in Latin because To translate is to betray)

So those who share Life are afraid to lose it, and living things share Life, because if death was ours then we couldn't lose it.

For theist philosophers like Plato, Plotinus, Aristotle, Socrates, and more, you should be afraid of death because at the end (when humankind dies) we will be judged by God if we have loved one another.

Another, more poetic, point of view come from 

https://books.google.it/books?id=4OfWWfRDAXcC&lpg=PP1&pg=PT29#v=onepage&q&f=false J.R.R. Tolkien, The Silmarillion, Ch. 1, Of the beginning of days


  [Iluvatar] But to the Atani I will give a new gift.


On the beginning of world that host the adventures of Middle-Earth Iluvator, that is the name of the Creator on Tolkien, gives the death is a gift to the Atani/humankind. Elfs instead remain alive still the World is life. Basically there are strong linked with the World but that doesn't make them happy indeed the prefer go back to Valinor where the Valar lives
Just because tigers do not speak English, it does not mean that tiger's pains and joys have no value. Humans are primates; primates are animals. What makes you think one animal's pain and joy have more value than another? Just because the other one cannot speak English? Is it ethical to take the bread out of a child's mouth to feed your own simply because that child is a tiger cub?

Historically, the tiger's silence has been abundantly taken advantage of by some famished English-speaking hominids.
The form of the reasoning here is: "we can not do everything, therefore we should not do anything", or more colloquially "nobody's perfect, so why bother". The commonly used names are the http://www.iep.utm.edu/fallacy/#Perfectionist perfectionist fallacy and the https://en.wikipedia.org/wiki/Nirvana_fallacy perfect solution or nirvana fallacy:


  "By creating a false dichotomy that presents one option which is obviously advantageous—while at the same time being completely implausible—a person using the nirvana fallacy can attack any opposing idea because it is imperfect. Under this fallacy, the choice is not between real world solutions; it is, rather, a choice between one realistic achievable possibility and another unrealistic solution that could in some way be "better"."


In addition to misguided perfectionism, there is another issue here, which is also potentially fallacious. "We can not do everything" is a more or less uncontroversial statement of fact, but "we should not do anything" is a norm, prescription. Deriving norms from facts, or prescriptions from descriptions, or "ought" from "is", in Hume's classical example, is generally a fallacy, called the https://en.wikipedia.org/wiki/Is%E2%80%93ought_problem naturalistic fallacy or Hume's guillotine. How things are need not imply anything about how they should be, or what we should do.

Very similar in spirit is the fallacy of relative privation, see https://philosophy.stackexchange.com/questions/24622/what-fallacy-dismisses-problems-by-presenting-bigger-problems/24623#24623 What fallacy dismisses problems by presenting "bigger" problems? The perfectionist fallacy also has a curious opposite, known as the https://en.wikipedia.org/wiki/Politician%27s_syllogism politician's syllogism:"We must do something.     This is something. Therefore, we must do this". Clearly, neither doing everything nor doing something random is the right approach.
This really depends on what the process is. You seem to be assuming several things:


All souls must be on Earth
All souls must be in use at once/as soon as you die you go straight into a new life.
Souls cannot be split
There was a set number of souls to start off with.


If any of these assumptions are wrong then there is a solution:


As one civilisation dies in another galaxy there is could be another popping up somewhere in the universe.
Maybe there are trillions of souls and they just wait their turn until a space is available, a soul might have been incarnated as one of the first humans and only just now get its ticket drawn for a second incarnation.
Perhaps it all started out as one soul and it gets divided up among people as there are more of us (might explain all the people thinking they were once Napoleon).
Maybe new souls are created as the need arises.


Where there are other possible solutions (especially when we know little about a process) we can't rule it out with assumptions that have no basis.
I see two possible lines of thought to answer this. First:


You see other people today in this world.
You can imagine the point of view of one of these other people, through being physiologically very similar to yourself.
The world exists now because you are in it and can observe it.
Similarly, you can imagine a different person being a main character, thus having the world exist without your existence.


Another point of view could be that being born and death are events contained within the world, and so is any idea involving the concept of time. The world cannot stop existing, or disappear because these are verbs relating to time, and only make sense within the world itself. If you think of the universe from a point of view outside the universe (so that time and space are just properties of this abstract object), you must conclude that as a whole it doesn't change in any way. Therefore whatever property you ascribe to it, such as existence, it either has this property or it doesn't.

If this sounds too alien, just consider that the universe could well be a mathematically definable construct similar to let's say - the Fibonacci sequence, or fractals or cellular automata. You wouldn't say that the sequence ever changes as a whole, although it contains a rule-based progression, and when following these sequences you might notice patterns come and go.

Neither of these explanations are of course complete, because existence can be defined subjectively, so that the word isn't meaningful without also specifying a subject. And if that subject is you - whose lifespan is only a tiny blip inside the universe, we must conclude that the universe's existence is also a tiny blip within itself. Or we might conclude that indeed the idea of existence doesn't apply to whole worlds, only specific things inside worlds.
Natural rights encompasses human rights. Humans are, after all, a component of nature. I would add the diagram, but couldn't do it now, as I am having problems attaching the pictures.

To get back to the topic, natural rights encompasses human rights. You could consider natural right as a universal set, and human rights as its subset. Hope this answer helps. 
It seems a good question to me and I would answer in the affirmative. But the idea needs tweaks. The word 'before' for the BB is okay in ontology (prior, or 'reducible to') but cannot be given a temporal meaning.

What emerges from the practices of Yoga is the very idea you put forward, that the source of consciousness is prior to the space-time world thus prior to the BB. This Source would be free of phenomenal attributes (cf. Kant's thing-in-itself), thus it would be incorrect to call it Something or Nothing.    

This would be an explanation for why it makes no sense that our Origin is Something or Nothing, as philosophers have always found. The view you are asking about is probably 'non-dualism', which is the rejection of all positive metaphysical theories and which some would call the 'perennial' philosophy. 

If this is your speculation then you have a lot of support for it. To turn it from speculation to knowledge would be no easy task, however, so the case for it usually best argued in metaphysics, where the idea that the world begins with Something or Nothing is famously unworkable.    
Perhaps the best way to understand hypotheses is in terms of logic. We use hypotheses to learn about laws or principles that govern the behavior of things, and a common form of expressing such a principle is:


  ∀x[Px → Bx]


This might be read as: For every x which has some property P, x will exhibit a certain behavior B.

A hypothesis is nothing more than proposing such a principle so that it can be put to the test. In your example about grouping a marbles, the grouping would pertain to some particular property P whether it be of size or color. However, what is absent from your hypothesis is a behavior B. You are grouping the marbles arbitrarily without attributing any particular significance to the grouping.

You have to ask yourself: What effect can be brought about by grouping marbles by color (or size)? According to what principle? Once you answer that, you can put the hypothesis to the test to discover if it conforms to reality.
You are confusing the empirical act of measuring versus the concept of measurement used in theories such as SR.

In theories such as SR, the idea is that if you take a measurement within any reference frame, it is "valid."  It describes the object correctly.  What is not included is how difficult this measurement is.  When it comes to the world of metrology, where we study how well we can measure things, you are traditionally correct.  The best measurements are those where we can lock down every unknown variable possible.  However, that is not a fact of spacetime.  That's merely an artifact of the hardware we use to measure.

If I may offer an example, consider a case where the stationary observer measuring the length of an object with a ruler is piss drunk.  They can barely see straight.  Meanwhile, my moving observer* is in a lab with millions of dollars of fancy measurement equipment.  I would expect the lab measurement to be more accurate than the drunkard's.  That has nothing to do with the extra mathematical details of the Lorentz boost, and everything to do with the fact that the moving lab has better measurement equipment.

* As nir pointed out in the comments, technically an observer in SR is not actually a person, but merely a frame of reference.  I use observer here in the colloquial sense because the original question appears to be using "observer" in the colloquial sense as well, and it looks like some of the confusion may stem from the fact that SR doesn't actually need an individual observing.
This is more a question of semantics than a question of philosophy.  Most can clearly see the similarities and differences between a biological living thing and a language.  

But whether we call language a living thing or not is merely to do with how broadly we use the term "living thing" in our speech rather than related to the true essence of "language" and "living things".
I parse the question into this: why cultural relativism (CR) cannot be justified from the standpoint of tolerance? For this, let's first clarify what CR is. CR can be descriptive or prescriptive. As a descriptive idea, CR states the observation that morality is relative to each culture. As a prescriptive, moral theory, CR asserts that morality must be founded on each culture. All moral values are nothing more than customs, social norms and/or legal practices. I focus on this moral theory side of CR in this post.

As a (prescriptive) moral theory, CR is already very problematic, CR as a moral theory is based on the so-called naturalistic fallacy (inferring what one ought to do from what one is). Also, under CR, a moral revolutionist (like M.L. King) becomes a common criminal. CR cannot explain why stoning couples for marrying across different castes is wrong. Indeed, CR can defend any clearly morally wrong practice insofar as the practice is the norm in the society. Consequently, CR cannot offer guidance for moral progress. For this reason, no moral theorists  are motivated to defend CR. 

Setting aside the fact that CR is an inadequate moral theory, can CR be defended nevertheless for the reason that it is based on the value of tolerance? Surely, tolerance has some goodness going on. Like individuals, each culture has its own idiosyncrasies due to existential contingencies. Many cultures around the world find it offending when some globally dominant culture judges their cultural norms as morally wrong or inferior. Not exercising tolerance can be viewed as imperialistic or paternalistic.  

Does this means that CR can be justified from the standpoint of tolerance? If tolerance is an intrinsic value, then CR can be, but it is not. Tolerance can be good or bad. While we should be tolerant towards differences in general, there are times when being tolerant borders on acting cowardly: e.g. tolerating a bully.  Tolerance is valuable when it serves some higher values. John S. Mill, for instance, argued for tolerance for the sake of liberty, which he viewed as a source of human happiness (cf. David Brink's Mill's Progressive Principles). John Rawls argued for tolerance (through epistemic modesty and neutrality on the good) to show the ideal of equal respect. Since tolerance itself has no steady foundation, CR justified by tolerance will be even more unstable, which is why the argument from tolerance to CR fails.    
Certainly in this case, not acting is itself an action. It takes no great physical effort to pull the lever. Your only choice is the mental decision whether or not to pull it. So that choice isn't acting or not acting; it's one action versus another action. You can't "not act"; either way you're "acting".

Consider a related problem, as follows. The train is barrelling down the tracks, but in such a way so that all six people will be killed. And your lever is in a "neutral" position such that all six will indeed die. But it has two alternative non-neutral positions you can push it to: one where five will die, and the other where one will die. Any question here??? Clearly, the only ethical choice is for you to "kill one". But do nothing and he dies anyway. So you're not really "killing one"; your three choices are "save five" or "save one" or do nothing and "save nobody".

Likewise, the original problem just boils down to "save five" or "save one". The "initial condition" that the lever happens to be in the "save one" position just isn't relevant to your ethical decision. Forget about the initial position. Just decide: do you want to save five or do you want to save one?
You're right, the tractarian language is a logically perfect language, a language which we don't actually know yet, but which must, so Wittgenstein believed at the time, underly every natural or scientific language. This perfect language is, at the level of elementary propositions, isomorphic to the world. This follows from a clear order of dependence: Facts are primary. Thoughts are "pictures" of facts. Propositions "express" thoughts, and thereby are also pictures of facts. The picturing/depicting relation is bijective.


  4.014 A gramophone record, the musical idea, the written notes, and the sound-waves, all stand to one another in the same internal relation of depicting that holds between language and the world.


The Tractatus does not offer a theory of natural languages. What it says, in a commenting way, is that the structure of a natural language can, and does, totally contort and obscure the underlying perfect, logical language. So that a natural language is not expected to correspond to the world in any apriori specifiable way. So, no homomorphisms, or any other morphisms, are expected to hold.


  4.022 Man possesses the ability to construct languages capable of expressing every sense, without having any idea how each word has meaning or what its meaning is — just as people speak without knowing how the individual sounds are produced. Everyday language is a part of the human organism and is no less complicated than it. It is not humanly possible to gather immediately from it what the logic of language is. Language disguises thought. So much so, that from the outward form of the clothing it is impossible to infer the form of the thought beneath it, because the outward form of the clothing is not designed to reveal the form of the body, but for entirely different purposes. The tacit conventions on which the understanding of everyday language depends are enormously complicated.

Assuming time has the following characteristics? I don't really agree with any of the characteristics there. What does it mean to say the present is fictional?

To me those characteristics are fairly characteristic of metaphysical assertions which transgress the boundaries of sense.

I think the river analogy is the most common one. Wittgenstein had some great things to say in his Blue and Brown books about how the analogies people make with time cause a lot of the philosophical muddles people get into. When people see an analogy, they tend to ask questions that don't make sense. E.g. there is a certain analogy between time and a river. That is all well and good, but it runs into problems when people carry the analogy too far (like Augustine did with his puzzlement about time). 

If one imagines a log flowing down a river, while we stand on the bank, we can imagine saying the log is now downstream, now in front of us, now upstream. That kind of analogy is used to describe events in time. One can ask meaningfully "the log has gone away, I wonder where it is now". But to carry this idea over into the analogy with time is the misstep in thinking. It causes one to ask questions like " oh, I wonder where the past has gone, and whether it exists"
There is another similar question active on this topic: https://philosophy.stackexchange.com/questions/14768/do-human-rights-exist Do human rights exist?


  What are human rights?
  
  With so many cultures in the world, is it possible to determine what rights should be intrinsic to all humanity?


There are several good answers in the other post, and rather than rewriting the whole thing, I am just going to point to that. 


  Is it fair to criticize a separate culture's "human rights" values within their own sovereignty? 


As an addendum to the answer I contributed, I would argue that since human rights are artificial, they must be strictly confined to the sovereignty of those who agree to them. The inexorable rights described in the US constitution are limited to those with in US borders and US citizens abroad who are still protected by the US government. There are several international accords and agreements that would be applied to the those participating nations, but to extend them further becomes a suspect endeavour.

To expand into my own opinion, the correct way to protect those rights universally would be to extend invitations to all people to join your culture and reap the protections that it awards them. As with other issues of sovereignty, it is not correct to dictate to those people who have not consented to your laws (either explicitly or tacitly). As I state in my other answer, the natural distaste for the suffering of others is intrinsic to the nature of humanity, but it cannot be justification to assert one's moral, political, religious or other perspective on those who do not ascribe to it. There is an equivalence in interference, regardless of justification, that cannot be parsed between right and wrong.
No.

In propositional logic, an argument is valid IFF (1) it is inconsistent to assert all the premises and the negation of the conclusion (semantic validity), or (2) the rules of inference allow you to derive the conclusion from the premises (syntactic validity). 

Let's go with definition (2) first. Suppose you have a valid argument P, Q |- R. That means you can derive R from P and Q. Adding extra premises, S, T, cannot prevent you from deriving R from P and Q.

Now let's go with definition (1). Suppose you have a valid argument, P, Q |= R. That means that {P, Q, ~R} is inconsistent. Adding extra premises cannot make that set consistent.
There is a gap between the fact that at a low level knowledge is hierarchical and the notion that it is so throughout.  At a relatively low height in the hierarchy, one clearly loses the ability to measure height across the structure and to define what is and what is not 'more knowledge'.  From a Montessorian perspective, what is really 'a better use of horme' is what accords with the given organisms pattern of growth, and that varies across a population even in ideal circumstances.

Having intimate knowledge of Kabbalah, or of how to best use a PDP-11, does not elevate your leverage on power, unless you are in very specific circumstances.  You cannot claim these do not constitute more knowledge.  But you can argue we should not value them.

The basic idea that public education aims for complete coverage at a given height reflects this intuition.  We want you to graduate, because we want to empower people as much as we can as long as we know we are actually empowering them.  We want to raise them to the degree that they are free to make their way in whatever direction they see as 'upward' from there, knowing this is going to vary almost full circle across the population.

And this divergence begins well before that point.  Classes in High School, or even in Middle School begin to be of real use only to certain people who will head in certain directions.  I would contend that this is not about our system of education, but is the nature of knowledge.  Even in some kind of education that fits with a goal of equality and not a traditional standard of 'excellence', this will still be the case.

I would suggest that anarchy (or even related approximations like ideal liberal democracy) is only going to be possible if you maintain access to some given height, that all of you can agree makes sense.  Otherwise, given people will automatically have power given to them by accidents or parental choices that put them on the right track toward useful knowledge, even if you undermine the accumulation of power completely in other ways.

If everyone can get near that point, and build out from there, and when massive changes occur, can return to that point and choose a different orientation, then one can always see one's own direction as 'upward', and others' notions of 'upward' as 'sideways'.  You can have leverage on reality of certain sorts, without absolutely differential access to control.
The issue in the example seems to be that the word "dominate" is used in two different senses. When this is done in an argument (it is not clear that this is so here) the fallacy is called https://en.wikipedia.org/wiki/Equivocation equivocation.

However, some language in the post suggests a different type of reasoning, namely inferring from the mere fact of (statistical) domination of men in the CEO positions, or women in teaching, that there is an organized effort to maintain male or female (power) dominance there. The equivocation might be of minor use to bring about the thought but the major fallacy is then different, it is a case of illicitly inferring intent, https://en.wikipedia.org/wiki/Correlation_does_not_imply_causation cum hoc ergo propter hoc (Latin for "with this, therefore because of this"), correlation implies causation.

Now, such reasoning is not necessarily fallacious when used for heuristic purposes. Correlation certainly gives one good grounds to suspect causation, and if not direct then perhaps a roundabout and harder to detect one. While there is no conspiracy of men to keep women out of CEO positions there is (and even more so in the past, was) a culture of behavior in big business that disadvantages women. It may not be maintained by men fully consciously, but they do maintain it by conforming to rather than challenging it. Of course, the mechanism for the teaching jobs is completely different.

Some of this complexity is captured by the legal doctrine of https://en.wikipedia.org/wiki/Disparate_impact disparate impact (as opposed to https://en.wikipedia.org/wiki/Disparate_treatment disparate treatment), which can be used to alter a hiring practice even if it has no overt intent to discriminate, to err on the side of caution, as it were.
This question is more like "Why does a dog move its tail?" Some people answer that for the obvious reason the tail cannot move the dog!
Then an obvious answer to the question why do we take pictures might be because pictures aren't photographers! When we ask "Why do we seat?" We might get simple answers like "Because we are tired." Or "Because there is a space to do so." The second answer makes more sense-obviously. In the same fashion we can give a multiplicity of answers why do we take pictures or why do we do anything. Descartes posed the statement "Cogito ergo sum" proving his own existence. Kant postulated the Noumenal World, an idea from the Greeks, like the Theory of the Forms. There are indeed deeper questions than to ask about why do we take pictures, etc. The fact that our senses are finite, and that we move around our own definite Phenomenal world, would be enough to convince us that no matter what we do, its is impossible to know for sure what is real and what is not. That is, of what exists and exists not. Only relativism give us our own subjectivity. Indeed seeing is believing. As to why do we take pictures with our camera or phone: "Because we have them and we get happy when we take them." The irony is that no pictures can be taken beyond Phenomena.
In discussions of appeal to ignorance, one common distinction is between "absence of evidence" and "evidence of absence."  "Absence of evidence" means that no or very little evidence is available either way (neither for nor against the proposition in question).  Is there life on https://en.wikipedia.org/wiki/Europa_(moon) Europa?  We don't yet have evidence either way, so concluding that there is life on Europe would be fallacious.  There's a lack or absence of evidence.  "Evidence of absence" means that there is evidence that something isn't there.  We've looked for life on the Moon, and gathered enough negative evidence to be reasonably confident that there is indeed no life there.  There's sufficient evidence to conclude that life is absent on the Moon.  

There are at least two complications worth mentioning here.  I'll illustrate this with a chemical safety testing example.  Suppose we want to know whether chemical X causes cancer.  We've run one small study, in lab rats, and the effect was not statistically significant.  

First, the basic difference between "absence of evidence" and "evidence of absence" is whether we have "sufficient" evidence, or can draw "reasonable" conclusions.  But different people may want to use different standards for sufficient evidence.  The manufacturer of chemical X might claim — based on the one small study in lab rats — that we have sufficient evidence to conclude that chemical X does not cause cancer.  They see this as a case of "evidence of absence."  But environmentalists might claim that this study doesn't provide sufficient evidence to draw any conclusion.  They see this as a case of "absence of evidence."  

Second, when we're talking about practical reason, we may have good reason to act as though a claim were true even if we don't have good reason to believe it.  For example, environmentalists might maintain that we should limit the use of chemical X — treating it as though it did cause cancer — even though we don't have good reason to believe that it really does cause cancer.  This doesn't mean that we should simply assume that chemical X causes cancer on the basis of no evidence.  Instead, the idea is that the default position is that chemical X causes cancer, and we need sufficient evidence to conclude that it's safe.  (The manufacturer, on the other hand, might argue that the default position should be that chemical X is safe, and we need sufficient evidence to conclude that it's dangerous.)  
"Revolutionary" is ambiguous here, depending on the context; it could mean either backward-looking (the bourgeois/capitalist revolution overthrowing feudalism) or forward-looking (the socialist revolution overthrowing capitalism) or both.  

To make things a little more concrete, consider https://en.wikipedia.org/wiki/Business_process#Adam_Smith Adam Smith's discussion of a pin factory.  In a pre-industrial economy, each pin is made by a single skilled individual.  (Smith gives a list of steps that's something like drawing out the wire, measuring and cutting it, pointing one end and capping the other.)  Putting a bunch of pin-makers in a single room together doesn't really make them more efficient or productive, because they're each still working individually, one pin at a time.  

A factory, by contrast, is organized in what Henry Ford dubbed an assembly line.  Each worker is responsible for exactly one step of the process.  This (according to Smith) allows for more efficiency and increases production.  (For some critical discussion and context, see http://www.nakedcapitalism.com/2014/06/michael-perelman-fraud-adam-smiths-pin-factory.html here.)  In the nineteenth century, assembly lines made effective use of steam engines (cf https://en.wikipedia.org/wiki/Cotton_mill#/media/File:Cotton_mill.jpg https://en.wikipedia.org/wiki/Cotton_mill#/media/File:Cotton_mill.jpg). This was a revolutionary increase in production relative to the old feudal system of piece-work manufacturing.  

That was the backward-looking sense of "revolutionary."  For the forward-looking sense, consider the social requirements for organizing a factory.  An individual craftsperson doesn't need to coordinate their work with anyone else — they work at their own speed, according to their own schedule (more or less).  But a factory run as an assembly line requires workers to be coordinated.  Each worker needs to work at a certain speed for the assembly line to run smoothly, and workers need to start and stop their work according to strict schedules.  Zooming out, whole industries need to be coordinated in the same way — raw materials have to be extracted, finished goods have to be sold to consumers, and everything needs to be transported in between.  At the highest level, the entire global economy should be organized to run smoothly.  So capitalism requires an elaborate infrastructure for coordinating and organizing productive activity and the transportation of goods, linking the work of each individual worker to the entire global economy.  

But who controls this infrastructure?  Clearly it would be developed by capital, over time, as ever-greater efficiency and productivity is sought for the sake of ever-greater profits.  But, once developed, there's no reason why this infrastructure can't be controlled by the workers, collectively, for the good of all.  Furthermore, the workers can use this infrastructure to coordinate their own political activities.  (Think of the way socialists used the printing press, cheap newsprint, the mail system, telegraph, radio, and so on to spread revolutionary ideas.)  Here's how https://drive.google.com/open?id=0B6oYmzobonqocGMxbEl1T0ZSSjA Robert Paul Wolff put it:  


  Marx expected, for sound reasons, that the technology of production,
  communication, and management required for the central planning and
  control of an entire economy would develop first within capitalist
  firms, in direct response to the pressures of competition and the
  demands of profitability. And so they have. An immediate consequence
  of this process is the transformation of economic calculations into
  political decisions within the firm. Thus, if by socialism we mean
  the rationally coordinated planning of an entire national economy in
  such a way as to transform the major economic choices of the society
  into political choices, responsive to the will of the people, then it
  is true that socialism has been growing within the womb of capitalism,
  or at least that the technical pre-conditions for socialism can be
  seen to be developing there.


So the association (productive organization) of workers is also revolutionary in this forward-looking sense of creating a critical piece of infrastructure for a socialist society.  

Is the forward-looking sense what Marx and Engels had in mind when they wrote the Manifesto?  That's probably a stretch:  they were only about 30 and 28, and industrialization was still a very new process.  It's somewhat more plausible as an interpretation of Marx's Contribution to the Critique of Political Economy, which was written 11 years later.  (Wolff's focus in his essay is on the Contribution.)  
It's not entirely clear here exactly what you're asking it sounds like a moral question as to how to apply Kant's ethics, which is how I've answered, but it could be a question about how far one can manipulate Kant's ethics without it no longer being 'Kant's', which seems an entirely trivial question which I won't engage with, so my apologies If I've missed the point.


  Fair enough, but can't we amend the rule regarding lying to be:
  "Don't ever lie, unless by doing so you are saving the life of an innocent person."


Yes, but only if you already believe that saving the Jews is the right thing to do and that telling the truth to the Nazis about where they are hiding is not going to bring that objective about. You have, in formulating such a maxim, already done the ethical work the maxim is pretending to help you do. You've already decided what outcome you think would be morally right (saving the lives of the Jews) and you've already decided what course of action will best bring about that outcome (not telling the Nazi's where they are). The point of the maxim is not to help you make such a decision, it is to help you justify it to yourself so you can live with the choices you make, as Bertrand Russell put it "without being paralysed by hesitation"

You could just as easily justify the decision (that you've already made) by utilitarian ethics (the most good will come about by saving the innocent from oppression), by virtue ethics of any kind (it is a virtue to defend the oppressed against violence), even by hedonism (standing up to such a powerful group as the Nazis to save innocents makes me feel like a hero which gives me a lot of pleasure). You make the ethical system fit your preferences, they will all require some manipulation of their inherent ambiguities to make them fit.


  Are imperatives still considered "categorical" if they contain unambiguous universally applicable "unless-then" clauses?


Considered by whom? Surely this will depend entirely on a completely subjective interpretation of "unambiguous". As Jobermark points out "innocent" and "saving" can be considered ambiguous, but so can "universal law", which is, I think, the crux of the question. How detailed do you make the law? Is it lying to anyone, lying to authority, lying to Nazis, or lying to these particular Nazis on this particular day that you are universalizing?
These ambiguities are a necessary component of any ethical system, not a flaw, they are there because if an ethical system actually did tell us how to act in every situation, it would undoubtedly be so over-simplistic and prescriptive as to be completely unworkable within days of everyone following it. But perhaps more importantly, what kind of idolatrous zealot would dismiss millions of years of cultural evolution to follow the prescriptions of one random 18th century German?

If, when faced with actual Nazis (or the modern-day equivalent) asking about the whereabouts of your neighbours, intending to do them real harm, anyone really wishes to rely on the advice of just one person against all their instincts and culturally learned values and judgements, then good luck to them. For the rest of us the philosophy is post hoc, so if makes sense, then it can be considered valid and a justification for what you already think is right of no greater or lesser value than any other equally valid justification.
There is a non-philosophical component to this which I think the other answers touched on, but did not quite get to.

The years leading up to Socrates' trial were hard on Athens. Athens had just lost the Peloponnesian War against its long-time rival Sparta, and Sparta had installed the https://en.wikipedia.org/wiki/Thirty_Tyrants Thirty Tyrants. Though their rule only lasted 13 months, it was extremely brutal, and quite unpopular. According to the Wikipedia article I linked, they killed 5% of Athens' population. This government also ran counter to many of the values Athens held dear: they were (unsurprisingly) anti-democratic and pro-Sparta.

Socrates, with his skepticism of democracy, was associated with the Thirty; in fact, one of the main leaders of the Thirty had been a student of Socrates'. Keep in mind that the political theory of the day was largely a tussle between democrats and oligarchs. By critiquing democracy, Socrates was effectively supporting (or seen as supporting) oligarchy, a system of government supported by Sparta and the Thirty. That was what part of "corrupting the youth" probably included: "it was your teachings that helped get these monsters into power."

The Thirty were overthrown in 403 BCE; the Trial of Socrates was a mere four years later. The people of Athens were still reeling from the Thirty, and were looking for revenge. The specific charges may have been trumped up.

Put in that light, his joking response that he should be rewarded for his role would have been particularly ill received. To the extent that his trial was implicitly an accusation that he had a hand in the Thirty's rule, he not only refused to deny that accusation, but in fact suggested that it was a good thing.
I think several different things are mixed together here. "Eating a cookie won't hurt me, because it's only 100 calories" is not fallacious, taken at face value it is valid. Bringing in "broader context" is called the https://en.wikipedia.org/wiki/Slippery_slope slippery slope argument, and at least traditionally doing that is considered fallacious. However, there are pragmatic defenses of the slippery slope in policy debates, see http://www2.law.ucla.edu/volokh/slipperymag.pdf Volokh-Newman's In Defense of the Slippery Slope:


  "To accept a slippery slope argument, detractors claim, is to say that “we ought not to make a sound decision today, for fear of having to draw a sound distinction tomorrow. It turns out, though, that the realities of the political and judicial processes can make the slippery slope — or, more precisely, several different kinds of mechanisms lurking behind the label “slippery  slope” — a  real  concern... But arguments such as “Oppose this law, because it starts us down the slippery slope”  have  earned  a  deservedly  bad  reputation,  because
  they’re too abstract to be helpful... What is valuable is the ability to identify ways in which slippage might happen and to tell listeners a plausible story about  how  this  first  step  might  lead  to  specific  other  ones."


Argumentation theorists concur, with similar reservations, see http://books.wwnorton.com/books/webad.aspx?id=4294977874 Kelley's Art of Reasoning:


  ""slippery slope arguments can be good ones if the slope is real—that is, if there is good evidence that the consequences of the initial action are highly likely to occur. The strength of the argument depends on two factors. The first is the strength of each link in the causal chain; the argument cannot be stronger than its weakest link. The second is the number of links; the more links there are, the more likely it is that other factors could alter the consequences.""


The difference in the last two examples is that the "broader context" is already present, it is not something that may or may not occur in the future. They both invoke reasoning from the https://en.wikipedia.org/wiki/Continuum_fallacy continuum fallacy, a.k.a. the https://en.wikipedia.org/wiki/Sorites_paradox sorites paradox (paradox of the heap): adding a grain to a non-heap does not make it a heap, therefore no amount of grains makes a heap. Or, therefore no one grain "contributes" to the heap. Here we have votes in place of grains. The reasoning ignores qualitative transitions in chains of small increments, which the media often dramatize by pointing out how in close elections a handful of votes would have changed the outcome.

The second example, unlike others, also has a "should", which brings in yet another issue, see https://philosophy.stackexchange.com/a/41882/9148 What fallacy argues that we should do nothing because we can not do everything? "We can not solve it perfectly, so why bother at all". The names are the http://www.iep.utm.edu/fallacy/#Perfectionist perfectionist fallacy and the https://en.wikipedia.org/wiki/Nirvana_fallacy perfect solution or nirvana fallacy. It creates a false dichotomy between perfection and doing nothing, and then dismisses not doing nothing as imperfect. This second part involves the fallacy of relative privation, see https://philosophy.stackexchange.com/questions/24622/what-fallacy-dismisses-problems-by-presenting-bigger-problems/24623#24623 What fallacy dismisses problems by presenting "bigger" problems? 

Even aside from this particular context, deriving a "should", what to do, from what is is problematic in itself on general grounds, see the https://en.wikipedia.org/wiki/Is%E2%80%93ought_problem naturalistic fallacy or Hume's is-ought guillotine. One could say that we should do what is right even when it is "pointless". But one can reconstrue the inference as an elliptic means-end reasoning: we should mitigate climate change, so we should reduce greenhouse gases as a means (in principle). But since we can not reduce them by much we shouldn't bother. The first part is now valid (assuming one accepts the scientific connection), but the second one is soritic.
Hegel considered mathematics as a subordinate science. Philosophy, as far as it shall be science, cannot borrow its methods from such a subordinate science as mathematics is. 


  Die Philosophie, indem sie Wissenschaft seyn soll, kann, wie ich anderwärts erinnert habe, hierzu ihre Methode nicht von einer untergeordneten Wissenschaft, wie die Mathematik ist, borgen, so wenig als es bei kategorischen Versicherungen innerer Anschauung bewenden lassen, oder sich des Raisonnements aus Gründen der äußern Reflexion bedienen. (Georg Wilhelm Friedrich Hegel: http://gutenberg.spiegel.de/buch/wissenschaft-der-logik-1653/2 Wissenschaft der Logik - Kapitel 2)


EDIT

From Hegel: http://www.zeno.org/Philosophie/M/Hegel,+Georg+Wilhelm+Friedrich/Ph%C3%A4nomenologie+des+Geistes/Vorrede Phänomenologie des Geistes, Vorrede, his contempt of mathematics should become crystal-clear:

Im mathematischen Erkennen ist die Einsicht ein für die Sache äußerliches Tun; es folgt daraus, daß die wahre Sache dadurch verändert wird. Das Mittel, Konstruktion und Beweis, enthält daher wohl wahre Sätze; aber ebensosehr muß gesagt werden, daß der Inhalt falsch ist. ... Die Evidenz dieses mangelhaften Erkennens, auf welche die Mathematik stolz ist und womit sie sich auch gegen die Philosophie brüstet, beruht allein auf der Armut ihres Zwecks und der Mangelhaftigkeit ihres Stoffs und ist darum von einer Art, die die Philosophie verschmähen muß. ... Das Wirkliche ist nicht ein Räumliches, wie es in der Mathematik betrachtet wird; mit solcher Unwirklichkeit, als die Dinge der Mathematik sind, gibt sich weder das konkrete sinnliche Anschauen noch die Philosophie ab. ... Eine Kritik jener Beweise würde ebenso merkwürdig als belehrend sein, um die Mathematik teils von diesem falschen Putze zu reinigen

Its contents is wrong, insufficent rekognition, poorness of its purpose and insufficiency of its stuff, ...
Problem

Your two examples have different logical structures. So I will treat them separately.

First example

https://www.logicallyfallacious.com/tools/lp/Bo/LogicalFallacies/132/Negating-Antecedent-and-Consequent Negating Antecedent and Consequent
(also known as improper transposition)

The logical structure simply is:

A → B, therefore ¬A → ¬B

The fallacy lies in not switching antecedent and consequent. Therefore, it is the inverse rather than the contrapositive. It is only the contrapositive that is logically equivalent to the original proposition and therefore already implied in it. You can read more helpful links and thoughts explaining these terms and their differences in https://philosophy.stackexchange.com/a/42957/17209 this related answer.

Second example and header

The header and the second example have another logical form. They correctly state the implication (NOT syllogism!) 

A → B, therefore ¬B → ¬A

But the validity (or truth value) of this depends on the truth value of 'A → B'. Therefore, arguing along these lines without further support is https://www.logicallyfallacious.com/tools/lp/Bo/LogicalFallacies/53/Begging-the-Question begging the question or petitio principii, as another answer correctly stated. It is a type of circular reasoning.

Explanation

Your second example basically argues that by attracting money, you become rich, and as you are not rich, you did not attract money. Here, you presuppose that it is only by attracting money you become rich, and from that, you gather the implication, the contrapositive. As both sides (before and after "therefore") are logically equivalent, it can be transformed into

A → B, therefore A → B

Or, if you like: X → X

It is like saying "There IS God, therefore there is God.", while thumping on the table. This is begging the question.
We don't know what Socrates would have said about modern evolutionary theory because he makes no real mention of about any form of evolution; it's not outside the bounds of possibility that he had made remarks on evolution but they simply weren't recorded in the dialogues by Plato (which accounts for much of what we know about Socrates philosophy) since there is a short remark by Aristotle on the notion of random change and selection in evolutionary biology - which he dismisses because of the lack of a telos. 

Dedalus' 'History is a nightmare from which I am trying to awake' has an echo in Marx's 18th Brumaire of Louis Bonaparte :



  Men make their own history, but they do not make it just as they please; they
  do not make it under circumstances chosen by themselves, but under circumstances directly encountered, given and transmitted from the past. The tradition of all the dead generations weighs like a nightmare on the brain of the living.



But since Joyce was not a Marxist, this is only a side comment. Eisenstein is a candidate as a Marxist who used Joyce. Take this quotation from a 1932 talk : 



  Radek's critique of Joyce was based essentially on one point. He said that we don't need things in such microscopic detail. We don't see that way, such phenomena don't exist. But that criticism is as if a person at some first-aid station saw an enlargement of something seen under the microscope on the wall
  and said: "Why is this necessary? After all, microbes aren't that big. After all, you don't see all that in real life." Do you understand the
  mistake here? The thing is that you have to study those charts in
  order to be able to know those invisible bacteria, those invisible
  elements, in order to possess them. And that's the significance of
  studying Joyce and it's on that level that he analyzes things so
  microscopically. (James Joyce Quarterly, Vol. 24, No. 2 (Winter, 1987), p.137.)


This can be applied to history and can remove its nightmarish qualities. Macro-history is as bad as Marx said it was but it ceases to be dark and oppressive when macro-history is replaced by micro-history : the life of one person in the course of just a day described in 700 pages. Of course 'Ulysses' is not real history but its microscopic style and level of treatment can be applied to history, which then looks very different from the Marxist grand narrative. It does not reject that narrative but alters our perspective on it. 
This probably isn't the type of answer you're looking for, but maybe it will offer a few insights.

From a scientific point of view, it can be argued that we have a duty to our species. Thus, our primary instinct of self-preservation can be overridden by the urge to protect our young or mates. This could also manifest itself in the form of concern for progeny that don't exist yet. For example, you might want to stop drinking or drug abuse before having children.

Empathy allows us to project similar feelings beyond our immediate relations. All of a sudden, we care about people on the other side of the world. And if we can care about people we may never meet, why not care about the unborn?

The "selfish gene" and/or empathy comes into play when we see a fellow human attacked by a predator. Our immediate instinct is to help the human (without getting hurt ourselves).

You say we can't steal from the unborn "in quite the same way."

That might be true, but our actions obviously affect them in countless ways. Our ancestors hunted many species, including the fabled elephant bird, to extinction. Thanks to them, I'll never be able to see an elephant bird. Nor will I ever see a herd of bison stretching to the horizon.

I live on a planet that's increasingly polluted, thanks largely to the efforts of past generations - people who invented various pollutants before I was even born.

My personal philosophy is that we are only temporary custodians of Earth, and we should try to leave things as pristine as possible.

On a tangent, it's sad that we can't go back in time and hold past generations accountable. But what about some wealthy heir who's worth $10 billion because his father invented Agent Orange or DDT? Do we have the right to garnish his assets? 
Well this is a question with an interesting cast of characters who come into play and I don't know if I'm up to the task of sketching it out completely, perhaps all I can do is sketch out a little of it. 

What does the end of history mean?  Maybe I can give you a little of the history of the end of history, or at least point you in that direction as I understand it. 

Have we found "nirvana" in capitalism? If so, history is over, and we can rest easy in that knowledge.  We just sit back and enjoy the fruits of capitalism, and nothing really changes. 

I will put aside for bit  talk of Fukuyama, and discussion of Alexandre Kojeve's influential brand of Hegel. I have an article for you below.  (By the way, Kojeve did not bow out of history after he gave his highly influential lectures on Hegel at the Sorbonne in the 1930s. He ended up in an important post in government administration in France, and he maintained a vital friendship with the philosopher Leo Strauss.   Understanding Kojeve's tremendous influence, which he asserted in his early lectures, and his later philosophical connections, are absolutely necessary to comprehend the whole end of history thing). 

Concerning today, the first thing I would mention, to be kept always in the back of the mind is the PE ratio, or price/earning ratio for publicly traded stocks. Stocks sell at a multiple of price to earnings, so built into share prices is an expectation of future growth, and this puffs up the market considerably. 

The expectation of growth, and in fact growth itself, is essential to the success of capitalism, and the classic end of history theory. 

So what strikes holy terror into the heart of the capitalist class (but NOT just them) is that somehow this expectation of growth will stop, and that "word" of something
will get out, and it will become common knowledge that this growth ain't happening in the future. 

We may surmise why it's not happening by considering topics like climate change and limits to growth, and limits to growth is not dead, it is still a live concept. The Paris Accords itself was a joke, it didn't do enough, and even so the US is withdrawing, etc.  

The above is not a tomorrow problem, or an end of century problem,  it is a today problem, in the sense that the market might collapse and stay collapsed starting soon.  Doubtful, but posssible. And this should give you at least some idea of why there are very, very strong forces trying to keep a lid on the full comprehension of this information.  

Now these super "rich" folks are NOT all bad. Remember, there are many middle class and lower middle class people whose retirement accounts are tied to the stock market. So if the rich go down, a lot of the regular folks do too, and not down the road, but soon. 

Question?  So do we need to change our economy so that we direct and control the ends of production so as to better preserve our existence on earth for a longer period of time?  Today, we don't really control the ends of capitalism. The capitalist can "waste" resources by making the goofiest things, as long as he can sell them for a profit.  

Key point: To change our economy is to engage with history again,  to "restart" history so to speak. 

From here I would suggest Joe McCarney's article at Marxists.org, below. This should make clear some things on Fukuyama, Kojeve, Hegel, etc. 

Here is a link to the McCarney article. It is a bit  outdated due to events, but still good and very important to read for background information:
https://www.marxists.org/archive/mccarney/fukuyama/shaping-ends.pdf https://www.marxists.org/archive/mccarney/fukuyama/shaping-ends.pdf

Can Marx helps us?   Marx is most valuable for the critical tools he gives us. The tools of analysis.   These are still very important in my opinion and we should retain them. 

As far as exact Marxian solutions, these may be, and probably are, outdated, or at least in part they are.  Marx himself was vague about how real communism would work, he was probably vague on purpose because he didn't exactly know himself, leaving it for future generations to figure out.  Of course there are different socialist flavors, permutations available also. 

And we should not just speak of Marx, we could very well end up with an authoritarian capitalist system at least for a period of time, because the dominant class is understandably nervous about the fast pace of change which may be necessitated by environmental changes and resource constraints.  The Chinese have a system of authoritarian capitalism right now. Chinese brand authoritarian "communist" capitalism.   (If this seems like an odd paragraph to you, you're right!  Welcome to the 2000s).

Anyway, we are in new territory. We have an advanced global economy that may be killing us.  In some ways we are stumbling forward almost blindfolded. The earth itself will force us to restart history, we humans have managed somehow to adapt and move forward in the past and the hope is we can do it again.  

P.S. People like Naomi Klein try to suggest that we can grow ourself out of climate change by building a lot of green things. Think of all the growth!  We'll build a lot of stuff and this stuff will protect us!  I'm not so sure. The world is a relatively closed system, already beginning to overheat. Construction creates disorder in order to make new order, and it uses energy, lots of it. What about entropy effects?  The physicists tend to take a narrow, specialist approach to this issue. I'm not so sure. Maybe we need to look into this, scientists should study this problem since earth/ atmosphere is a relatively closed system. Efficiency will be key to solutions. 
You are drilling yourself into a hole without any basis to get back out of it -
 look at the things you are asking yourself step by step.

First of all, to the extent that what we mean by death is the complete and irreversible cessation of all cognitive functions (perhaps an unnecessarily strong definition but suitable for these purposes), there simply is no perspective of the dead person. Since all cognitive functions have per our definition ended, you cannot speak about the subjective experience of the dead person, only of the subjective perspective of the dying person. Can it seem like the universe is ending to the dying person? Sure but that isn't relevant to anything meaningful. One can, after all, hallucinate that the universe is coming to an end under the influence of drugs or dreams but one wouldn't consider this experience epistemologically meaningful.

In other words, the answer is simply that the perspective of the dying person is individually determined by the person and the circumstances of their death and it's not clear why any rationally thinking person would put much stock in this experience. A dead person is just as much a person as a table or a rock is and as such experiences nothing. "Nothing" here doesn't merely mean they are sitting in a dark room, it means they don't experience space, the passage of time, the existence of themselves and others and definitely not the end or the existence of the universe.


  Something makes me think that when you cease to exist "an infinite amount of time passes"


I don't know why you would think that when it's evidently false. You understand how many people die every day? Even as you read this answer here? Not once has an infinite amount of time passed because of that. Maybe what you mean is that when you're dead, you have no experience of the passage of time anymore? Sure, that's true, but lacking an experience of the passage of time in no way means time stopped. Time also doesn't stop when you're unconscious, under anesthesia or when you're functionally dead (though reversibly unlike the above definition) but then brought back by doctors. You experience no time in all those situations and even sometimes when you sleep but that in no way means that no time is passing. 

This means, though, that "to the ex-dead person", if you bring them back from death somehow, they will feel as though the time interval between their death and their resurrection was 0, i.e. time passed infinitely from "their perspective". But this is an illusion because like I said they don't have a perspective. The human brain would simply interpret this absence of memories of the passage of time as an instantaneous jump in time.

TL;DR: There is no instant passage of time happening when people die, not even from their own perspective since they have no perspective. If you somehow brought somebody back long after their death, they would feel as though no time had passed between their death and now but this isn't epistemologically meaningful and it's easy to understand why the brain would make this mistake.
Perhaps not surprisingly, different philosophers disagree.  http://www.iep.utm.edu/kantaest/#H2 Kant centers aesthetics around the judgement of the viewer.  Beardsley focuses on the https://plato.stanford.edu/entries/beardsley-aesthetics/ artistic experience.  Danto considers aesthetic quality to be a function of https://en.wikipedia.org/wiki/Arthur_Danto social context.  http://www.rowan.edu/open/philosop/clowney/Aesthetics/philos_artists_onart/bell.htm Bell focused on formal properties of the art object.

Art is a subject with very little consensus in philosophy.
You are correct, arguments do not take place in a void, the arguers have to share common principles or presuppositions to make an argument possible. If both are allowed to reject each other's premises and background assumptions without limitations they will just be talking past each other. 

Logical pragmatics, tacit and context dependent aspects of arguments, is a big part of modern https://en.wikipedia.org/wiki/Argumentation_theory argumentation theory. The language of "inside and outside the argument" was used on the http://lesswrong.com/lw/3be/confidence_levels_inside_and_outside_an_argument Less Wrong blog recently. The more traditional term is "begging the question against the opponent", a move where one of the parties argues from, or presupposes, a premise she knows (or should know) the opponent would reject (or rejects a premise he accepts). In your example Bob begs the question against Jim, he is "outside" the argument. 

Unfortunately, Wikipedia's article on begging the question overlooks its pragmatic dimensions, and simply identifies question-begging with circular reasoning. But circular reasoning may well be pragmatically justified in a context if the supposedly circular premise happens to be shared by the participants. It is only begging against the participants' premises that violates the tacit "pact" made at the start of the argument. You can look at http://www.dougwalton.ca/papers%20in%20pdf/94begging.pdf Walton's Begging the Question as a Pragmatic Fallacy for a discussion:


  "Robinson argued that there are only two proper ways of condemning an argument - because the conclusion does not follow from the premises, or because the premises are not acceptable to the person to whom the argument was directed. Arguing that begging the question does not fit into either category, Robinson concluded that it is not a proper criticism of an argument.
  
  Robinson, continued to build up his skeptical case by arguing that begging the question has traditionally been thought to be a fallacy because it is a breaking of the rules of the old-fashioned game of elenchus (two-person contestive question-reply argumentation as found in Aristotle). Each participant has a conclusion (question) to be proved, and one of the rules, according to Robinson, was that a question must not directly ask for conclusion."


Robinson gave an example similar to the OP's:"God has all the virtues. Therefore, God is benevolent". On the traditional account the argument begs the question by assuming its conclusion. But as long as the premise is "acceptable to the person to whom the argument was directed" it is contextually valid nonetheless. The objection is "outside" the argument. On the other hand, if the purpose of a debate is a "search for truth" then the strictures of the elenchus may not apply, as Robinson himself argued. This may reflect a pragmatic pact among the participants to keep their assumptions open to external evidence. For instance, https://ojs.uwindsor.ca/ojs/leddy/index.php/informal_logic/article/download/2902/2309 Rips in Argumentative Thinking describes an approach of Hahn, Harris, and Corner:


  "This approach toward argumentation takes the strength of an inference to be the conditional probability of its conclusion given available evidence, where the evidence in some cases may be drawn from outside the argument itself. In the case of arguments from ignorance, for example, the strength of an argument such as “Drug X is safe because JAMA reported 10 studies with no side effects,” depends on the strength of the evidence (the number of studies) and on the reliability of the source (JAMA)."


Chomsky argued, controversially, that adherence to "established" premises in public debates is aided by "concision" (saving time) in the media, which leads to conformism and "thought control", see http://cornerstone.lib.mnsu.edu/cgi/viewcontent.cgi?article=1076&context=speaker-gavel Dimock's Critiquing Debate:


  "Debaters  who  keep  their positions  within the  very narrow range of the dominant paradigm have a considerable  tactical  advantage  over  those  who  attempt  to  argue  from  outside  that paradigm. Concision  and  the  overreliance  on  authority  are  practices  which  make  it very  difficult  to  challenge  the  dominant  paradigm.  Because  they  reinforce  the dominant  ideology,  which  has  tended  to  favor  some  groups...  while marginalizing others."

As American patent law sees it, you still have the right to use your idea independent of them.  It is 'prior art' and they cannot monopolize it.  But you have no right to any part of their success.  They are the ones who have made the investment in your idea.

You already had the opportunity to take your idea to someone else who would make that investment in a way that would benefit you, instead of them.  If you did not recognize the value of your own idea, they deserve the benefit of that realization, which you did not have.

'Capitalism' is the name for a market economy, but the primary thing markets acknowledge is not the holding of capital.  Money is no longer based on actual property, it is now an entirely market phenomenon, based on the recycling of debt.  So modern markets run on the successful estimation of value and a proper sense of the associated risk.

You did not properly estimate the value, or take the related risk.
All the definitions listed (short of the 'absolute' pacifism noted by @Conifold) do all apply to various people who call themselves pacifists.

A primary division is the question of whether pacifism is primarily about organized violence or whether it is about all violence.  (As usual, I would suggest that deciding violence in defense of others is always bad is an entrenched anti-male bias that one should seriously consider bigoted.)

One historical, minimal point of reference for some Quakers is George Fox's exhortation to '...live in the virtue of that life and power that takes away the occasion for all wars.'  (The phrase 'life and power' is often replaced by 'light' in this quote.)

It is not uncommon to argue that restricting mutual defense or adequate policing itself invites folks to attack one another more liberally, and therefore is not 'in that virtue'.  To some degree, civil disorder causes wars, because at some point some neighbor (or global busybody) will invade to establish order and protect the weak.

This often exists alongside explicitly anti-war behavior, like minimizing reportable income, on the deduction that we use or military far too liberally just because we have already paid for it and therefore our tax structure actively causes wars.

At the extreme end of both of these positions, many clans of Rainbow People prepare to defend their sovereignty with guns, in order to continue to avoid using government-backed currency, which they do to further the cause of pacifism.
It's both.  Analogy is never officially logically valid, but it is rhetorically cogent.  And reasonable logic is never so precise that some degree of analogy is totally absent.

It is a natural part of every argument that is not about a simple, individual case.  No useful generalization is made strictly upon the defined attributes of the things being generalized over outside of mathematics, and even then we generally do the generalization first, and then find the attributes that help us extend it further.

An analogy is an allusion to a parallel argument that could presumably be made by translating existing arguments for the analogous case.  If the analogy is very strong, for instance, simply substituting a different individual for a given individual in the case at hand based upon a number of similarities between their circumstances, we would never expect the entire parallel argument to be made over again.

But what makes an analogy strong?

This is an aspect of the problem that Quine captures in discussing Natural Kinds.  We do not know when two things are so much alike that they are substitutable and which things are not.  Trying to reduce this kind of thing to rules fails in general.  But we know how to navigate substitutions and other transformations needed to stretch existing cases to more general uses.  We do it so naturally without analysis that when we attempt to subject it to analysis, we find very little ground to stand on.

We have to have ways of determining how broadly an argument based upon parallel properties actually applies.  But when we have passed over from useful paraphrase to mere metaphor is something that is not a solvable problem.
I am not sure i would say there is a fallacy here.  The message i get from the statement is that there is an equal wrong doing here not a comparison of which is right.   The statement suggests all of the acts are equally wrong.  So how can you single out one act as wrong and not the others.
It certainly would, and you can observe it doing so.  Our understanding of one another cannot be complete.  There can be no true communal standard as to what 'rabbit' actually means, and as we move away from the central anchor of 'rabbit-hood' we will disagree on what features things need to retain in order to remain 'rabbits'.

This variation is sterner and broader in children than adults.  Notoriously, way too many things are doggies for a while, and kids disagree a whole lot more on what doggy-hood is depending on whether they have contrasting notions of kitty-hood or horsey-hood mixed into their experience.  They make those guesses and fight with one another about them, or test them by how adults respond to their usage.  Language is a game more like 'tea time' than like 'Monopoly', we learn what the rules are by taking part, not by being told.

Humans don't learn definitions or grammar as children, because we know that is impossible, basically from logic like Quine's (more specifically, but less clearly, Wittgenstein's).  Instead, we converge on vague complexes of meaning and usage until we have enough structure to conceive of the notion of a definition and its surrounding grammar.  Then we change the way we talk about words, because that structure is a lot more efficient, even if it is less than honest.

This is part of the motivation for Chomsky presuming children have a native ability to vet guesses about meaning against pre-established patterns of grammar.  It seemed insurmountable to him that children could both deal with vagueness and learn grammar at the same time, without some built-in instinct for what parts of language were which sort of thing.  But apparently we do.
Well the issue with the principle of indifference is that it only applies if there can be only one item in a set. For instance if you have 4 possibilities that all contradict each other, and thus only 1 can be true, you're looking at a situation where most are false, and it is most likely any one is false. However if you take 4 possibilities that do not contradict each other, and multiple can be true, it is also possible that most or even all of the possibilities are true, and thus you cannot say that it is unlikely for any particular one to be true.
The broader question is answered under https://philosophy.stackexchange.com/questions/33490/in-what-fundamental-ways-if-any-does-husserl-break-with-kant In what fundamental ways, if any, does Husserl break with Kant?

As for the title question, the answer is that we can not. Husserl's "things themselves" are nothing like Kant's "things in themselves". Although the two philosophies are incommensurable, to the extent that we can compare them the "things themselves" are Kant's appearances in their least contaminated form, phenomena as they come. This is why Husserl calls his philosophy transcendental phenomenology. Husserl's motto "back to things themselves" was a play on Helmholtz's earlier motto "back to Kant", which in turn was a reaction against the dominance of Hegel-styled speculations about rarefied abstractions in the German philosophical scene of the first half of 19-th century. Back to things themselves meant back to phenomena, and away from the German idealism.

Husserl considers the notion of Kant's "thing in itself", and of unperceivable things generally, to be "countersensical" or "materially nonsensical" (Widersinn, as opposed to Unsinn, logically nonsensical). While logically coherent it involves a vain attempt to imagine perceiving what is at the same time is asserted to be in principle unable to ever enter the imagined situation. Here is from https://archive.org/details/IdeasPartI Ideas I:

"The hypothetical assumption of something real outside this world is, of course, ‘‘logically’’ possible; obviously it involves no formal contradiction. But when we ask about the essential condition on which its validity would depend, about the  mode  of  demonstration demanded  by  its  sense...  we  recognize  that something transcendent  necessarily must be experiencable... as a demonstrable unity relative to its concatenations of experience... If there are any worlds, any real  physical things whatever, then the  experienced  motivations constituting them must be able to extend into my experience and into that of each Ego."

In fact, Kant himself characterizes application of the categories of experience (such as "thing") beyond any possible experience as "transcendental illusion", so he appears to agree that "thing in itself" is "countersensical". Already Fichte pointed out that on Kantian terms we should not even be saying that "thing in itself" is unknowable, we should not be making this combination of words at all, let alone putting it into sentences. The usual response is that Kant only uses it analogically and metaphorically, to allude to the limits of experience and knowledge.

There is another possible interpretation. One response to the charge that unexperiencable experience is countersensical is that while we can never perceive things in question we can conceive of an intellect that can do so, and this is enough (e.g. it can be God, but it need not be a god of any known religion, and it need not exist, only be possible). In §§76-77 of Critique of Judgment Kant admits the conceivability of such a non-discursive intellect, intellectus archetypus as he calls it, but proceeds to argue that neither we nor anything like us possesses anything like it, see https://philosophy.stackexchange.com/questions/31683/did-kant-come-to-believe-that-we-have-access-to-things-in-themselves-after-all Did Kant come to believe that we have access to things-in-themselves after all? 


  In fact our Understanding has the property of proceeding in its cognition, e.g. of the cause of a product, from the analytical-universal (concepts) to the particular (the given empirical intuition)... We can however think an Understanding which, being, not like ours, discursive, but intuitive, proceeds from the synthetical-universal (the intuition of a whole as such) to the particular, i.e. from the whole to the parts. The contingency of the combination of the parts, in order that a definite form of the whole shall be possible, is not implied by such an Understanding and its representation of the whole. Our Understanding requires this because it must proceed from the parts as universally conceived grounds to different forms possible to be subsumed under them, as consequences."


Still, this means presumably that the "thing-in-itself" is perceivable to something conceivable to us, which would give it a shadow of sense, however tenuous. And Kant was of course a devout Christian, who, in his own words, had to "limit reason to make room for the faith".

Perhaps the most detailed discussion of meanings and shades of meaning of Kant's "phenomenon", "appearance", "noumenon" and "thing in itself" is http://staffweb.hkbu.edu.hk/ppp/ksp1/KSP6.html Palmquist's Two Perspectives on the Object of Knowledge.
Kant, in 'Lectures on Ethics' is reported to have said that "in giving to an unfortunate man we do not give him a gratuity but only help to return to his that of which the general injustice of our system has deprived him." So he would appear to be framing giving in terms of restitution and therefore firmly within the duty of the giver, not on the actions of the beggar themselves. The concept here would be that where evidence of past injustices are presented one has an imperfect duty to rectify them, the beggar's actions are simply the presentation of that evidence.

He also says “We shall acknowledge that we are under obligation to help someone poor; but since the favour we do implies his well-being depends on our generosity, and this humbles him, it is our duty to behave as if our help is merely what is due to him or but a slight service of love, and to spare him humiliation and maintain his respect for himself” (6:448) Metaphysic of Morals.

In both these statements Kant refers specifically to the poor man, not charity or social institutions in general. One cannot see how our giving to charity would directly humble a person, nor how any action on our part could spare him such humiliation, so he must be referring to direct giving.

Of course, being Kant, he contradicts himself elsewhere, "Alms-giving is a form of kindness associated with pride and costing no trouble, and a beneficence calling for no reflection. Men are demeaned by it."

Lucy Allais reconciles these seemly opposed views by arguing that it is the lack of information within a begging exchange which makes it unreasonable. The beggar is offering the potential giver no evidence of injustice and so placing them in a position where they themselves are being used as a means to an end (the relief from poverty) rather than an end in their own right (respecting that they would want some evidence that their giving will actually achieve their objective of alleviating poverty). Her paper is reproduced https://www.google.co.uk/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=0ahUKEwj-v9_T17XWAhWCLVAKHUkuD8UQFggmMAA&url=http%3A%2F%2Fwiser.wits.ac.za%2Fsystem%2Ffiles%2Fseminar%2FAllais2012.pdf&usg=AFQjCNG5I2QsFeEd01qnT_qk0SKHlm1sVg here, but it is still quite critical of begging and so may not give you the answers you're looking for
It's about using the right words. You wouldn't say "If a cat barks, it isn't a cat". Presumably, you'd say "If an animal barks, it isn't a cat". 

Therefore, a statement should be "If a shape/an object has 2 sides, it isn't a triangle". It might not be the answer for the question, but i think that kind of statements shouldn't be used whatsoever.
I don't think your question makes any sense given the fact nihilists themselves rarely consider any outside emotional reactions to their beliefs from 3rd parties and even another nihilist can be considered a 3rd party. The idea behind https://en.wikipedia.org/wiki/Nihilism nihilism is that one can deny the value of virtually everything including thyself, therefore, prescribing any value to anything, including pity, doesn't "fly" well within nihilism.

In such circumstances one wouldn't bother too much to deny the intention behind everything (including pity) even from someone who could be considered a comrade. In the sense nihilism is used in philosophy today it makes it possible for a person to respond negatively to any emotion, even the ones who are positive when directed at him/her. This is why a "good nihilist" shouldn't have any problems denying the intentions of any other party including other nihilists and be able to live up to the promises of this worldview without any kind of "head-ache".

P.S. However, the question remains is such a worldview really something worth adhering to? I personally wouldn't advice for nihilism but it's probably the only philosophy able to deny everything thrown at its prescribers, so questions like yours user3293056 really don't make any sense to a "self respecting nihilist".
I'm assuming you mean is it inherently illogical to use a mass shooting as a reason to repeal gun control? In which case the answer is definitely no. In some cases it can be an appeal to emotion. I think the easiest way to explain this is with some example arguments.

Fictional example argument for appeal to emotion:


  Many peoples' loved ones were killed and injured in the Vegas shooting, so we should ban guns.


Example argument that is not an appeal to emotion:


  The Las Vegas shooter was able to kill 59 people and injure many more because he had access to many semi-automatic weapons, modified to fire like fully automatic weapons. Without the ability to fire as many rounds as fast the Las Vegas Shooter would not have been able to fire off as many rounds and kill as many people. Therefor banning assault weapons could reduce the death toll in future shootings.


The first argument is clearly an appeal to emotion (fear), and does not address any logical reasons that assault weapons should be banned. The second argument however is a logical argument for why we should ban assault weapons. 

Quite simply the claim that politicizing mass shootings is actually an attempt at censorship. The goal is to shut down the discussion about guns because this event and events like it supply logical reason to ban assault weapons.
You can see the Introduction to https://books.google.it/books?id=-j-GaDjLdhYC&printsec=frontcover Plato's Symposium (Sheffield & Howatson editors), Cambridge UP (2008), for an overview of the various speeches of the dialogue.

"Phaedrus, the first speaker, puts the issue of the role of love in moral education firmly on the agenda. He argues that a love relationship has the greatest power when it comes to acquiring excellence (arete) and happiness, as he conceives of such things. [page xi]" 

"There is, Socrates supposes, an end, or a greatest good, towards which our desires and actions ultimately aim. What we really want as desiring agents is the possession of the sort of good that will satisfy our desire for happiness. This reflection suggests to Socrates that people are mistaken to suppose that eros refers to sexual desire exclusively; in fact, it is happiness quite generally that is desired and sexual desire is just one way in which this broader aim is manifested. [page xv]"

"One thing that is clear is why Socrates’ account will move from an analysis of the nature of such desire to an account of knowledge and its acquisition; for if we all have a desire for our own good and happiness, the issue becomes how to identify correctly the nature of this good. [page xvi]" 

See also https://plato.stanford.edu/entries/plato-friendship/#LovAscBea Plato on Friendship and Eros.

The conventions of a society determine, fix, what is regarded as ethical in that society - to the extent that the society has consensual conventions. These have become rare in the culturally heterogeneous societies of the present day. 
If there is an objective moral reality, independent of human beliefs, i.e. if moral realism is true, then there are (some) moral truths which are independent of whatever the conventions of any society. These 'truths' may coincide or agree with social conventions but they are no less truths (if you are a moral realist) whether they do or not. 
Ancient Greek morality cannot be characterised in any single or simple way. Plato held that there is an absolutely objective morality which reflects a realm of values embodied in the Forms, transcendent realities knowable only by and to an intellectual elite. Aristotle's view was that there is an objectively correct way in which for a human being to develop and that this will involve (at least for free adult males) participation in citizenship. On the other hand, sophists such as Thrasymachus, mocked in Plato's Republic, are widely believed to have thought that ethics, morality generally, was a set of beliefs instilled in the 'strongest' to induce obedience to their rule. (Thrasymachus's exact position as depicted in Plato's 'Republic' is actually full of ambiguities and subject still to scholarly dispute.) Glaucon in Plato's 'Republic' is represented as putting forward (not holding) a 'social contract' theory of morality : morality is a set of rules of convenience which we'd all like to break but which are necessary for a safe and organised social life. 
Even if the ethics of society does in some sense dictate what is ethical - morally right - either makes it actually right (conventionalism) or determines what is merely regarded as right - there is no logical link with majority opinion. Thraysmachus might be correct in holding that morality is decided by the interests of the strongest, who could be a minority, or it might be the case that moral opinions about right and wrong are transmitted by, and inherited from, tradition which the majority like everyone else accept as the guide to ethical behaviour.


The Greeks were greatly exercised, at least in the 4th and 5th centuries BCE, over whether morality was rooted in nature (phusus) or convention (nomos). There were powerful exponents of both views, with philosophers such as Plato and Aristotle coming down in their different ways on the side of phusis and the so-called sophists, including Protagoras, Gorgias, Hippias, Prodicus and Thrasymachus, opting for nomos. (But the actual nature of the sophists' views really needs careful handling; I am presenting at best a historically influential view of their ideas.)
The short version is that good is virtue, not pleasure, and bad is the loss of virtue. If someone does something immoral for pleasure or to avoid pain then they are not taking good actions. For instance in the Apology Socrates states that it would not have been good of a soldier to abandon his post, though it may help him avoid death, because it would be a breach of his duty. 

However I suggest you share notes with one of your class mates so that you can get the most accurate sense of what your professor intends for you to learn.
What if you accidentally break something that does not belong to you and nobody notices. Is it then ethical to not say anything about it? I do not think so, and so the same goes for accidentally killing someone.
The classification of arguments for God's existence into ontological, cosmological and teleological is due to Kant, and Kant had to group together arguments rather coarsely to fit the broad variety of them suggested by his time. Within each group multiple versions are distinguished today, and it is also not surprising that some arguments do not neatly fit into Kant's groups. Basically, ontological arguments are supposed to argue for the existence of God a priori from the definition of "God", while cosmological arguments argue for God as the first cause, and hence involve a posteriori elements concerning the nature and operation of causes. https://en.wikipedia.org/wiki/Proof_of_the_Truthful Avicenna's "proof of the truthful" happens to involve both aspects. 

On the one hand, Avicenna proceeds from a "necessarily existent by virtue of itself", an entity that cannot not exist, which sounds similar to Anselm's starting point of "that, than which nothing greater can be conceived". On the other hand, the reasoning goes that the set of contingent things must have a cause that is not contingent, because otherwise it would be included in the set. This is similar to the traditional https://en.wikipedia.org/wiki/Cosmological_argument cosmological argument, but this way of putting it is parallel to Anselm's inference that "than which nothing greater" must already exist in reality because otherwise that same thing, which also exists in reality, would be greater (this type of reasoning is disallowed today because it  leads to the https://en.wikipedia.org/wiki/Burali-Forti_paradox Burali-Forti and https://en.wikipedia.org/wiki/Russell%27s_paradox Russell paradoxes). To complicate things further, Avicenna does not exactly make the "necessarily existent" into God by definition, instead he offers a series of ancillary arguments to derive its attributes, which ultimately allow him to identify it with the God of Islam.

The debate over the nature of Avicenna's argument is reviewed by https://academic.oup.com/jis/article-abstract/12/1/18/721290 Mayer in Ibn Sina's ‘Burhan Al-Siddiqin’. Given its heterogeneous nature taxonomic disagreement among scholars is to be expected. For example, https://archive.org/details/167142938DavidsonHProofsForEternityCreationAndExistenceOfGod1987Pdf Davidson writes in Proofs for Eternity:


  "Avicenna does not regard the analysis of the concept necessarily existent by virtue of itself as sufficient to establish the actual existence of anything in the external world. He does not, in other words, wish to offer an a priori or ontological proof of the existence of God, but rather a new form of the cosmological proof".


https://books.google.com/books?id=pZQp4GpdwzAC&source=gbs_navlinks_s Morewedge in "A Third Version of the Ontological Argument in the Ibn Sinian
Metaphysics" (p.188) says the exact opposite, and classes the argument along with "ontological proofs, which are based purely on his analytic specification of this concept". His reading is based on Avicenna's distinction between being and existence (not unlike Anselm's existence in mind and existence in reality). Avicenna's being "is more determinable and more extensive than both 'existence' and 'essence'", and so he can posit the the "necessarily existent" initially as only being, which then, due to "necessarily", must also be existent. http://onlinelibrary.wiley.com/doi/10.1111/j.1478-1913.1984.tb03452.x/abstract Johnson in Ibn Sinā's Fourth Ontological Argument for God's Existence takes an intermediate position of classing the argument as cosmological overall, but containing an ontological aspect. Mayer's surmise shows that classification attempts are even more hopeless:


  "So even if it is ruled that Ibn Sinā's argument is cosmological or ontological, it will remain to be answered which kind of cosmological or ontological argument it is. If it is an ontological proof, St. Anselm of Canterbury alone is now credited with two different versions. Then, if the proof is read as cosmological, William Lane Craig has shown that there are three subordinate types of cosmological argument: the kalām, the Thomist, and the later Leibnizian type. Arguments of the first type 'maintain the impossibility of an infinite temporal regress', those of the second 'maintain the impossibility of an infinite essentially ordered regress', and those of the third 'have no reference to an infinite regress at all'.
  
  [...] Davidson (just quoted to the effect that Ibn Sinā's proof was definitely cosmological) claims that Ibn Sinā hit on an argument which could entirely dispense with reasoning from the absurdity of an infinite regress. This would mean that, in terms of Craig's typology, the shaykh had produced a cosmological argument directly foreshadowing the Leibnizian (or Spinozist) approach. However,
  Davidson surprisingly finds that Ibn Sinā 'due to the influence upon him of other proofs of the existence of God illogically forced his own proof into the mould of familiar cosmological proofs that do explicitly reject an infinite regress of causes'".


It is noteworthy that the Burali-Forti paradox can also be equivalently presented as employing "infinite regress" (https://en.wikipedia.org/wiki/Transfinite_induction transfinite induction) or as a reductio that avoids its explicit use.
To me the above example sounds like being related to the category of fallacies that arise due to missing the point. Fallacies of this category include strawman,  Non Sequitur, Ignoratio Elenchi and red herring.

Among these, red herring fits the description best. The fallacy occurs when an irrelevant subject is interjected into the conversation to divert attentions away from the main issue. More specifically, in offering an argument toward a certain topic, the arguer changes the topic and provides evidence that is irrelevant to the original topic, and then draws a conclusion about the original topic through the distraction. This is an example:

I believe that Yahoo is a better search engine than Google or Bing. Yahoo hired a female CEO, which shows its commitment to the gender equality issue in tech industry. By contrast, Google and Microsoft, the owner of Bing, had done nothing to mitigate the inequality problem. Clearly, Yahoo is the winner in the field of search engine technology.

The fallacy gets its name from the practice of training a hunting dog. A red herring, a fish with very strong scent, is dragged across the trail with the aim of leading dogs astray. Only the best dogs will follow the original scent. 

  By rights, one means an entitlement a person has to do something. 
  Abortion, on the other hand, does not concern the pregnant woman doing something. 


Your definition of rights is overly restrictive. That a woman ought not to be obliged to carry a pregnancy to term also falls within the traditional scope of a right:

From https://plato.stanford.edu/entries/rights/ Stanford Encyclopedia of Philosophy: "Rights are entitlements (not) to perform certain actions, or (not) to be in certain states; or entitlements that others (not) perform certain actions or (not) be in certain states."


  People don't do abortions on themselves. 


As in you're stating that were it not for the involvement of a third party, an abortion would not occur? Because the concept of clinician-administered abortions is a relatively recent development in the history of abortions. Ultimately, the capacity for a woman to have an abortion is not at the mercy of a third party i.e. it's a procedure that, if required, she would be able to perform on herself.


  Rather, abortion, as it is usually understood, refers to the act an external party, the doctor, performs at an external location, the hospital, all made readily available by yet another external party, the government. Further, there are costs and logicistal issues associated with this entire procedure, which, again, are external matters.


In the United States, the use of federal funds to pay for an abortion are prohibited under the Hyde Amendment. In circumstances where the costs of an abortion are borne by private parties, the  matter of a right now appears to be whether a woman can enter into a commercial contract with another party.
I think that the problem is an undistributed middle term.


The perpetrator is white, male, 25-35, and aggressive.
Jack is white, male, 25-35, and aggressive.
Jack is the perpetrator.


This syllogism is AAA in the second figure. The middle term is "white, male, [&c]", and is undistributed in both premises. There is thus no link between the two statements.

By the way, this reasoning is the error that underlies guilt by association.
Your question presumes that discrimination and appreciation are mutually exclusive. However, a wine enthusist has a highly discriminant pallet which in large part is due to his great appreciation of wine.
No, for two reasons:

Firstly, possession of a snake is not germane to the utility of anti-venom i.e. everyone possessing a vial of anti-venom would be equivalent (and indeed better though not especially useful).

Secondly, anti-venom is a different kind of protection than carrying a gun. The former is a passive defence while the latter is an active deterrent. A more accurate equivalent to anti-venom would be for everyone to wear a bullet-proof vest. As no-one is suggesting the latter, the former is not really a strong argument.

Now, if you carried something that snakes were fearful of (nothing springs to mind except possibly a gun) then that would be a reasonable equivalent. But, presumably, that doesn't really achieve your aim.

BTW this isn't intended as an implied defence of gun ownership but a critique of the snake argument.
It is said, in the tradition, revenge is for a hurt, justice is for a wrong. I read, somewhere, not long ago, in a thinker, the statement that: even a dog distinguishes between being kicked and being stumbled over. The pain of being stumbled over might be worse. Yet, by nature, if the dog is any measure, this is not something that deserves reprisal. 

Eventually, starting from such dialectical and simple points, one can come to a theory of the injustice of revenge, and the justice of punishing a legal wrong based on intentionality. Yet, one can never leave it at that. And the discussion is ultimately thrown into the abyss, seeing how the instinctual opinions of dogs, may be nothing but conditioning, i.e., evolution, and of men, perhaps the trauma of a Freudian Father Figure. If all such considerations are baseless, or merely a matter of various forms of control, whether societal or through physical determination, etc... 

Did you mean, however, that revenge means taking the law into one's own hands, rather than that of the authority of a public agency? Conifold, as you see, speaks correctly, in saying that here we have a vast area, and some qualification is needed to usefully attempt to elucidate and so to investigate the matter of a specific consideration. 
To treat someone merely as a means you need to be not considering the value of the outcome for them as a person.  You have to be intending they should see no benefit or choice, despite their involvement and significance in the situation.

When it is done ethically, imprisonment is meant to serve an reformative or an expiative effect through its restriction of the individual.

The potential for reform or the affirmation of the person's individual responsibility and power of self-determination and the release of their emotional guilt means that this is not use of the prisoner as a mere means, as long as it is genuine.

The latter is kind of subtle, but I think it is the aspect Kant would emphasize, because he minimizes the significance of predictions of the future.  (No human skill should be a direct requirement for moral action.  So the ability to understand likely outcomes should not matter. Guessing that reform is more or less likely is a computation -- which requires a skill.)

So let me explain 'expiation'.  Tying criminal acts to consequences means that one is legitimately making a decision when one acts as a criminal. Not following through on the deal under which that decision was made is unfair to the person making the decision.  It undercuts their sense of structure and fairness in the world -- a world in which they live as much as everyone else.  It handles them paternalistically, and reduces them to having a childish lack of traction on their future.

So rules are good for everyone, including those who break them.  Stating those rules, but then not following them, without making some other demand or plan that everyone involved would agree is in fact 'expiative' is bad for everyone involved, including the perpetrator, as it leaves the self-loathing of guilt and the fear of retribution on the table.  (As usual, a genuine Kantian solution allows for both secular and religious interpretations.  The notion of expiation allows for the Christian alternative of genuine contrition as an alternative to actual justice, but only, again, when it can be proved genuine to the victim, who gets a sense of righteousness from his forgiveness.)
A related question and its answers appear at https://philosophy.stackexchange.com/questions/46301/where-does-aristotle-mention-inequality-and-his-solution-to-this/46310#46310 Where does Aristotle mention inequality and his solution to this?

My answer appears there, also. In short, it looks like Aristotle and John Rawls arrived at similar conclusions for different reasons.
This is an interestingly and painfully complex situation.

▻ UTILITARIANISM

There seems little scope for a rule utilitarian resolution here. Take rule utilitarianism to be the requirement to act on a rule which, if adhered to by everyone, would (probably) produce the greatest amount of good. 

'Good' can be interpreted in a variety of ways (happiness, pleasure, the satisfaction of interests or preferences among them). Interpreting the nature of good is not the problem here; rather the problem is that no rule of sufficient complexity is likely to be available in this situation. The moral life would be impracticable if rules had to accommodate such detailed circumstantiality. 

Then the option appears to be act utilitarianism. I take this to be the requirement to do that action which will (probably) produce the greatest amount of good. 'Good' again is open to latitude of interpretation. 

It can only be down to the act utilitarian to think her or his way through the situation and to decide what, all things considered, will produce the greatest good. What that decision will be will depend on assessments of probable consequences (that's built into act utilitarianism as defined above) but beyond that it is too situational for an outsider to fix objectively what the doctor - 'you' - should do. The doctor must trust to 'moral luck' that she or he has made the right, or at least a defensible, act utilitarian decision. 

My formulations of rule and act utilitarianism are rough and approximate but more sophisticated and qualified formulations would not deflect the points I've made. 

▻ KANTIANISM

I think a similar indeterminateness, for quite different reasons, impacts the Kantian agent. The clearest light is thrown not by the 'Groundwork' but by 'The Metaphysics of Morals' ('MM'). In MM II.1.9 Kant observes that 'The supreme principle of the doctrine of virtue is : act in accordance with a maxim of ends that it can be a universal law for everyone to have. - In accordance with this principle a human being is an end for himself as well as for others' (MM, tr. M. Gregor, Cambridge, 1996, 157). Familiar stuff but he adds that if the moral law 'can prescribe only the maxim of actions, not actions themselves, this is a sign that it leaves a playroom (latitudo) for free choice in following (complying with) the law, that is, that the law cannot specify precisely in what way one is to act' (MM, II.1.7 : Gregor, 153). 

I should argue that whatever maxim the doctor adopts in choosing between the options you identify, that maxim could respect 'The supreme principle of the doctrine of virtue ... : act in accordance with a maxim of ends that it can be a universal law for everyone to have' and treat all the human beings involved as ends for themselves as well as for others. 

▻ CONCLUDING REFLECTION

John Rawls in one of his early papers referred to the need, or at least the desirability, of a 'decision procedure' for ethics. Given the need for the ethicist, not to simplify the moral life but to represent its real complexity, I find reassurance in the failure of either utilitarianism or Kantianism to cut a knife through the dilemma you describe and specify a single, determinate solution.

J. Rawls, 'Outline of a Decision Procedure for Ethics', The Philosophical Review, vol. 60, No. 2 (Apr., 1951), pp. 177-197.
The notion that we are naturally war-like is popular but has a few holes, actually. From archeological data, it may be entirely possible that we never waged war before we started having private property.

And if many civilizations on earth crumbled at some point, it was almost never to the point of destruction. Rome wasn't erased from the map in the 5th century, it continued to exist but stopped being the powerhouse it was before.

Today, we see a lot of signs showing the human race realizes some of its limitations and is taking steps to correct its course. Maybe tens of millions of people will die before we do it enough, maybe even a few billions. But that doesn't mean human extinction.
The difference is expressed by the late David Lewis in terms of two examples ('Counterfactuals, 1973) : 


Indicative conditional : 'If Oswald did not kill Kennedy, then someone else did'.
Counterfactual conditional : 'If Oswald had not killed Kennedy, then someone
else would have.'


As Lewis points out, 1. is probably true and 2. may very well be false. Certainly, accepting 1. does not commit us to 2. and accepting 2. does not commit us to 1. 

Everything to do with counterfactuals soon becomes technical but I hope this note gives some idea of what the difference is.
The premise 'If nobody did it' or 'If everybody did it' is not questionable at all, it is often a staple in Kantian arguments, where broad universal rules are the main goal of many arguments.  It is always a good thing to consider, because it clears away a lot of details in which we might otherwise be excessively invested.

But your specific deductions are not necessarily valid.  (Daniel Dennett agrees with you, but I find this less than compelling.)

How do you decide that the greatest number of beings is really part of the goal of a species?  Humans, from the Anglo world to Japan, and most notoriously China, are deciding that quality trumps quantity and under-reproducing themselves in large numbers, following what seems to be the forward direction for their species.  How can you know that cows would not somehow be happier if they were less numerous but free?  How do you know that we have not kept them from pursuing a greater evolutionary destiny by including them into ours?

At the same time, what obliges us to imagine we know anything about the well-being of cows in the first place?  We can only apply human standards, not understanding the essential psychology of a cow, and those are most surely just wrong.

Both sides of this argument, as you present them, seem to be about psychological projection, and not logic.  To the degree that human empathy automatically creates psychological projection, we should think this through, because what we think of ourselves matters.  But we should not lose track of the fact that empathy is about the psychology of the one empathizing, and does not really reflect the autonomy of the one empathized with.

  Is it moral to act kindly?



It is always moral to be kind.
It is sometimes moral to act kindly.


~ https://en.oxforddictionaries.com/definition/kindness Kindness is defined as the quality of being friendly, generous, and considerate.

~ Aristotle said that https://www.cwu.edu/~warren/Unit1/aristotles_virtues_and_vices.htm friendliness is a virtue.

~ https://en.oxforddictionaries.com/definition/virtue Virtue is defined as behavior showing high moral standards.

~ https://www.merriam-webster.com/dictionary/be To be means to equal in meaning.

With the given facts, logically: "It is always moral to be kind." Because if you are kind, then you are moral. Now,

~https://www.merriam-webster.com/dictionary/act To act has two different meanings, relevant to the question:


Doing or deed; and 2. a display of affected behavior : pretense ; examples: put on an act that deceived nobody. His friendly concern was just an act.



  The reason you are motivated to be kind is mostly determined by several factors. The very first one is your https://en.wikipedia.org/wiki/Intention#Intentions_and_behaviors intention.



So if you intend to be kind, your acts or deeds are moral.
But if you are intentionally unkind; or, if your kind acts are unintended or unintentional, they are in reality only a https://plato.stanford.edu/entries/imagination/ pretense though they may appear moral to some people.


See also: https://plato.stanford.edu/entries/fiction/ Fiction

In conclusion, as most philosophers are aware of the fact that 
https://www.cwu.edu/~warren/Unit1/aristotles_virtues_and_vices.htm truthfulness is also considered a virtue (along with kindness), so it should be apparent to most of you that a pretense of kindness is not true kindness, and therefore is not moral.
To be fair, the notion is really 'more perfect', 'more free of defect' or 'closer to ideal', rather than 'greater'.

Anselm would consider not existing in reality to be a defect, so whatever does not exist in reality could be better if it were to exist in reality.  So if you decided God were not real, you did not actually correctly identify the thing most free of defects as God.  So there is a real implication there.  You have taken a candidate for perfection and improved it by removing a flaw.  That is not circular or degenerate.

But that candidate would have to exist.  The first part of the deduction does have the weakness you indicate.  The notion that the lattice of perfect or defect-free things has a single maximal value is kind of bizarre, given that we know that, for instance, the lattice of integers doesn't. We can create one, slap it on there, and name it 'infinity' but it does not actually exist, and proposing it creates all kinds of confusion and numerous contradictions to be evaded. It can only exist outside the integers...

There is also a basic problem with the idea that 'existing in reality' is a property things can have or not have. Where do we keep the things that do not exist in reality -- do we have another place to put them, other than reality? In fact things might exist 'modally', as fictional ideas, as potential creations, as inchoate wishes, etc... but the idea of 'reality' is a red herring, since any given notion of reality will include some of these and exclude others, kind of according to taste.
Short answer is you actually don't. In point of fact, there's a better chance statistically of you being a https://en.wikipedia.org/wiki/Boltzmann_brain Boltzmann Brain  because it would be far simpler for the universe to create your brain in isolation with all its memories than it would be to create a complete universe in which you could come to pass conventionally and collect all those memories directly.

To conduct a thought experiment; imagine it's possible to take the memories in a single mind and transfer them completely to another mind in another body, wiping out the original memories in the process (full transfer of 'consciousness'). What happens when you take the set of memories in a blind person and put them into the mind of someone who can see?

Worse yet, take a healthy person's memory and put them in the mind of a person who's been given a lobotomy?

A part of the consistency of memory is that they are consistent with our own personality, brain chemistry and ability to sense the world around us. That doesn't make it clear that our memories are real, but it does say that if they're manufactured, then someone has gone out of their way to make it seem as real as possible. It would be cases where you can't remember seeing (but now you can) that would be the clear indication that something's wrong along the lines that you mention.
Even animals care for their young ones (their feelings and emotions also). This is an already-installed quality in most living things.  Haven't you seen mother birds hatching their eggs? Will they hatch if no care is given? ...Bees caring their larvae? Are they all necessary to maintain the equilibrium of nature?

What would happen if no care (for feelings and emotions) is given to the disabled and the aged? 

Think about the development of your language and your favorite pieces of music. If no care was given would they develop as you use or enjoy them now?

After reading the following, please don't blame me.

If nobody cared for you, your parents would leave you as a waste within a few hours of your birth. And you wouldn't get any food even for the first few days. Your mother also wouldn't get any care from others. I don't need to explain further. Here I linked 3 persons only. You may make this chain longer.    Considering others also you may make or think of a web.

I don't need to explain your character (if you survived) if your parents and grandparents didn't get love and care from anywhere.   I leave it for your thought.  You will understand how civilizations develop.


  We must exercise certain deeds hailed in holy texts.


This is not necessary everywhere (or in all categories). Try to read other holy texts also. 

=========================================================================

P.S.

Man is a social being.  He can't grow or develop without others' help.

From your question I guess you are asking about man as a being neutral in character (Whose deeds cannot be considered as good or bad).  

(Although most often it is relative,) There are people having good and bad characters.  You can live in this world without doing anything good.  But you should be ready to accept its consequences without any complaints.  And before saying "Yes", you should know one thing -- Every thing you are tasting/enjoying now is because other people and animals are maintaining love and care for others' feelings and emotions and also because they are doing good.  Otherwise you would be living as member of an uncivilized primitive tribe.  [I think even those primitive men care for their progenies' some feelings and emotions.]

Humans have developed brain and many things they experience will make impressions in their mind for a long time.  Normally, he feels attachment towards things, places, people etc and becomes stronger as time passes by.  You begin to say "these are my things, my people" etc.  And their wishes, feelings etc are more varied than other animals'.  As you know humans can control their feelings as well.  Also, they have some special feelings like pride, shyness etc.  They wish to be loved and cared by others. Since they can think better they always try to do and enjoy everything in a better way.  The best way to fulfill their wishes, feelings etc is to be good and do good to others. And then only their mind will be free from worries. They know that.

Please understand this: 

You may live in the society without any good character; without doing anything good.  You feel problem only when you begin to ask some questions to yourself.  (This happens especially in one's old age.) 

E.g. I have enjoyed my life with the help of others.  Did I do anything good for them in return?

Who am I really? What is the aim of human life?

I am born with many abilities and interests.  Did I have a past life?  If so will I have a posthumous life?  

Where will I go after death?  

Will I become a mere nothingness after death?

I wish to enjoy this world again. Will I be born here or somewhere else?  

If these types of questions don't worry you, and if you don't do anything bad to others, there is no problem to them (i.e., to others). 

But the final authority to decide whether there is any problem to you, is none other than you. 


  Mind is everything. It controls your whole life. Upon it depends your
  happiness or misery, success or failure. "Mana eva Manushyanam Karanam
  Bandhamokshayoh" thus say the Upanishads. Again, 'Yena Manojitam Jagat
  Jitam Tena' is the great truth. As you think, so you become. Do you
  fully realise now the great importance of controlling, training and
  overcoming the mind? So long you have neglected the care of the mind.
  Attend to this vital subject from now. Mastery of mind means success
  in all fields of life. To achieve this mastery you must study the
  mind. You must understand its nature, habits, tricks and the effective
  methods of bringing it under restraint.


To read the full text see: http://sivanandaonline.org/public_html/?cmd=displaysection§ion_id=476 http://sivanandaonline.org/public_html/?cmd=displaysection§ion_id=476

If you wish to know the differences between man and animals, please read this:

http://blog.practicalsanskrit.com/2010/01/human-or-animal-what-is-difference.html http://blog.practicalsanskrit.com/2010/01/human-or-animal-what-is-difference.html

If you found the things I mentioned above logical, (You may ignore the religious texts mentioned here) you may take it as base and try to understand  holy texts. And you will certainly understand whether they are useful or not.

To do good implies to be good. To know more about its ultimate importance, please read the following extract from a website:


  Satsangatve Nissangatvam Nissangatve Nirmohatvam Nirmohatve
  
  Nischalatattvam Nischalatattve Jeevanmuktih ... Bhaja Gonvidan, Bhaja
  Govindam
  
  Meaning: Sat sangatve - through the company of the good, Nissangatvam
  - (there arises) non-attachment, Nissangatve - through non-attachment, Nirmohatvam - (there arises) freedom from delusion, Nirmohatve -
  through the freedom from delusion, Nischala - Immutable, Tattvam -
  Reality, Nischalatattve - through the Immutable Reality, Jeevanmuktih
  - (comes) the state of 'liberated-in-life'.
  
  Substance: The company of the good weans one away from false
  attachments; from non-attachment comes freedom from delusion, when the
  delusion ends, the mind becomes unwavering and steady and from an
  unwavering and steady mind comes Jeevat Mukti (liberation even in this
  life).

Ιn the philosophy of quantum mechanics some authors who are wave function realists (i.e. who believe that quantum states have an ontology which is independent from our knowledge about them) claim that the "universal wave function" belongs to a unique ontological category which is nothing like anything else we experience. 

They make this claim in response to other views about the ontology of the wave function as a field, a law, a blob etc. since all these views face important difficulties.

So I guess there isn't a yes or no answer to your question.It depends on what kind of metaphysics one adopts. I guess a better question would be: do we have reasons to believe that there are physical entities of a unique ontology?
"Archon basileus" translates as "king magistrate" So says Wikipedia:
https://en.wikipedia.org/wiki/Archon_basileus https://en.wikipedia.org/wiki/Archon_basileus
Socrates's trial does not seem to have had a presiding official in the sense of an American trial judge. https://en.wikipedia.org/wiki/Trial_of_Socrates https://en.wikipedia.org/wiki/Trial_of_Socrates

Euthyphro was a character, with Socrates, in Plato's Euthyphro dialogue. The full surviving text of the dialogue is available at The Internet Classics Archive: http://classics.mit.edu/Plato/euthyfro.html http://classics.mit.edu/Plato/euthyfro.html 
From "Alchemy" (1995) by Eric John Holmyard: "the symbols for fire, air, water, and earth indicat the property of the first two to ascend and that of the second two to descend..." So fire and air face upward while water and earth face downward. I am not 100% certain about the lines themselves, but you might be able to find more information in the book. They seem to indicate the "top" and "bottom" however, with water and fire in the middle. 

This http://symboldictionary.net/?p=1914 site, while not as scholarly, may provide additional information. Notice that if you overlay air and earth, the extra lines do connect, suggesting a relationship between the two elements.
Wouldn't it depend on what you mean specifcally with 


  doesn't have any lasting effects


You obviously have the feeling of Pain that you are refering to.
However there might also be effects during the duration of healing. For example you being limited in doing certain actions. 
Imagine following thought experiment: You and and idealized you are running on a track and the only difference is that you have a minor incident where you have to tie your shoes during the race. This limits your ability to run during this time. You will therefore be behind the idealized you for the rest of the race. Despite the effects not being directly in place.
You're misunderstanding the term.

The biological imperative does not mean organisms have to do particular things, it means that there are certain.. we'll say, something like natural laws which guide how we understand living things, and how living things are likely to behave.

In a nutshell, the biological imperative is the idea that living things exist to produce copies of themselves. A few corollaries of this imperative are that living things also need to both survive, and find mates to produce those copies, in addition to whatever properties stem from that.

As a scientific term, then, it is an important concept because everything else about the biology of an organism ties back to this imperative. Everything about you, everything you do, how your brain works, is ultimately aimed at surviving, reproducing, and creating copies of yourself. So when scientists study an organism, they make the assumption that this is the purpose of any evolved features.

For most animals this imperative is bullet-proof and always plays itself out. Where humans differ is that we've adapted a strong ability to adapt to new things, which can include our own desire for reproduction. Still, though, our bodies will usually be inclined to seek out partners, make money, and survive, regardless of whether we actually produce children.

In sum, there is nothing we have to do, it is an imperative that any biological organism is oriented toward survival and reproduction.
You might consider looking at Catherine Malabou on this problem today, who is concerned with the plasticity of the brain — both its capacity for growth and dynamic development, but also its potential for destruction, breakdown; dissolutions. Damage to memory and cognition eventually wear away the substrate of “personality” — but even though “you” have been effectively lost beyond recovery, this was all still in a way “you” since these are transformations your organs were always capable of undergoing, part of their internal possibilities. (The plasticity of the brain cuts both ways — allowing us to learn and heal, “route around” damaged components; but also permitting collapse, degeneration, erasure, etc.)
I think there are two main elements or aspects to Ortega's 'The Dehumanisation of Art' (La deshumanización del arte e Ideas sobre la novela, 1925; Princeton tr., 1968). 

The first can be illustrated by a passage from the book :


  A great man is dying. His wife is by his bedside. A doctor takes the
  dying man's pulse. In the background two more persons are dis
  covered: a reporter, who is present for professional reasons, and a
  painter whom mere chance has brought here. Wife, doctor, reporter,
  and painter witness one and the same event. Nonetheless, this identical
  event-a man's death-impresses each of them in a different way.
  So different indeed that the several aspects have hardly anything in
  common. What this scene means to the wife who is all grief has so
  little to do with what it means to the painter who looks on impas-
  sively that it seems doubtful whether the two can be said to be
  present at the same event' (1968 : 145).


The dehumanisation here, which has nothing negative about it, is the artist's emotional detachment from the scene - 'psychic distance' in one sense of Edward Bullough's phrase. 

The second element or aspect is significantly diffferent, or seems so to me. It centres on the use of the artistic image - in poetry, painting or whatever - to 'denigrate' human lived experience. For instance a poet might use the image of a paperclip to describe the appearance of a bent and frail elderly person. The image is fresh but the result is to denigrate the person or whatever the object - it's to say, really this person is less significant than the image I use to describe her or him. 

Another aspect of denigration unfolds. A long poem might be written about the contours and texture of (wait for it !) a human wart; an entire wall might depict a toe-nail cutting. Such art pulls us down to consider what we normally take to be beneath our notice or what we regard as distasteful or repellent. 'You humans might create great beauty but you also have warts and toe-nail cuttings. Don't think too well of yourselves' - these are not Ortega's words but if I'm right they express his attitude. 

Ortega is a difficult writer; and his ideas changed. But this is my best effort to explain what I think he meant by the dehumanisation of art. What I've got wrong or omitted, doubtless other answers will correct and supply. 
Your premise is that God is a good being. But already the bible shows us that if God exists then he is certainly not a good being in the sense of humans, but either indifferent or deliberately bad. (Thererfore it would be better if he does not exist.)

As already the bible shows us God cannot simultaneously be omniscient and infinitely merciful: 

Tree of knowledge amidsts the Garden of Eden

Creation of female and snake

Order to Abraham to sacrifice his son

Plagues and ruin of the Egypts

Israel's sons kill Sichem's men and rob the town but God managed that they did not get punished.

The Flood 

Every torture of a living being

But God is certainly not even omnipotent. Why did he need 6 days (and another day of rest) to create what an omnipotent God would have accomplished immediately?

It is clear that, if he exists, not all his properties are infinite (consider the scholastic proof that he cannot make a stone so heavy that he cannot lift it or the other way round).

If he exists he can be understood like a player of games like Age of Empires which would be boring if everything evolved without conflicts.

So the answer to your question is: Your premise is false.
Take it as a fairly simple test case in Kant.  Could we universalize this?

If everyone were responsible for ensuring others' morality you would end up with a degree of meddling that most folks could not wish for.  (We already see how intolerable the culture's judgement of people's parenting is, and this would make everyone effectively the parent of everyone else.)  Also, given that most duties are contingent anyway, and related maxims can be stated many different ways, our autonomy to reach the truth by our own path would lead to constant problems with interpretation.

So no.  At least according to one of the most popular deontologists, making others do their duty should not be a direct duty.

Making sure that institutions to which you contribute encourage others to do their duty is another thing entirely.  The notion of a community, generally a state, allows for a limited and codified kind of meddling which people have accepted voluntarily or according to tradition and character.

So somebody in the community should enforce a given shared interpretation of a minimal compliance with duty.  And you should cooperate with that process.  But each of us independently should not.  Vigilantism is not in order.  You are not obligated go out and search for ways to prevent murder.  But you should require that your state do its best at this, to the extent you are able.

This is much simpler, and not recursive, only reciprocal.  We are to properly judge the state so that it judges everyone else's morality properly.
Yes, Whitehead builds on Leibniz view of monads.  The initial picture is that each monad reflects all others, so every thing is defined by its effects on everything else.  Whitehead explicitly adopts this visual analogy in Science and the Modern World.  He backs off from it moving forward with Process and Reality, because it is still too static.  But it captures the idea.

As I see it Whitehead's notion of organic identity does not really fit the more simplified notion of category theory.  A category has to have identity relations that carry each object back into itself.

This calls out what is and what is not a basic object in a way that would not be really possible in a process reality.  Some collections of things have an identity, and others do not.  So you would need some kind of model that derives identity from the other relations by minimizing in some lattice.

Category theory makes only half a step in the right direction.
Outside of the context of a tightly controlled thought experiment, there's nothing to suggest that your personal death would do anything substantial to improve conditions for the rest of us.  The Earth actually has more than enough resources to sustain http://www.bbc.com/earth/story/20160311-how-many-people-can-our-planet-really-support an even larger population than the one we have.  The real problem is that we are consuming resources wastefully, inefficiently, and http://money.cnn.com/2017/12/07/technology/bitcoin-energy-environment/index.html needlessly.  As Gandhi said, https://www.thenational.ae/opinion/comment/the-earth-provides-enough-to-meet-everyone-s-needs-1.426562 "The Earth has enough resources to meet the needs of all, but not enough to satisfy the greed of even one person."

There is also little to suggest that your own death would inspire the needed cultural change to shift to sustainable living, or that even the voluntary suicides of massive percentages of the population would keep the remaining people from offsetting any gains with increased consumption.

If you are willing to die to promote sustainable living, a better, and potentially more effective option might be to devote your life to that same end (while, of course, personally living as sustainably as possible).
https://en.m.wikipedia.org/wiki/Cybernetics Cybernetics is a scientific discipline that deals with closed-loop systems and open loop systems. It's related to https://en.m.wikipedia.org/wiki/Control_theory control theory in engineering.

Many systems on Earth, from technical or engineering systems to markets and social systems, can be "controlled" (as in made to behave in a desired way) by intervention. This intervention is either due to moving an https://en.m.wikipedia.org/wiki/Actuator actuator, like an electric motor to rotate a shaft, in an engineering system, or due to laws and policing in a social system. The systems are dynamical systems (differential equations), so their states change over time. 

Open loop systems are steered in a desired direction by an actuation system. They are not very robust (not very good at achieving their desired goal) if there are externalities that influence and disturb the system. Such external forces can be both physical and social depending on the system.

Closed loop systems have feedback from the current state of the system to the desired state of the system. The difference between desired and actual current state is called the control error. This difference goes into a controller, which determines the required actuator position to make the actual state follow the desired state. Closed loop control systems are much more robust than open loop control systems.

The following diagram, taken from https://en.m.wikipedia.org/wiki/Control_theory Wikipedia, shows a closed loop system.

https://i.stack.imgur.com/J7CFv.png 
Science tells us that a human fetus is a distinct human being. Abortion ends the life of that human being. Therefore, a right to abortion includes a right to end a human life.

It's not hard people. I know we want to be politically correct and everything, but in this case, it's just a matter of scientific consensus.
This question can be best answered by the motivation of the participant.

Let's say you have a tennis star. He loves tennis; he plays it all the time. The fact that he's so good at it that he gets paid to do it full time is a side point.

This man is a hobbyist. Tennis is his hobby, and he's so good at his hobby that he's found a way to do it all the time.

On the other hand; you have a tennis star who is ultra-competitive. He loves tennis, but the part of it that he loves is competing (and beating when he can) the best in the sport. He craves holding that trophy aloft, in front of adoring fans. The money is just a means to the end of reaching that lofty height.

This man is a sportsman. Tennis is his chosen medium of competition and he works at it to be the best in his field.

Finally, we have a man who's very good at tennis, but it's not a passion. He's in it to win because that gives him a bigger paycheck that he can use on charitable work, buying a bigger house; whatever. His motivations for the money are immaterial.

This man is a performer. His job is to bring in crowds who pay to see him, and he'll adapt his playing style accordingly. He'll bring flair and attitude to the game so he attracts interest, which translate to appearance fees at smaller tournaments and the like. He'll have large contracts with companies like Armani, Longines, or similar long before others in the sport. He'll sport a fashionable or interesting haircut and will always be dressed in the most modern style possible.

Whether we like it or not, the business model for most sporting events around the world these days is 'put on an interesting show, get people to pay to watch it'. That makes those sporting events attractions in some form. There is an artistic side to this, and there are certainly some parallels with concerts or the theatre. Certainly, some 'sportsmen' are adapting by being more flashy rather than competitive, meaning that they bring their own audiences to events and therefore have higher bargaining power with the organisers.

But to be fair, not all sportsmen (I'm using these terms in a gender neutral context by the way) feel that way. Some love their sport, love competing against the best. To them, it's not about entertainment, it's about doing your best.

So, while ultimately the answer to your question is YES, that doesn't mean its the primary motivation for all people in that sport and it would be unfair to paint all sportsmen as performance artists or entertainers. As such, sport is what you get from it; it's just that in today's world, the major participation model is spectating.
Cheating genetics? Yes, absolutely. In precisely the same way that wearing clothing cheats genetics.

Cheating evolution? Definitely not, at least not evolution by natural selection. That's exactly what's going on here. Our hero is adapting to his environment to ensure genetic success. 
Reiterating what Mauro said, your use of the word "chain" is a clue that you are not referring to the concept of reason in general, but the causal chain associated with events (reasons why.)

In the case of an individual women, a causal chain can be observed which factors in the Agreeableness trait, and its implications for the woman's salary, etc. However, as soon as you "zoom out" to the level of a company, a nation, a species, you are no longer looking at a direct causal chain of events, but overlapping distributions.


  If someone says women earn less because they are women, is this
  correct?


Notice that by removing the middle link of your causal chain, you have created a lower resolution representation of the initial premise. Technically the statement is true, but cannot be demonstrated as true to someone ignorant of the lack of assertiveness mentioned in the original chain. This is because the bottom chain has collapsed into something which can no longer be represented as a syllogism (which requires a minimum of three related claims.)

Demonstrated as True:


Less assertive people earn less money than those who are more
assertive
Women are less assertive
Therefore, women earn less money.


True but not demonstrated, because of a baked-in premise:


Women earn less because they are women


The latter is no different than saying:

Women earn less.
I am not an expert but I will try my best. My understanding is that God is a being/existence that would have adequate capabilities to create the known universe as based on God's own will.
The advocate has the burden of proof. Regardless of developments in society, B always retains the burden of proving "X and Y". This duty remains in place whether A denies Y, C affirms "X and Y and W", or whether people offer any other combination of statements.

A takes on the task of showing that "X and not Y" is true, and will always retain that burden.

As a practical matter, given the popularity of A's new idea, or the effectiveness of A as an orator or writer, B might have to act sooner than B wants to. But this practical burden is different from the logical burden.
I think that with your interpretation you are adding in travel between these realities as well as between times. If indeed we can be this omniscient time traveler we could (almost) all find "our price" for betraying our morality and settle in that reality/branch of decisions.

Before now I do not believe I have heard of an interpretation where we can have this ability. If we had this power even in our own single reality, I'm sure we could also find a perfect path for ourselves among the near limitless options in front of us.

While a lot of people here may have some inherent problems with the notion of asking a "should/shouldn't" question, I would have to agree that it would be within our best interest and potentially the best interest of humanity itself to explore a lot of these options before "deciding."

As for your final question. We are certainly hampered or limited, whether for better or worse, by our reality and the constructs within. Note that some of these will certainly be for the better though. For every "good" scenario stemming from the many worlds theory there can be at least just as many "bad" realities.
I think the idea of what is natural, and what unnatural, in ethics stems from beliefs about the proper or normal development of a human being. Such beliefs need a context. Aristotle, for instance, in his 'Politics' and 'Nicomachean Ethics', plainly thinks of a human being as having an inbuilt developmental pattern such that he or she can flourish - enjoy objective well-being - only if a range of virtues or dispositions are possessed and exercised. These are the ethike aretai and the noetikai aretai : moral traits such as courage, justice, self-control, and intellectual traits such as phronesis or practical wisdom. 

It's important to see that Aristotle regarded a life in which the moral and intellectual virtues were possessed and exercised as the right, the proper, the 'natural' life for a human being, just as natural as an acorn growing into an oak tree. All sorts of assumptions sit behind this view but mainly the idea of there being an authentic pattern of life which, as a matter of fact, human beings have an inbuilt tendency or drive towards and without achieving which they objectively cannot flourish. One of Aristotle's views was that the proper life for a human being involved participation in the self-government of the polis or city-state. He really believed this; political life is 'natural' to a human being, an essential element in a flourishing life : hence his famous phrase that we are 'political animals'. So Aristotle can tell us quite clearly what is unnatural - for one thing, it is life without political participation. 

A similar view of proper or normal development can be traced in Christianity. A philosopher such as St Thomas Aquinas will tell you that you were born to live in a certain way : to have faith, to worship God, to act charitably towards your neighbour, to forgive offences against yourself, to avoid pride, to see the present world as only a prelude to the perfection of heaven, and so on. All this is presented as the objective essence of a human life - literally what we were created for. To refuse to follow this authentic pattern and model of human life is to deny our God-given nature. It is literally to act unnaturally. 

When homosexuality is denounced as 'unnnatural' it is usually the Christian model that informs this view though Islam and Judaism also hold that homosexuality is unnatural - as not what the Creator intended in bringing us into existence. 

You correctly point out that violence and anger are 'natural' urges and that this doesn't make them right. I think what Aquinas would say is that homosexuality can be a 'natural' urge in the sense of being something to which people (some people) are inclined involuntarily but that this does not make homosexuality 'natural' in the sense of being any part of the authentic model and pattern of human life. It is a deviation from what God intended, an imperfection in the individual who is inclined towards it even if the inclination is involuntary.

I am not trying to sell Aristotle or Aquinas to you. I just hope I have explained how homosexuality can be both 'natural' in the sense of being involuntary to the individual but 'unnatural', if this is what you think, in the sense of being a deflection from the authentic model and pattern of human life. 

Both the examples I have used, Aristotle and Aquinas, belong to the Western tradition of thought. Similar examples could be taken from Eastern thought; I have left them out only because I do not have the expertise to discuss them. 

My own views about homosexuality are neither here nor there. 
The ship of Theseus can still sink
Somehow in this situation, the death drive or death instinct,  which is normally directed outward as aggression, must be directed in some other way or it could "go to ground" and be directed against the self; the self would discharge tension back to the peace of the womb which at an extreme could be suicide. Could aggression be discharged, along with libido, the sex drive, in play? Possibly so. Children do it. 

This is the beginning of the model, Freud's model in "Civilization and its Discontents" described by Marcuse here:   https://www.marxists.org/reference/archive/marcuse/works/eros-civilisation/ch01.htm https://www.marxists.org/reference/archive/marcuse/works/eros-civilisation/ch01.htm  You can see how Marcuse works that out in his later writings from his book "Eros and Civilization" onwards. If we do have the tremendous use of robots and automation, then somehow we must have a guaranteed income, or there will be social unrest and a lack of consumptiom. From this point forward we can see the importance of your question. Of course, countervailing forces may require all hands on deck (something like full employment) in a continued fight with nature due to global warming and so on. 

Summary: You ask a very sophisticated question. What will happen to the aggression if we do not need dominance any more in order to survive, in a world post-scarcity?   Far from revolution we could have mass suicide; aggression directed at the self. Can play be a solution to this problem? As far as I know, Marcuse is the only one who even tried to deal with this question in his book, "Eros and Civilization", 1955. Caveat/2018: we may have further need of human labor & outward aggression in an attempt to ameliorate climate change, etc. Economics, the problem of scarcity, would be briefly "solved" only to plunge back into scarcity again.
You are not stupid at all for experiencing this concept in this way --arguably this is exactly how this concept is meant to be experienced.  In point of fact, the early Platonic dialogues (believed to be closer to the philosophies of the historical Socrates, who is the main character in them) almost always end in https://en.wikipedia.org/wiki/Aporia aporia, a state of confusion, and suspended belief, in which the hearer understands and acknowledges that he believes things that are incompatible and paradoxical --the very state that you describe.

The later dialogues introduce more of an explicit metaphysics, and end less often in aporia.  Many readers, however, myself among them, believe that Plato https://philosophy.stackexchange.com/questions/43687/why-there-are-so-many-blunders-fallacies-in-platos-dialogues/43699#43699 deliberately left paradoxes and flaws in his metaphysics.  If so, his goal was perhaps to force the reader to think past any ideas or concepts that we can grasp, formulate, teach or learn in this earthly plane, and ultimately to grapple directly with a deeper and greater truth that cannot be accurately reflected from within our degraded version of reality.

If this viewpoint appeals to you, you might be interested in https://www.iep.utm.edu/neoplato/ Neoplatonism, a highly influential ancient Roman interpretation of Platonic thought, that advocated a more mystical approach to Plato than the literalist Aristotelian one that was current at the times.
Meritocracy is almost universally rejected by political philosophers.  Their reasons for rejecting it vary wildly, however.

Libertarians like Robert Nozick say that meritocracy violates rights.  If you own a firm, you are allegedly at liberty to hire whomever you like. This may be the most meritorious applicant; or it may be a friend, or a member of your preferred race, or whatever.

Utilitarians point out that a merit-based society may not be a maximally happy one.  (Perhaps it omits potential transfers from the rich to the poor that increase the general welfare.)

Egalitarians interested in "diversity" will often (e.g.) endorse gender-based discrimination so that a profession has the same male/female gender ratio as society at-large.  The meritocrat, in contrast, rejects all discrimination on grounds irrelevant from the point-of-view of merit.

John Rawls explicitly rejects meritocracy, and on several grounds.  (E.g.: One does not deserve one's natural traits, and so therefore one does not deserve anything that one obtains by way of them.)

In fact, there is only one contemporary political philosopher who defends robust meritocracy--me.  I explain why meritocracy is uniquely just, and why these other theories are wrong, in my recent book, https://www.routledge.com/Justice-and-the-Meritocratic-State/Mulligan/p/book/9781138283800 Justice and the Meritocratic State.
This really belongs somewhere like worldbuilding, not here.  But I tend to just answer stuff knowing the result will be discarded...

Mary Gentle has a similar species, who get a gender at the dawn of puberty.  She actually points out that in a society that is still very labor-intensive and leans toward huge families as a consequence, it can be very inconvenient for someone to not have known whether they will get the added boost of muscle development and size, or whether they will be in charge of a household of their own children.  She has characters who are very well-practiced in fighting, but never attain the height necessary to make it a profession because they become female.  Or who grow up male and miss the time they used to spend with children because they are required to earn money for their own spouses, who are tied up at home with their children.

So I think the answer has to depend upon the level of technology and how much that sorts the kinds of jobs people do by gender.  In a modern world, where most people don't, for instance, carry things for a living or simply apply force to things for hours and hours a day in attempts to shape them to their will, where motherhood is not a permanent full-time job in its own right, because we don't have as many children as possible, and where even effectiveness in war and personal safety are equalized by technology, there would be no reason to care what gender you were growing into.
This is probably a physics question more than a philosophy question, but the (not so) simple answer is that space and time are indeed related. This concept of space-time stems from the idea that the universe operates in a 4 dimensional construct, with 3 primary axes defining physical direction, and the last one defining time.

Perhaps one of the best descriptions I've seen of how this works was in Richard Feynmann's '6 (Not So) Easy Pieces', which presented some of his 'edited' lectures on physical topics, one of which describes relativity and that touches quite strongly on the nature of time in our environment.

To go into the specifics mentioned in your example though; with no phenomena around us to observe, a human mind without the benefit of learning that we all take for granted would have no conceptual framework for time, and would be incapable of understanding it the way we do. There's nothing from which to derive concepts of cause and effect or any other time dependent concept. That said, just because such a human could not perceive time doesn't mean that time doesn't exist for that human. We all breathe for several years before learning what air is, for instance.

This is similar to a thought experiment I use to describe the difference between consciousness and intelligence; storing 20 years worth of knowledge and experience on a HDD does not make the HDD conscious, or even intelligent. On the other hand, putting a newborn human in a sensory deprivation chamber for 20 years leaves you with a conscious person with absolutely no intelligence. The difference here is that the human has the capacity to learn, but is starting from scratch the moment he or she is thrust back into a real world.

So it is with this question. Putting a human in an environment devoid of stimuli merely restricts the human's opportunity to learn about time, not his or her capacity to learn about time if stimuli are reintroduced. In such a circumstance, it's not time itself which is removed, merely the observable phenomena from which the mind can learn about it.
