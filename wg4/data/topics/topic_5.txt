"http://en.wikipedia.org/wiki/Intuitionism Intuitionists" believe that mathematics is just a creation of the human mind. In that sense you can argue that mathematics is invented by humans. Any mathematical object exists only in our mind and don't as such have an existence.

"http://en.wikipedia.org/wiki/Platonism Platonists", on the other hand, argue that any mathematical object exists and we can only "see" them through our mind. Hence in some sense Platonists would vote that mathematics was discovered.

He assumed that if God exists then 

1a. humans are immortal (as a necessary condition for 1b)

1b. God decides whether they are tortured eternally or happy eternally (one of them is enough to profit infinitely, though)

1c. God bases his decision on whether they believe in him (if this is not the case, the wager does not work, because believing in God or believing in God because of the wager might cause infinite torture)

1d. God bases his decision on whether they follow religious law (again, the wager does not work if God does not like people following religious law)

1e. God is the Christian God and you have to believe in exactly the right doctrinal points and those are the points that Pascal was raised with (again, the wager does not work if God does not like people to follow a particular religious law)
It is not true that you have nothing to lose if there is no afterlife and you have spent all your life following the instruction of the religion even though you would have preferred to do something else.
He also assumed that he can rationally decide to believe. 


Summary: 
The wager assumes that there is a positive probability for God rewarding a particular behaviour with eternal bliss and a zero probability for God punishing this very behaviour with denial of eternal bliss. There is no reason for this assumption. 

If the probability for a reward is in fact zero, it does matter if you sacrifice all the life you have for a non-existing reward.

If the probability for God disliking the behavious is not zero, then you have to weigh more than one infinite expectance values.

Even if everything works fine, you would have to be able to believe or disbelieve just because you want to. (Like, say, an atheist who reads a study that believers have a better healing chance for a particular cancer and decides to become a believer as a therapy.)

It's logically fine if someone believes all the assumptions and then says that it is a good idea to believe. But this person believes in the first place, they do not believe because of the assumptions, they are just happy with their belief because of what they believe.
http://www.stanford.edu/~hammond/HarsanyiFest.pdf Harsanyi's theorem is often used as a justification of utilitarianism and basically involves averaging of individual utilities.

Other aggregation procedures don't seem applicable. Geometric averaging of individual utilities would be an odd thing to do, for instance. I can't quite give you the reason why it would be odd, but I have an instinct that it would do weird things like massively favour the best off or the worst off, or something like that.

There's a lot of work on judgement aggregation that involves using procedures different from arithmetic averaging, but I don't know whether one could apply it to aggregating utilities...
Specifically, which branch of philosophy are we talking about here? Since you've tagged the question /questions/tagged/logic logic, is it reasonable to assume that you're talking strictly about formal logic?

Because in that case, the proofs look an awful lot like mathematical proofs, and use many of the same basic patterns mentioned in your question. For a more thorough introduction to each of the types and the symbolic notation, see http://faculty.matcmadison.edu/alehnen/weblogic/logproof.htm this page. Also see http://mypage.iu.edu/~bmhorvat/misc/SentLog1.pdf this complete sample proof for a fairly simplistic problem.

But if you're talking about moral philosophy or any of the more http://www.thefreedictionary.com/touchy-feely "touchy-feely" branches of philosophy, then no. The best you get is an analytically-reasoned argument. And sometimes you don't even get that.
There is some instability in the terminology here. Many authors use Reductio Ad Absurdum (RAA) as meaning the same as proof by contradiction and indirect proof. More careful authors distinguish them, taking both RAA and indirect proof to be a species of proof by contradiction.

In what follows, I use P and Q for propositional meta-variables, ∧ for conjunction, ∨ for disjunction and ¬ for negations. (I do wish for LaTeX!)

RAA proceeds by assuming some proposition P, on that basis deriving some contradiction such as Q ∧ ¬Q, and, having reduced P to absurdity, concluding ¬P. In the context of a natural deduction proof system for logic with introduction and elimination rules for each connective, this is ¬-Introduction.

Indirect proof is the very similar method of proof whereby you assume ¬P, derive a contradiction such as Q ∧ ¬Q and then conclude that P. In the sort of natural deduction presentation just mentioned, this is the rule of ¬-elimination.

These two principles are very much worth distinguishing. If you take a classical propositional logic natural deduction proof system with introduction and elimination rules for each connective and remove ¬-elimination, the result is intuitionistic logic. This logic is more often thought of as being characterized by the denial of the law of excluded middle (P ∨ ¬P), but the two characterizations are equivalent for propositional logic.

Intuitionistic logic is one of the best studied and oldest non-classical logics, and one which plays a prominent role in many debates in metaphysics.
I think I've found a good answer to my own question. Basically the confusion comes from the fact that what scientists call realism is different from what most of the philosophers do! The philosophical consequence of Kochen-Specker's theorem is that a realism which requires knowable pre-existed reality, regardless of being justified, is impossible. Although here the justification doesn't necessarily require consciousness. It can probably be done by any system of high entropy. 

I've written this as a record. Let me know if you find it unclear but interested to know more about it.
Kant famously argued that http://en.wikipedia.org/wiki/Ontological_argument#Kant%3a_existence_is_not_a_predicate existence is not a predicate in his analysis of the ontological argument for the existence of god. 


  Kant argues that "'being' is obviously not a real predicate" and cannot be part of the concept of something. That is, to say that something is or exists is not to say something about a concept, but rather indicates that there is an object that corresponds to the concept, and "the object, as it actually exists, is not analytically contained in my concept, but is added to my concept". For objects of the senses, to say that something exists means not that it has an additional property that is part of its concept but rather that it is to be found outside of thought and that we have an empirical perception of it in space and time. A really existing thing does not have any properties that could be predicated of it that differentiate it from the concept of that thing. What differentiates it is that we actually experience it: for example, it has shape, a specifiable location, and duration. To give an example of Kant's point: the reason we say that horses exist and unicorns do not is not that the concept of horse has the property of existence and the concept of unicorn does not, or that the concept of horse has more of that property than the concept of unicorn. There is no difference between the two concepts in this regard. And there is no difference between the concept of a horse and the concept of a really existing horse: the concepts are identical. The reason we say that horses exist is simply that we have spatio-temporal experience of them: there are objects corresponding to the concept. So any demonstration of the existence of anything, including God, that relies on predicating a property (in this case existence) of that thing is fallacious. (wikipedia)

I would say that generally, the burden of proof falls on whomever is making a claim, regardless of the positive or negative nature of that claim. It's fairly easy to imagine how any positive claim could be rephrased so as to be a negative one, and it's difficult to imagine that this should reasonably remove the asserter's burden of proof.

Now, the problem lies in the fact that it's often thought to be extremely difficult, if not actually impossible, to prove a negative. It's easy to imagine (in theory) how one would go about proving a positive statement, but things become much more difficult when your task is to prove the absence of something.

But many philosophers and logicians actually disagree with the catchphrase "you can't prove a negative". http://departments.bloomu.edu/philosophy/pages/content/hales/articlepdf/proveanegative.pdf Steven Hales argues that this is merely a principle of "folk logic", and that a fundamental law of logic, the law of non-contradiction, makes it relatively straightforward to prove a negative.

In practice, I think the truth lies somewhere in the middle. Hales seems to be making the argument that it's possible to assemble a formal logical proof of a negative statement. He doesn't guarantee the possibility of conclusively proving all of the premises of such argument. That is all well and good, but the average person rarely finds formal logic proofs very persuasive. The real problem is that negative claims often make assertions about things that we are in practice either unable to observe altogether, or that are difficult to observe in finite time.

Consider, for example, I make the claim that "there is no intelligent life on other planets". Certainly it seems intuitive that I possess the burden of proof for such a statement. But as discussed, it would be extremely difficult, if not impossible, for me to actually provide a compelling proof of this claim, because it's impossible to conclusively examine the entire contents of this and every other universe, looking for intelligent life (even putting aside such technical issues as what barometer we use to measure "intelligence", or even "life"). 

Certainly, following Hales's example, I could make the following "logical" argument:


  Premise A: If intelligent life were to exist elsewhere in our, or any other, universe, we would be able to make contact with it.  
  
  Premise B: We have been unable to make contact with any intelligent life in our, or any other, universe.
  
  Conclusion: Therefore, intelligent life does not exist in our, or any other, universe.


But I guarantee that anyone reading that argument is immediately going to object to the first premise. Some would probably even quibble over the second. In a strictly logical sense, my argument is sound: if the premises hold, then the conclusion follows. But that doesn't mean it will manage to convince very many people. The reality is that because negatively-phrased statements often make such sweeping claims, it's very easy to conceive of potential counter-examples or poke holes in the premises of those proofs.


But I don't think it's accurate to say that the burden of proof falls only on those who claim non-existence, either. Consider that I were to make the argument that Santa Claus exists. Why should the burden of proof be on you to disprove that argument? Certainly in making a claim, I should possess at least a minimal burden of proof to substantiate that claim, right?

So my general rule, and one widely followed in philosophical debates, is that the person who is making a claim always holds the initial burden of proof. Once that claim is made and the burden of proof is overcome, the burden of proof falls to any challengers of that argument, because what is a challenge to an argument but a claim to the contrary? 

The way I see it, it's logically disingenuous to allow people to get away with making any type of argument without providing some sort of proof for that claim. For what it's worth, I've never heard the premise that you lead with in your question, and it strikes me as downright specious. A person who claims that "God exists" should have just as much burden of proving that assertion as a person who claims that "God does not exist". Why should I be free to spout nonsense just because I rephrase it as a negative?
Some claims of existence are mathematical: is a given set of properties consistent? is there a number/object which satisfies a given set of constraints? Whether you set out to prove the positive or the negative, the burden is on the claimant, there's no need to worry about whether it is positive or negative existence or non-existence. There may still be an issue of difficulty (or as your example shows, issues of constructibility and reverse mathematical -logical- axioms (like "p or not p") are allowed).

Other claims are scientific: is there a an instance in the 'real' world? Here the properties are not inconsistent, but not necessary either. Is there a unicorn dancing on my head? (evidence shows not). Is there an atom of atomic number 120? (theoretically its possible, but we can't scan the entire universe, and our current technology only gets us so far). 

So for your primes example, existence or non-existence, it doesn't matter (any quantification can be converted from existential to universal or back again with a couple extra negations).

For your 'murderer' vs 'not natural cause' example, you're still playing with the properties of the concepts, which is...mathematical.
With help from the http://de.wikipedia.org/wiki/Wirklichkeit German Wikipedia on Wirklichkeit, one can roughly put it this way: 

Wirklichkeit is a philosophical modality of the being; that of which is the actual case. Realität is used instead of Wirklichkeit in the ordinary German language.

In contexts that seek to distinguish, the concept "Wirklichkeit" is reality, limited to things that have an effect or exercise an effect.
The German word Wirklichheit was an translation by Meister Eckhart of the Latin actualitas (related to Aristotle's concept of energeia and ergon). 

Actuality and reality could perhaps be used.

Disclaimer: I'm far from good at German, and used Google Translate as an aid. Also, it probably depends on which philosopher you're reading.
No, it is not a flawless argument even if you accept the premise. It simply does not follow that, from the premise that existence is a part of essence, the most complete essence must exist; it is a non sequitur. 

Consider Anselm's argument: 


  Premise: God is that than which nothing greater can be thought.
  
  Argument: The greatest thing must exist, or it is not the greatest thing.
  
  Conclusion: Hence god exists.


Compare this to the following argument:


  Premise: The perfect number is the number who has no greater number.
  
  Argument: The perfect number must exist, or it is not the perfect number.
  
  Conclusion: Hence a greatest number exists.


Here it becomes obvious what the error in the argument is: even if we accept that the existence of perfection is necessary for it to be perfect, perfection can still just as well simply not exist. 

So if existence is a part of essence, that does not mean that the most complete essence exists. It only says that, if it doesn't exist, it is not the most complete essence. So all you say is that, if the most complete essence doesn't exist, then it doesn't exist, but if it exists, then it exists.  Well, we can agree on that. :-)
That all depends what you mean by the number 1.

You see, mathematics is- associated by whatever means, and in whatever terms, the reader's philosophy dictates- a linguistic construction that reflects our intentional phenomena in regarding things.  

But as such, just as one is duty bound in thought, regarding a single thing, only to have a phenomenal intention that incorporates that of 'a single thing', one must be duty bound in one's mathematics only to incorporate the mathematics of 'a single thing'. And the mathematics of single things, unconstrained in sub- and superstructure, is rich indeed- perhaps the whole of mathematics.

Is it still '1' if it is one hour on an analogue watch, where twelve is indistinguishable from nothing? Is it still '1' if it is each 6 hour leap of the same- where 1 and then 1 is indistinguishable from nothing? If it is one kahler manifold? The mereological 'Top' object? The number two? The category with one element? If it is simply 'a small amount', and more will make 'much'*? 

If you answered 'yes' to all of the above, then perhaps. But the extent to which the existence of a thing 'proves' the existence of the number 1 (in any of its wide variety of senses) depends upon (can be 'proved by') a fact about the world depends in turn on the (as yet unresolved!**) question of how to conceive of the relationship of one's intentional states to the world. 

* This is a reference to the Piraha tribe, see link posted in comments

** And I would not wait around for a consensus
You seem to be playing with a variant of http://en.wikipedia.org/wiki/Russell%27s_paradox Russell's paradox.

If this sort of thing interests you, there are plenty of examples to be found-- second order logic is full of these types of problems, and attempts and formalizing them.

Personally, I don't find these paradoxes to be very interesting, unless they lead us to something more profound.
One way to evaluate a new different logic is to see what new things you can prove in it and what old things you can't prove; and also whether you want these new changes or not. That is, start with the restriction (e.g. relevance logic only allows proofs where propositions have antecedents that appear in the consequents (i.e. hypotheses have relevance to inferences)) and then see if you get something you don't like (like p -> (q -> p)).

Another way to decide is how -long- your proofs are; removing rules of inference or restricting their action can make some things not provable, but sometimes it leaves them provable but just with much longer proofs (see for example http://en.wikipedia.org/wiki/Cut-elimination_theorem cut-elimination)
Yes, your last statement is still a valid statement, and still an axiom, or at least can get considered an axiom for some formal system, since you can always join any theorem say in a natural deduction context as an axiom if you wish.  There exists no question, that your last statement comes as a "valid" statement or "tautology" in classical logic, and consequently for the complete system of classical propositional logic it will also come as a theorem.  Why?  Basically because of truth-functionality.  You've basically applied "the rule of (uniform) substitution" which can get proven as a metatheorem of classical and, I think, any truth-functional logic.

Since (p->(q->p)) comes as valid, it holds for all truth values in {T, F} or equivalently {1, 0}.  The material conditional "->" comes as a truth function.  This means that for any ordered pair of inputs (x, y), x, y belonging to {1, 0}, the material conditional "->" assigns a unique member of {1, 0}. "¬" comes as a unary truth-function also taking any member of {1, 0} uniquely to a member of {1, 0}.  So, given any values of A, and B in {1, 0}, (B → ¬A) evaluates to single value in {1, 0}.  If you continue on like this, you can show that as long as "A" and "B" get assigned truth values consistently, that the entire formula, and every subformula of it (once properly parenthesized with parentheses around the entire formula in this case) will have some unique truth value.  Now, since ((p->(q->p)) always comes as valid for all truth values in {1, 0}, and the subformulas of (A → (B → ¬A)) → ((B → (B → A)) → (A → (B → ¬A))) only take on truth values in {1, 0}, we have the larger formula as always taking on truth value of 1, so it also comes as valid.

You might also want to see S. C. Kleene's Mathematical Logic and The Schaum's Outline of Boolean Algebra for proofs of the rule of (uniform) substitution in general.
It depends on how you are using the term 0. In your question you used it as both a http://www.philosophyofreligion.info/theistic-proofs/the-ontological-argument/st-anselms-ontological-argument/existence-is-not-a-predicate/ predicate (a property of an object) and as a existential "flag" (non-predicate).

In other words, you could use to it refer to the existence of something, i.e. with 0 essentially representing the existence or non-existence of something:

1 apple = an existing apple  
0 apple = a non-existing apple


Or, you could also (in a way) use it as a descriptive characteristic, like a property; in this regard it might be seen as a property all things share. I.E. All existing things share in their non-nothingness; all non-existing things are equal in their nothingness. However in many philosophical contexts, existence is not seen as a property of an object; it is simply viewed a relation between objects (the subject and the object). In other words, it doesn't really say anything about an object's features/characteristics to say that it exists or not.

The overall point here is that you should be careful to avoid linguistic traps with the concept of 0.  

E.G.


  
  The Devil is greater than nothing.
  Nothing is greater than God.
  Therefore, the Devil is greater than God.
  


See the brief but useful section on http://en.wikipedia.org/wiki/Nothing#Language_and_logic Language and Logic on Wikipedia about this. You might also be interested in philosophical discussions of nothingness in the http://plato.stanford.edu/entries/nothingness/ article from SEP.
In technical logic, a predicate is an entire statement. In your usage, with respect to grammar, the predicate is the verb and object (or other parts) which apply to the subject; the predicate is usually some relation about the subject. I think the latter concept of 'predicate' is what you are referring to.

In your statements, the subject is 'I' and the predicate is 'am me' or ' am my father's son'. The predicate relation, in both instances, is an equivalence relation ('is', equals'). The object of that particular predicate is proposed as equivalent to the subject. As such a proposition is then referred to as an equivalence.

The first one, because the pronouns refer to the same thing, is a tautology, because  x=x is already an axiom of equivalence relations. The second is not, because you could be female.
I think the answer are largely historical. However, there are some nice mathematical properties about the system in question, which suggest they are intuitive:


[K] p → q → p
[S] (p → q → r) → (p → q) → p → r  
[contraposition] (~p → ~q) → p → q


Modus Ponens: p → q , p / q

A related (complete) system replaces contraposition with double negation: ((p → ⊥) → ⊥) → p . This is due to Alonzo Church, Alan Turing's PhD supervisor and father of the λ-calculus.  As far as I can tell, this system is also rather popular.  On the other hand, it can be argued that its less intuitive, as I write below.

Axioms 1 and 2 are quite old: they appear in Frege's Begriffsschrift (1879), §14.  Forms of double negation appear in §17 and §18; Frege does not mention contraposition in his text.

Outside of philosophical logic, axioms K and S play an important rôle in combinatory logic, as part of the http://en.wikipedia.org/wiki/Curry%E2%80%93Howard_correspondence Curry-Howard Correspondence.  This connects a fragment of propositional logic to computer programs.  To demonstrate, here's how you'd write S and K in the programming language Haskell, along with their type declarations:

k :: p -> q -> p
k x y = x    

s :: (p -> q -> r) -> (p -> q) -> p -> r
s f g x = (f x) (g x)


Under the Curry-Howard reading of S and K, modus ponens corresponds to function application.

One can express all of the http://en.wikipedia.org/wiki/Simply_typed_lambda_calculus simply typed λ-calculus, using a very old embedding algorithm due to Schoenfinkle from his On The Building Blocks of Mathematical Logic (1924). Schoenfinkle appears to have developed S and K independent of Frege, which indicates they have some mathematical naturality.

Using the combinatory logic and the Schoenfinkle embedding of the simply typed λ-calculus, one can quickly prove theorems by constructing little programs to specification.  A classic example is s k k :: p -> p; the program s k k is the identify function given my above definition. It can also be thought of as a proof of "p → p" via the Curry-Howard correspondence.

S, K, and modus ponens are also complete for the implicational fragment of intuitionistic logic.

Finally, axioms regarding negation are natural to mathematicians and have a long traditional in philosophy. For one thing, contraposition dates back Aristotle's Organon; virtually no philosopher disputed its validity until intuitionists in the 20th century.

Double Negation is not as intuitive as contraposition - this might be why its not as popular.  In rhetoric, litotes makes use of double negatives to change the meaning of sentences in order to make understatements. It also has a rocky history.  It was embraced as early in the 3rd century by the Stoic Alexander of Aphrodisias, however by the middle ages logicians argued over its validity. In particular, Peter of Abelard in the 12th century rejected double negation.  The subsequent rise of consequentialist interpretations of logic would ultimately settle this dispute.  By the time of the 17th century, Arnauld and Nicole's Port Royale Logic (1662) presents double negation elimination in Chapter III, Rule III.  Double negation elimination was later embraced by almost every western philosopher after the 17th century, including Leibniz, Kant, and in Boole's Laws of Thought (1854).

  It seems to me that truth is merely the absence of all lies, and as such can be considered a privative. Is this the case?  


An absence of lies may be sufficient for truth, but does not necessarily mean that a rational assessment of "non-lying" statements will be true. The question is a bit like comparing horses and buggies.  

Truth is a condition of propositions (statements, sentences, etc.) which is satisfied when utterance corresponds to (matches, fits...) what is the case. The absence of truth can be falsehood, or the absence of truth can be a statement which is not rationally assessed a truth value, such as an opinion or an order.  

A lie is an intentionally false or intentionally misleading statement. The conditions of satisfaction requisite for a lie to be identified are different (intentionalistic, agentive). Now if someone intentionally misleads with true statements, the absence of the lie is not truth.  

Imagine you work in an office with no windows. The weather forecast called for possible rain and you are wondering if you should bring your umbrella with you when you go to lunch. A coworker who has a corner office with windows happens by your office door and you ask them to tell you what the weather is like outside. Unbeknownst to you your coworker suffers a delusional mental disorder such that when in fact the sun is shining, they think it is raining. Furthermore, your coworker decides to play a trick on you and tell you that the weather is other than it appears to them. When they return to your office and tell you the truth that it is sunny, did they lie?  
I see two parts in your question. Firstly, what did Wittgenstein think about rule-following and what is his critique meant to teach us? Secondly, how can we, in the light of this paradox, ever know we are following a rule. I take them in turn - my answer to the second question will be informed by the Wittgensteinian analysis that grows out of the first.

There is something one always needs to keep in mind when reading the Investigations, which is that Wittgenstein's principal aim is to criticize philosophical parlance, not philosophical arguments per se. What Wittgenstein is doing here is not, as Kripke would have it, presenting us with a skeptical paradox, namely that there is no fact of the matter that makes it true that at any given instance I am following a particular rule. What he is criticizing instead is the philosophical temptation that leads us into thinking that the concept of a rule can only be approached through the concept of "interpreting a rule." 

It is, according to Wittgenstein, a paradigmatically philosophical conceit to put distance between a rule (viewed in abstraction) and the understanding of a rule. Only in philosophy are we tempted to talk like this: in reality (i.e. in language games that arise naturally in our form of life) rules are followed in the same way that commands are carried out, viz. as a reaction to prompts and ostensive definitions carried out in a shared form of life (what in our case we may call a shared 'linguistic environment'.) This is what Wittgenstein means when he says that by "interpretation" we should understand simply alternative ways of expressing (as opposed to understanding) the same rule. 

The fundamental grammatical mistake (there is always one of those in the later Wittgenstein) therefore is that of adopting too wide a scope for the term "interpretation" when we are doing philosophy. It is this that leads us into temptation and it is this that we must get rid of if we are to clear the 'misunderstanding' and realize that there really is no problem with rule-following at all: the fly, as it were, was never in the bottle to begin with.

So, to get to your second question: "How do we know we are following a rule?" This is a question that ONLY becomes meaningful if you accept there is a separation between grasping a rule and the rule itself. Think of the situations where it becomes useful to exclaim: "I know I am following a rule!" Perhaps in a maths class, an innumerate classmate challenges you to prove to him that the scribbles in your notebook are not arbitrary doodles. When he remains unconvinced, the epistemic exclamation: "I know I am following a mathematical rule!"  is meant to convey to him a certain regularity in your behaviour, perhaps in an attempt to convince him not to dismiss mathematics altogether. Here the exclamations make sense. 

What is crucial is that we must not let ourselves take this proclamation literally. There is no fact by virtue of which I can know that I am following a rule. To say that I do is just a manner of speaking. There is a feeling of being 'guided' by the rule, of being led to the next step as though by an invisible hand. That is the feeling we try to convey when we say: 'I have grasped this rule! I now know how to apply it in all future applications!' The mistake is to understand this invisible guidance in terms of an interpretation that somehow, as it is grasped, magically fits all the facts, past present and future. The feeling is no more mysterious than the feeling of guidance one gets when one sees an arrow pointing right. One knows where the arrow is pointing at simply by virtue of being a member of a linguistic community (a form of life) that employs arrows in such a manner.

  I've considered "reductio ad absurdum" but it feels wrong here. 


Actually, it's exactly right here.


  There could be a contradiction, but instead we could simply draw a ridiculous conclusion (in which case the argument is logically sound, but has no supporters).


No: the argument would be http://en.wikipedia.org/wiki/Validity valid, but not http://en.wikipedia.org/wiki/Soundness sound.  And that's the distinction that makes a difference.  A reductio does not need to result in a contradiction; it can also result in an absurd conclusion which is clearly false.  (This is technically a reductio ad falsam, but it is generally considered a subtype of http://www.iep.utm.edu/reductio/ reductio ad absurdam.
Yes and no.

This question yearns for the comparison between two views: Moral  Absolutism and Consequentialism.

http://en.wikipedia.org/wiki/Moral_absolutism Moral Absolutism:


  Moral absolutism is an ethical view that certain actions are absolutely right or wrong, regardless of other contexts such as their consequences or the intentions behind them. Thus stealing, for instance, might be considered to be always immoral, even if done to promote some other good (e.g., stealing food to feed a starving family), and even if it does in the end promote such a good.


http://en.wikipedia.org/wiki/Consequentialism Consequentialism:


  Consequentialism is the class of normative ethical theories holding that the consequences of one's conduct are the ultimate basis for any judgment about the rightness of that conduct. Thus, from a consequentialist standpoint, a morally right act (or omission) is one that will produce a good outcome, or consequence.


These two contrasting views dictate what you may and may not do.

While Moral Absolutism is only with respect to the barebones of one's actions (and cares about nothing else), consequentialism takes into account the final objective/result that one strives to achieve.

Let me give you an analogy:

Imagine you have committed a crime, and thus, you are now on trial. You are given a drink that forces you to only tell the truth when prompted.

Scenario A (Moral Absolutism):

The judge asks you whether you committed murder. You respond by saying, "yes". The judge then immediately sentences you to 25 years in prison. 

The judge does not know (and does not care) that you were abducted, kidnapped, and tortured, and that the only way to escape was to kill your captor while he was distracted, take the keys from his pocket, and unshackle yourself and escape.

Scenario B (Consequentialism):

The judge asks you whether you committed murder. You respond by saying, "yes". The judge further asks  questions to find out why you committed the crime. He finds out what the true reasoning the murder was and thus, merely sends you to a rehabilitation center to prepare you to re-enter society.

Hopefully, these analogies gave you an idea of each view.

Each view has its pros and cons. Of course, scenario A was fairly unjust (no pun/oxymoron intended) because the basis of the deeds was not taken into account. But on the other hand, consequentialism can also be twisted to reflect Machiavelli's "The ends justify the means" and be taken advantage of.

Many questions come on the borderline and are difficult to answer such as killing one innocent person to save a thousand.

The ultimate answer to your question lies with your decision of which view you believe to be the most suitable for the situation.
The symbol

∈

denotes membership in a set.

For example, if we say that


  A ∈ Δ


then that means the constant A is a member of the set Δ.


The opposite would be

∉

which denotes that something is not a member of a set.

For example, if we say that


  X ∉ Δ


then that means the constant X is not a member of the set Δ.


Also see the http://en.wikipedia.org/wiki/Element_%28mathematics%29 Wikipedia article on elements (in mathematics).
There are some philosophical issues with the Higgs mechanism, but they are not the types of things considered in any traditional sense part of philosophy. The issue here is which type of language is appropriate for describing the phenomenon, particle language or field language. The two are very subtly dual, and it is difficult to make a precise particle description out of a field description. But to fully understand the Higgs mechanism, you need both, and it is not clear to what extent the particle picture is available.

In field language, the Higgs mechanism is when a scalar field which is charged has an expectation value in the vacuum, so that the lowest energy state gives a vacuum which is full of charge condensate which is not invariant under gauge transformations. When this happens, the associated photon (the particle carrying the interactions appropriate to this charge) becomes short range. The phenomenon is completely described by field Hamiltonians, and is easy to calculate and straightforward to understand in this classical field language. For electromagnetism, this was understood by Stueckelberg, but it wasn't appreciated fully until it was done for nonabelian gauge fields by Brout/Englert, by Higgs and by Guralnik/Hagen/Kibble in this order in 1964, following Nambu's ideas about fields in a vacuum having expectation values, something which was suggested earlier by Heisenberg, who was the first to understand and formulate the notion of spontaneous symmetry breaking, something which emerged from Heiseberg's work in the early days of quantum mechanics with Neville Mott regarding a more traditional philosophical problem of how measurement works.

The philosophical issues however come from the fact that the Higgs mechanism was first discovered using a different language, where it is not natural or straightforward, in the language of particles, not quantum fields. The Higgs mechanism was first discovered by (arguably) Landau, Bardeen/Cooper/Schreifer, or by Anderson, all of whom were thinking about coherent condensates of charged particles like electron-pairs inside a metal. When you have a coherent superposition state of many nonrelativistic particles which are charged, the end result is that the nonrelativistic quantum field describing these particles has a vacuum expectation value, so that there is a Higgs mechanism, and the electromagnetic photon gets a mass. This is the Meissner effect of superconductivity, the fact that photons can't penetrate a superconductor to a given length.

All the philosophical issues come from the question of particle-field duality. To what extent is it appropriate to describe the quantum field in particle langauge?

Particle/Field duality

When you have a nonrelativistic quantum field, there is no philosophical issue. The quantum particle language and the quantum field language are entirely mathematically equivalent, and a many-identical-particle wavefunction evolving according to the many-particle Schrodinger equation with additional interactions can be equally well described by a quantum field evolving according to a nonlinear Schrodinger equation in space. This duality is described on Wikipedia under "Schrodinger field".

But when you have relativity, the particle and field pictures are only dual if the particles are allowed to propagate back in time. The scale at which this happens is the Compton wavelegth, which, even for the electron, is very small compared to atoms. So the relativistic complication doesn't appear in condensed matter systems, and the nonrelativistic theory, where the duality is mathematically fully worked out, is applicable.

But in the relativistic context, the field picture is well defined, but the particle picture is only defined in a Feynman expansion through perturbation theory, and it isn't fully clear what the description is outside of the perturbation series. You can't give a wavefunction for the particles at one time, because the particles can turn around later and come back to the same time "later". So your description of the wavefunction for the particles must be either on an asymptotic incoming trajectory, where this problem is sidestepped because the effective fields are free, this is the point of view of scattering theory, or alternatively by using additional parameters for the proper times of the intermediate particles and describing the wavefunction as a function of these new unobservable proper times.

The issue with introducing intermediate proper times is that the interactions happen when the particles are at the same position, not at the same proper time. So that if a particle at one proper time hits a particle with another proper time, they can interact. This means that the description doesn't have a one-way time where interactions happen only in the future direction, and this makes the description a mess. The mess is only simplified in a perturbation series, and this doesn't give one confidence that the formalism is completely consistent.

Anyway, putting the well-definedness aside, in particle picture, the Higgs mechanism is a condensation of relativistic charged particles in the vacuum which make a charged condensate, a superconductor, just like an ordinary material superconductor. The superconductivity picture of the Higgs mechanism is hard to make precise, because one usually defines the particles relative to a given vacuum. For the superconductor, the particles are defined relative to a symmetric uncharged vacuum that does not exist. To the extent that relativistic particle field duality makes sense (and it should make perfect sense), the Higgs mechanism is a condensation of the charged space-time localizable particles corresponding to the quantum field with an expectation value.

So the philosophical issue that I see is to what extent the particle field duality is true, that is to what extent is the quantum field theory also a theory of point particles interacting at points. The resolution to this philosophical question will come with a better formalism that gives a consistent particle description of field theory, which allows you to fully make sense of the wavefunction of a photon, or the wavefunction of an up quark, two particles that are never nonrelativistic. Such a formalism is only half-way developed always.

Non-positivist questions

In addition to this question, which is investigated by physicists, there are other questions which physicists do not consider real questions, because they aren't logically positivistically well defined.


Ontology: Is quantum field theory about particles or fields?


This question asks whether the objects in the theory are "really" particles making a superconductor, or "really" fields making a charged condensate. This question is nonsense to a logical positivist, so I won't consider it further. All questions of ontology are generally meaningless to a positivist, and in my opinion, the subject is closed forever. You can take any ontology you like that is consistent with experience, and the result is just a change in point of view, a change in philosophical gauge, and to debate which gauge is real is no more meaningful than debating which gauge is real in electromagnetism, or which coordinate system is real in relativity. The answer is clearly "none of them".


Epistemology: How can we be sure that there is really a charged condensate, and not just terms in a Lagrangian fooling us into thinking that there is stuff in the vacuum?


This has to do with the fact that we don't see the condensate directly, rather we see the oscillations of the condensate, which have their own interactions which can be defined without direct reference to the condensate. This is again positivistically meaningless, since the two ideas of a conspiratorial Lagrangian and an actual condensate can't be distinguished by experience, since they are mathematically identical.

The fact that philosophers don't accept logical positivism anymore makes it so that it is generally impossible for physicists and philosophers to communicate anymore. The philosophers ask all sorts of questions that make no sense as questions, and do not accept that these questions make no sense. This is why physicists always do their own philosophy, and this will not change until philosphers accept positivism as a principle for sorting out the meaningless from the meaningful questions.
Primitive notions are undefined concepts within a theory. For example, a theory of meaning may have causation as a primitive notion. A theory of causation, obviously, will not.

First principles are propositions that are not argued for in a theory -- maybe because they are considered self-evident. For example that we know our Ideas and Impressions, in Hume's philosophy; that (Descartes) thinks, in Descartes's philosophy.

So, a primitive notion is expressed by a noun phrase: "causation", "the good". First principles are expressed by sentences: "I think", "We know our Ideas and Impressions".
It seems some variance of the http://en.wikipedia.org/wiki/Circular_reasoning Circular Reasoning.
A -> B is just implication:

A B A -> B
t t   t
t f   f
f t   t
f f   t


A <- B is the same as B -> A, so the above truth table holds, but with different variable names.
Are you attempting to read the Critique of Pure Reason on your own, outside of a philosophy class?

If so, I'd strongly suggest against doing so, unless you already have a very good background in philosophy.  Your question about "necessity" would tend to indicate that this is not the case.

In any event-- the answer to your question is in the sentence preceding the one you quoted: the notion "that it could not possibly exist otherwise."
OK, I think I've got it now:


A certain logic validates the Law of the Excluded Middle (LEM) if the following is a theorem in the logic: p v not p
A certain logic abides by the Principle of Bivalence (PB) if every well formed expression according to the logic has exactly one truth value: true or false


Some semantics may make it the case that LEM is true and PB is not true. Consider the following http://en.wikipedia.org/wiki/Supervaluationism supervaluationist treatment of vague predicates. A statement such as


  Schiphol is bald


will be supertrue (superfalse) iff under all (no) acceptable precisifications of the predicate "bald", the sentence comes out true. A precisification will have the form "... has n hairs", where, e.g., n = 0 is acceptable, but n = 10^6 is not. Sadly, the sentence above is supertrue -- which is the supervaluationist criterion for accepting it as true.

Luckier folk, such as, say, Andy, might come out bald according to some precisifications and not bald according to others. Thus,


  Andy is bald


is neither supertrue nor superfalse: it lacks truth value, according to supervaluationism. PB, therefore, is false: that sentence is neither true, nor false. Now, what happens with a sentence of the form [p v not p], such as


  Andy is bald or Andy is not bald


Well, such sentences will be true for all precisifications, because either Andy has n hairs or he doesn't, for all n. Therefore, the sentence comes out supertrue -- this is the supervaluationist for accepting it as true. Its negation ("it's not the case that Andy is bald or Andy is not bald"), by the same token, comes out superfalse.

The same will happen with every other vague sentence: the supervaluationist semantics validates LEM. Supervaluationism is a semantics that validates LEM but not PB.
First thing, the title of your question seems to imply that your difficulty lies in translating the formal (bi-)conditional into plain English, however the body seems to imply that the problem is in translating plain English into formal logic.

Translating formal logic into plain English shouldn't be difficult and many sources will give one a list of accepted readings. Translating in the other direction (plain English to formal language) is a different story.

The biggest problem with translating any natural language as it is used in day to day life into any formal language is that natural languages tend to be ambiguous without context (this is in part why machine translation of natural language is so difficult).

Formal languages try to strip away as much context as possible in order to eliminate the ambiguity of natural language. Another example of this is how the English words, ‘and’, ‘but’, and ‘yet’ all typically get translates into the conjunction, as the attitudes or beliefs of the speaker carried with these words is unnecessary cruft as far as formal logic is concerned. 

Another difficulty is that while formal logics usually only employ a single conditional construction with a clear and distinct meaning, English has several conditional constructions (counter factual conditional for instance) which, again, require context and carry with then implicit information.

So the long and short of it is that if you are looking for an easy/mechanical method for translating plain English into formal language then you're out of luck. Such translations require that you fully understand what you are translating and being able to fill in the gaps of what is not be said.

Quine's “Methods of Logic” [http://books.google.com/books?id=liHivlUYWcUC&pg=PA53&lpg=PA53&dq=Quine+%22Methods+of+Logic%22+%22Words+into+Symbols%22&source=bl&ots=JJIlBxa8AU&sig=4nM0-3KlpIbQBiLoDSQ_B_KrYjU&hl=en&sa=X&ei=S8sSUNyGGZOm8gS97oD4Aw&ved=0CFAQ6AEwAg 1] has a chapter devoted to the subject of translating plain English to formal logic which may help hone one's skill in this respect.
To understand what the permissive law is we have to examine the origination of the civil society as Kant sees it. We will then see that the permissive law isn't ad hoc at all but an important part of Kant's whole philosophy of law. As I did not read enough to give a systematic summary of that yet, I will leave this part to others and focus on the steps required for the argumentation.


  We ordinarily assume that no one may act inimically toward another except when he has been actively injured by the other. This is quite correct if both are under civil law, for, by entering into such a state, they afford each other the requisite security through the sovereign which has power over both. Man (or the people) in the state of nature deprives me of this security and injures me, if he is near me, by this mere status of his, even though he does not injure me actively (facto); he does so by the lawlessness of his condition (statu iniusto) which constantly threatens me. Therefore, I can compel him either to enter with me into a state of civil law or to remove himself from my neighborhood. The postulate which is basic to all the following articles is: All men who can reciprocally influence each other must stand under some civil constitution. (Kant, Perpetual Peace, Footnote 3, my emphases)


The compulsion of the others to enter into a state of civil law can be violent. What we see is this: There is a prohibitive rule to use violence in general, but there is a permissive law that allows violence in certain circumstances, namely for the erection of the state, a legal constitution. It is similar for states which, between themselves, act as individuals. As long as there is no international constitution, individual states are "in the state of nature". To overcome this state and to constitutionalise their interactions, certain otherwise prohibited acts become temporarily permitted. Every thought in this book serves one purpose: the creation of a legal condition between states. It is Kant's philosophy of law on a higher level all over again. This is also why we don't find the permissive law in ethics. 
Syllogistic logic is one of the things that one does with quantificational logic, just as one occasionally works with arithmetic of integers when working in real analysis or group theory (both of which generalize integer arithmetic). We do not take special note of the structure of syllogisms, because they are only a special case of the usual rules of inference; because we have more flexible tools, we do not put as much effort into memorizing what syllogistic formulae are valid or invalid, except as examples of valid or invalid reasoning in general.

In the following, I will let x:A denote that an object x takes a property A.

All A are B:  ∀x: (x:A) ⇒ (x:B)

Some A are B: ∃x: (x:A) & (x:B);

Some A are not B: ∃x: (x:A) & ¬(x:B);

No A is B: ∀x: (x:A) ⇒ ¬(x:B).

Syllogistic reasoning can be performed as it was traditionally, albeit with extra instantiation and generalization steps, and often using rules of inference such as modus ponens and modus tollens. Here are two typical examples of syllogistic reasoning, where the first two lines are classic syllogistic premisses, and the final line is the classic syllogistic conclusion.

Example 1.


  P1. ∀x: (x:Human) ⇒ (x:Mortal)
  Premise — gloss: all humans are mortal 
  P2. Socrates:Human 
  Premise — gloss: Socrates is human 
  UI. (Socrates:Human) ⇒ (Socrates:Mortal)
  universal instantiation — gloss: if Socrates in particular is human, then he is mortal
  MP. Socrates:Mortal 
  modus ponens — gloss: therefore, Socrates is mortal


Example 2.


  P1. ∀x: (x:Rabbit) ⇒ (x:Furry)
  Premise — gloss: all rabbits have fur 
  P2. ∃x: (x:Rabbit) & (x:Pet)
  Premise — gloss: some rabbits are pets 
  EI. (Flopsy:Rabbit) & (Flopsy:Pet)
  existential instantiation — gloss: consider Flopsy, a (generic) pet rabbit
  UI. (Flopsy:Rabbit) ⇒ (Flopsy:Furry)
  universal instantiation — gloss: if Flopsy in particular is a rabbit, then Flopsy is furry
  MP. Flopsy:Furry
  modus ponens (with a conjunctive simplification) — gloss: therefore, Flopsy is furry
  CI. (Flopsy:Pet) & (Flopsy:Furry)
  conjunctive introduction (with a conjunctive simplification) — gloss: therefore, Flopsy is a furry pet
  EG. ∃x: (x:Pet) & (x:Furry)
  existential generalization — therefore, some pets have fur 


Syllogistic reasoning is therefore a special case of quantificational logic; it is a framework in which one can confidently reason syllogistically, without being limited only to syllogisms. But one should not feel absolutely restricted to using quantificational logic, or any particular logic; it suffices to use some logic which is both effective and reliable for coming to usable conclusions.
The various truth assignments don't modify the proposition "If there is God, then there's a human"; they're assignments to whether or not there are gods or humans, and the truth value of A ⊃ B represents whether or not the hypothetical world being described — with or without gods, and with or without humans — is consistent with the statement that if there's a god, then there's a human.

It might help you to rephrase this as "There is God only if there is a human", which is equivalent, and can be easily understood as a constraint on the conditions in which God can exist. 


A=0, B=0 ⇒ there's no God and no human; as there's no God, the constraints on its existence is not violated, so A ⊃ B = 1.
A=0, B=1 ⇒ there's no God, but there are humans; similarly to the above, the constraint on God-existence is not violated, so A ⊃ B = 1.
A=1, B=0 ⇒ there's God without humans; this violates the constraint, so  A ⊃ B = 0.
A=1, B=1 ⇒ there's God, but there are also humans; the necessary condition for God's existence is met, so  A ⊃ B = 1.


In the first two cases, A ⊃ B is often described as vacuously true, as the premise of God existing is false in those cases — equivalently, the condition which is restricted by the consequences fails to hold anyway, so the constraint is satisfied by that very failure.
You already have the truth-table corresponding to the conjunction connective, so it should be transparent for you to recognize the answer to your question from the syntactic point of view of the calculus.

Asserting ¬(P.Q) [Premise 1] is logically equivalent to assigning a binary truth-value of zero to (P.Q). So, looking at the table, you see there are three possible assignments to the truth-values of P and Q that yield that result. Then, by asserting ¬(Q) [Premise 2] and using the same argument, you assign a truth-value of zero to Q; looking again at the truth table, this restricts the possibilities to two cases (namely, one in which P's truth-value is zero, and another in which it is one). So, the assertions do not imply a definite binary value for P's truth state.

From the semantical point of view, the interpretation of the conjunction connective in this case follows (informally) this way. You know that either P is false while Q is true, or Q is false while P is true, or  P and Q are both false; and you also know that Q is false. This last statements eliminates the first possibility (Q being true while P was false), and you are left with the other two; so, what you seem to think to be justified (the second possibility, that Q being false then P must be true) excludes the third situation when both P and Q are false, and so is fallacious.
Also read sometimes as: p implies q.  All this means is that p being true defines a value for q.  When p is false, q can be anything it likes.  

The "True" (not undefined or not known) in the truth table means that these values do not contradict the rules, which is true in this case.  It is more that the RULE is (can be) true when p and q have these values, than saying that the rule proves these values, which is how you seem to be trying to read it.
If-Then is not symmetrical.  Consider the following cases:


If it is raining, then I am wet.
If I am wet, then it is raining.


We can find a counterexample for each that is not a counterexample of the other.


Suppose it is raining, but because I am inside, I am not getting wet.  What this demonstrates is that it is not the case that if it is raining then I am wet.  That's because we've identified a case where it is raining (the left hand side of the implication is true) but where I am not wet (the right hand side of the implication is false).  But we haven't thereby shown that it's not the case that if I am wet, then it is raining.  That's because I'm not wet, so our current example doesn't tell us anything about what happens when I am.
Suppose I'm wet, because I've just fallen into a pool, but it's a sunny day out.  This demonstrates the converse - that it is not the case that if I am wet then it is raining - since we've found a case where I am wet (the left hand side of the implication is true) but it's not raining (the right hand side of the implication is false).  This, on the other hand, doesn't show that if it is raining then I am wet, because it's not raining, so we aren't usefully informed about what the consequences are of it raining.


You might take away as a lesson from this that if you are interested in chasing up the consequences of a particular proposition or propositional fragment, and seeing whether a particular conclusion follows from that proposition, you should put the proposition on the left hand side.

Can we do better, and demonstrate a counterexample to the symmetricality?  Sure.  How about:


If you're a Texan, then you're an American.
If you're an American, then you're a Texan.


is easily true, because Texas is in America, and therefore every situation in which someone is a Texan is also a situation in which they're an American.  But 2. is clearly false, because California is in America, and because there are Californians that are not Texans.  There is a situation in which we have an American (a Californian) that is not a Texan.  So 2. fails, even where 1. succeeds, and hence we have an example of the Asymmetricality of If-Then.  



That doesn't mean it's Anti-symmetric - we do have some cases where the two implications match up.  Consider:


If you're an unmarried man, then you're a bachelor.
If you're a bachelor, then you're an unmarried man.


These implications both go through.  In Philosophy, rather than talking about two different propositions here, we sometimes use a shorthand form, which might be what's confusing you.  This shorthand form is written as follows:


You are an unmarried man if and only if (or sometimes iff) you are a bachelor

How to think about "P ⊃ Q" in plain English

In propositional logic, P ⊃ Q is what is called a material implication. It doesn't mean that P and Q mean the same thing (they might not have the same truth value); all that it is, is a claim that if P is true, then Q is also true — without making any more claims than this.

An alternative way of considering P ⊃ Q is as a "constraint" that someone claims holds for the state of affairs. That constraint is either satisfied (in which case P ⊃ Q is true) or it is violated (in which case P ⊃ Q is false). This picture is somewhat different from the usual way that we think of "if-then" statements, which is more like causation than constraint. For this reason, it might be helpful to describe P ⊃ Q as either 


"P only if Q" (if P holds, then Q had better hold for the constraint to be satisfied); or
"either ¬P or Q" (if ¬P is false, then Q has to be true if the constraint is to be satisfied).


In this respect, we can think of "satisfying the constraint P ⊃ Q" as being something like a theory about how the world is. If it is true, then it is never violated; but if you can find one counterexample, then it must be false.

Necessary conditions and sufficient conditions

In the case that P ⊃ Q actually does hold, the two different phrasings "if P then Q" and "P only if Q" allow us to easily describe the relationship between P and Q in terms of either necessary conditions or sufficient conditions.


"If P then Q" intuitively means that P is enough to ensure that Q holds. In this respect, P is called a sufficient condition for Q.
Conversely, "P only if Q" intuitively means that Q is a precondition of P holding true; even though P implies Q, P also cannot hold without Q holding. In this respect, Q is called a necessary condition for P.


The notions of a "sufficient condition" and a "necessary condition" are complementary to one another; if one condition is sufficient for another, then that second condition is necessary for the first condition. 

The truth-table for P ⊃ Q

Thinking about P ⊃ Q in terms of constraints allows us to explain the truth-table for conditionals:

 P  Q  |  P ⊃ Q
-------|--------
 F  F  |    T 
 F  T  |    T
 T  F  |    F
 T  T  |    T


We can describe these simply in terms of the conditions in which P is "allowed" to be true.


If P is false — no matter what the value of Q is — then we haven't violated the constraint P ⊃ Q, because it only imposes constraints on when P can be true. For instance, suppose P is "you succeed in climbing Mt. Everest", and Q is "you try to climb Mt. Everest". Because climbing Mt. Everest is difficult enough that we would presume that it has to be a deliberate undertaking, it is reasonable to say that you can only succeed in climbing Mt. Everest if you try: that is, P only if Q. But it is possible to fail to climb Mt. Everest in two different ways:


either by not trying, or
by trying but having problems (weather conditions, a serious accident, etc.) which prevent you from succeeding.


In the case where P is false, we say that P ⊃ Q is vacuously true — it is true only because the constraint that it imposes on P being true is not put to the test.
In the case that P is true, the constraint P ⊃ Q is put to the test, and that constraint is satisfied only if Q is true. If P and Q are both true, then the constraint is satisfied, and so P ⊃ Q is true. However, if P is true and Q is false, then it cannot be that P only if Q; the constraint is violated, so that P ⊃ Q is false.


Is Q ⊃ P equivalent to P ⊃ Q, or does it allow you to infer P ⊃ Q ?

The short answer is that they are not equivalent, and neither one allows you to infer the other. There are two easy ways to see this.


P ⊃ Q is a constraint on when P can be true, while Q ⊃ P is a constraint on when Q can be true. In general, these are not comparable constraints; neither one allows you to infer the other.
We can make reference to the truth-tables for each, using the table we've already computed for P ⊃ Q to find out the values for each row  in Q ⊃ P:

 P  Q  |  P ⊃ Q  |  Q ⊃ P
-------|---------|---------
 F  F  |    T    |    T
 F  T  |    T    |    F
 T  F  |    F    |    T
 T  T  |    T    |    T


We see that when only one of P or Q is true, then one of P ⊃ Q or Q ⊃ P is true — but not the other. Because P ⊃ Q and Q ⊃ P are not both satisfied under all of the same conditions as each other, we see that they are not logically equivalent.


We can see from the above that both P ⊃ Q and  Q ⊃ P are true, if P and Q are either both true or both false — if P and Q are themselves equivalent. This is just the observation that P ≡ Q ("P is equivalent to Q") is logically equivalent to (P ⊃ Q) & (Q ⊃ P). But precisely because P ⊃ Q and Q ⊃ P are not logically equivalent, P ≡ Q is a stronger logical statement — a tougher constraint to satisfy — than either conditional alone.

Note that because P ≡ Q is equivalent to (P ⊃ Q) & (Q ⊃ P), it is possible to summarize P ≡ Q as saying that "P is a necessary and sufficient condition for Q", where the 'necessity' comes from the conditional Q ⊃ P, and the 'sufficiency' comes from P ⊃ Q.

Contrapositives

The conditional statement  P ⊃ Q does have another equivalent conditional form, known as the contrapositive: ¬Q ⊃ ¬P. This is a way of rephrasing the constraints on when P can be true, as a constraint on when Q can be false.


"P only if Q" means that if Q fails, then P had better also fail; that is, "if ¬Q then ¬P". 
Similarly, "either ¬P or Q" means that if Q fails, then ¬P must hold; again, "if ¬Q then ¬P".


We can show that ¬Q ⊃ ¬P also implies P ⊃ Q using a symmetric argument, and double negation (¬¬P ≡ P). And if you compute the truth-table for both formulae, you will find that they have the same values:

 P  Q  | ¬P  ¬Q  |  P ⊃ Q  |  ¬Q ⊃ ¬P
-------|---------|---------|---------
 F  F  |  T   T  |    T    |     T  
 F  T  |  T   F  |    T    |     T
 T  F  |  F   T  |    F    |     F
 T  T  |  F   F  |    T    |     T


So we can see just by calculation that they are logically equivalent propositions, or "constraints".

Does  P ⊃ Q  mean that there's a cause-and-effect relationship?

Something that trips up a lot of people is thinking about logical conditional statements as though they were cause-and-effect relationships. While cause-and-effect is one kind of conditional statement, it is not the only kind; so you should be careful not to assume that a logical "if-then" statement is saying anything about cause and effect.

For instance, you might worry about conditional statements such as


  "If I eat an apple, then I will die"


— which might be true for someone severely allergic to apples, but is also true for absolutely anyone who is mortal. (Though I'm not personally allergic to apples, I do expect that I will die some day. So it is true that I will die whether or not I eat any more apples; so if I eat an apple, I will die.) You might complain that the condition P — if I eat an apple — has little to do with the conclusion Q — I will die. This doesn't seem to get much better if we present it as a constraint of the form "P only if Q" (I will only eat an apple if I will die) — again, what does dying have to do with whether I could eat an apple?
What makes the proposition hold is the formulation "either I won't eat an apple, or I will die" is true. Indeed, in the chain of reasoning I applied to myself, I asserted the proposition Q — I will die — and then reasoned that the logical status of P (my eating an apple) had no bearing on that truth. My arriving at the conclusion "if I eat an apple, then I will die" is in this sense true, but somewhat perverse; it omits the fact that if I don't eat an apple, I will also die then.

We can make similar comments for vacuously true statements, such as 


  "If I were elected the King of France, I would make it rain ice cream every day".


With our current understanding of weather — or at least the cost of ice cream production and jet fuel — it seems unlikely that I could cause ice cream to rain down every day under any circumstances, regardless of whether or not I were the King of France. If my claim seems outlandish to you, it is because you're not evaluating P ⊃ Q (where P = "I am elected the King of France" and Q = "I would make it rain ice cream every day"), but only Q on its own (you can't see any reasonable circumstance in which I could make Q true). Of course, the conditional P ⊃ Q is a weaker assertion than Q: it is possible for Q to be false and P ⊃ Q to be true; it's equivalent to "either I will not be elected the King of France, or I will cause it to rain ice cream from the sky every day" (which now sounds like a surreal ultimatum rather than a promise). Given that France doesn't have any kings at all at present, let alone elected kings, this proposed constraint on the way the world works will likely be satisfied by virtue of the fact that I will not be elected the King of France.

In both cases, the condition had little to do with the consequence; eating apples is unlikely to cause me to die, and becoming King of France is unlikely to give me ice-cream-rain-god powers. But both of the conditional are true, because the constraints that they propose for the nature of the world are satisfied in either case.

When we do want to think of conditionals P ⊃ Q in terms of cause and effect, we usually want to work in models of the world in which it is possible for P to be true, and where it is also possible for Q to be false. The reason why the examples above might seem strange is exactly because in one case Q is unavoidable (I can't avoid dying), and in the other P is unreasonable (I can't be elected the King of France) — at least not with the current governmental systems and levels of medical technology. 
We may consider all possible values of each atomic proposition, and compute the corresponding truth-values of the formulae that arise as a result. Then we may restrict to those rows of the truth-table in which the assumed propositions take the value 'true'. If any atomic proposition obtains a constant value across all of those rows, we may infer that it is correspondingly either 'true' or 'false'.

Taking the example from your question, we may compute:

  A  B  |  A ⊃ B  | ¬A ⊃ B
--------|---------|----------
  F  F  |    T    |    F
  F  T  |    T    |    T       ⇐ satisfies (A ⊃ B) and (¬A ⊃ B)
  T  F  |    F    |    T
  T  T  |    T    |    T       ⇐ satisfies (A ⊃ B) and (¬A ⊃ B)


If we delete all rows which do not satisfy the propositions which are asserted, we obtain

  A  B  |  A ⊃ B  | ¬A ⊃ B
--------|---------|----------
  F  T  |    T    |    T      
  T  T  |    T    |    T       


from which we infer that B is true, although A could still be either true or false.

Edited to add — in reference to your question, "is it just that we assume some values which we can use to prove the argument to be valid or invalid?" It isn't enough to assume some values to prove the argument valid — what we are doing here is showing that all possible values of A and B which satisfy the two imposed constraints (A ⊃ B and ¬A ⊃ B) which are used as hypotheses, validates the inference of B. By the same token, because we see that there are two different values of A are compatible with the imposed constraints, and so no valid inference can be made about A in either direction.
"Some", (∃x), is left-open, right-closed interval - the number of animals is in (0, x] or 0 < n ≤ x

"Not all", ~(x), is right-open, left-closed interval - the number of animals is in [0, x) or 0 ≤ n < x.

"Some" means at least one (can't be 0), "not all" can be 0.

"No", ~(∃x), allows only number 0.
The main difference between  (∃x)(Ax . Bx) and (∃x)(Ax ⊃ Bx) is the committment they make to the existence of As and Bs.  In fact, this is read directly off the truth table you gave.

You can interpret (∃x)(Ax . Bx) as saying that there is something that is both A and B.  So you know that there is at least one A and at least one B.  There is only a single 1 in the truth table, which is the case where A(x) gets the value 1 and B(x) gets the value 1.  The object you're committed to has both properties.

(∃x)(Ax ⊃ Bx) on the other hand says that there is at least one thing such that if it is A, then it is B.  All of the 1s in the truth table describe consistent cases. You don't necessarily know that there is something that is A; perhaps for every object, A(x) gets the value 0, so (Ax ⊃ Bx) works out true.  In fact, you only need there to be one thing such that A(x) gets the value zero to make (∃x)(Ax ⊃ Bx) come out as true.

You can read (∃x)(Ax ⊃ Bx) as equivalent to (∃x)(¬Ax v Bx), which is in turn equivalent to (∃x)(¬Ax) v (∃x)(Bx).  So the commitments we make are only to either there being some object that is not A, or there being some object that is B.

"Some A is B" has the former commitments rather than the latter, because we're saying that there is an A, such that it is also B.
The proof is not valid and it isn't even true.  In particular, (z)Fz does not imply Fa for some a.  If the universe is empty, (z)P is true for any P (including both Fz and ~Fz).

If you can assume a non-empty universe, the proof is correct.  To demonstrate it mechanically, you should write it out more completely as well-formed formulas and apply transformations like ((P => Q) && ~Q) => ~P to demonstrate correctness.  This usually ends up being a pointlessly tedious exercise after you've done it a few times to verify that it works.
A brief answer - It is possible to prove that something "can't exist", which in turn implies that it doesn't exist.  But trying to prove that something doesn't exist directly is a futile effort.  

iphigenie's links will give you more detail than I have time to re-hash at the moment though :)
There is a simple model theoretic proof. We build a model where the formula is false in for a possible world. This implies that the formula is not valid and hence not a theorem.

Consider a model M, where the set of possible worlds is {w1,w2}, and the accessibility relation R={(w1,w2)}. Then since there are no possible worlds accessible from w2, for any formula A, M(A) is false at w2. Therefore, L(M(A)) is false at w1; hence L(M(A)) is not valid. Given that K is complete, then L(M(A)) is not a theorem for any formula A.

In general one cannot prove using proof-theoretic techniques that a formula is not a theorem.    
To be honest, I suspected that in the normal way of Kant being a Kant, he must've derived his categories transcendentally, which is his modus operandi when it comes to primary concepts. So I searched, and struck gold:

http://plato.stanford.edu/entries/kant-transcendental/#LogForJudCat 1.4 Logical Forms of Judgment and the Categories


  In §§19–20, Kant contends that the vehicle that brings about synthesis
  is judgment, and that this vehicle employs certain forms of judgment,
  which are in turn intimately related to the twelve categories. By
  connecting [synthetic knowledge] to judgment, and the forms of judgment to the
  categories in this way, Kant aims to show that we must use the
  categories in the synthesis of experience.


Keep in mind that opinions on what constitutes a synthetic judgment are strongly divided. Kant himself had a very clear idea of what he meant, but may have failed to communicate an exact-enough definition. Julian Baggini points this out in his http://rads.stackoverflow.com/amzn/click/B005UQLGC0 Toolkit, noting that a way to view the distinction between synthetic and analytic judgments is in whether or not a judgment "adds something to the subject" (§4.3).

What this means for Kant is that in any judgment, there is some experience which is being had. Providing evidence for any proposition requires that an experience of the evidence be possible in the first place. Kant's deduction then unfolds from the argument that categories are a necessary feature of such an experience.

It may appear that Kant has thus plucked his categories out of thin air, but the details of his argument for why experience would be impossible without categories is compelling. 
I think that Catch-22s (Catches-22?) are different from paradoxes, contradictions, or circular reasoning, though they do resemble these things, and are interesting for this reason.

It seems to me that a Catch-22 consists essentially of:


a desideratum D: something that one would like to accomplish
a necessary condition N for accomplishing D. This may often also be a sufficient condition (modulo some actions which one assumes are easy), but often it is something which must be satisfied before D can be accomplished.
a sequence of necessary conditions N1, N2, ... , Nn where N = N1, where for each j < n, the condition Nj+1 is a necessary condition for Nj, and where either 


D or some Nj is a necessary condition for Nn, and each of the necessary conditions must be satisfied before each one which depends on it (a chronological Catch-22); or
D or some Nj is the logical negation of Nn (a logical Catch-22).



The similarity to circular reasoning is, I think, in the cycle of conditions.†

The classic example is drawn from the novel Catch-22, with respect to Orr being allowed not to fly anymore in the Allied campaign in Italy during WWII. Without commenting on whether or not these premises are valid propositions in themselves, the structure of that Catch-22 is as follows: Orr will only be relieved of pilot duty if he is crazy; but if he were crazy, he wouldn't request to be relieved of pilot duty. We may analyse this by:


D = Orr relieved of pilot duty
N1 = Orr is crazy
N2 = Orr doesn't request to be relieved of pilot duty
N3 = Orr not relieved of pilot duty


As D ⇒ N1 ⇒  N2  ⇒ N3 ≡ ¬D, we have a logical Catch-22.

The other, "chronological" Catch-22 involving a cycle of preconditions is actually a special case of a logical Catch-22; albeit one which is more commonly found in practise, for instance in bureaucratic situations involving approval or identification. The "logical" contradiction in this case arises from the need for the temporal ordering: if a condition Nj is initially not satisfied, and if for some time t the condition  Nj can only be satisfied if (through a cycle of preconditions) it is also satisfied at some earlier time t − t', and if we suppose that there is some lower limit to the amount of time required to satisfy each subsequent condition given that its preconditions have been satisfied, then we can conclude that Nj and any condition depending on it will never be satisfied, by induction. This chronological Catch-22 differs from circular reasoning in that we do not suppose that we can actually achieve any of the preconditions; in fact we conclude that we can achieve none of them. 

The logical status of a Catch-22 is therefore a reductio ad absurdum which demonstrates that the desideratum D cannot be achieved, because a logical contradiction would be required to achieve it. The only reason to confuse this with an actual contradiction or circular reasoning is if you assume that it is possible to  achieve; but that is merely a bad assumption. The emotional power of a Catch-22 is when the desideratum D is more obviously compelling than one or more of the preconditions Nj, so that although D is impossible (and so 'technically' absurd), it is the logical structure which renders D impossible which seems absurd instead. This usually arises because the logical structure of the Catch-22 is established by some entity — again, usually a bureaucracy — whose priorities are more strongly aligned with the logical structure that it has created than with the desideratum D. The absurdity then comes from a clash of priorities between the agent trying to achieve the desideratum D, and the agent or more general setting which they find themselves in which is not very sympathetic to their priorities.

† This can also be used to capture a more complicated and Byzantine collection of preconditions which together form a set of necessary conditions, by taking the disjunction of preconditions and tracing them backwards until the collection of preconditions converge on a fixed set.
A predicate, from the Latin praedicare, which is itself a translation of Aristotle's κατηγορῆται, is something that is 'said of' something else. Thus in 'Socrates is bald', the predicate 'bald' is said of Socrates. To say that existence is not a predicate means that existence is not really said of any individual, i.e. is not a property of an individual in the way that being bald, being white etc are properties. 

If it were a predicate, then 'Socrates does not exist' would be saying of some individual that he lacks some property, namely existence. But that is absurd: how can there be some individual such that there is no such individual?  "Blue buttercups do not exist" is not saying that there are such things as non-existent blue buttercups. Rather, it is saying that no buttercups are blue.
http://plato.stanford.edu/entries/aristotle-logic Aristotles logic is more complex than your brief question suggests. He's popularly known for the syllogism. He also enunciates the law of the excluded middle, but he then goes out to point out that this cannot apply to our knowledge of the future where possibility is the rule.

A major goal of his book On Interpretation is to discuss the thesis that, of a pair of proposition one of which asserts what the other denies, is that one and only one can be true, and the other false. However he discusses an example where this is not possible because of the nature of time, the future is not actual but  a spectrum of possibility. Contemporary logic has a subdiscipline called http://plato.stanford.edu/entries/logic-modal/ modal logic that discusses this. 

It has been suggested he adopted, or at least flirted with, a three-valued logic for future propositions, or that he countenanced truth-value gaps, or that his solution includes still more abstruse reasoning. There are more points of contact between contemporary logic and that of Aristotle than the pre-modern commentators suggested which focused more on the syllogism and the law of non-contradiction that he enunciated in his book metaphysics.

http://plato.stanford.edu/entries/dialetheism/ Dialethism is the position that there are true contradictions, of which one form is http://plato.stanford.edu/entries/logic-paraconsistent/ Paraconsistent Logics which denies the principle of explosion, that is given one true contradiction then every proposition is true is a major contemporary research area. http://en.wikipedia.org/wiki/Dign%C4%81ga Dignaga and http://en.wikipedia.org/wiki/Dharmakirti Dharmakirti founded and developed the Buddhist school of logic which was dialethistic. This allowed them to assert that atoms were spherical but also point particles (ie without extensions)

http://en.wikipedia.org/wiki/Luitzen_Egbertus_Jan_Brouwer Brouwer reacting against Hilberts formalism advocated http://en.wikipedia.org/wiki/Intuitionism intuitionism that denied the validity of http://en.wikipedia.org/wiki/Law_of_noncontradiction the law of the excluded middle, this was formalised into http://en.wikipedia.org/wiki/Heyting_algebra Heyting algebra which codified the laws of intuitionistic logic in the same way that boolean algebra codified that of classical propositional logic. Contemporary work in this discipline is done in http://en.wikipedia.org/wiki/Topos Topos Theory in which intuitionistic logic is hybridised with that of http://plato.stanford.edu/entries/type-theory/ type theory, amazingly enough it also has a geometric interpretation (as a category of sheaves on a space).
If you defined an isomorphism between the natural numbers and some element of a physical theory, this would imply that there exist statements about the physical theory could not be proven or disproven within the theory. It certainly doesn't say that every statement in the theory is beyond falsification; and it proves nothing whatever about what might happen experimentally. The physical theory could still be wrong, if it conflicted with experiment.

Note that if the physical theory were disproven, this would not represent a disproof of number theory; it would only prove that the isomorphism between number theory and the physical theory doesn't necessarily expose any number-theoretic structure of reality. That is to say, the pre-image of the physical theory via the isomorphism would be a number-theoretic expression of an invalid physical theory. Not that a priority of number theory is to directly express physical physical structures, of course.

  What exactly is meant by transcendential idealism?


Transcendental idealism, or critical idealism as Kant preferred to call it, is the view that our experience can only give us representations of things (i.e. how they appear to us; this is referred to as phenomenon), and that we can never know how these things are in themselves (this is what he referred to as  noumenon or ding-an-sich). This does not mean that there is no real, objective world; it just means that this objective world is beyond the categories of human reason. We cannot know these things-in-themselves, only that they must exist in order to form the representations that we can experience. 


  Is it simply the idea that we can't possibly observe things-in-themselves directly (like idealism), so (unlike idealism) we know they exist, but not what they exist as?


This seems accurate, although I would say that idealism, without any further specification, does not mean anything more than the view that our thoughts make up fundamental reality. "The idea that we can't possibly observe things-in-themselves directly" is something Kant introduced; not all idealists make this distinction between things-in-themselves and things as they appear to us. 
There is https://mathoverflow.net/questions/9864/presburger-arithmetic/10027#10027 a very closely related post on MathOverflow to this topic, which I was pointed to recently in the context of Gödel's Second Incompleteness theorem. The following is a paraphrase of a response there, and some of the comments to it.

Presburger arithmetic does not capture enough of arithmetic to prove its own consistency. Its only function symbols are addition and successor, which are not sufficient to represent Gödel encodings of propositions. However, there are axiom systems which are self-verifying, and which are consistent if and only if Peano Arithmetic is consistent. To do so, one must include enough arithmetic to make Gödel codings work (i.e. so that the system can encode a wide variety of recursive functions), but not enough to allow the incompleteness theorem to apply. 

Such an axiom system is presented in Dan Willard's http://www.cs.albany.edu/~dew/m/jsl1.pdf Self-Verifying Axiom Systems, the Incompleteness Theorem and Related Reflection Principles (disclaimer: I have not had the chance to read it yet), which replaces addition and multiplication with subtraction and division. This is just enough to allow the system to represent itself using arithmetic, but not enough to allow it to prove that multiplication is a total function, presumably because division and subtraction aren't total functions on the non-negative numbers. This would seem to prevent Gödel's Fixed Point theorem (c.f. the http://plato.stanford.edu/entries/goedel/ Stanford Encyclopedia's entry on Gödel, Theorem 2) from being provable about the theory:


  Gödel's Fixed Point Theorem.  If φ(x) is a formula of number theory, then there is a sentence ψ such that P ⊢ ψ ⇔ φ(⌈ψ⌉), where ⌈ψ⌉ denotes the numeral of the Gödel encoding of ψ.


As Gödel's incompleteness theorems both rely on this theorem, they do not apply to this particular self-asserting formal system.
What do you mean does not apply?

  P   Q  ~P  ~Q  P->Q  ~P v Q   P->~Q  ~P v ~Q
 =================================================
  0   0   1   1   1   =   1      1   =   1
  0   1   1   0   1   =   1      1   =   1
  1   0   0   1   0   =   0      1   =   1
  1   1   0   0   1   =   1      0   =   0


As for me, everything matches. 
The "fallacy fallacy" is that you reject a conclusion because the argument is a fallacy - even though the conclusion may be right. In general form:


If P, then Q.
P is a fallacious argument.
Therefore, Q is false.


You have only given step 1 and step 2; you didn't accept step 3, rejecting the consequent (Q). Since the implication (P->Q) holds (step 1) and P is indeed a fallacious argument (step 2), there is no fallacy. If you had concluded that 2+3+1=7 was wrong based on the antecedent - and not on your own 'mathematical intuition', then it would have been a fallacy.

The implication "If 2+3=6, then 2+3+1=7" is true, since both the antecedent and the consequent are false. You can look at the truth table for this:



An example of an argumentum ad logicam is the following:


If 2+3=6, then 4+4=8.
2+3=6 is false.
Therefore, 4+4=8 is false.


There is nothing wrong with step 1 and 2. The implication (step 1) holds and "2+3=6" is false (step 2). It does not logically follow, however, that "4+4=8" is false (step 3). 

Note that the implication in step 1 also holds, but that there is a difference with the previous example I gave. Here the antecedent is false, but the consequent is true (which can lead to the fallacy fallacy). Truth table:



(As a side note: a general rule to determine the truth value of an implication: if the value of q≥p, the implication holds (even if q and p are completely unrelated!). If you find this a bit absurd, you're not alone. Many Logicians find it a quite troublesome.)

Or I can give a more absurd example:


If New York is in China, then New York is in the U.S.A.
New York is not in China.
Therefore, New York is not in the U.S.A.


The truth table in this case is the same as in the previous example:



Here, the implication in step 1, as absurd as it may sound, is true. This may illustrate the point I made in the previous example: the implication can be quite troublesome at times.

In real-life, this will often occur in a more subtle manner. For instance:

Argument by proponent:


Socrates is a sentient being.
All humans are sentient beings.
Therefore, Socrates is human.


Counter-argument by opponent:


You have just http://en.wikipedia.org/wiki/Affirming_the_consequent affirmed the consequent.
You have committed a fallacy; your reasoning was wrong, so Socrates is not human.


While it is of course true that the argument given by the proponent is fallacious, his conclusion was correct. The opponent could point at his flawed reasoning, but he could not reject the conclusion based solely on this flawed reasoning. This is an example of the fallacy fallacy or argumentum ad logicam.
Yes, usually natural deduction systems that you would get in intro textbooks have no axioms. A logical theory could be just a set of theorems, and so have no inference rules. I think the use of such theories is that sometimes you want to put an emphasis on what the theory says and not how it delivers those results.

For instance, in ontology you'll often want to apply a criterion for ontological commitment (like Quine's "to be is to be the value of a variable") to a theory. Since the criterion will (often) just read off commitments from quantified statements, you don't really need the inference rules. To make sure you get all of a theory's commitments, you'll often see authors taking the closure of a theory under logical entailment.

So, if by a "propositional logic without inference rules" you just mean a set of theorems, then yes I think they are sometimes useful. I think the general rule of thumb might be that such formulations are preferable when only the content of the theory matters for your purposes. 
Like perception, introspection, memory, and testimony, a priori justification is fallible. One might be justified in believing something a priori, e.g., that every event has a cause, that is actually false. Many physicists think that some subatomic events occur at random and so have no cause. Besides being fallible, it seems that a priori justification is defeasible, a priori justification can be defeated by further evidence. Why couldn't a priori justification be defeated by empirical, not just a priori, considerations? Something like this has actually happened, Kant was a priori justified in believing that every event has a cause but, because of developments in sub-atomic physics, we are not, and that the Greeks were, all things considered, a priori justified in accepting Euclidean geometry but we are not because of developments in cosmology.


  Some philosophers changed the can in the definition of a priori
  knowledge to a must in order to solve some problems. Who are these
  philosophers? And what would be examples of a priori knowledge using
  a"must"? I won´t go further into the problems that might arise with
  the notion of a prioricity here. I will say that some philosophers
  somehow change the modality in this characterization from can to must.
  They think that if something belongs to the realm of a priori
  knowledge, it couldn´t possibly be known empirically.


Putnam (1) e Kitcher (2). Philip Kitcher thinks that if there is such a thing as a priori justification, then “a person is entitled to ignore empirical information about the type of world she inhabits. Hilary Putnam thinks that if a person is entitled to ignore empirical information, or it is always rational for her to believe something no matter what the empirical evidence is, provided she is a priori justified in believing that thing, then a priori justification is indefeasible by experience.

Putnam and Kitcher maintain that if the belief is justified a priori, then the belief  is not rationally revisable in light of experiential evidence. But, they contend, the propositions traditionally alleged to be knowable a priori, such as mathematical propositions, are rationally revisable in light of experiential evidence. Therefore, knowledge of such propositions is not a priori. Consider the empirical sources that have been alleged to justify mathematical propositions empirically: counting objects, reading a textbook, consulting a mathematician, and computer results. Each of these sources is fallible in an important respect. If  belief that p is justified by counting a collection of objects and arriving at a particular result, then it is possible that recounting the collection to arrive at a different result. If  belief that p is justified by a textbook or mathematician or computer result, then it is possible that to encounter a different textbook or mathematician or computer result that states that not-p. In each case, the latter result is an empirically justified overriding defeater for the belief original justification. The argument, however, is problematic on two counts. First, there are grounds for denying that it captures Kant’s conception of a priori justification since his arguments in support of a priori knowledge do not address the issue of whether experience can defeat one’s justification for believing mathematical propositions. They focus exclusively on the source of such justification. Second, it settles by fiat a substantive philosophical question since it rules out the possibility that mathematical propositions are justified both a priori and by experience

But why think that  a priori justification implies either that a person who has that sort of justification is entitled to ignore empirical information or that it is always rational for her to believe what she does no matter what the empirical evidence is? A priori justification must be “independent of experience,” which implies that it must be independent of empirical evidence. But there is an interpretation of that sort of independence that does not imply that the person is entitled to ignore empirical information or that her justification will remain no matter what the empirical evidence is. Suppose being justified independent of experience simply means that experiential sources do not provide the justification, that the justification is provided solely by some non-experiential source. That does not imply that the experiential evidence could not defeat that non-experiential justification. A priori justification does not imply that the justification will remain where experience is not silent. It allows that experience might defeat a priori justification. There might be three categories of justified propositions: 1- those whose justification is wholly independent of experience, 2- those whose justification does not rest on, but can be defeated by, experience, and 3- those whose justification rests, or depends on, experience. A priori justification might be applied to categories 1 and 2. It is harder to say positively what it means, but on one standard interpretation non-inferential, a priori justification is justification based solely on understanding the proposition at issue.

The notion of a priori knowledge, construed as a notion of non-empirically grounded knowledge, is not the same as a notion of epistemic certainty. Philosophers have understood ‘epistemic certainty’ in various ways: for instance, as epistemically indubitable belief or as self-evident belief. A belief is epistemically indubitable if and only if it would not be epistemically justifiable to doubt that belief under any circumstance. It is not obvious that a priori warrant for a proposition requires epistemic indubitability of this proposition. A priori justification for a proposition apparently can be subject to ‘epistemic defeat’ given a change in a priori evidence. A self-evident proposition is justified but does not depend on anything else for its justification. The problem in linking a priori warrant to such self-evidence is that a priori warrant is compatible with inferential warrant, wherein a proposition owes its warrant to inferential relations with other propositions, as might a theorem in a mathematical system.

The notion of a priori knowledge depends on a notion of a priori warrant, not on a notion of a non-empirical origin of the concepts. A notion involving special conditions for the justification of a believed proposition is not automatically a notion involving special conditions for either the origin or one’s understanding. 

The primary dispute between apriorists and radical empiricists is over the source of the knowledge in question. A theory of a priori knowledge requires limitation of the set of propositions knowable a priori. Such a theory must avoid confusing the notion of what is a priori with the notions of what is necessarily true, what is analytically true, what is innate, and what is certain. It must also draw a clear distinction between what is a priori and what is a posteriori. Many prominent apriorists maintain that truth conduciveness is a necessary condition for epistemic justification.The claim that a source of beliefs is truth conducive or that it is not error conducive is a contingent empirical claim that need be supported by empirical investigation.

(1) Putnam, Hilary. “‘Two Dogmas’ Revisited.” In Realism and Reason: Philosophical Papers. Vol. 3. New York: Cambridge University Press, 1983.
(2) Kitcher, Philip. The Nature of Mathematical Knowledge. New York: Oxford University Press, 1983.
You ask two questions, (1) whether it is beneficial to think in a logically consistent manner, and (2) whether it is always beneficial to think in a logically consistent manner. To answer these, we have to take a look at two different aspects of thinking.

First aspect: Belief Systems

Regarding your beliefs, I would say the answers are Yes to (1) and No to (2). It is generally beneficial to have consistent beliefs and as a rational animal you strife to eliminate inconsistencies by further thinking, experimentation, belief revision, and so on. It would not be beneficial to always have consistent beliefs (and also not be possible, because people make mistakes), because you aggregate beliefs and form knowledge from various different sources: perception, your own thinking, and other people by testimony. At least for the latter, it is beneficial to keep track of possibly inconsistent information in order to evaluate it later. It would be harmful to immediately revise all your beliefs in light of contradicting evidence or ignore contradicting evidence in order to forcefully maintain a consistent belief system. 

Second aspect: Drawing Inferences

Here again the answer is Yes to (1) and No to (2), but for different reasons. First of all, not all inferences are logical in the narrow sense of being 'deductive' (as in classical logic). You also need to make inductive inferences and possibly abductive inferences, where it must be mentioned that the existence and justificatory status of abduction is highly controversial. These are not logical in the narrow sense but the have been studied by philosophers, logicians, computer scientists and mathematicians in great detail, too. Secondly, there are general complexity considerations that speak against the view that one can or ought to always make perfect deductive or inductive inferences. With an increasing number of variables and background assumptions deductive and inductive inferences become computationally intractable. You'd be paralyzed by your inferential processes, which would take too much time to complete. Instead, humans make a lot of heuristic shortcuts, which only work sometimes and lead to erroneous results in other cases (cognitive biases). In computer programming, shortcuts are also used, or alternatively incorrect simplifying assumptions are made. (For example, for a Bayesian network to become tractable you need to make strong assumptions about the independence of random variables which are rarely fully justified.)

Bottomline: No, logical thinking is not always beneficial, but it is so most of the time - at least if time permits.
Your assumption is indeed correct.
Technically, actually, your sentence doesn't even have to be true. Consider: 

"There's a man in Memphis who paints pictures of dogs in the park for a living. I don't know what his name is, but let's just call him Jerry."

If I were to say next "Jerry's name is 'Jerry'," I would (probably) be speaking falsely, because 'Jerry' is just the name that we, as storytellers, are using to keep track of this man (possibly in an indeterminate way); it's not this person's actual name. The same applies to "Barack Obama's name is 'Barak Obama'".

If your original sentence was "Barak Obama is Barak Obama," then Kripke would say, without hesitation, that the truth of this sentence is both a priori and necessary. But with your original sentence, "Barak Obama's name is 'Barak Obama'", I think Kripke would have to say this is contingent, since as you said different worlds could be employing different languages or different assignments of names to objects. 

That does not, however, refute Kripke's position on names being rigid designators. The point is that names in our world rigidly designate the same object in every possible world, not that in every world names pick out the same object. Of course Kripke would agree that there's a possible world just like ours except the names are all mixed up, so that my name (in that world) is 'Brad Pitt', and Barak Obama's name is 'Hilary Clinton', etc. But given the (ideally unique) reference of names in this world, in our actual language, those names pick out the same people as we consider different worlds. The analogy is similar to the language-metalanguage distinction; while it's true that the objects of the possible worlds could be employing different languages, or using the same language differently, still names in the metalanguage, the language we're using to describe these possible scenarios, pick out the same object across these different worlds. (Of course, not everyone agrees with this line of thought, but it's certainly a reasonable position to take.)
http://plato.stanford.edu/entries/phenomenology/ Phenomenology, was, perhaps, the most sustained effort to excise this division from ontology. It begins with Husserl's notion of http://plato.stanford.edu/entries/consciousness-intentionality/ intentionality and his famous dictum that "consciousness is always the consciousness of something", but finds its full force in Heidegger's Being and Time and his notion of http://plato.stanford.edu/entries/heidegger/#BeiWor being-in-the-world. Perhaps even more explicitly, Merleau-Ponty's Phenomenology of Perception is the criticism of the mind-body distinction. Although phenomenology is a movement, not a set of doctrines, the main idea shared by phenomenologists is that 'things' (objects) cannot be separated from those who perceive them. Hence the emphasis on examining meanings rather than things, and hence the erasure of the subject/object distinction. Phenomenology does not so much resolve Cartesian dualism as shows how it is a superfluous distinction.
The definition of any of these terms is going to be controversial, but your first problem is that your definition of a priori is wrong. The traditional definition is "knowable independent of (any particular) experience". Now, analytic truths (traditionally conceived) are a priori knowable, but just because the analytic truths are a subset of the a priori truths doesn't muddy the distinction.

Regarding  "'A priori' and 'analytic' refer to 'deduction'; this leaves 'synthetic' and 'a posteriori' to share 'induction'." There is certainly no reference relation between any of these terms. You might think that the means to uncovering a priori or analytic truths is typically deductive whereas the means to uncovering synthetic truths is typically inductive. But you can reason deductively about synthetic truths (this happens whenever you give a deductive argument with synthetic truths as premises) and likewise for reasoning inductively about a priori or analytic truths. 

Quine's "Two Dogmas of Empiricism" is the locus classicus for criticism of these distinctions. Kripke rehabilitates a few of them in Naming and Necessity.

See the following two SEP articles for more general information:

http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=8&cad=rja&ved=0CF4QFjAH&url=http%3A%2F%2Fplato.stanford.edu%2Fentries%2Fapriori%2F&ei=JaihUfa3LKqK0QGtu4GgBQ&usg=AFQjCNHan89OTJhAVYIU5BJum9zrN5jsqA&bvm=bv.47008514,d.dmQ The a priori

http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=8&cad=rja&ved=0CF4QFjAH&url=http%3A%2F%2Fplato.stanford.edu%2Fentries%2Fapriori%2F&ei=JaihUfa3LKqK0QGtu4GgBQ&usg=AFQjCNHan89OTJhAVYIU5BJum9zrN5jsqA&bvm=bv.47008514,d.dmQ Analytic/Synthetic
a) It is 2nd-Order Logic with Full Semantics. The semantics is "full" because the second order variables range over the full powerset of the first-order domain. Categoricity obtains in many cases as a result of the failure of Lowenheim-Skolem in a second-order setting.

b) It is not true of every axiomatic system, or at least we do not have proofs of categoricity for every axiomatic system. Some tend to view categoricity as crucial for a proper axiomatization of non-algebraic theories like PA. It's hard to see whether algebraic theories should be categorical, though.

http://plato.stanford.edu/entries/philosophy-mathematics/#Cat Section 5.2 of the SEP article on Phil of Math has some info you might find useful.
There may be a connection between model theory and type theory if you dig enough, but I don't see the analogy between assigning types to variables and assigning objects to variables. My knowledge of type theory is a bit fragmented, but I would say that, if anything, type theory has closer ties to computability theory or proof theory than it does to model theory.

For one thing, types/terms can be systematically combined in meaningful ways to produce other types/terms, whereas objects in model theory don't combine in this way. Moreover, in model theory, the focus is on models, and which theories are satisfied in which models; it's more concerned with worldly structure, you might say, and less concerned about the particulars of the language (though which language a model is in certainly matters). By contrast, type theory is more concerned with the particulars of the language, and less concerned with the way the world is set up. 

That's a very rough (and possibly misleading) characterization, but for instance, in model theory, you might be concerned about determining what the models of the theory of algebraically-closed fields or the theory of divisible ordered groups look like (are there saturated models, prime models, homogeneous models, etc.) or what properties these theories have (are they complete, decidable, categorical, do they have quantifier elimination, etc.). But you wouldn't bother yourself with proof systems in model theory, and in fact you can easily get in model theory by without ever mentioning them! By contrast, type theory might be concerned with naming proofs and showing how to systematically code proofs into terms of a certain type. You wouldn't need to talk about the way the world is, in type theory, but you would need to talk about syntactic functions.

It can also be confusing, because model theory uses the word "type" in a very different unrelated sense from type theory. A type over a parameter set X with respect to a theory T is a set of formulae consistent with T which may use members of X as parameters in the formulae. It then becomes an interesting question in model theory whether or not there are models of T that omit certain "types", or how many types there are with respect to T, or in what way the different types with respect to T are related, etc.
Logic enable to check validity, not verity. You take a set of axioms (basic independant not contradictory propositions) and a set of rules to combine axioms : logic is the game that explore where you can go from this axioms with this set of rules. Also you may take an "arbitrary" proposition and ask yourself, "can I go there from my given axions and rules?". According to https://en.wikipedia.org/wiki/Consistency Gödel's incompleteness theorems, there are propositions for which the answer is "may be I could reach it with my given sets, but there's no general short cut and while I may find a way, I may just as well never leave this logical incertitude".

Whether axioms are true is somwhere between arbitrary decision and trust in your perceptions.

From a logical point of view, an absolute https://en.wikipedia.org/wiki/Solipsism solipsism is as sound as realism.

From a logical point of view, "I think so I am", or any equivalent formulation – even those which may possibly grammaticaly hide the subject of the action like "thinking implies being" or "cogito ergo sum" –, is just a tautology : "I is I", "I ⇒ I".

If you decide that tautologies are sources of truth, and that you believe that "I" (you are existing), then you decide that your own existence is true.

Of course, logical analyze is far from your only source of data which feed your "decision process", even in ontological process. If you are hungry, chances are good that you won't cogitate for long on the ontological status of hunger before you decide to make actions that you think required to make you satiated.
Your assumptions are wrong this time. Gödel used http://en.wikipedia.org/wiki/Type_theory#History Russell's type theory from http://en.wikipedia.org/wiki/Principia_Mathematica Principia Mathematica. Thus Gödel uses a higher-order theory in the original proof (though I'm not sure without taking a deep look how much of the higher-order apparatus is used). In fact, the original paper's (translated) title is http://en.wikipedia.org/wiki/On_Formally_Undecidable_Propositions_of_Principia_Mathematica_and_Related_Systems "On Formally Undecidable Propositions of Principia Mathematica and Related Systems".

Full http://en.wikipedia.org/wiki/Second-order_logic#Metalogical_results second-order logic is also incomplete.

I believe the only requirements on a system to have incompleteness obtain are the following (I might be missing something):


Capable of representing primitive recursive functions (at the very least the arithmetical ones).
Having a recursively enumerable axiom set.


Once those two hold (modulo my concern that I might be forgetting something), incompleteness applies to that system.

EDIT:

I believe my statement that those are the only two requirements is actually correct (so my concerns were misplaced), Gödel states the conditions as follows:


The class of axioms and the rules of inference (i.e., the relation "immediate consequence of") are recursively definable (as soon as the basic signs are replaced in any fashion by natural numbers).
Every recursive relation (Note: here Gödel is referring to what are now called primitive recursive functions) is definable in the system.

Von Neumann in the early days of Quantum Theory came up with Quantum Logic. He noticed that that the mathematical apparatus describing a system can be seen as a generalisation of ordinary logic. This of course only a formal similarity but he believes in taking this as a serious hint about how to think about Quantum Theory. 

Formally he interprets Quantum Theory (solely in its mathematics) as a probability calculus based on a non-classical lattice. (Technically speaking it is measure theory generalised from sigma-algebras to ortho-algebras).  This lattice is not distributive & is ortho-complemented, whereas the classical boolean lattice is distributive and complemented. 

The lattice is non-classical because there are measurements we can make that cannot be done simultaneously. This is the content of Heisenbergs Uncertainty principle.

Now as far as Logicians are concerned this is only a fragment of a logic. There is no mechanism for inference, nor its truth-functionality. That is we cannot say whether the logic is complete or sound - which to Quine was a serious argument for taking FOL as logic.

Well before Krauss, it was Finkelstein & Putnam in the sixties that began to aggressively push the non-classical logic centre-stage, mainly it seems to get the mainstream take these kinds of logics seriously.

More recently, Isham & his colloborators are using Topos Theory to provide semantics & a proof theory to quantum logic in the form of the Bohr Topos. Notably the proof system is not classical either it is higher order typed intuitionistic logic. It is also complete & sound. One interesting result is that the kinematics of the Bohr Topos when viewed internally is classical & when viewed externally is quantum-mechanical. 

This is a new way to take seriously Von Neumanns dictum that it is the quantum logic that is essential in QM. Its far too early to say how important the results are though.

Finally, the double-slit experiment doesn't challenge our logic but our physics as then conceived. That is then Physics had a simple deterministic interpretation - symbolically the 'clockwork universe'. In fact this experiment does have a classical interpretation via Bohmian Mechanics - but it is non-local. One should note that the Greek atomists did include indeterminism in their physics. As Newton did preface his Principia with an excerpt from Lucretious De Rerum Natura, it probably would not have come as a surprise to him to see its advent again.  

And yes, 2+2=5 is a joke - and not to be taken seriously - other than to provoke a little thought amongst the mathematically or logically inclined.
Answer to Question #1

Paraconsistent mathematical theories will not always be complete. Depending on what the theory takes to be true, and the strength of its deductive system, there might well be unprovable truths. As I'm sure you are by now aware, all paraconsistent theories give up ex falso quodlibet (the rule that allows you to derive anything from a contradiction) as well as principles that entail it (like disjunctive syllogism: from "A or B" and "not-B" deduce "A"). This means that inconsistencies within these theories will not "explode" allowing the proof of any statement of the language. Thus, paraconsistency is no guarantee of completeness. Embracing inconsistency does, however, open the door to the possibility of a complete theory whose classical counterpart would be essentially incomplete. For a toy example, a paraconsistent theory which keeps ex falso quodlibet (though, such a theory wouldn't really be paraconsistent anymore) as an admissible inference will be trivially complete (I imagine this is something like what you had in mind). 

Answer to Question #2

Well, many interesting paraconsistent theories won't be consistent, so those theories certainly shouldn't be able to prove their own consistency--- that would be bad. I'm not quite sure what else you had in mind, but it is interesting to note that Tarski's corollary of Gödel's results--- http://en.wikipedia.org/wiki/Tarski%27s_undefinability_theorem Tarski's Undefinability Theorem ---is no longer much threat. If you look at the linked Shapiro article (in "Further Reading") you'll see that the theory he develops is a paraconsistent arithmetic (a http://plato.stanford.edu/entries/dialetheism/ dialetheist arithmetic to be more precise; I suspect that many of your questions about paraconsistent theories are really meant to be about dialetheist or otherwise inconsistent theories) which contains its own truth predicate. It can prove its own soundness and its own Gödel sentence.

Further Reading

http://plato.stanford.edu/entries/mathematics-inconsistent/ SEP Article on Inconsistent Mathematics

http://www.iep.utm.edu/math-inc/ IEP Article on Inconsistent Mathematics

http://mind.oxfordjournals.org/content/111/444/817.abstract Incompleteness and Inconsistency; by Stewart Shapiro
Actually, it's the opposite. Here's a simplistic overview of why: Suppose you do a finite amount of mathematics (prove a finite number of theorems from a finite axiom set). But by the incompleteness theorems, there are some theorems that simply cannot be proved. Thus, no finite amount of mathematics suffices to encompass all of mathematics.

It's instructive to look at a specific case study of what situations like these actually look like in the course mathematical research. The first and still most famous case is the ancient question of whether or not Euclid's parallel postulate (PP) can be derived as a theorem from the first four axioms (the postulate states that There is at most one line that can be drawn parallel to another given one through an external point).

One way geometer's posed this problem was to attempt to re-prove all the old Euclidean results from scratch without invoking the axiom of parallels, and see how far they could get. This system of geometry, Euclidean minus parallels, is called Neutral Geometry because it is neutral about whether or not the parallel axiom is true. The question is then, does Neutral Geometry end up being equivalent to Euclidean Geometry anyway?

And of course, they failed time and again to prove PP within Neutral Geometry, and so they eventually started focusing their research on more indirect approaches, like considering what taking "PP is false" as an axiom would imply and presumably something absurd would result. One way to falsify PP is to say that we can find at least two parallel lines through a point, in contrast to at most one. Counter-intuitive results followed, but it this system was surprisingly proved to be consistent regardless, and it came to be called hyperbolic geometry. Whereas before geometry was a single unified system codified by Euclid, both complete and consistent, was shown to be but a stem of an ever branching family geometr*ies*.

This is what really makes the implications Godel's incompleteness theorems so profound. It guarantees the openendedness of mathematics. In the beginning there was geometry and arithmetic, and then algebra and eventually calculus/analysis. The subject matter of mathematics could be understood in completely taxonomic terms: space, quantity, structure and change. But today we can see that mathematics exceeds any definition in terms of subject matter. On might say that mathematics is more of an art: the art of being creatively logical through the media of abstraction.
I do not have this book, so I cannot know what the phrase "to relative an oracle" is supposed to mean, but I can explain the relativity of models in mathematics that results from this theorem.

In a given universe U of set theory, consider a theory T and an infinite model M of T.
According to that theorem, the cardinal of M may be any infinite cardinal (countable or uncountable, as viewed by U).

Now let us change the viewpoint, and consider the universe U with the system M inside it, from the outside : U is just a model of set theory among others.
If U is a countable model of set theory, then M will be countable too when viewed from outside U, even if it was uncountable when viewed by U.

Set theory also admits models U where the set N of natural numbers it contains, is uncountably infinite (when viewed from outside). In this case M will also be uncountable when viewed from outside (as it contains a copy of this uncountable N), even if it is countable when viewed by U.

I also wrote a site on the http://settheory.net/ foundations of mathematics, that contains these issues.
The issue is a little bit complex.

See :


  John Etchemendy, TARSKI ON TRUTH AND LOGICAL CONSEQUENCE, in The Journal of Symbolic Logic, Vol.53 (1988)
  
  Mario Gomez-Torrente, Tarski on Logical Consequence, in ND Journal of Formal Logic, Vol.37 (1996)
  
  Craig Bach, TARSKI'S 1936 ACCOUNT OF LOGICAL CONSEQUENCE, in Modern Logic, Vol.7 (1997)
  
  the new translation of Alfred Tarski, On the Concept of Following Logically (1936), in HISTORY AND PHILOSOPHY OF LOGIC, 23 (2002).


I'll try to summarize the central issue, trying to avoid technical details (due also to the lack of LaTeX editor).

Reference is, of course, to Wilfrid Hodges' entry in SEP : http://plato.stanford.edu/entries/tarski-truth/ Tarski's Truth Definitions.


  We say that a language is fully interpreted if all its sentences have meanings that make them either true or false. All the languages that Tarski considered in the 1933 paper were fully interpreted [...]. This was the main difference between the 1933 definition and the later model-theoretic definition of 1956 [...].
  
  In 1933 Tarski assumed that the formal languages that he was dealing with had two kinds of symbol, namely constants and variables. The constants included logical constants, but also any other terms of fixed meaning. The variables had no independent meaning and were simply part of the apparatus of quantification.


Today logicians work with formal languages, whose extra-logical constants are uninterpreted (until an interpretation is fixed in a particular case). To provide an interpretation or model of such a language, one typically specifies a domain or universe of discourse and assigns to each individual constant a unique object in the domain, to each monadic first-order predicate a subset of the domain, and so on. 

Tarski [1936] worked instead with what he called formalized languages, in which the extra-logical constants are interpreted and the domain is fixed.

As Hodges explains :


  Model theory by contrast works with three levels of symbol. There are the logical constants (=, ¬, & for example), the variables (as before), and between these a middle group of symbols which have no fixed meaning but get a meaning through being applied to a particular structure. The symbols of this middle group include the nonlogical constants of the language, such as relation symbols [like ∈ in set theory], function symbols and constant individual symbols [like 0 in arithmetic]. They also include the quantifier symbols ∀ and ∃, since we need to refer to the structure to see what set they range over. This type of three-level language corresponds to mathematical usage; for example we write the addition operation of an abelian group as +, and this symbol stands for different functions in different groups.
  
  By the late 1940s it had become clear that a direct model-theoretic truth definition was needed. The version we use today is based on that published by Tarski and Robert Vaught in 1956. 
  
  The right way to think of the model-theoretic definition is that we have sentences whose truth value varies according to the situation where they are used. So the nonlogical constants are not variables; they are definite descriptions whose reference depends on the context. Likewise the quantifiers have this indexical feature, that the domain over which they range depends on the context of use. 


Because Tarski [1936] uses a formalized language with interpreted extra-logical constants, he must first replace extralogical constants by variables of the same type and then consider what sets of objects satisfy the resulting sentential function.

The concept of a model in Tarski [1936] is not the contemporary concept. In contemporary mathematics, as Hodges points out, a model or structure is roughly a collection of elements with relations defined on them. A set of objects is not such a structure.
Everything you said seems fine. You propose to think of intensions as functions from possible worlds to subsets of the domain of individuals. That's a pretty standard way of thinking about intensions (goes back to at least Carnap's work in semantics). That particular criterion of analyticity, namely that:


  Sentence "s is P" is analytic iff concept(P) ⊂ concept(s)


doesn't require an intensional treatment, because concept(x) could simply denote the extension of x in a given world. That being said, I think an intensional interpretation of concepts is much stronger and therefore less susceptible to criticism. The criterion above can be slightly modified to work in a possible worlds framework exactly as you proposed.

I'm not familiar with any objections to the intensional interpretation of this particular criterion of analyticity (Kant gave another two: one also in CPR A7, another in Prolegomena p. 266). But there might be. There are lots of different ways of explicating analyticity; this is just one, old version of it. Carnap has more interesting formulations of it (see, for example, Appendix B of his Meaning and Necessity). 

Since Quine's "Two Dogmas of Empiricism", analyticity has been subject to a lot of debate. But Quine's attack wasn't against any particular formulation of analyticity, but with the very distinction. If you think there is such an informal distinction between analytic and synthetic truths, then you will not find Quine's objections problematic for this intensional formulation of Kant's criterion. If you're interested in the messy details of this debate, G. Russell's http://rads.stackoverflow.com/amzn/click/0199694737 Truth in Virtue of Meaning is a very readable, recent defense of the analytic/synthetic distinction. I highly recommend it.

  http://en.wikipedia.org/wiki/Common_knowledge_%28logic%29 Common knowledge is a special kind of knowledge for a group of agents. There is common knowledge of p in a group of agents G when all the agents in G know p, they all know that they know p, they all know that they all know that they know p, and so on ad infinitum.1
  
  1: Osborne, Martin J., and Ariel Rubinstein. A Course in Game Theory. Cambridge, MA: MIT, 1994. Print.


(http://en.wikipedia.org/wiki/Common_knowledge_%28logic%29 source)



Everyone sees 99 agents with blue eyes. Therefore, everyone knows that there are at least 99 agents with blue eyes. However, does everyone know that everyone knows this?

Suppose one of the agents (A) wants to know what any other agent B knows. He knows that every other agent knows that there are at least 98 agents (all 99 that are seen by A except B himself) with blue eyes. 

Now suppose that A wants to know what B knows about what a third agent C knows. A knows that B sees at least 98 agents that have blue eyes, of which one is C. Therefore, A knows that B is sure that C knows that at least 97 agents (the 100 except A, B and C) have blue eyes.
We cannot count A because in a worst-case for A there might actually be only 99 blue-eyed agents (everyone except A), which means that for B the number of blue-eyed agents is 99 or 98 (100, without A, and with or without himself). Then if B wants to know what C knows in this worst-case scenario he has to subtract C as well, which makes the number 98 or 97. Therefore A is not sure if B is sure that C knows that there are 98 agents with blue eyes.

We can go a level deeper, in which we will see that A can only be sure that B is sure that C is sure that D is sure about there being at least 96 blue-eyed agents. 

Because common knowledge is "ad infinitum", and we need to subtract one from 100 for every level we go deeper, because we can never count the agents in this chain A, B, C, ..., there is no common knowledge except that there are at least 0 blue-eyed agents.

Disclaimer: I'm not entirely sure about this. I have no qualification in this field.
http://plato.stanford.edu/entries/logic-deontic/ Deontic logic is concerned with what is optional, recommended, forbidden, etc.

http://en.wikipedia.org/wiki/Doxastic_logic Doxastic logic, on the other hand, is about beliefs.

I would argue that policies (as per the OP) can be expressed by using deontic logic and perhaps doxastic logic as well if you want to capture actual adherence to stated policies. As a practical example, http://en.m.wikipedia.org/wiki/ISO/IEC_24744 ISO/IEC 24744, a standard language for the description of methodologies, uses deontic markers to express whether specific tasks are compulsory, recommended, optional, discouraged or forbidden.
Yes it is through a process called "Deductive reasoning". Deductive reasoning implies that if all of the premises are true and if the inferences are valid, it follows that the conclusion must be true. Here's an example.


  I have a bag full of black marbles. I will pull out a marble and record what colour it is until the bag is empty. It follows that I will only have recorded that there are black balls in the bag.


Let's break this argument down:


I have a bag full of black marbles.
I will pull out a marble and record what colour it is until the bag is empty.
I will only have recorded that there are black balls in the bag.


Given that premise (1) and (2) are true, it follows that the conclusion (3) must be true.

To answer your question, in the event of an argument that uses deductive reasoning, it is possible for something to be known in absolute certainty. This works in theory, however in practise it is harder to say that all given premises are true.
If you take A -> B to be a material conditional, such that A -> B iff ~A v  B is a prior definition rule MC, then a very simple proof might go something like this:


  A->A (established in the subproof below:)
  
  
  A (suppose for a conditional)
  A (by reiteration)
  A->A (by conditional proof
  
  
  ~A v A (by MC)


But the actual details of your logic system will make a massive difference to whether this kind of proof will work.

E: Now that we do have the specifics of the system, you could do it like this:


  ~A -> ~A  (by subproof below)
  
  
  ~A  (suppose for conditional)
  ~A -> ~A  (by conditional introduction, discharging supposition 1.)
  
  
  A v ~A (by the equivalent MC rule, lines 12-15)

Note that P(x), without qualifiers or any information describing x as a constant, is neither true nor false: x is a free variable, which is to say a place-holder for an not-yet-specified term. As such, P(x)⇒Q(y), equivalently ¬P(x) ∨ Q(y), is a constraint on as-yet unsupplied terms.

In asking whether ∀x∈∅.P(x)⇒Q(y) is true for some y, we ask whether there is any value of x for which the constraint may be violated. There is not: and this is not because ∀x∈∅.¬P(x) per se (though this is also true), but simply because there are no values of x, and thus none for which P(x) is true.

If you prefer, using the construction ∀x∈D.Φ ≡ ∀x.x∈D⇒Φ, the cause of the truth of ∀x∈∅.P(x)⇒Q(y) is not because P(x) is always false, but because x∈∅ is always false.
Russell, in the introduction to the Tractatus elucidates this. He says:


  facts cannot strictly speaking be defined, but we can explain what we mean by saying that facts are what make propositions true, or false.


That is the proposition 'grass is green' is true because of the fact that grass is green. 

Wittgenstein is defining the world as all such facts, rather than in conventional language the world of things, like grass itself.

  Is logics (logic rules, arithmetic and logic inference) universe-dependent or not.


Possibly, but we have only one universe, so its certainly not amenable to physical experiment.


  In other words: are logic rules ultimately physical laws of the universe (as gravity, quantum laws and electromagnetism are) or are them not?


According to Wittgensteins Tractatus, the facts of the physical worlds, and natural laws occupy only a small place in the space of all logical possibilities; and these possibilities are the horizons of our vision. 

However, according to Kant logic is not in the world (noumena) but in phenomena; Wittgenstein doesn't say where as he doesn't specify the location of logic. He also says there may be unlogical worlds, but we haven't the capacity to imagine them.


  Can this question ever be ever answered, either logically or experimentally?


Not experimentally; perhaps philosophically, that is with the aid of reason or logic. But then we may have more than one answer.


  Are there any related papers or books in this regard?


I wouldn't read the Tractatus, as its a difficult and obscure book written in a carefully cryptic and aphoristic style. Logicomix is much easier, its a graphic novel, and what it covers logic in the 20C when that area was kicked off by Frege.
Good reasons include if the new theory is simpler, has a wider explanatory scope, is more accurate, and/or produces a new set of testable predictions that are subsequently confirmed.  
The general fallacy here is a typical confusion with quantifier order, and I would call it a "quantifier-order fallacy".  This is a phenomenon that tends to be revealed much more commonly and clearly in the study of contemporary first-order logic than it is in ancient syllogistic logic, simply because it is quantification and particularly the importance of quantifier order that is made clear and emphasized in first-order logic.

The assertion "for every A there is a corresponding B" is asserting merely that the relation is total, that every A has  corresponding B. But the assertion "every B has a corresponding A" is asserting that the relation is surjective. 

Students in mathematics are quite commonly confused about this distinction. For example, they commonly conflate what it means to say that a function f:X→Y is one-to-one, or that it is onto Y. For example, suppose that f:X→ Y is a function, which means that for every a∈ X there is a unique b∈ Y such that b=f(a). 


the assertion ∀a∈X  ∃b ∈ Y  b=f(a) is saying merely again that f is a function.
but the assertion ∀b∈Y  ∃a ∈ X  b=f(a), with the quantifiers reversed, is saying that f is an onto function, that every point in Y is hit by the function. 
meanwhile, the related concept that is often confused is the assertion that f is one-to-one, which is ∀a,a'∈X  f(a)=f(a') → a=a'. 


Perhaps it is fair to say that one can make quite sophisticated and complex mathematical statements with just a few alternations of quantifiers. This distinction is what underlies the difference between continuity of a function and uniform continuity, which was classically confusing to many prominent mathematicians. 

Meanwhile, there are abundant natural language examples of the quantifier-order fallacy:



Every boy was kissed by a girl, versus 
There is a girl who kissed every boy. 


Every child at the circus was supervised by an adult. 
There was an adult that supervised every child at the circus.


every time the train runs, there is someone who is mugged.
there is someone who is mugged every time the train runs.



But actually, if one looks at your fallacy, it isn't just the quantifier order that is reversed, but the actual quantifiers themselves are swapped. So some more accurate natural language instances would be:




Every boy was kissed by a girl, versus
every girl kissed a boy.


every child was supervised by an adult, versus
every adult supervised a child.


every person at the performance was in a seat, versus
every seat at the performance was occupied by a person.



This is the same issue that arises in the distinction between 


f:R→R is continuous iff for every x, for every positive ε there is positive δ for every x', if |x-x'|< δ → |f(x)-f(x')|< ε. 
f:R→R is uniformly continuous iff for every positive ε there is positive δ for every x, for every x', if |x-x'|< δ → |f(x)-f(x')|< ε. 

Ok, let me try. (translation is off the top of my head, so check with the oxford translation).

Ὁμώνυμα λέγεται ὧν ὄνομα μόνον κοινόν, 

"Things are said to by `homonyms' which have one name in common, . . ."

ὁ δὲ κατὰ τοὔνομα λόγος τῆς οὐσίας ἕτερος, 

" but the account of its being corresponding to the name (kata tounoma) is different . . . "

οἷον ζῷον ὅ τε ἄνθρωπος καὶ τὸ γεγραμμένον τούτων γὰρ ὄνομα μόνον κοινόν, 

"just as a living thing and a drawing of one have one name in common . . ."

ὁ δὲ κατὰ τοὔνομα λόγος τῆς οὐσίας ἕτερος ἐὰν γὰρ ἀποδιδῷ τις τί ἐστιν αὐτῶν ἑκατέρῳ τὸ ζῴῳ εἶναι, ἴδιον ἑκατέρου λόγον ἀποδώσει.

" because the account of being corresponding to the name differs, . . ." the rest of the clause gets confusing after that though. 

I'd say that what Aristotle means by λόγος τῆς οὐσίας has pretty well got to be what a contemporary philosopher would call a real definition. On Aristotle's view, we get definitions by empirical inquiry--gathering things together into genera and species in terms of their likenesses and differences, and then a definition (horismos or logos) which expresses the essence of the thing is just a summary of these genera and the specific differences, e.g. ``Man is a rational animal.'' 

His point in this paragraph is that mere linguistic use can be deceptive. Sometimes we use the same sound to name two things that in themselves have radically different natures. Aristotle's example is a man and a picture of a man, which apparently could both be called an 'anthropos' in Greek. A better English example might be a 'bank', which could be either the side of a river, or a financial institution. There is nothing that riversides and financial institutions have in common, so the use of the word "bank" is ambiguous in just this way. 

The importance of Aristotle's point here will only really become apparent in the Analytics, when he is describing the kind of logical unity required for scientific inquiry. There he lays down the condition that you can't transfer scientifically proven conclusions about the nature of humans to pictures of humans. This is Aristotle's way of pointing out the fallacy of equivocation. 

There is one more wrinkle, that is a famous issue in Aristotle's metaphysics and that is that Aristotle recognizes a special subset of homonyms where there is enough relation between the different senses to allow us to investigate them all under one science. He calls this "pros hen equivocity" although it's sometimes known in the literature as "focal meaning" or in older medieval texts as "the doctrine of analogy". This is how Aristotle is able to reconcile his claim that "being" is not a univocal notion--chairs and redness don't "exist" in the same sense, with his claim that metaphysics is the science of being qua being.
lets first rewrite Russels proof in in modern notation:

Id(*3.01) = |- (((P & Q) -> R) -> (~(~P v ~Q) -> R)
transp                         -> (~R -> (~P v ~Q))
Id (*1.01)                     -> (~R -> (P -> ~Q))
comm                           -> (P -> (~R -> ~Q))
transP.syll                    -> (P -> (Q -> R))) |-. prop


I do think Russel uses the rule of syllogism (|- P -> Q, |- Q -> R => |- P -> R ) without mentioning it, because it just makes the proof unnescesary complicated.

to add them to the proof it becomes

Id(*3.01)   = |- (((P & Q) -> R)   -> (~(~P v ~Q) -> R)
transp      = |- (~(~P v ~Q) -> R) -> (~R -> (~P v ~Q))
--syllogism-- |- (((P & Q) -> R)   -> (~R -> (~P v ~Q))
Id (*1.01)  = |- (~R -> (~P v ~Q)) -> (~R -> (P -> ~Q))
--syllogism-- |- (((P & Q) -> R)   -> (~R -> (P -> ~Q))
comm        = |- (~R -> (P -> ~Q)) -> (P -> (~R -> ~Q))
--syllogism-- |- (((P & Q) -> R)   -> (P -> (~R -> ~Q))
transP.syll = |- (P -> (~R -> ~Q)) -> (P -> (Q -> R)))
--syllogism-- |- (((P & Q) -> R)   -> (P -> (Q -> R)))


it just makes the proof 4 lines longer and much less whitespace, but if you find it better then do it this way
(ps but even here don't you want to go back to the axioms?
The difference is essentially the same as that which Gottlob Frege discusses in his seminal work http://philo.ruc.edu.cn/logic/reading/On%20sense%20and%20reference.pdf On Sense and Reference. Putnam essentially uses the word meaning in place of Frege's sense, presumably because it seemed more intuitive for Putnam to splinter reference from meaning rather than make meaning a redundant term in our language.

The distinction between meaning and reference is simple: meaning/sense is the intension or the word, name or symbolic representation of an object; the reference is the thing to which the intension corresponds in the world—the reference is the object to which you are referring, if you will.

For instance, if you think of a battery, you have the word battery and you have symbols for batteries, such as:






and yet you also have the battery as a physical object, as an actual battery in the world, an extension. You have that which you mean and your ways of meaning it. The referent and the symbol to refer to it. An interesting parallel between analytic and continental philosophy is Frege's distinction between sense and reference, which is also acknowledged in semiotics as the distinction between signifier and signified.

Putnam differs from Frege in where meaning and reference sit. For Frege and others like him, meaning is a social, communicable sign/symbol and reference is in the world. For Putnam, both meaning and reference are in the world, because reference influences the formation of the symbols and names we use for meaning. Putnam sought to prove his argument with his famous http://en.wikipedia.org/wiki/Twin_Earth_thought_experiment Twin Earth thought experiment.

  But doesn't this assume a somewhat self-righteous perspective which facilitates this conclusion?


Yes, law of identity is an axiom, something that is asked to be considered true without proof. Generally, the only argument provided to support such a statement is its "self-evidence". But not all philosophies agree that identities exists. Probably the most famous saying expressing an other point of view is "No man ever steps in the same river twice", attributed to https://en.wikipedia.org/wiki/Heraclitus Heraclitus.


  Continuing along this line of reasoning, what need is there for The Law of Non-contradiction and the Law of the excluded middle?


You will probably want to document on https://en.wikipedia.org/wiki/Constructivism_%28mathematics%29 constructionism. 

(Please try to put a question in your title and to use the description to specify your request.)
If we can make an equivalence between "Ouch!" and the claim "It is the case that Person X is experiencing pain", then "Ouch!" is a proposition. At the most basic level, a proposition is something that asserts a fact about the world. On a pedantic level, every utterance can be converted to a claim and a proposition. But a question that follows is whether what is said is meant to be propositional in nature.

During the earliest era of the philosophy of language, the answer would be no, "Ouch!" is not a proposition as it does not assert anything about the world. Everything needed to be converted into claims and anything else needed to be thrown out.

It did not take long for philosophers to recognize the problem with this. Ludwig Wittgenstein looks at this question in Philosophical Investigations and spends quite a bit of time trying to unpack precisely what this sort of claim means (or else we just have some arbitrary notes we're mistaking for something).

There's some argument as to what he means by it, but the basic problem is that I feel sentences are not exactly verifiable claims about the world. As one of the other answers suggests, there is a senses in which it is true that I am not trying to make a claim, I am more so reacting to something I am experiencing.

I think a better solution is to recognize that we make many types of utterances, which can bear meaning even if they are not identical to propositions. This is a fundamental benefit from incorporating Speech-Act Theory -- recognizing that some utterances are meant to accomplish things. Expanding on this a bit, there's Herbert Fingarette's Confucius: The Secular As Sacred which looks at how rituals without words form the Confucian tradition can also be meaning-accomplishing. 
If you're asking whether this is a valid argument, with the third line understood as the conclusion,


If something exists then it must adhere to a standard
If it does not adhere to a standard, it will cause suffering.
If it will cause suffering, then it should not exist.


No, it's not. The antecedent of the third line affirms the consequent of the second, and so doesn't have any implications concerning the first line.
This is a deceptively complex question, and very on the nose when it comes to Tractarian interpretation.  My line on this is to say that we need to pay attention to the distinction in the semantics of TLP between "Propositions" (the German "Satz") and "Elementary Propositions" ("Elementarsatz"), and to note the theoretical difficulties in explaining how to cash out the more general notion of Proposition in terms of Truth functions over Elementary Propositions, as in section $5, without sacrificing the Picture Theory approach that makes the semantics of Elementary Propositions seem appealing

For Wittgenstein, although propositional (Elementarsatz, at least) meaning is tied to the existence of states of affairs, how do we explain the meaningfulness of contingently false sentences, such as “It is warmer in Britain than in Australia”?  If there is a state of affairs of objects joined in the way indicated by the sentence, then it is clearly a non-actual one. On the other hand, if there is no such state of affairs, then how do we resolve the reference of the sentence to some other state of affairs so as to preserve sensibility?

We have a choice to make between accepting that non-actual states of affairs can be the subjects of utterance and fleshing out some mechanism by which we speak of the facts of the matter even in error.  Max Black, in his http://books.google.co.uk/books/about/A_Companion_to_Wittgenstein_s_Tractatus.html?id=uzY9AAAAIAAJ “Companion to Wittgenstein's Tractatus”, denotes these two interpretations as Possibility and Fact Theories, respectively, and different philosophers have developed both interpretations of the Tractatus as in keeping with the themes and approaches of the original text (in as much as any interpretation can be in keeping with the TLP).

If we can take it as read that the Truth functions exist as mathematical functions over a Boolean-valued domain, such that there are only two possible truth values that any given proposition might take, then this is a fairly cheap Fact-Theory way to get the additional abstraction over our base level of elementary propositions that we need to speak "falsely about" the facts of the matter.  Elementary propositions describe states of affairs, these states of affairs combine together into "facts" in a way strictly mediated by how complex propositions are structured in boolean functional terms, and the "negative fact" is just a consequence of the negation operation in boolean logic, rather than any kind of complicated alternative possibility.  In the example above, Britain is at least as cold as Australia, and so the falsity of Britain being warmer can be accounted for from the state of affairs describing the relevant climates of the two and the boolean function taking truth to falsity and vice versa.

(In effect, the "possibility" of truth or falsehood in a proposition doesn't commit us to an ontology of alternate universes of states of affairs if we can get this kind of abstract truth function construction going.  We do need such an account, because the more natural way to interpret negative propositions in the picture theory is to point to an inflated ontology with non-actual states of affairs for them to correspond to.  And if we can account for negated propositions in this kind of mathematical way, we get the bivalence of any proposition as part of the package.)

This interpretation is very along lines that Bertrand Russell might have endorsed.  So naturally as Wittgenstein scholars we might want to be skeptical that it's the intended one.  Indeed, there seems reason to suspect that W himself might be interpreting negation some other way.  There is a distinction to be drawn in TLP between "~" negation (the German "Verneinung", or "denial") and the "N(ζ)" "Negation" described in $5.5 in terms of repeated schematic applications of the Sheffer Stroke - the former, if thought of as Boolean negation, is a Semantic conception of negation and the latter Syntactic.  It's the Syntactic version "N(ζ)" that is called upon in $6 to explain the logical form of Truth Functions (rather than the more contemporary tactic of evaluating the definable functions set-theoretically), and hence that informs his conclusion about the limits of what can be logically expressed.

Mathematically, if we think the syntactic conception of negation is the intended one, then as Benubird pointed out above, we don't necessarily thereby commit ourselves to a strictly Boolean logical framework.  For Wittgenstein in the Tractatus, it is entirely possible that problems around infinity and countability hadn't yet come into view (his http://plato.stanford.edu/entries/wittgenstein-mathematics/ Philosophy of Mathematics work would come later and largely miss the massive developments happening in Set Theory at the time) but issues around how exactly to cash out this notion of repeated application of functions and the mathematical disputes around foundations and axioms in maths would be very relevant here.  We could entirely well say something very similar about the formal limitations of assertion using Heyting Algebras or some alternative semantic framework as would be favoured by Intuitionists; in which case, we would arrive at a different conclusion of what possible descriptions of the Facts there might be too.

Given what Wittgenstein says throughout about the use of Truth Tables, and his position in the history of mathematics, it seems reasonable to suppose that he was relying on the technology of boolean logic over finite models.  Since model theory, proof theory and set theory have come a long way since the 1920's, we might be more prepared from our current standpoint to deal with pluralities that Wittgenstein as a non-mathematician may not have been aware he was skipping over.
There are two ways of interpreting it, one in the spirit of the binary operation asking "are these mutually exclusive and exhaustive options", in which case the sentence is


  ... is true when one, and only one, of the n sentences is true.


as Christopher says.

The other is like the result of putting this into a computer. As XOR is both commutative and associative, we can put any statement of the form

A1 xor A2 xor ... xor An


into a cannonical form, putting all the trues first and all the falses last

T xor T xor F xor T xor F


could be written

(((T xor T) xor T) xor F) xor F


so it is only the number of trues and falses that matters. http://en.wikipedia.org/wiki/Exclusive_or XOR can be identified with addition mod 2, with false being 0 and true being 1 . So it is the number of ones (trues) makes a difference to the final result, i.e.

X xor F -> X (nothing changes)
X xor T -> not X (value flips)


so, the truth is determined by whether there is an odd or even number of "trues". Then we have

T xor T is false


so even numbers of trues give false, and the sentence you would be looking for is


  ... is true when the number of true sentences is odd?

Tautologies are logical truths in the context of propositional logic:


  φ is a tautology       =def   φ is assigned ⊤ by all rows of the truth-table for φ.


Logical truths are something more general, and can be defined as follows:


  φ is a logical truth   =def   a true interpretation of the logical constants occurring in φ makes φ true.


Logical consequence is similar to that:


  φ is a logical consequence of ψ   =def   every true interpretation of ψ makes φ true.


Logical necessity is a modal notion, and can be defined using state-descriptions:


  φ is logically necessary   =def    φ is true in all state-descriptions.


All of these definitions are inspired by Carnap, but may differ from his actual definitions.
See https://en.wikipedia.org/wiki/Vacuous_truth Vacuous truth:


  A statement S is "vacuously true" if it resembles the statement P => Q, where P is known to be false.


Be careful! It is the conditional "if A, then B" that is vacuously true; as you said, B can be true or false... A being false, the truth-value of B does not matter, i.e. it does not influence the truth-value of the conditional.

It does not follow that B must be true.
Intuitionistic logic is currently studied... and "it works".

See http://plato.stanford.edu/entries/logic-intuitionistic/ Intuitionistic Logic and http://plato.stanford.edu/entries/intuitionistic-logic-development/ The Development of Intuitionistic Logic.

But also http://plato.stanford.edu/entries/brouwer/ Luitzen Egbertus Jan Brouwer and http://plato.stanford.edu/entries/intuitionism/ Intuitionism in the Philosophy of Mathematics and http://plato.stanford.edu/entries/mathematics-constructive/ Constructive Mathematics.

Some books dedicated to intuitionism :


  Arend Heyting, 1956, Intuitionism: An Introduction, Amsterdam: North-Holland Publishing (3rd revised ed, 1971)
  
  Anne Sjerp Troelstra and Dirk van Dalen, 1988, Constructivism in Mathematics: An Introduction, Amsterdam: North-Holland Publishing
  
  Michael Dummett, 1977, Elements of Intuitionism (Oxford Logic Guides, 39), Oxford: Clarendon Press (2nd edition, 2000)
  
  Grigori Mints, 2000, A Short Introduction to Intuitionistic Logic, KLUWER ACADEMIC PUBLISHERS.


For a "standard" textbook in math log with a chapter about intuitionistic logic, see :


  Dirk van Dalen, Logic and Structure (5th ed - 2013), Chapter 6: Intuitionistic Logic.

So, there is a very facetious answer to your question; it depends what you mean by large cardinal axiom.

For instance 0-sharp is something like a large cardinal axiom without asserting the existence of a cardinal explicitly. It's rather about sets of indiscernibles that code the construction of L, and is very naturally viewed as a (Delta^1)_3 non-constructible real. However, 0-sharp fits very nicely into the large cardinal hierarchy, so is something like a large cardinal axiom; it's a principle of strength.

If we're talking about principles of strength then, there is a very obvious axiom that is inconsistent with ZF, namely 0=1. However, this is obviously not what you're looking for, rather you want an axiom that is in some sense "natural" or "conceivable" or "contentful" (or some other similarly philosophically problematic notion; cans of worms everywhere!), but that is inconsistent with ZF. Hugh Woodin and Peter Koellner have been working on thinking up such a large cardinal notion inconsistent with ZF; the hierarchy is roughly outlined in the Wikipedia article you cite (I would be somewhat careful of that, however; there are a couple of errors in the text). No inconsistency has yet been found, however.
It should be understood, to start, that any answer to this question will be entirely speculative — this is not a question of logic, but of physics, i.e. it is an empirical question (albeit about a phenomenon that no-one has ever knowingly observed and reported). Any answer we provide is merely a proposal which could possibly serve as a principle for the development of physical theory, in any theory of physics which possibly allows for time-travel.
With that being understood — 

The classic paradoxes of time-travel are not ones of logical consistency so much as they are ones of consistency of the historical record. There are different solutions to this, which are so well-worn that they are well-represented, for instance, in science-fiction literature.


Alternative worlds: our history is one of many possible histories, each of which are developing at the same time. Your presence in the past is a parameter which serves to partially describe these histories. When you travel back in time, you may not end up in the same historical line that you started off in: you end up in a subjectively different — but objectively equally real — timeline. Furthermore, if you travelled again in time, there's no particular reason to suppose that you wouldn't ever end up back in your original time-line, so long as your appearance were consistent with the history of the timeline.

This requires that we can posit more worlds than we currently observe, and may prove unsatisfying for this reason. Of course, we haven't developed time-travel, either; and time-travel would be one way that we could attempt to determine the existence of co-evolving divergent historical timelines.
History as a fixed point of time-travel: one can answer your question "is the very simple act of just being in the past contradictory" with another question: how do you know you don't exist in the past already? There is no reason why you couldn't travel to the past, immediately be hit on the head by a falling object, and be a mysterious dead body discovered by the police or a wild animal. Or: you could be a John Doe in an asylum or burned at the stake for making the obviously crazy claim that you're a time-traveller from the future.

Or: you could be wily and fit in to your society — possibly succeeding so well that there is little to no trace of anything strange in your having appeared there. So little trace that no-one suspects; and in particular, you never found out that you were already there in the past, before you decided to travel back in time.

In fact, how can you be sure that you aren't already an ancestor of yours? The chances of your genes mixing with others over several generations to accidentally produce an exact clone are vanishingly small, of course — the key word being "vanishing", as in extremely small to start with and diminishing further over generations, but not actually impossible. Rather than a "grandfather paradox", you become a "grandfather fixed point": history and the chance meeting of your ancestors has developed in just such a way so that your birth and travel backwards in time are logically consistent with a single historical record, and — possibly — so that your later contribution to the genetic pool gives rise to yourself as if by accident.

This may seem dissatisfying because it involves a circular pattern of cause and effect: but again, there is nothing actually logically inconsistent with such phenomena — as with time-travel, we just haven't observed any examples of it, and so have no particular reason yet to believe that they occur.

We can feel comforted by the existence of truths which are not merely subjective because the alternative that there are none is quite uncomfortable. Were there no truths, and nothing other than subjective perceptions, we should find ourselves with at least these two very uncomfortable thoughts:


If when we are harmed by someone else, there were no facts of the matter about what happened, there could be no hope that judges and juries could approach an accurate understanding of what happened. Our understandings of situations are always limited by our perspectives and our limited information, but we can often acquire some further understanding about those limitations themselves. We can acquire knowledge of the ways in which and degrees to which our knowledge is limited. If there are no facts, then it is not coherent to say we are epistemically limited in this way. Rather, each of our subjective impressions is as good as any other. So if a crazy person says I stole his money, that claim is on equal epistemic footing as my claim that I clearly did not. Without the ability to describe claims as better-grounded or worse-grounded, our ability to resolve situations justly, situations where we are harmed or accused, is fundamentally undermined.
Without the sense that we can trust that aspects of the world are stable to some degree, and can be known to some degree in more objective ways, it would be quite scary doing ordinary activities like walking and driving. It would be paralyzing to think that I can't know at all whether the sidewalk on which I'm about to step is going to be solid or liquid or gel or plasma. If it's a merely subjective matter, I have no ways of assessing that I'm probably right that the sidewalk is solid. Similarly, it would be uncomfortable not to be able to trust that there are facts that the engineers of cars and bridges and tunnels have more access to than does a child or a person using LSD. That is, it's frightening to imagine that neither I nor they have beliefs that approach any facts.

1 Let's start with a simpler case first. While it is true that the statement


  (a) "I am moving"


is true at some points in time and false at others, it doesn't mean that given any point t in time we have:


  (b) "I am moving at time t " ∧ "I am not moving at time t ".


Given any time t, either you are moving at time t or you are not. The fact that there are times at which you are moving and that there are times at which you are not moving, does not contradict that fact.

2 Similarly, "I am moving" can be true with respect to some objects in space and false with respect to others. For example, if you're on a train, you're moving with respect to the external environment of the train, but are stationary with respect to the person sitting next to you. It doesn't follow from this that: 


  (c) "I am moving with respect to an object o " ∧ "I am not moving with respect to that o ".


Given any object o in space, either you are moving with respect to o or you are not. The fact that there are objects (e.g. the Sun) with respect to which you are moving and that there are objects (e.g. the Earth) with respect to which you are not, does not contradict that fact.

3 The impression that you have that there is some trick being played here might be caused by the fact that the context of utterance is often implicit. The context determines things such as the time and place of the utterance, the person uttering the sentence, and so on. Once the context is made explicit, it's possible to make formally precise the idea that the same sentence can have different truth-values depending on different context-sensitive factors, such as the time of utterance and so on. 
Options 1 to 3 are based on a fundamental misconception, since physical theories cannot be proven mathematically for correctness. At best, they can be proven mathematically as self-consistent. A "good" physical theory provides predictions which could be falsified by appropriate experiments, if one of the assumptions (axioms) is inconsistent with nature.

We know, that the two most successful theories, general relativity, and quantum theory, aren't  compatible.
A mathematically consistent theory covering quantum theory and general relativity in all aspects regarding consistency with experiments and observation, and predicting correctly yet unknown phenomena neither of the two generally accepted theories is able to predict correctly, and describing all known cosmological parameters, and constants of nature, would be accepted as an intermediate theory of everything.
If it also predicts the results of all experiments and observations to be carried out in future, it would become the canon, as long as no contradicting observation is made.

For this, it would be necessary, to show, that the axioms of quantum theory, and the axioms of general relativity occur as approximations of the new theory for parameters which have been confirmed experimentally thus far.

Hence option 4 of the options gets closest to what can be expected.

Consistency and completeness at the same time aren't necessarily ruled out by Gödel's incompleteness theorem, since the theorem just applies to sufficiently complex formal systems, like arithmetics, or set theory. Any finite structure, as an example, can be complete and consistent at the same time.

A "final theory" would be unexpectedly simple, and obviously not possible to be simplified even more. It would be capable to predict phenomena, which wouldn't be expected to be predictable by such a simple theory, such that incidential correctness would be exceedingly unlikely.

But even then "they" wouldn't know for absolutely sure.
This fallacy is called wishful thinking. To give a more formal definition, the http://www.fallacyfiles.org/wishthnk.html "fallacy files" defines it as follows:


  I want P to be true.
  
  Therefore, P is true.


Notice also the two exceptions where a valid form of reasoning can look like wishful thinking at the link. Along the same lines, if you are discussing a practical syllogism, it might matter that the conclusion would make you happy such as the following form:


  If I were married to Sally then I would be happy, so I should marry Sally.


In such a case, the form is not fallacious because happiness is material to the conclusion of what you should do. Of course, it may be wishful to think that marrying Sally would produce that effect, but that's not quite the same thing as this fallacy -- that may just be folly.

The fact that you can't apply concept of emptiness to the object doesn't make object non-empty. It could be just irrelevant to the concept.
This is weird treatment of concept of emptiness either. Emptiness (in the most general definition) is the absence of (usually false) knowledge in the true knowledge. And in particular, absence of atman in the dharmas (skandhas, etc).

One of justifications of some dharmas being empty is that they are dependently originated. But some other dharmas are still empty without being dependently originated, like nirodha or tathata.
So one supposedly non-empty thing is atman, but it doesn't exists, as it's false concept.

The reason why argument by analogy could be called invalid hinges on a technical definition in formal logic. Viz., "invalid" means not attaining to formal validity either in sentential logic or one of the many types that depends on it (e.g. deontic logic, modal logic).Thus, the following argument is invalid:

(1) If Japan did not exist, we would not have hello Kitty.
Ergo, (2) the earth orbits the sun.


The conclusion is true. The premise is true. But the argument is not valid.

A second example:

(1) If the earth orbits the sun, then there are aliens living in my basement. 
(2) the earth orbits the sun  
Therefore, they are aliens living in my basement.


This is valid. But one of the premises (i.e. (1)) and the conclusion are false.

Arguments by analogy cannot be valid. Instead, they can be strong or weak depending on how convincing they are. The same is true of inductive arguments.

The distinction has to do with what an argument can accomplish. A valid deductive argument is "truth-preserving" meaning that if its premises are true, its conclusion is necessarily true. A strong inductive argument is not truth-preserving, it just more or less probably true. Thus, the belief (seen here as a conclusion reached on looking at a large amount of evidence) that there are only white swans had a good degree of support but its conclusion turned out to be false. It didn't mean the argument for the claim was weak -- just that it was a type of argument that is always defeasible.

Invalid is not a bad thing for inductive and analogy arguments. It's just part of the nature of the beast.
If this were an argument, and not a request, we would call it http://www.logicallyfallacious.com/index.php/logical-fallacies/130-moving-the-goal-posts moving the goalposts, which is where the original conditions are met, but additional ones are subsequently added.
The first question a mathematician looking at a new concept is not to ask whether it exists but how it proves itself useful. The differential calculus made it easy to write down the equation of motion of Newtonian Physics and Descartes coordinate geometry made it possible to formulate the same equations in a natural way.

This might be called the http://en.wikipedia.org/wiki/Pragmatism pragmatic answer, in the sense we think of it as a tool to predict, describe or solve; or a http://en.wikipedia.org/wiki/Coherentism coherent answer, which rejects the (metaphysical) reduction to a foundation, (philosophically this view is called http://en.wikipedia.org/wiki/Foundationalism foundatinalism) to a set of beliefs (axioms) that are secure or justified, and is non-inferential ie axioms. 

So how do sets prove themselves useful in mathematics. This is connected with http://en.wikipedia.org/wiki/Hilbert%27s_program Hilberts programme to reduce mathematics to logic, and one is lead to this thought when one considers that sets are aligned with logic, consider http://en.wikipedia.org/wiki/Venn_diagram Venn Diagrams and observations that all the integers for a set and so on. 

A different approach, called http://en.wikipedia.org/wiki/Platonism Platonism asserts the existence of any set of axioms that is internally consistent. This draws upon Platos theory of Forms which gives existence to pure ideas. This is essentially a metaphysical supposition, and evidence of such  a position can only be given in the most indirect kind of way. 

Another position that came out of the early work on mathematical logic is http://en.wikipedia.org/wiki/Formalism_%28mathematics%29 Formalism which drops any need to ground a theory in some Platonic world and instead observes that  consistency is all that is important when it comes to manipulating axioms and theorems. This is actually grounded in logic, and observes that a mathematical theory can be seen as simply as a set of strings that are rewritten according to some set of rules.

Finally, the analytic or positivist approach that Rostomyan mentions with regards to Carnap, and which came out of a philosophy of science in Vienna,  also dispenses with metaphysics by rendering them 'meaningless' - the metaphysical voice is silenced. 


  A standard set-theoretic way of defining the natural numbers[1] 1,2,3,... is based on the empty set


Note that we can start from any other set, the set of cups for example: Define 0:=Cups, 1:={0}, 2:={1} etc. Of course the void set was chosen for two reasons, it is a given, and also it is the minimal set that is included in any other set. Another point to notice is that we can say 2 is a member of 3 which doesn't make much sense. Finally note we can just define the natural numbers as a http://en.wikipedia.org/wiki/Ring_%28mathematics%29 ring with certain arithmetic properties or via the http://en.wikipedia.org/wiki/Peano_axioms Peano Axioms.

As one notes from this, there are quite a few distinct positions and which position one adopts is most likely a function of ones own inclination as the schools of thought one is aligned with.
Seems like the "straw man" to me. It's attacking a position that nobody is actually defending. 

Edit: In response to a comment below. It's worth noting that what S is encouraging his or her listeners to do is to commit a http://en.wikipedia.org/wiki/Use%E2%80%93mention_distinction use-mention error.
What you call "Positivism" there is but a crude description of a principle that has been associated with them. Let's consider a more faithful description of the so-called empiricist criterion of meaningfulness:


  Criterion. (Hempel 1965b) A sentence makes a cognitively significant assertion, and thus can be said to be either true or false, if and only if either:
  
  
  it is analytic or contradictory, or
  it is capable, at least potentially, of test by experiential evidence.
  


In the first case, the sentence is said to have a purely logical meaning; in the second case—an empirical meaning or significance. Hempel calls this principle the testability criterion of empirical meaningfulness, and ascribes it not only to empiricism, but also to operationalism and pragmatism (appropriately or not isn't the topic of our discussion here). The criticism as applied to Criterion would be


  Criticism. (Graviton) Criterion is self-refuting, because it cannot be empirically verified.


Of course, Hempel's version wasn't the one being criticized. I welcome Graviton to modify the critique to this particular case, if it too suffers from the same flaw. Until then, we should start by noting that the Criterion is of a disjunctive form: either condition (1) is met or (2). It is obvious that Criterion, like the claim "0 ≠ 1", is incapable, even potentially, of being tested. The arithmetical claim is analytic (logically true with respect to Peano's Axioms, say). But so is Criterion:


  Defense. Criterion is analytic, and is thus: meaningful according to Criterion.  


Neither is capable of being tested. You can reject Peano's Axioms so that you won't have to accept that 0 ≠ 1. You can also reject Criterion so that sentences like "John's aura is powerful" don't become cognitively insignificant. Tolerance (another positivist 'doctrine') is the key here. Consider the definition: 


  Criterion 2. A binary relation R is called 'symmetric' if and only if for all x, y, xRy implies yRx.


Is Criterion 2 meaningful? Yes. Is it capable of empirical testing? No. Is it analytically true? Yes. It is, of course, possible to say that definitions like Criterion and Criterion 2 are not the sorts of things that can be true or false, but I'm hesitating to commit to that stronger view of definitions to keep things simple.

The topic has spawned a lot of literature over the last (half a) century. It would be good news to some if positivism was dead, for then, there would arise the opportunity to skip reading stuff from Russell, Wittgenstein, Carnap, Ayer, Hempel, Reichenbach, Menger, Hahn, and others on all sorts of interesting topics. But if we're going to bury positivism, the least we can do is shove the right body into the coffin.

                                                                     References

Lewis, D. (1988) "Ayer's First Empiricist Criterion of Meaning: Why Does it Fail?"
Hempel, C.G. (1965a) Scientific Explanation: Essays in the Philosophy of Science.
Hempel, C.G. (1965b) "http://www.contrib.andrew.cmu.edu/~kk3n/80-300/hempel-empcrit.pdf Empiricist Criteria of Cognitive Significance: Problems and Changes".
All physical measurements in the real world are approximate. To the extent that calculus is used to model physical phenomena, it is an approximation to reality [if one believes in an underlying reality at all ... a separate question.]

However, within the scope of mathematics, calculus is exact. That's because calculus is based on a logically rigorous theory of the real numbers, which are based on a logically rigorous account of set theory.

That is, we can start from the Zermelo-Fraenkel axioms of set theory, published in the early 1920s, and we can then develop the theory of the real numbers and calculus from those axioms. In that respect, calculus is an exact theory of some imaginary or fictional entities; since (as far as we know) there are no real numbers in the physical universe.

So, you are entirely correct that calculus is only an approximation to the real world, whatever that may be. But within the domain of pure mathematics, calculus is exact. That's because the real numbers and sets in general are only abstract mathematical entities. They do not necessarily have any analog in the physical world. 
There's a problem in your translation. Let's forget about the quantifiers for the bit, and ask how to translate 'if a is negative and b is negative then a x b is positive'.

You have something like:


  Na & Nb & Tab → ¬Nc


But where does c come from? We want to say that c is the result of multiplying a and b, and Tab doesn't tell us this.

Instead, we need a ternary relation to say 'a x b is c', let's call that 'Tabc'.

Then, we get as the translation of 'if a is negative and b is negative then a x b is positive' the following:


  ∀x(Na & Nb & Tabx → ¬Nx)


(Why do we need the quantifier? Because, for all we've said about T, it might be that more than one number is the result of multiplying a and b. Ideally, we'd want to write down some more sentences which tell us that there's always exactly one answer.)

Then to translate that the product of any negative numbers is positive, we do:


  (1) ∀x∀y∀z(Nx & Ny & Txyz → ¬Nz)


A functional approach

Alternatively, we can consider versions of logic which feature function symbols.  A n-ary function symbol f (compared with an n-ary predicate) is such that, whenever a1,...,an are terms (e.g. variables or constants) then f(a1,...,an) is a term as well.

It's then very natural to translate your sentence into a language with a function for multiplication. We let T be a binary function so that 'T(x,y)' means 'x times y' (in fact, given this, we can straightforwardly abbreviate this as x x y).

Then the translation of the sentence would be much simpler:


  (2) ∀x ∀y( Nx & Ny → ¬N(T(x,y)))


Proving these

Now, how would we prove either of these? The answer is, we can't, not if we're trying to do a proof in pure logic, as you are trying to. That's because the fact that (1) and (2) are true is not a matter of pure logic – that is, it is not a matter purely of the form of the sentences. Instead, it depends on the details of what 'times' and 'negative' mean. (Suppose, for example, that 'N' meant 'is negative', but 'T' meant addition. Then (1) and (2) would be false.)

In order to prove (1) and (2), we need additional facts about negativeness and multiplication, probably in the form of axioms. These will tell us what the properties of these predicates and functions are. We would then use these axioms as additional premises in the proof.

We could try to give some such axioms directly, but it probably wouldn't be very interesting. The reason is that, if we want to give some very basic principles about how these interact, the chances are that something like (1) or (2) would appear in our axioms, so the proof would be a boring one liner.

A more interesting proof – and one which is much closer to how it would actually be done – would be to select some axioms for arithmetic in general, such as the [http://en.wikipedia.org/wiki/Peano_axioms][Peano http://en.wikipedia.org/wiki/Peano_axioms][Peano Axioms].* From these, we then define 'negative' and 'times' [but see edit below], and then prove (1) or (2) (depending on whether we've defined 'times' as a predicate or a function).

.* Actually, the Peano Axioms won't really do without a lot of additional work. The reason is that they're only about positive numbers, but we obviously want to say something about negative numbers. We could either move towards a set of axioms which also tells us about negative numbers, or we could do some tricks to define negative numbers within the positive numbers (this sounds weird, but it can be done in strange ways). 

EDIT: Rereading this old-ish answer of mine, I notice I've made a mistake. In particular, in the (first-order) Peano axioms we can't define multiplication. Instead, we need to take multiplication and primitive together with some corresponding axioms. In second-order Peano arithmetic, we can, however, define multiplication as repeated addition.
Are all true mathematical statements provable?

An interesting look for Gödel Theorem.... Check :

http://www.youtube.com/watch?v=Q_q_DtxaMYg&index=21&list=PLIljB45xT85Bfc-S4WHvTIM7E-ir3nAOf  Are all true mathematical statements provable?
We can be pretty certain of tautologies, which are axioms of logic based on the principle of non-contradiction. After all, 'cogito ergo sum' already presupposes that something must exist in order to think, and presupposes a notion of 'ergo', both of which are (to Descartes at least) at least as certain as the full phrase itself. 

http://en.wikipedia.org/wiki/Logical_truth Necessary truths, which are defined either as being true in all possible worlds, or by their falsification leading to contradictions (both of those definitions are essentially the same) are also certain truths, because no matter how we may imagine a universe to look, it cannot sustain contradictions. Therefore, the classic example 'all bachelors are unmarried', is a necessary truth, because the definition of a bachelor is someone unmarried. For this statement to be false, a man would have to be both married and unmarried - an impossibility. Hence, this statement cannot be false and therefore must be true. (I think. See https://philosophy.stackexchange.com/questions/14373/the-truth-falsehood-dichotomy-and-logic here and https://philosophy.stackexchange.com/questions/14376/belnaps-four-valued-relevance-logic here)

In addition, like logical proofs, the results of mathematical proofs are similarly trustworthy, so to speak. If a rectangle is defined as a four sided polygon whose sides interact at right angles, and we prove that a polygon has these properties, it must necessarily be a square. The same idea (https://philosophy.stackexchange.com/questions/14388/are-mathematical-statements-necessary-truths I think) applies to numbers. 

Some may argue these points, but some argue on Descartes' argument as well. It seems like these are some things that we can be at least as sure of as we may be of 'cogito ergo sum'. 

  Is it a paradox to assert "1+1=2 is true, but I don't know it"?


Assuming that


p is "1+1=2 is true",
"it" refers to p,
your statement is a compound statement A & B,
A and B are of the form: propositional attitude + p,


then the crucial question is: What is the http://en.wikipedia.org/wiki/Propositional_attitude propositional attitude wrt p in A?

1) If we assume that it is "I know that p", then your conjunctive statement is stating that 


  I know that 1+1=2 is true & I don't know that 1+1=2 is true


which is, obviously, a contradiction. (Or, there are two different meaning of "knowing" involved.)

2) If we assume that it is "I assert that p", then your conjunctive statement is stating that


  I assert that 1+1=2 is true & I don't know that 1+1=2 is true


by which you say that p in A is true, but that you can't fulfill some further condition required to actually know that p (e.g. p is true but lacks justification). No contradiction here.
The paradox that you reference is the Liar Paradox. LP has a number of famous variants. The one you cite is the simplest. To learn more about the paradox and attempts to resolve it, go to: http://plato.stanford.edu/entries/liar-paradox/ http://plato.stanford.edu/entries/liar-paradox/ It covers the paradox in great detail. Separately, if you need to understand bivalence, check out: http://en.wikipedia.org/wiki/Principle_of_bivalence http://en.wikipedia.org/wiki/Principle_of_bivalence
On the issue of Euclidean Arithmetic, see by Ian Mueller, http://rads.stackoverflow.com/amzn/click/0486453006 Philosophy of Mathematics and Deductive Structure in Euclid's Elements (1981 - Dover reprint).

All Ch.2 is devoted to this topic; see page 58 :


  In books VII-IX Euclid develops the subject of arithmetic in almost complete isolation from the remainder of the Elements. [...] [In contrast to previous books, we] find no specifically arithmetic postulates in the Elements.


We have definitions regarding numbers in Book VII [see http://aleph0.clarku.edu/~djoyce/java/elements/bookVII/bookVII.html#defs Euclid's Elements] : 


  Book VII : Definitions
  
  Definition 1 : A unit is that by virtue of which each of the things that exist is called one. 
  
  Definition 2 : A number is a multitude composed of units. 
  
  Definition 3 : A number is a part of a number, the less of the greater, when it measures the greater; 
  
  Definition 4 : But parts when it does not measure it. 
  
  Definition 5 : The greater number is a multiple of the less when it is measured by the less. [...]


But there ara no Axioms; see Mueller, page 59 :


  Euclid does not prove that 2+2=4 or that a 2 and a 2 combined yield a 4, nor does he even have the apparatus for doing so. Such facts, insofar as they are used in the Elements, are used without proof.

The intuitive version (that seems justified to me) is a statement of pragmatism, not truth: if A and B explain things equally well, and A is simpler, why would I bother with the extra headache of B?

I think there's a truthier version that is entangled with http://en.wikipedia.org/wiki/Kolmogorov_complexity Kolmogorov complexity and http://plato.stanford.edu/entries/dynamic-semantics/ dynamic semantics in deep ways.  I've never seen anything approaching a proof of this, but the intuition is that although it is easy to write a true statement, writing a true statement that conveys a lot of information (in the Kolmogorov sense) is very difficult.  Almost everything you try to say will either have very little content ("My name is not Joe") or will be wrong ("My name is Fred") or will not take advantage of context and thus be impossibly bulky ("My name is Matt, where by 'name' I mean that verbal utterance and corresponding written string of symbols by which I am commonly referred, in contrast to the legal name on my birth certificate...").

Occam's Razor is then a statement of three things: first, how special it is to find a compact description of anything; second, that we have to a large extent organized language to match causally-separable or independent processes; and third, that we observe that very often there is a single proximal causal process rather than an indecipherable muddle that gives rise to recognizable patterns.  With these three together, you have reasonable hope that if you find one of these rare compact but effective descriptions, you're really onto something.

Even so it's just a rule of thumb, but I think it's a deeply and subtly true rule of thumb.
We're given two premises:


A;
(A ∧ B) ∨ (C ∧ D).


The goal is to assume (1-2) and derive: ¬(C ∧ D) → (A ∧ B). I should start by saying that premise (1) is useless and in what follows we'll simply use premise (2). Here is one strategy you can take. It's pretty system-neutral, so try to implement it in your particular proof system and feel free to ask for further guidance if you're not completely sure how to proceed. Hope you find it helpful and good luck.


  Proof. Assume ¬(C ∧ D). Our goal is to get (A ∧ B). Premise (2) tells us that either (A ∧ B) is true or (C ∧ D) is, so we proceed to http://en.wikipedia.org/wiki/Proof_by_exhaustion prove by cases. Assuming the first case: (A ∧ B) immediately gives us (A ∧ B). Assuming (C ∧ D) contradicts our initial assumption  ¬(C ∧ D), so we get a contradiction (symbolically: ⊥). Then we eliminate the contradiction, obtaining (A ∧ B). This elimination step is warranted by the fact that in classical logic, http://en.wikipedia.org/wiki/Principle_of_explosion from a contradiction anything follows. Since both disjuncts of premise (2) lead to (A ∧ B), we conclude, by disjunction-elimination (aka proof by cases) that (A ∧ B) is true. Since having assumed ¬(C ∧ D) we were able to derive (A ∧ B), we conclude by ∧-introduction (aka http://en.wikipedia.org/wiki/Direct_proof direct proof) that ¬(C ∧ D) → (A ∧ B).


  Alfred Tarski asserts that a satisfactory definition of truth must be both formally correct and materially adequate. 


I can't speak for philosophical logic, but for mathematical logic, Tarskis  assertion became a definition; in the sense that these are good requirements to have for a mathematically tractable definition of truth; that this is a good definition (mathematically) is shown by the consequences of his definitions and theory; and this entailed the development of Model Theory. 

Here, Godel showed in his http://en.wikipedia.org/wiki/G%C3%B6del's_completeness_theorem completeness theorem that:


  is a fundamental theorem in mathematical logic...[it] establishes a correspondence between semantic truth and syntactic provability in first-order logic. 


That is a proposition p is syntactically true, that is derivable by the laws of inference if and only if p is also semantically true, that is satisfied by its model(s).

The forward direction is called completeness & the backwards direction soundness - in mathematical logic at least.

To give a simple example of this, take the theory of (elementary) Euclidean Geometry (EG), and first-order logic, and modus tollens as the law of inference. Then every arithmetic theorem that one can prove formally by the rules of logic, inference and the axioms of PA, can be shown to be satisfied by  the actual plane; and the converse also holds.
Bivalence and supertruth

Yes, clearly a supervaluationist makes a distinction between the truth of a particular precisification and the supertruth of a statement true for all possible precisifications, which on the face of it could imply multiple truth values rather than the pure true/false dichotomy of bivalence.

If we take for example the statement "Andrew loves maths", this is clearly not an example of a supertruth because we can make this statement false or true by making it precise in different ways, whereas "Andrew is a banana or is not a banana" is supertrue since its truth is not affected by how it is made precise.

However, you can interpret a supertruth as a collection of true precisification rather than as something which is "more" true than an individual precisification - there's no need to introduce a multi-valued logic to use this concept. 

[Conversely, you could also think of supervaluation as using the set of true precisification of a statement to determine a partial order on statements (assuming you fix a particular language), and the maximum element is the supertrue one, and the minimum element is the superfalse one. This can be viewed as consistent with a boolean algebra with more than two truth values, but it's not necessary to interpret it this way.]

Some logicians would require that any statement be completely disambiguated before assigning a truth value to it, so could argue that none of these had a truth value, which links nicely with my next point about borderline cases and bivalence.

Borderline cases and bivalence

The supervaluationist claims that borderline cases have neither truth nor falsehood, precisely because by definition they are intrinsically undecidable. You can argue that this is inconsistent with bivalence by saying that you now have three truth values: true, false and indeterminate, but a supervaluationist could counter that indeterminacy was exactly an absence of a truth value, not a third truth value.

Correspondence and bivalence

Correspondence may indeed seem to presuppose bivalence. Classical logic definitely presupposed bivalence, but nevertheless it was found that more generally any boolean algebra would do. 

The correspondence between a statement and reality can be seen as a relation in the mathematical sense, with a pair of statement and fact being in the relation or not in a binary yes/no way, but there's no inherent problem with marrying correspondence to a non-bivalent logic.

For example, we could assign a number between zero and one to indicate the extent to which a statement corresponds to a fact in reality; it's possible to argue that bivalence is not implied by correspondence, regardless of whether any protagonists have seen the two as part of a single theory of truth.

Indeterminacy, borderline cases and correspondence

There's a difference between unclear to a given person at a particular time and intrinsically inquiry-resistant. (Interestingly, quantum mechanics asserts that some things are unknowable, and  Godel's incompleteness theorem implies that some things are undecidable, but note that neither is the result of multiply-interpretable statements.) 

Indeed, as you note, in practice many things aren't known at the time, and many things an individual such as you or I "know" are merely learned and classed as fact rather than determined as fact. It's even possible to argue that nothing can be known, but this contradicts neither correspondence nor supervaluation. It's consistent to assert that the correspondence relation exists without demonstrating or knowing any of it.

In fact, borderline cases could be described as the cases where a statement cannot be classified as either in or out of the correspondence between true statements and facts, because of its inherent vagueness of language. This is an entirely different issue to whether a given non-vague statement is knowable as true or false.

The fact that it doesn't make sense to say whether "yellow < 7" or not doesn't spoil the fact that we can compare any two real numbers with "<". Similarly, a supervaluationist rejects some statements as truth-valueless but needn't feel that this spoils correspondence theory. To quote Dr Vassili Corbas (very much out of context): "It's worse than wrong, it's meaningless!". A supervaluationist may take this view of borderline cases - they're external to the theory rather than contradictory to it.

Summary


A supervaluationist could happily accept that they know few of correspondences with fact in the actual world (they are as yet indeterminate in practice), without admitting them as borderline cases (inherently indeterminate). 
They could happily see the existence of inherently indeterminate statements not as a rejection of correspondence theory, but rather as an externality to the correspondence relation itself. 
They could retain bivalence by asserting that lack of a truth value is by definition not itself a truth value and also asserting that a supertruth is a collection of truths rather than a greater truth. 
Alternatively they could reject bivalence and retain correspondence by using multiple truth values associating statements to facts.


Finally, it's worth noting that you can see supervaluation as your favourite valid, purely formal logical system without asserting anything whatsoever about its applicability to life!
This is undeniably difficult.  The section at 4.1212 onwards is where he gives his take on the internal/external relations doctrine. The holding of internal relations cannot be asserted by propositions, but rather shows itself in the propositions (in den Saetzen), by an internal property of the proposition which presents a state of affairs. 
 A property is internal if it is unthinkable that its object does not possess it (4.123). 

He also says (4.121) that propositions cannot represent logical form, which mirrors iself in propositions. “That which mirrors itself in language, language cannot represent”. A proposition shows the logical form of reality, or exhibits it (er weist sie auf).

So in this sense, what can be shown (logical form, internal structure etc) cannot be said.

Passing to “The limits of my language mean the limits of my world”.  I imagine he means, consistent with what he says above, is that whatever falls within the limits of our language includes all that can be said. What can be shown, however lies outside our language, or perhaps ‘at the limit’. 

Note also his remark later on at 5.6331 about the form of the visual field – hard to explain without his diagram, which shows the eye and the visual field on the page itself. He means that the eye itself would never appear in the visual field. “The subject does not belong to the world but it is a limit of the world”. 

So in summary, “What can be shown, cannot be said” and “The limits of my language mean the limits of my world” are not necessarily contradictory, if ‘what can be shown’ is at the limits of language and the world. 

Note also, if what can be known is limited to what is said by a proposition, rather than what is shown, then there are things that can be shown, but which we cannot be said to know. If that is the case, his first statement does not imply there are things that we can know (and be shown), since what is shown is unknowable, in the sense we cannot say what it is, in a proposition of the form 'S knows that p'.
This question in the philosophy of logic is studied as the nature of the relation of Logical Consequence http://plato.stanford.edu/entries/logical-consequence/ (SEP Article).  Broadly speaking, a construction relating a number of premises to a conclusion is a valid argument if and only if for every interpretation in which the premises hold, the conclusion/s hold.  But what kinds of interpretation are we interested in?  Perhaps an argument that seems logically valid to one person who only accepts a certain model of acceptable interpretations might not to someone who accepts a more narrow delineation of the possible ways things might be.  (perhaps some of our core concepts are Intensional ones, which invites a much more complex connection between how things are and how things might be in our interpretive understanding)

Formal work tries to use mathematical fields like Model theory and Proof theory to help explore how we might split up our domains and our arguments in a structured way so as to help explore what a logical consequence might look like.  Intuitively we do want to use certain connectives with well-established behaviour (like And, Or, Not, All, Some etc.) to help standardise some of what we do in fact appeal to in our practice of logical reasoning and inference.  The mathematics of these connectives can be very informative as to what makes them tick, but this is more of a scaffolding than an explanation of what, exactly, you might mean in a given validity claim.
Start with "intuition" :


  if there is a black cat, then there is a cat


is an argument that "sound" quite correct.

To prove it "semantically" we have to assume an interpretation I in which the premise is true.

This means that there is an object o of the domain of the interpretation I for which both F and G holds.

But this implies that for that object o the "property" F holds, and this boils down to say that in I there exists an x such that F(x) is true.

Thus, being I an interpretation whatever, we have proved it for all interpretations; i.e. in every interpretation in which the premise is true, also the conclusion is, and this is the definition of logical consequence :


  A ⊨ B.




Note

In proving it by contradiction, we have to assume that the conclusion ∃xFx is false; this amount to saying that for every object x in the domain D of the interpretation, we have that F does not hold for it. 

But this is in contradiction with the fact implied by the assumption that there is o ∈ D such that F holds for it.
You can see my answer to this https://philosophy.stackexchange.com/questions/10350/is-the-real-number-line-actually-real-when-we-construct-it post and the relevant quotation from :


Sara Negri & Jan von Plato, http://rads.stackoverflow.com/amzn/click/0521068428 Structural Proof Theory (2001), page 26 :



  Classical logic contains the principle of indirect proof: If ¬A leads to a contradiction, A can be inferred. Axiomatically expressed, this principle is contained in the law of double negation, ¬¬A → A. The law of excluded middle, A ∨ ¬A, is a somewhat stronger way of expressing the same principle.
  
  Under the constructive interpretation, the law of excluded middle is not an empty "tautology," but expresses the decidability of proposition A. Similarly, a direct proof of an existential proposition ∃xA consists of a proof of A for some ["witness"] a. Classically, we can prove existence indirectly by assuming that there is no x such that A, then deriving a contradiction, and concluding that such an x exists. Here the classical law of double negation is used for deriving ∃xA from ¬¬∃xA.


Thus, it is correct to say that, from a constructivist point of view, tertium non datur [i.e. excluded middle] does not apply in general. 

Its application to existence proofs impies that the existence of a witness of A is undecided/unproven until we are not able to "show it".
For the abstractionist, possible worlds don't (necessarily) have the properties they represent certain possibilities as having--- they merely represent.

If you're a concretist, like David Lewis, you can claim that a possible world represents a certain possibility by simply being that possibility. So, to represent itself as being concrete the world just needs to be concrete. Similarly, if a world is to represent the possibility of a talking donkey, it will do so by containing a talking donkey as a part.

For the abstractionist all possible worlds--- even the actual world ---are abstract. They represent possibilities in different ways. David Lewis lays out three different types of abstractionism (he terms the view "ersatzism" and the worlds "ersatz worlds"; that terminology has stuck in the literature, imo) based on how the worlds represent possibilities (See On The Plurality of Worlds, ch. 3).


Linguistic Ersatzism: This is the version defended by Robert M. Adams (will have to dig for a citation later) and holds that possible worlds represent by saying a possibility is a certain way. One version of this view holds that possible worlds are maximal consistent sets of propositions, completely specifying some way the world might be. The actual world is represented by a maximal consistent set of propositions describing the way the world actually is.
Pictorial Ersatzism: Possible worlds represent by isomorphism, like a picture or map. A map of the USA represents the USA in virtue of depicting 50 states in the right spatial arrangement, etc. This is how possible worlds represent, they're like pictures.
"Magical" Ersatzism: Possible worlds represent by magic. This is what Lewis calls views, like the property view of Robert Stalnaker, that don't explain--- and, Lewis claims, leave mysterious ---how worlds represent.


The point here is that whereas a concretist can identify the actual world with this concrete thing we inhabit, an abstractionist will generally (always?) have an abstract surrogate for even the actual world. None of the abstractionist's worlds are (or could be) concrete, though many, if not all, represent possibilities involving concrete objects. Remember that not even "the actual world" is concrete for an abstractionist--- though it certainly represents a concrete possibility (ours!).

TL;DR
Your argument contains a false assumption: abstract worlds could not possibly be concrete, they just represent possibilities involving concrete individuals.
This is the closest I've been able to find in St. Thomas, where he discusses whether truth is immutable (http://dhspriory.org/thomas/summa/FP/FP016.html#FPQ16A8THEP1 Summa Theologica I q. 16 a. 8 ad 3):


  A proposition not only has truth, as other things are said to have it, insofar, that is, as they correspond to that which is the design of the divine intellect concerning them; but is said to have truth in a special way, insofar as it indicates the truth of the intellect, which consists in the conformity of the intellect with a thing. When this disappears, the truth of an opinion changes, and consequently the truth of the proposition. So therefore this proposition, "Socrates sits," is true, as long as he is sitting, both with the truth of the thing, insofar as the expression is significative, and with the truth of signification, insofar as it signifies a true opinion. When Socrates rises, the first truth remains, but the second is changed.


And from his http://dhspriory.org/thomas/QDdeVer1.htm#6 Quæstiones Disputate De Veritate a. 6, on whether truth is immutable:


  The truth of a thing is the cause of the truth of a proposition, for a statement is said to be true or false insofar as a thing exists or does not exist.


and 


  it is essential to a proposition that it signify that which it has been made to signify.


Thus, according to St. Thomas, the liar's paradox "propositions" are not real propositions at all because there is no corresponding thing for them that "exists or does not exist" and they do not "signify that which" they have "been made to signify."
I think that in some situations it is a fallacy to think too logically, particularly when human relations are involved.
I would give the following reasons for not thinking too logically:


you might fail to see that the situation is too complex for you to grasp. Logic applies well with certainty, i.e. when working on certain premisses to derive a certain conclusion. In real life, bayesian reasonning would be more appropriate to deal with uncertainty but sometimes you can't even evaluate degrees of certainty, let alone take into account some crucial parameters whose existence you don't even imagine. Using logic could be profundly misguided. Trusting others could be the most rational move.
logic does not tell you what you ought to do. At best it tells you how to achieve something you ought to do. You might sometimes pursue a wrong goal just for the sake of logic, because this goal is amenable to quantification and logical reasonning- here is a somehow stereotyped example: you might decide to earn money at the expanse of a happier life because money is amenable to utility decision while happiness is difficult to evaluate, if possible at all. In other words: when you have a hammer in hand, everything looks like a nail. It can be rational, sometimes, to try different tools...


For these reasons, it might be rational in some situations to rely not on pure reasoning, but also on the built-in adaptative system on which your higher-level abilities rest, i.e. your intuitions and emotions.
It seems to me that this is particularly true in social situations, because the situations are usually complex, and because "what you ought to do" in life might be an irreducibly social matter in the end.
The word “contingent” in broader usage means subject to chance or depending on the way things are.

However, philosophers use the word “contingent” and “necessary” specifically to describe ways of being or existing (for both things and properties). Something exists contingently if and only if it does not exist necessarily. Something exists necessarily if and only if it must exist—if it cannot fail to exist. Something necessarily has a property if and only if it must have that property and cannot fail to. So, in contrast, something exists contingently if and only if it can fail to exist, under some circumstances. Philosophers say of such things that they are contingent. The term implies that the things existing or failing to exist depends on other things, on circumstances, or the like.

If nothing is contingent, then everything is necessary, meaning that everything that exists must necessarily exist and could not have failed to exist. This is equivalent to the thesis called “determinism.” (In turn the thesis of determinism is often thought to have the implication that actual free choices are impossible—that there is, that is to say, no free will.) If you are interested in learning more about Determinism, you might try http://plato.stanford.edu/entries/determinism-causal/ the SEP article on causal determinism. 
Chris is right, of course, about the fact that infinitely many conclusions can be drawn from those premises (provided that there is at least one applicable inference rule). What I want to do here is to simplify one of the sentences to reveal an obvious conclusion that can be drawn from them.

Given that F is false, (1) is equivalent to (W → M). Here is one way of showing that:


(01) (W → (F ∨ M))
(02) ¬F
(03) | W
(04) | (F ∨ M) (by →-elimination from 1 and 3)
(05) | | F
(06) | | ⊥ (by ⊥-introduction from 2 and 5)
(07) | | M (by ⊥-elimination from 6)
(08) | | M
(09) | | M (by reiteration from 8)
(10) | M (by ∨-elimination from 4, 5-7, 8-9)
(11) (W → M) (by →-introduction from 3-10).


So the three assumptions are:


(W → M)
¬F
(M → A)


A nice conclusion now follows by the transitivity of →, viz. (W → A). This can be proven, for instance, by using → introduction and elimination rules:


| W
| M (by →-elimination from 1 and 4)
| A (by →-elimination from 5 and 3)
(W → A) (by →-introduction from 4 to 6)

Short answer: it's not the exact middle of two extremes.

Intuitively, this seems implausible because one of the two extremes so balanced might unnecessarily weigh the notion of virtue / excellence / arete) one way or the other. Moreover, this would also seem to have the counter-intuitive result that the notion of virtue / excellence / arete would change depending on the circumstances under which it was considered.

Alright, enough intuition. Here's some textual evidence. First, in Book II of the Nicomachean Ethics, Aristotle makes the following observation:


  moral qualities are so constituted as to be destroyed by excess and by
  deficiency
  
  Aristotle, Nicomachean Ethics, 1104a


Having made this observation, he goes on to define virtue / excellence / arete as follows:


  Virtue then is a settled disposition of the mind determining the
  choice of actions and emotions, consisting essentially in the
  observance of the mean relative to us, this being determined by
  principle, that is, as the prudent man would determine it.
  
  And it is a mean state between two vices, one of excess and one of defect.
  
  Aristotle, Nicomachean Ethics, 1106b-1107a


This then is the mean: a point that lies between the two extremes of excess and deficiency, so as to be what the "prudent man" would choose.

But there are some caveats to be observed about this notion of mean. First of all, there is not always a mean to be found:


  Not every action or emotion however admits of the observance of a due
  mean. Indeed the very names of some directly imply evil, for instance
  malice, shamelessness, envy, and, of actions, adultery, theft,
  murder. All these and similar actions and feelings are blamed as being
  bad in themselves; it is not the excess or deficiency of them that we
  blame. It is impossible therefore ever to go right in regard to
  them—one must always be wrong; nor does right or wrong in their case
  depend on the circumstances, for instance, whether one commits
  adultery with the right woman, at the right time, and in the right
  manner; the mere commission of any of them is wrong.
  
  Aristotle, Nicomachean Ethics, 1107a


And so, certain actions are to be altogether avoided, despite virtue being defined as a mean. But, Aristotle goes on to point out that being virtuous is hard, and all the more so because:


  of the two extremes [i.e., excess or defect,] one is a more serious error than the other
  
  Aristotle, Nichomachean Ethics, 1109a


And so the mean will tend to be a little more to one side than to the other. Moreover, as a practical principle:


  one should lean sometimes to the side of excess and sometimes to that
  of deficiency, since this is the easiest way of hitting the mean and
  the right course.
  
  Aristotle, Nichomachean Ethics, 1109b


Depending on which way is less opposed to the middle, i.e., which extreme is less bad than the other. And this is why Aristotle's notion of virtue / excellence / arete is sometimes referred to not just as the mean, but as the mean of the mean.

All that said, the "sweet spot" as you put it will be somewhere in the middle, just not necessarily the exactly, mathematical middle.
It's absolutely possible to prove both. I don't know your set of inference rules, so I'm going to make it easy for me by including material implication to solve the second one and to just let things follow directly from a contradiction for the first:

1. G & ¬ H   A
2. ¬ H ⇒ H   A
3. ¬ H       &E1
4. H         MP2,3
5. H & ¬ H   &I 3,4
6. S & I     From contradiction at 5.


If you aren't allowed to just draw any conclusion from a contradiction, then you bracket this by assuming : ¬ ( S & I) as a subproof as step 3, drawing the contradiction (now 3-6), then proof by contradiction using the subproof to get S & I

1. H        A
2. ¬ B v H  vI 1
3. B ⇒ H    Material Implication 2
4. ¬ S v ( B ⇒ H ) vI 3
5. S ⇒ ( B ⇒ H ) Material Implication 4

According to Whitehead & Russell's Principia Mathematica, existence is a case of a propositional function being true of at least one value of the variable. 

"There exists y such that φ(y)" is equivalent to "not all y such that φ(y) is false."

"There does not exist y such that φ(y)" is equivalent to "for all y, φ(y) is false."

It is important to understand that existence can only be asserted of a description.


  We can say 'the author of Waverly exists' and we can say 'Scott is the author of Waverley', but 'Scott exists' is bad grammar. It can, at least, be interpreted as meaning, 'the person named "Scott" exists', but 'the person named "Scott"' is a description, not a name. Whenever a name is properly used as a name it is bad grammar to say 'that exists'.


If unicorn is interpreted as "the creature described in wikipedia-Unicorn", then

"Unicorns exist" means "there is at least one creature that fits the description given in wikipedia-Unicorn." 

"Unicorns do not exist" means "Of all the things in the universe, none fits the description given in wikipedia-Unicorn." 

Based on above definition, none-existence involves "all", and therefore is not so easy to prove as it first appears. In order to prove unicorns do not exist, one would have to examine every thing in the universe from the beginning til the end of time. Thus, as far as current human knowledge can warrant, whether unicorns exist or not remains unknown. 

*Source: Russell, Bertrand. My Philosophical Development. Principia Mathematica: Philosophical Aspects. New York: Simon and Schuster, 1959
It can be useful to go back to the source of formal logic : http://plato.stanford.edu/entries/aristotle-logic/ Aristotle.

An argument must be valid "by virtue of form alone".

In Aristotle's logic :


  
    A deduction is speech (logos) in which, certain things having been supposed, something different from those supposed results of necessity because of their being so [emphasis added]. (Prior Analytics I.2, 24b18-20)
  
  
  The core of this definition is the notion of “resulting of necessity” . This corresponds to a modern notion of logical consequence: X results of necessity from Y and Z if it would be impossible for X to be false when Y and Z are true. We could therefore take this to be a general definition of “valid argument”.
  
  Aristotle proves invalidity by constructing counterexamples. This is very much in the spirit of modern logical theory: all that it takes to show that a certain form is invalid is a single instance of that form with true premises and a false conclusion. However, Aristotle states his results not by saying that certain premise-conclusion combinations are invalid but by saying that certain premise pairs do not “syllogize”: that is, that, given the pair in question, examples can be constructed in which premises of that form are true and a conclusion of any of the four possible forms is false.




Consider for simplicity arguments like the above, with two premises : P-1 and P-2 and call C the conclusion.

1) 

Saying that :


  "the premises CANNOT all be true without the conclusion being true as well"


means :


  it is not the case that : P-1 and P-2 are true and that C is false


that is equivalent to saying that :


  if we have that P-1 and P-2 are true, then also C is true.


The "it is not the case that ..." means that we cannot find a "situation" where ...

Instead :


  "the premises CAN all be true without the conclusion being true as well"


means that :


  it is the case that P-1 and P-2 are true and that C is false


i.e. we can find a "situation" where ...

In a "situation" where we have one of the Ps true and the other false, we cannot said nothing about C, i.e. the truth values of the premises do not "force" the truth value of the conclusion.

2)

If :


  "the premises CANNOT all be true without the conclusion being true as well"


is the defintion of valid argument, then an invalid argument is defined by the negation of the previous statement.

An argument with 


  premises P-1 and P-2 and conclusion C is invalid precisely when (i.e. : iff) it is the case that P-1 and P-2 are true and that C is false


i.e.


  iff "the premises CAN all be true without the conclusion being true as well".


3)

The argument : 


  if P and not P, therefore Q


is valid because there is no "situation" where the premises CAN be both true : it is a case of http://en.wikipedia.org/wiki/Vacuous_truth Vacuous truth.

We have to consider the definition of invalidity; in order to assert that the above argument is invalid, we have to find a "situation" where both premises are true and the conclusion is false; but we cannot find such a situation, because in every prossible situation, if one premise is true, the otehr is false.

Thus we cannot assert that the argument is invalid; but if it is not invalid, it must be valid.
It may never affect your everyday life, but it has weakened our trust in rigid logical methods, as a culture.  If even mathematics cannot attain to this kind of complete coverage of a domain, there is a good reason to think we habitually overvalue the role of rules in science.

I think that the shift toward seeing more of the human side of scientific inquiry, and admitting that it is deeply affected by personal faith, was unchained by the brake this kind of result put on logical positivism.

It is in effect the first post-modern fact.  Even if you don't go down the whole trail of postmodernism, it keeps the bug in your ear that says absolute modernism strives for more than can be realistically attained.  Sociology, faith, human nature, etc. really do matter in the end, and will not just be steamrolled by the sheer power of any system.
The best expression of how quantum mechanics conflicts with realism is the so-called measurement problem. Assuming scientific realism (that the theory describes an independent reality), one faces the following trilemma:


either the wave-function is an incomplete description (Bohm's theory)
either its evolution is non-linear  (collapse theories)
or measurement outcomes are not definite, but superposed (many-world interpretation).


It is not very clear where to situate copenhagen's interpretation, because of its vagueness. One option is to view it as a collapse theory, but it does not specify when, where and how the collapse occurs (only that it is related to measurement).

Another option is to view it as a realist theory with regards to classical states and an instrumentalism with regards to quantum theory ( which indeed has kantian flavours). The idea is this: quantum theory is a tool for predicting classically described outcomes, given classicaly described initial state, but it is not in any sense a description of reality. Bohr emphasised the fact that you need a "classical" vocabulary to interpret a wave-function and the observables. It has been claimed that it assumes an arbitrary microscopic-macroscopic dichotomy (and a collpase) but this is probably a realist misreading of Bohr's view.

Note that some of the physicists who elaborate quantum theory participate to the Vienna circle (logical positivist philosophy group) which was more or less influenced by neo-kantian philosophy. Logical empiricist's attempts to reduce theoretical knowledge to an observation vocabulary (phenomenism, operationalism) is clearly anti-realist.

As for the philosphy of contemporary physicists: I think there are various views, some being instrumentalists, others assuming many-worlds or a somehow vague collapse interpretation, and many being pragmatists without an explicit philosophy. 
HINT 

Here is a proof with Natural Deduction :

1)  ¬(¬¬¬P ∨ P) --- assumed

2) ¬P ∨ P --- Law of Excluded Middle (is a tautology)

3) P --- assumed [a]

4) ¬¬¬P ∨ P --- from 3) by ∨-introduction

5) ¬P --- assumed [b]

6) ¬¬¬P --- from 5) by Double Negation : φ ⊢ ¬¬φ

7) ¬¬¬P ∨ P --- from 6) by ∨-introduction

Now, from 3)-4) and 5)-7) we have derived  ¬¬¬P ∨ P from both assumptions : P and ¬P. Using ∨-elimination (proof by cases) from 2) we conclude with :

8) ¬¬¬P ∨ P --- discharging assumptions [a] and [b]

Now 8) contradicts 1); thus, from Ex Falso Quodlibet : φ, ¬φ ⊢ ψ we conclude with :


  9) Q ∧ R.


Thus, from 1)-9) we have :


  
    ¬(¬¬¬P ∨ P) ⊢ Q ∧ R
  




Note

We can prove LEM :  ¬φ ∨ φ using the "other half" of DN : ¬¬φ ⊢ φ.
Galileo's argument shows that the magnitude which determines the speed of free fall must be intensive, not extensive.

In the case of charges, the relevant magnitude is charge per mass unit. Same in the case of cross sections: it is cross-section per mass unit. These magnitudes are intensive. When attaching two objects, you double the resistance of the air, but you also double inertial mass, so the two objects will not fall slowlier or faster than each separately (contrarily to what you are saying). Galileo's argument remains valid in this case. Idem in the case of charges attracted by a magnet: two charges won't be attracted faster when tied together, because inertial mass adds up as well as charge.

The premiss on which the argument rests is that a continuous change in the arrangement of a situation cannot produce a discontinuous change in the dynamics of these arrangement (change an arrangement a little and the dynamics should change a little). If an extensive quantity is involved in movement then the principle fails: there'd be a discontinuity between the dynamics of two spheres with an infinitely small gap and two connected spheres, which seems absurd.

In conclusion: you might quibble on the formulation, but given this premiss of continuity of change, Galileo's argument is sound.

EDIT: perhaps the argument is even more general and does not depend on continuous change. Consider a composite system S1+S2. The extensive magnitudes of S1 and S2 will add up while the speeds of S1 and S2 won't (you'll take average speed for the composite system). Insofar as nature's behaviour does not depend on how you name and group things, extensive magnitudes cannot be directly relevant for movement.
I'm not really grasping where the question you is.

You state that for Spinoza, we can know either God's infinity or God's attributes. Spinoza calls what is grasped under the former "God" and under the latter "nature". It's not so much that nature is God or God is nature, but that the terms nature and God refer to the same thing comprehended differently...
Your specific wording suggests the statement must be true, but a reworded version of the question may provide sufficient linguistic ambiguity to permit it to be false.

In our exchange in the comments, you mention that this question stems from the Deflationary Theory of Truth.  If I may show my ignorance and quote http://en.wikipedia.org/wiki/Deflationary_theory_of_truth wikipedia:


  In philosophy and logic, a deflationary theory of truth is one of a
  family of theories that all have in common the claim that assertions
  that predicate truth of a statement do not attribute a property called
  truth to such a statement.


I will repeat your question here, just to put all of the necessary phrases in one place:


  Assuming P means the same as Q and Bob believes P and is aware that P
  means the same as Q, can we conclude he believes Q?


Intuitively, it makes sense that we cannot conclude he believes in Q.  Language has been evolving for a long time, and it would be risky to simply assume there is waste to be shaved off from it.  While the "day-star" vs "evening-star" argument provides one argument, a common phrasing in religion provides a counterpoint:


  Person 1: "Do you believe in God?" (P)
  
  Person 2: "Yes"
  
  Person 1: "Do you truly believe in God?" (Q -- alternatively worded "Is it true that you believe in God?")
  
  Person 2 answers


That dialogue would be nonsensical or irrational under the argument that P and Q say the same thing.

The reason this is rational is because our knowledge is imperfect.  We, on a regular basis, make decisions on things we think are true.  However, we may not be comfortable with the transitive closure of those things we think are true; this is demonstrated in the classical style of debate which seeks to cause cognitive dissonance in the student in order for them to identify a "better" truth.

The logic of this position can be exemplified with http://en.wikipedia.org/wiki/Bayesian_inference Bayesian Inference.  In Baysean Inference, each statement is in the form of a probabilistic statement.  The truth value of P and Q may not be known a-priori, but "priors" can be used to estimate the distribution P and Q are drawn from.

There reaches a point where it is no longer reasonable to keep track of the statistics because the wording is no longer sufficiently terse to be optimal.  At this point language will begin throwing around the word "true" in a sense which is less than the sense of "truth" used in Deflationary Theory of Truth.

Keeping track of these lesser flavors of "truth" is the key to my counter argument: your wording changes in the middle of the question to favor Deflationary thinking


  Assuming P means the same as Q and Bob believes P and is aware that P
  means the same as Q, can we conclude he believes Q?


For P and Q, the phrasing is "believes."  However, he "is aware" that P means the same as Q.  I believe this wording choice is what forces Deflationary thinking, because it implies he is certain P and Q mean the same thing, but he merely believes P.  In order to support Baysean style thinking, it must be changed:


  Assuming P means the same as Q and Bob believes P and believes that P
  means the same as Q, can we conclude he believes Q?


Now we can see that the path from P to Q can be phrased as a Baysean inference.  In this case, the connection between P and Q is imperfect, based on his statistical beliefs regarding that relationship.  Those imperfect statistical beliefs can be sufficient to allow "Bob believes P" and "Bob believes Q" to have different truth values.  Baysean inference would say they have different likelihoods of being true.  A more frequentist approach would suggest that there is a cutoff for believability that P achieves but Q does not.

This argument depended on rewording your question.  If, in fact, "P means the same as Q" and "Bob is aware that P means the same as Q" are logical truths, then logically Bob must believe Q iff he believes P.  There are predicates for which "Bob is aware" is possible.  However, many rational philosophers argue that the questions of value are all such that we cannot "be aware" of their sameness.  Thus, for this class of "interesting" questions your statement is true merely by trivialization:  "If Bob is not aware of anything regarding this interesting class of questions, then the statement is true regardless of Bob's believe in P and Q because of the rules of logic."  If the question is reworded to work around this triviality by changing "awareness" to "belief," then Baysean Inference shows a rational reason Bob may "believe P" and not "believe Q."



As an addendum, with respect to your comment:

You asked about a slightly different question, which pre-supposes the Deflationary Theory of Truth to hold.  This is a difficult question to answer, difficult enough to be worth editing it into the answer instead of a comment, so that I may correct it if I get the wording slightly wrong.  It is difficult because I have brought up an example with Person 1 and Person 2 regarding belief in God which I believe would be considered a rational line of questioning by a reasonable portion of the populace, and I'd like to avoid needing to define "fairly rational," which is used in your question.  I find defining most of humanity as irrational is... uncomfortable.

The question is regarding language semantics and syntax, so my instinct is to turn to http://en.wikipedia.org/wiki/Model_theory Model Theory and http://en.wikipedia.org/wiki/Proof_theory Proof Theory.  Model theory is a strange little discipline designed to associate semantic truth to statements.  Its dual is Proof Theory, which seeks to define syntactic manipulations which do not modify the truth of a statement.  To pre-suppose Deflationary Theory of Truth is to admit a particular proof calculi, while the question is clearly directed at the semantic truth of the statement; this is much better approached in Model Theory.  To be honest in my answer, I must phrase it in a format which may appear frustratingly circular.


  If "Cort" believes that there exists a proof calculi which admits a
  series of operations on a model which can prove the truth value of
  your question to be "true," such as the Deflationary Theory of Truth,
  and "Cort" is aware that "there exists a proof calculi and a model" is
  the same as "for all proof calculi and all models", then you may
  conclude that 'Cort' believes you may reach the aforementioned
  conclusion rationally.


However, I would challenge that my awareness of the inherent sameness of "there exists" and "for all" is very limited and should not be relied upon in any rational discussion.
There is no strict universally accepted convention...

In mathematical logic, ≡ is logical equivalence, as you said, and it is a connective between proposition (in propositional calculus) or formulae (in predicate caculus).

Example :


  (p → q) ≡ (¬p ∨ q).


Usually, = stands for identity and it is a binary relation between "objects", like numbers in arithmetic.

Example :


  ∀x (x+0 = x).


Finally, := is derived from programming languages, and is used (less often then the two above) as an "assignment" function : "let x be ...".

Example :


  ψ := (x = y), stand for : let the formula ψ be (x = y).




Note : except for programming languages, := is not usually a symbol of the (object) language, but of the meta-language, while the two above are usually symbols of the language.

But we have to use = also in meta-language contexts, e.g. to express the identity relation between expressions; we can avoid confusion according to the context, but in some "pedantic" cases we introduce a different identity symbol for the meta-language, like ≐.
Mathematical platonism -- or platonism more generally (with the lower case 'p') -- holds the following http://plato.stanford.edu/entries/platonism-mathematics/#WhaMatPla three theses about mathematical objects: they (i) exist, (ii) are abstract, and (iii) are independent of intelligent agents. This is typically all that mathematicians mean when they say they are platonists, and the differ in their further commitments. Plato would hold these three premises as well (in that way Platonism is compatible with platonism) but his metaphysics is traditionally taken to extend further, committing him to stances that most mathematicians do not hold. Thus, I would say that mathematical platonism is not compatible with Platonism.

For a first example, let us consider ethics and the existence of moral truths. Obviously, for Plato, forms like the Good and Justice have the same sort of existence as Triangle. For mathematicians, there is a lot of variation. Some, like Godel, will commit themselves to a thorough metaphysical platonism that extends to ethics, hence http://en.wikipedia.org/wiki/G%F6del%27s_ontological_proof his ontological argument for the existance of God, for instance (although note an important difference, Plato would not go to the same extent as Godel to see if some Form exists or not). Others, like Russell, do not think there was an objective Good; to see this read over his very pithy https://escarpmentpress.org/russelljournal/article/view/1679/1705 "Is there an Absolute Good?". Note that with Russell it is a little difficult to see if he held mathematical platonism and his moral positions at the same time, given that he went through https://egtheory.wordpress.com/2014/02/21/change-progress-and-philosophy-in-science/ at least three distinct stages and mathematics is mostly early Russell while ethics is middle and late Russell. However, from my personal experience, many mathematicians would not be platonists with respect to ethics.

For a second example, let us consider epistemology. For Plato, we simply 'remembered' the Forms, we do not discover them. A lot of mathematicians might object to this perspective. Although again, there is some heterogeneity here, for some we simply 'see with our intuition' (not that different from Plato's remembering) a mathematical truth and then construct a proof to guide others to what we saw, much like Socrates might guide the slave boy.

For a third example, let us consider particulars. For Plato -- most famously in the allegory of the cave -- particulars are of the same 'type' as the Forms but are degraded by being corporeal. The Forms are, in some way, causes of the particulars. I don't think that many mathematicians would subscribe to this view. I think that many of them would implicitly subscribe to a form of dualism similar to Frege's -- another notable mathematical platonist -- and think of separate worlds of the abstract and of the empirical with the effectiveness of mathematics in the sciences being either a selection-bias or http://en.wikipedia.org/wiki/The_Unreasonable_Effectiveness_of_Mathematics_in_the_Natural_Sciences unreasonable.

For a fourth example, let us consider structure. As you noted, the mathematicians' forms are not solitude and the universe mathematicians imagine is structured in some ways and they use that structure to navigate that universe. It is not clear that Plato's Forms have this structure, under most readings the various forms are rather isolated from each other.
On page 22 (emphasis mine):


  Suppose that your probability for rain tomorrow, conditional on a sudden
  drop in temperature tonight, is 0.8, whereas your probability for rain
  given no temperature drop is 0.3. The temperature drops. What should be
  your new subjective probability for rain? It seems intuitively obvious that it
  ought to be 0.8.
  The Bayesian conditionalization rule simply formalizes this intuition. It
  dictates that, if your subjective probability for some outcome d conditional
  on another outcome e is p, and if you learn that e has in fact occurred (and you do not learn anything else), you should set your unconditional subjective
  probability for d, that is, C(d), equal to p.


So indeed, this is stating the obvious, or, 'formalizing an intuition'. 

So why is this important? I'm guessing here, but I think it has to do with chronology. Before, the notation P(A|B) was only used when A would be an effect of cause B (and thus, B would occur before A). Now Bayes' rule, talking about hypothesis and evidence rather than cause and effect, says that it also works the other way around: we can also use P(A|B) when A is an hypothesis which implies B (and thus, in some sense, A would occur before B). Bayes' rule teaches us that conditional probability is independent of time.
Think of some examples. Here's a classic Thomist "analogical" term: "healthy". Properly speaking it is only bodies that are healthy, and for a body to be healthy is for it to be in good working order. For medicine to be healthy isn't for the medicine to be in good working order, it is for the medicine to have the power to put bodies into good working order. So here we clearly have two distinct but senses of "healthy".

So let's consider an example of a syllogism.

(1) If something is healthy, it is good.
(2) This medicine is healthy.
(3) Therefore, this medicine is good. 

Understand the occurrence of "healthy" in the first line as meaning healthy in the sense that bodies are healthy. Do we have a valid syllogism here? According to Thomists yes; according to Scotists and those who are suspicious of analogy no.
Consider the system (ZFC + ~PC) (that is, ZFC with one additional axiom, namely the negation of the Poincare Conjecture).   Any proof of the inconsistency of this system is also a proof that ZFC implies the Poincare conjecture (and vice versa).  Because we only recently learned that ZFC implies the Poincare Conjecture, we only recently learned that (ZFC + ~PC) is inconsistent.  And the shortest known proof of that inconsistency (i.e. the shortest known proof of the Poincare Conjecture) is very long and complicated.
Yes.

An argument can instantiate several argument forms. For example, the argument

The sun is shining
If the sun is shining, it doesn't rain
Therefore, it doesn't rain


instantiates the (valid) form

p, p->q => q

but it also instantiates the (invalid) forms

p, r => q

p, p->r => q

p, r->q => q

For an argument to be formally valid, it doesn't have to instantiate only valid forms. This, in fact, seems impossible. It is enough for the argument to instantiate one valid form, for it to be formally valid. And that's how the above argument, in the example, is valid.
This question highlights an important point --that identifying a specific fallacy is really only important in as much as it helps craft a response. With that said, I would see this as both an ad hominem attack against Paul (he doesn't have such-and-such an experience, therefore his argument must be bad) and as an appeal to authority (the arguments of those who have had this experience must be correct).

It's important to realize that the attacks against you would not be fallacies in two specific cases --if you really have no other argument in favor of your position than just your positive personal experiences (then the argument against you is correct, not fallacious) or if the person who had the negative experience really is the correct authority AND would disagree with you (for instance, if it could be shown that having that negative experience is a common or reasonably expected result of your position).  Thus, your defense lies in debunking those two possibilities:


What do you think would change about what I have just said if I had that experience? (against the ad hominem)
What argument would a person who had that experience make against my position? (against the appeal to authority as directed against your position)
What makes the opinion of a person who has had that experience decisive? (directly against the appeal to authority)

If we were to agree with Kant in defining morality by adhesion to self, then an ontology will always yield a moral axiom (I'd call it a 'maxim') which aims at the definition of self in the ontology.

As far as formalizing morality into axioms, a problem occurs. According to Gödel's incompleteness theorem, an axiomatic system cannot be both consistent and complete. Thus, if we define morality with an axiomatic system, then either we get contradictions or we get undecidable propositions.
Informal fallacies are judgment calls about problems in the structure of an argument. Their applicability is often debatable. (Formal fallacies like affirming the consequent are different in that they show the actual invalidity of an argument).

The fallacy of tu quoque is to fallaciously deny an argument because the source of the argument does not follow their own advice. But whether it is fallacious to point out that they don't follow their own advice depends on an interesting problem that we see described as far back as Aristotle.

Aristotle talks about akrasia, which we often translate as weakness of the will. It refers to a problem in the "practical syllogism". The practical syllogism is the idea that if I know what I ought to do, then I will do it. It turns out we often fail despite appearing to have full knowledge of what we ought to do.

Giving advice about, say, weight is an example where it seems fully plausible that a person giving it may rightly know what they are talking about even if they fail to follow it. Or to put it another way, if the leading scientist on obesity also happens to be fat that does not make what he's saying fallacious. On the contrary, we'd be guilty of a tu quoque to dismiss his knowledge in that arena. There is something bizarre seeming about following his advice on that point, but it's not fallacious to do so.

Consider for instance another informal fallacy: argument from authority. If we listen to the world's leading expert on obesity and do something about our weight because she says it, then that's an argument from authority. But that doesn't mean it's fallacious to follow her advice. Why? Because informal fallacies require judgments about the relevancy of her expertise.

To sum up, informal fallacies are patterns that are generally errant due to a problematic feature. But that feature is not always problematic, and we have to make a judgment about whether it is or not.

I don't generally take weight loss advice from really fat people, because (all other things being equal) it is difficult to accept they are experts on how to accomplish weight loss. But there may be exceptional reasons that make it so it's not fallacious. Conversely, there are contexts where tu quoque is perfectly appropriate (e.g., imagine one smoker telling another that it's easy to quit when the other smoker can't quit themselves. It's perfectly appropriate proof that their claim is wrong that they themselves empirically invalidate it).
It isn't universal, though like you point out it seems it must be.

For example, we can say that given two objects, all of whose properties are the same, we can say that they are similar and not the same. In Leibnizs language they are indiscernible. 

This point of view is used in modern algebra, where the technical notion is called isomorphism.

And it's taken as a basic principle in Category Theory, where one 'ought' not to say that two objects are the same, but isomorphic. 

This might be seen as a hair-splitting distinction; and it is, but it turns out to be a useful one.
Neither SEP nor Wikipedia has heard of "material logic". In the https://books.google.com/books?id=lJnuAgAAQBAJ&pg=PA10 writings of http://plato.stanford.edu/entries/maritain/ Jacques Maritain, it apparently means "applied logic" or thereabout; he goes into an elaborate argument why "material logic is the "Greater Logic". 

https://books.google.com/books?id=-u4IVP-3qh4C&pg=PA6 According to Granström, which has a historical [re]view of the topic, "material logic" is a notion that was central in the scholastic period; http://en.wikipedia.org/wiki/John_of_St._Thomas John of St. Thomas defined it to be more or less what today is called epistemology. With the turn to formal logic in the 18th century, "material logic" has faded from modern treatments (of logic).
If we have an axiom system with a finite number of axioms, we can always reduce them to only one, replacing the set of original axioms with their conjunction.

Thus, every non-trivial axiom system that is finitely axiomatized can be formulated in an equivalent form with a single axiom.

http://plato.stanford.edu/entries/goedel-incompleteness/index.html Gödel's Incompleteness Theorems apply to systems that (in addition to otehr conditions) have a set of axioms that is finite or at least decidable; http://en.wikipedia.org/wiki/Robinson_arithmetic Robinson arithmetic, for example, is finitely axiomatized and it is enough for G's Theorem.
Assuming you're looking for some intellectual understanding of this:

What the statement "If A then B" means in logic is that there is never a case where A is true and B is false.  If A is true, then B will be true.

Another way of saying that is that either B will be true or A will be false. Therefore

!A V B

or 

B V !A

are both alternate ways of expressing A -> B

You can confirm by assigning A and B values and confirming that the overall value of those three expressions always matches.
I'm not convinced that hearing jokes all the time would make them not funny. As long as they are original there does not need to be a problem. There's also no reason to assume we'd run out of original jokes. 

But if we'd assume that hearing jokes all the time implied that they wouldn't be funny anymore, then yes, the categorical imperative in that formulation tells you to never tell jokes. 

  How would I represent a AAA categorical syllogism with symbolic logic?


The AAA categorical syllogism looks like this:

1. All P is Q.
2. All Q is R.
∴  All P is R.


If by "symbolic logic", you mean propositional logic, then the answer is that you can't (exactly). The closest you can come is the hypothetical syllogism:

1. P ⊃ Q
2. Q ⊃ R
∴  P ⊃ R


If by "symbolic logic", you mean quantificational logic, then it would look something like this:

1. (x)(Px ⊃ Qx)
2. (x)(Qx ⊃ Rx)
∴  (x)(Px ⊃ Rx)


The above notation is preferred by many philosophers, while mathematicians might prefer notation like this:

1. ∀x(P(x) → Q(x))
2. ∀x(Q(x) → R(x))
∴  ∀x(P(x) → R(x))


Both of these notations are semantically equivalent, so use whichever seems more intuitive unless you are operating under additional constraints such as course requirements. 

For both notations, it is common to read the first premise symbolically as "for all x, if x has property P, then x has property Q," the second premise as "for all x, if x has property Q, then x has property R," and the conclusion as "for all x, if x has property P, then x has property R."
Kant's theoretical-practical distinction is not a Kantian innovation, but was customary at the time (and has been ever since in German speaking philosophy). 

Aristotle's legacy

It is directly formed after Aristoteles distinction between practical wisdom (φρόνησις), and theoretical wisdom (σοφία) (e.g. in Nicomachean Ethics).

The various modes of reason inform also his systematics of the sciences: theoretical (θεωρία), practical (πρᾶξις) and productive (ποίησις) knowledge.

As the relevant http://plato.stanford.edu/entries/aristotle/ SEP entry on Aristotle states:


The theoretical sciences include prominently what Aristotle calls first philosophy, or metaphysics as we now call it, but also mathematics, and physics, or natural philosophy. Physics studies the natural universe as a whole, and tends in Aristotle's hands to concentrate on conceptual puzzles pertaining to nature rather than on empirical research; but it reaches further, so that it includes also a theory of causal explanation and finally even a proof of an unmoved mover thought to be the first and final cause of all motion. Many of the puzzles of primary concern to Aristotle have proven perennially attractive to philosophers, mathematicians, and theoretically inclined natural scientists. 
Practical sciences are less contentious, at least as regards their range. These deal with conduct and action, both individual and societal. Practical science thus contrasts with theoretical science, which seeks knowledge for its own sake, and, less obviously, with the productive sciences, which deal with the creation of products external to sciences themselves. Both politics and ethics fall under this branch.
Finally, then, the productive sciences are mainly crafts aimed at the production of artefacts, or of human productions more broadly construed. The productive sciences include, among others, ship-building, agriculture, and medicine, but also the arts of music, theatre, and dance. Another form of productive science is rhetoric, which treats the principles of speech-making appropriate to various forensic and persuasive settings, including centrally political assemblies.


Kant's distinction(s)


  Or is it that Kant views practical reason, or ethics, the basis by which one acts in the world; (even whilst he demands 'the moral law' as a pure aspect of inner being); as opposed to interpreting it, which is his pure critique?


It is important to note that "pure" is not a synonym for "theoretical".

Kant uses two orthogonal distinctions: 


theoretical (theoretisch) and practical (*praktisch)
pure (rein) and applied/empirical (angewandt/empirisch)


Kant distinguishes between theoretical and practical reason. But those two types of reasons are "ultimately one and the same reason which has to be distinguished simply in its applications".1 Those reasons can, further, be "pure" or "applied". As a first approximation, this distinction can be understood in analogy to the a priori/a posteriori distinction. (Be aware that Kant branches these distinctions the other way around too: there is pure and non-pure reason, which can be uses in a theoretical and in a practical way.)

So, Kant's critique of pure reason is, in fact, a critique of pure theoretical reason. 

And he starts his preface in the Critique of practical reason explicitly https://ebooks.adelaide.edu.au/k/kant/immanuel/k16pra/complete.html addressing his title:


  This work is called the Critique of Practical Reason, not of the pure practical reason, although its parallelism with the speculative critique would seem to require the latter term. The reason of this appears sufficiently from the treatise itself. Its business is to show that there is pure practical reason, and for this purpose it criticizes the entire practical faculty of reason. If it succeeds in this, it has no need to criticize the pure faculty itself in order to see whether reason in making such a claim does not presumptuously overstep itself (as is the case with the speculative reason). For if, as pure reason, it is actually practical, it proves its own reality and that of its concepts by fact, and all disputation against the possibility of its being real is futile.


Kant's Die Metaphysik der Sitten (http://en.wikipedia.org/wiki/The_Metaphysics_of_Morals The Metaphysics of Morals, 1797), on the other side, is concerned with applied or empirical practical reason, which includes virtues and vices, foundations of law and politics.



1 "… am Ende nur eine und dieselbe Vernunft…, die bloß in der Anwendung unterschieden sein muß" (Grundleg. zur Met. d. Sitt., Vorr., p. 19).
Unless I'm missing the point, none of your axioms offer any definition of proof or state that all true statements in your system must be provable.  Given that, there's no reason a statement might not be both true and not provably true --or, for that matter, false, but not provably false.

In my opinion, your axiomatic system would need a formal definition of proof, as well as a definition of what kinds of statements belong in its domain of discourse, in order for this question to be meaningful.

You're basically retreading ground that was covered at the dawn of modern symbolic logic.  You may want to look into the work of Tarski, Russell and Godel to see how these questions originally arose, and how they were handled.
There is no "immaterial" implication.

The term "material implication" originated with Bertrand Russell, http://en.wikipedia.org/wiki/The_Principles_of_Mathematics The Principles of Mathematics (1903); see Part I : Chapter III. http://fair-use.org/bertrand-russell/the-principles-of-mathematics/s37 Implication and Formal Implication for :


Two kinds of implication, the material and the formal.


See in W&R, http://plato.stanford.edu/entries/pm-notation/ Principia Mathematica the notation for implication (the "horseshoe") ⊃; in the "material" usage, it is a connective between propositions :


  
    *1.2    ⊢ : p v p . ⊃ . p,
  


while in the formal usage it is a relation between "classes" :


  
    *10·02 φx ⊃x ψx . = . (x). φx ⊃ ψx.
  


See :


Alfred North Whitehead & Bertrand Russell, https://books.google.it/books?id=ke9yGmFy24sC&pg=PA7 Principia Mathematica to *56 (2nd ed - 1927), page 7.




Today, the material conditional has to be compared with other conditionals : the subjunctive and the counterfactual conditionals; see :


Graham Priest, https://books.google.it/books?id=rMXVbmAw3YwC&pg=PA13 An Introduction to Non-Classical Logic : From If to Is (2nd ed - 2008), page 13.

The premise


  Premise 2. But God does not exist.


is begging the question. Whether it's true is clearly very debatable.

The second 'variant' is actually a different argument because the second premise has a very different meaning. Yet, also this second premise is debatable. Unfortunately, the author doesn't support the premise at all in the rest of his 'essay'.

In short: don't believe everything on the internet (in particular note from the author's biography that he has no relevant education or publications, except for his own website).
Yes. Both features of the relations of ideas, analyticity and a-priority, are spelled out in http://www.philosophy-index.com/hume/enquiry-human-understanding/iv.php §30 of the Enquiry, a bit after the one you quoted from.

Hume associates relations of ideas with demonstrative reasoning:


  All reasonings may be divided into two kinds, namely, demonstrative reasoning, or that concerning relations of ideas, and moral reasoning, or that concerning matter of fact and existence.


He then associates demonstrative reasoning with the law of non-contradiction:


  That there are no demonstrative arguments in the case seems evident; since it implies no contradiction that the course of nature may change, and that an object, seemingly like those which we have experienced, may be attended with different or contrary effects.


This implies that relations of ideas are analytic, in Kant's sense of the word. Hume then associates  demonstrative arguments, and non-contradiction, also with abstract  a priori reasoning:


  Now whatever is intelligible, and can be distinctly conceived, implies no contradiction, and can never be proved false by any demonstrative argument or abstract reasoning a priori.

The three issues (observer effects, proposition theory, correspondence theory ) do not seem to me related. Kant seems to speak, in that place,  about empiricial psychology and about the introspection of mental events . Frege and Early Wittgenstein did not deal with introspection or psychology. Both related truth and falsity  to abstract linguistic (not mental) entities. Early Wittgenstein did held a version of the correspondence theory of truth, but Frege did not.
For an n-ary function symbol, there are k^n elements of the domain; for each one you need to choose one of k elements to send it to. So the total number of possible function interpretations would be k^(k^n).

For an n-ary relation, you have 2 possibilities for each of k^n possible n-ary lists. So what would be the total number of possible relation interpretations?
In the "standard" treatment of first-order logic, the domain of the interpretation contains objects; this is the semantical side.

The language contains "names", i.e. individual constants; this is the syntactical side.

When you interpret a language you have to assign "meaning" to the symbols of the language, like assigning a reference to the individual constants. This amounts to assign to every name of the language an object as its reference.

We can have distinct names having the same objcet as reference.

In your example, Eric Blair and George Orwell are two names of the same individual.
I would say it is impossible to invalidate logic as a whole, just as it is impossible to disprove "science."  Logic, at it's base, is the formalization of rational thought.  If/when a more reliable method of reasoning becomes available, that becomes logic (and the old methods of reasoning are either adapted or abandoned).

In that way, though, it is very possible for one method of logical reasoning to prove that some other method of logical reasoning is unreliable.  http://en.wikipedia.org/wiki/Reductio_ad_absurdum Reductio ad Absurdum is an ancient technique often used for such purposes, and http://en.wikipedia.org/wiki/History_of_logic the history of logical reasoning is filled examples of old methods of logical reasoning giving way to more improved versions.
The Answer You're Probably Looking For

Under a common "critical thinking" or "intro to logic" in philosophy approach, the following definitions apply:

validity: an argument is valid if it is the case that the conclusion cannot be false when all of the premises are true.

consistency: it is possible for all of the premises to be true.

The answer is that you do not need a truth table on these definitions, because inconsistency in the premises means that it is impossible for all of the premises to be true. In turn, this means the argument is valid.

Behind this is that the definition of validity is this: were the premises all to be true then the conclusion could not be false. Since an inconsistent argument can never have all of its premises true,  it can never attain a state with all premises true and a false conclusion.



The Answer if You are Doing Formal Semantics

(please upvote the answer by Badrinath if this is what you were seeking)
Note that if you are referring to Tarskian model-theoretic semantics and some other advanced contemporary approaches to logic that this no longer obtains -- because validity and invalidity only apply to models, and models only occur when:


  A set T of sentences is called a (first-order) theory. A theory is satisfiable if it has a model \mathcal M\models T, i.e. a structure (of the appropriate signature) which satisfies all the sentences in the set T. Consistency of a theory is usually defined in a syntactical way, but in first-order logic by the completeness theorem there is no need to distinguish between satisfiability and consistency. Therefore model theorists often use "consistent" as a synonym for "satisfiable". (http://en.wikipedia.org/wiki/Model_theory#First-order_logic wiki)


On such an account, no theory could be simultaneously inconsistent and valid, because only consistent theories are valid or invalid.
In math in general, how things came to be is less important than why they stayed that way.  (Bourbaki notwithstanding, our traditional notations persist because they either conform to intuition or shape it productively.)

So skipping the history, these two signs are related because they define related lattices.

Both symbols are replaced by the 'greater than (or equal to)' symbol '>=' in broader (or more typographically constrained) contexts.  (Before the 40's, the undecorated containment symbol more often meant simply containment and not proper containment.)

The implying statement seems to have 'more truth', and to have some part of that truth be the truth of the implied statement.  And literally "False <= True" when these are 0 and 1.  You can read <= between booleans as 'whenever' if => is 'implies'.  So "A implies B means B whenever A" gets written 'A=>B = B<=A' in various computing languages.

The superset is clearly 'larger' than its subset.  So the sign is obvious there.

But instead of a nice symbol polymorphism we have a duality relation: (A => B) = (a in A <= a in B).

This captures the natural set vs state lattice duality of properties.

The set of states of the world that a proposition is true in is larger when the set of objects it applies to is smaller.  If A implies B, any state (of a machine, universe, etc.) where A is true is a state where B is true, even though any element satisfying B is an element satisfying A.

(So dualizing twice, you can get a coincidence: 
(a in A) => (a in B) == 
A <= B == 
worldsWhere(a in A) >= worldsWhere(a in B) )
Physics never has implications for mathematics, only the other way around.  Mathematics is not an experimental science, but an 'exact' one, and is only expanded, never altered, by the discoveries in other sciences.

Surely there are productive extensions of Euclidean geometry that capture the newly discovered notions of space and of matter.  (Back in the 80's I have seen folks define slight variations of topology that give open sets a quantum-logic penumbra, so that the resulting points are 'sort of fat' in a way that simplifies quantum and relativistic geometry, to the degree that quantum logic ever simplified anything.)  But the old system captures something so natural to human beings that it will never cease to be our common reference point for what geometry means.

From an intuitionist point of view, Euclidean geometry is part of our genetic inheritance, deeply embedded in our psychology, there to be discovered.  This is why it was the perfect example of anamnesis for Plato.  Folks who take a more functional or a more ideal approach to mathematics are also going to agree that this has the most immediate survival value, or that this occurs naturally so often that it must be part of underlying reality.

None of them is going to expect it to change due to discoveries in physics.
Let's look at the translations (into first-order logic):


  (1) ∃x : Man(x) ∧ Married(x).
  
  (2) ∃x : Man(x) ∧ ¬Married(x).


The first is true in universes where there is at least one married man; the second is true in universes where there is at least one bachelor. To show that the argument from (1) to (2) is not valid, consider the counterexample: a universe with only one object, a married man. This is a counterexample because there is no bachelor in such a world. That shows the invalidity.

Now let me say a word about why I think you were thinking that this argument is sound or at least valid. Logically:


  "there are φs" == "there is at least one x s.t. φ(x)"
  
  e.g. "there are married men" == "there is at least one x s.t. x is a married man".


But colloquially, "some" is often used in a stronger sense to mean only some, so for example:


  "sometimes I'm sad"


simply means that there are moments when the agent is sad, but it also seems to imply (in the literature they call this implicature) that there are times when she is not sad. To show that this is an implicature and not an implication, we can provide the following defeater:


  "sometimes I'm sad; which makes sense, since I'm always sad."


It's kind of a silly example, but you get the idea. True implication cannot be defeated.
You're right. Here is a counter-example. Suppose T means having a very rare disease : among 1000 persons only 10 have it.

Suppose S means having blond hair.

(2) says that most of the 10 persons having the disease have blond hair. Imagine it's 9 persons.

(3) says few of the 990 persons without the disease have blond hair. Imagine it's 99.

(4) says most people with blond hair have the disease, which in this case is false: 99 don't have it, only 9 have it.

Now add clause (1): most people have blond hair. Then the disease cannot be a very rare disease after all (that would be inconsistent with (3)) and (4) turns out true:

(1) Most people have blond hair. Imagine 900 have blond fair, 100 don't.

(3) most people without the disease are among the 100 non-blond. People without the disease cannot exceed 100 by large. Imagine it's 110 and only 20 are blond

(2) most people with the disease have blond hair. This is already constrained by (1) and (3): 880 over 890.

From which (4) follows as well: most people with blond hair have the disease: here 880 over 900.
Obviously no counterexample will be found in this case because we started with no particular assumption.
I think the standard response here is to call upon two particular parts of the Frege/Russell tradition - the first being the concept of a http://plato.stanford.edu/entries/propositions/ Proposition (SEP) and the second being the concept of http://plato.stanford.edu/entries/logical-form/ Logical Form (another SEP).

Consider a simple question like "Is it raining outside?".  One way to go about working out what it is that this question is asking is to try to determine under what conditions would this question receive an affirmative answer, and what would receive a negative answer.  Naturally, we would say that it would be answered affirmatively if, and only if, it is raining outside, and negatively if, and only if, it is not raining outside.

Now in one interpretation of the analytic philosophical project, what we're doing is constructing a theory of propositions (that we take to represent the bearers of semantic value) in a formal framework, and then using this theory to further interpret assertion more generally by saying that the logical form of a statement or argument reduces to the expression of either propositions or of relations featuring at least propositions and speakers as constitutive elements.

The logical form of my question then will feature the proposition "It is raining outside" as a proper part.  Is there something else that might be necessary here? Well, perhaps we might also add that in asking a question, I (the speaker) am addressing this question to you (a prospective answerer), in a manner which suggests that I do not know the answer but hope/believe that you do and want you to tell me if you do.

So.  Let's suppose we have a proposition-forming operator 'k' to form a proposition [k] from the sentence k. Perhaps a candidate for a correct logical form of "Is it raining outside?" asked by A to B would be something like this:

¬knowsThat(A,'it is raining outside') ^ ¬knowsThat(A,¬'it is raining outside')
 ^ believesThat(A,knowsThat(B,'it is raining outside') v knowsThat(B,¬'it is raining outside'))
 ^ (desiresThat(A,
     knowsThat(B,'it is raining outside') -> asserts(B,'it is raining outside') 
     ^ knowsThat(B,¬'it is raining outside') -> asserts(B,¬'it is raining outside') 
   )


A bit unwieldy perhaps, but then that's why we have natural language to simplify all of this!
The fallacy here is that D is not L (by which I mean D is not equivalent to L). L is general whereas D is particular.

Counterexample:


P1: Canids (Bears) are almost always viscous killers.
P2: The tamed circus bear is a canid.
C: Therefore, the tamed circus bear is almost always a viscous killer.


This is not valid. You are taking a general set with its own existential characteristics and applying it to an individual in that set which cannot logically be done. The only characteristics that you can say that each individual of the set have are the characteristics defined by the set. This doesn't inherently say anything about the set itself.

For another example, take the set of all positive, whole numbers from 1 to 100. The value is 5050:


P1: The individuals in the set of all positive whole numbers from 1 to 100 are almost all double-digit numbers.
P2: 100 is an individual in the set of all positive whole numbers from 1 to 100.
Therefore, 100 is almost always a double-digit number.


Do you see how this cannot be done? I'm by no means a logician, so I'd appreciate it if someone can help me understand WHY this is the case.
The key point is the sentence:"Even if this proposition is never true, it is nevertheless significant", I italicized "significant", as Russell does in https://books.google.com/books?id=XjncL40ZIR0C&pg=PA85&lpg=PA85&dq=Wittgenstein+objected+as+follows:+%27Russell%27s+definition+of+%22%3D%22+won%27t+do;+because+according+to+it+one+cannot+say+that+two+objects+have+all+their+properties+in+common.&source=bl&ots=WMcODDSTGh&sig=a6sTzP-bzMTOFLbmzTp9WGwPPYg&hl=en&sa=X&ei=KDNpVea-HIS8sAWfmYGAAw&ved=0CC0Q6AEwAg#v=onepage&q=Wittgenstein%20objected%20as%20follows%3A%20%27Russell%27s%20definition%20of%20%22%3D%22%20won%27t%20do%3B%20because%20according%20to%20it%20one%20cannot%20say%20that%20two%20objects%20have%20all%20their%20properties%20in%20common.&f=false his text. Russell is talking about manipulating objects in a formal system (of Principia Mathematica). What he says is that while objects can be "actually" distinct if they are "equal" in his system this fact is  insignificant (irrelevant), contrary to Wittgenstein. Because one can "never conceivably discover that a and b were two" without creating a "diversity" property, which would make them un"equal".

This becomes clear from the following paragraphs, where Russell describes his definition of 2 as a class with non-"equal" x, y, one of which is always "equal" to any z in this class, and then writes:"if two things have all their properties in common, they cannot be counted as two, since this involves distinguishing them and thereby conferring different properties upon them". In other words, his definition of 2 still works even if the defining class "actually" has more than two objects. 

This is now well understood in mathematical model theory. Peano arithmetic admits non-standard models, for example, with "infinite" numbers, despite the induction axiom suggesting that they are not there. How does this happen? There is no way to distinguish between "finite" and "infinite" numbers in first order logic, so whether they are there or not is "insignificant" as far as arithmetical properties are concerned. 

Wittgenstein was fond of mixing language with meta-language, even inclined to admit contradictions resulting from that, see http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.650.3454&rep=rep1&type=pdf Berto's The Gödel Paradox and Wittgenstein’s Reasons. But Russell points out that Wittgenstein's alternative of always using "different letters for different objects" is technically unworkable, and would make mathematical logic "impossible". "To say of one thing that it is identical with itself is to say nothing, and to say of two things that they are identical is nonsense" is a nice quip, but the second part is questionable, and both parts are beside the point of "equality". In interesting cases we do not know, and in some cases we can never know, if "equal" things are identical or not, e.g. when manipulating formal expressions representing them. And even the first part needs some twisting to accomodate examples like "the evening star is the morning star".
This is a problem really only in the case that you are a mathematical http://en.wikipedia.org/w/index.php?title=Philosophical_realism&redirect=no realist, meaning that you believe that a number like 5 has some independent ontological existence that infinity lacks.  In the case that you think that both 5 and infinity are just useful concepts then the problem disappears.

There are plenty of concepts we find useful in life and in science without assigning to them any larger or deeper reality.  For instance, in his book QED, Richard Feynman, the great theoretical physicist, asks us to picture light as little rotating arrows.  He is in no way a realist about those arrows.  He does not think that light is a little rotating arrow at any magnification, or under examination by any type of instrument.  But he does find it useful to conceptualize light as rotating arrows, because it makes certain difficult theoretical concepts and calculations easier to visualize.

Even in the case that you want to be a realist about 5, you could still maintain that infinity is a convenient fiction, helpful for calculations of some types.  In that case, however, you might be asked to explain what makes one of those numbers "realer" than the other.
Frege is the founder of a program called logicism that aimed to reduce all of mathematics to logic. In order to reduce mathematics to logic Frege had to expand what is meant by logic. Before him Locke, Kant and others understood by logic only http://plato.stanford.edu/entries/aristotle-logic/#Syl Aristotle's syllogistic, which is a manipulation of simple implications (syllogisms). http://plato.stanford.edu/entries/logicism/#Fre Frege's Logic went much further, and encompassed all of arithmetic in particular, if not all of set theory. But derivable from logic alone is what all three meant by "analytic". Under logicism all of mathematics would be analytic, including much of what Kant called "synthetic a priori" (but not the parts related to geometry and physics). So both Locke and Frege were right, they just meant different things by "analytic". 

"Frege's conception of the analytic was suitably broader than Kant's. Kant required that conceptual containments be evident within the sentence, rather than that the sentence be displayed as a conclusion following logically from axioms whose own logical or conceptual truth was self-evident, and which might contain expressions not occurring in the sentence in question... Kant did not regard ‘7 + 5 = 12’ as an analytic truth. The Fregean, by contrast, is able to exploit the internal structure of the numerals, and to invoke the recursion axioms for addition (which themselves would have to have been derived in logicist fashion). So, for the Fregean, even if not for Kant, ‘7 + 5 = 12’ is an analytic truth."

The best attempt to realize the logicism program was Russell and Whitehead's book Principia Mathematica. However, after Gödel's results on incompleteness the program came to be viewed as a dead end, and was largely abandoned. Later, Quine argued convincingly that the http://plato.stanford.edu/entries/analytic-synthetic/#ProLog analytic/synthetic distinction itself, even in a revised form adopted by logical positivists, can not be maintained at all. According to Quine, all knowledge, including the laws of logic, is synthetic, and ultimately empirical, this dealt a further blow to logicism. Although it is no longer believed that "all of mathematics" reduces to logic, the modern mathematical logic is much closer to Frege's conception of logic than to Kant's or Locke's.
See https://en.wikipedia.org/wiki/Kripke_semantics Kripke Semantics of modal logic :


  A Kripke frame is a pair (W,R), where W is a (possibly empty) set, and R is a binary relation on W. Elements of W are called nodes or worlds, and R is known as the accessibility relation.
  
  A Kripke model is a triple (W,R,⊩), where (W,R) is a Kripke frame, and ⊩ is a relation between nodes of W and modal formulas, such that:
  
  
    w ⊩ ¬A if and only if w ⊮ A,
    
    w ⊩ A → B if and only if w ⊮ A or w ⊩ B,
    
    w ⊩ □A if and only if u ⊩ A for all u such that wRu.
  
  
  We read w ⊩ A as “w satisfies A”, “A is satisfied in w”, or “w forces A”. 
  
  The relation ⊩ is called the satisfaction relation, evaluation, or forcing relation. The satisfaction relation is uniquely determined by its value on propositional variables.


According to these semantical specifications, the truth-value of p → □q must be computed as :


  w ⊩ p → □q, i.e. “p → □q is satisfied in w”


as follows :


  either p is not satisfied (i.e. is false) in w, or q is satisfied in every u such that wRu (i.e. in all worlds accessible from w).


In simpler words : if p is true in a world, q must be true in all worlds accessible from it.

  How can this be axiomatic?


Spinoza is using an axiomatic method to think through a certain emanationist theory.

Axioms by their nature are to be taken as granted; and it is the reasoning that follows that is usually subject to considerations of soundness.

To put this into context, consider the first few definitions in Euclidean geometry as Euclid theorised and conceived it.


  1.1A point is that which has no part
  
  1.2 A line is a breadthless width
  
  1.3 The extremities of lines are points


Now if one began to do geometry by disputing all of these (and they can be disputed - there are objects with no points, there are points which always have parts, or have extension) then one is hardly learning geometry; and the disputation is of no (mathematical) consequence, because as a sustained critique it came after, and not before; thus to display this knowledge, charitably can be excused as enthusiasm; less charitably as flattering themselves with a little purloined knowledge.

The point I'm making is that if one is not interested in geometry then one ought not to be taking a class in geometry.

Similarly, if one isn't interested in neo-Platonism and how it has intersected the Abrahamic traditions it seems quixotic to dispute it, before one has begun to understand it.

A secondary but still important point is that Euclid, far from inventing geometry placed it on coherent foundations; and this is part of the nature of axiomatic systems - if one looks into the history of axiomatics one generally notices a pattern of accumulating propositions until they are placed in a coherent manner - to give a recent example structuralism in Algebra before being put into a coherent axiomatic form through the notion of Category and Functor (by MaClane and Eilenberg).


  God can be concieved as not existing


This is incoherent, and like saying 'I can concieve that Green is not Green', which might make for an interesting poetics, but logically inconsistent.

If one accepts the axiom that God exists, and this in the sense described by the three monotheisms; then one can't immediately deny it.

What one can do is simply deny the first axiom: that there is a God; and many people do make this move - and not as a move on a game but as a reality.
The beautiful, the true, the good, the just - they are the typical ideas which Plato uses to illustrate his theory of forms.

The only candidate I know from Plato for identifying the beautiful (kalon) and the true (alethes) is Symposion 212a.

On the other hand, in Symposion Plato often combines the beautiful and the good (agathon). For the closest relationship see 202c.

In the politeia Plato distinguishes all ideas. In addition, he promotes the good to a position higher than the other ideas. The parable of the sun (Politeia 508ff) compares the idea of the good with the sun. Because of the sunlight we can see the objects. Secondly, the sun supports the life of all livings. An analogous role is ascribed to the idea of the good in the realm of forms. 

In my opinion, one should distinguish the ideas when speaking as a philosopher. But a poet is free to mix the concepts and to evoke associations as may best please him.

G.H. Hardy, when doing mathematics, is even stricter in his concepts and in his wording than a philosopher. But apparently, also Hardy sometimes enjoyed to speak in a poetical manner.   
The http://www.iep.utm.edu/val-snd/ Internet Encyclopaedia of Philosophy has an entry on validity (and soundness, which is often confused). While there are some issues with the entry, as Conifold points out below, the author has the definitions right:


  A deductive argument is said to be valid if and only if it takes a form that makes it impossible for the premises to be true and the conclusion nevertheless to be false. Otherwise, a deductive argument is said to be invalid.


A tautology is always true. Therefore, if the conclusion of the argument is a tautology, the conclusion is always true, which means it's impossible for the premises of the argument to be true and the conclusion nevertheless false, which is the definition of the argument's validity.

It's somewhat peculiar that that textbook talks about validity without first defining it. It's a pretty straightforward definition, but usually these books are very precise.
Solipsism can be more subtle than that, https://askaphilosopher.wordpress.com/2012/04/16/berkeley-as-solipsist Berkeley's for example, although there is a linguistic disagreement on whether to classify his philosophy as solipsism, and Berkeley denied the label. Basically he contends that "to be is to be perceived" (esse est percipi), there is no material or physical substrate to that, and each soul has a whole perceived world all to itself. In this Berkeley anticipated Kant's idea of appearances (phenomena). 

This does not mean that there can be no other souls however. According to Berkeley God is one of them, and so are other people. God produces perceptions in each other soul independently however, so there is no shared world in any sense, and they are all solipsists. See http://www.jstor.org/stable/2216814 Grey's The Solipsism of Bishop Berkeley.

Wittgenstein makes a cryptic remark in the Tractatus, which is perhaps the only "endorsement" of solipsism by a major philosopher


  "In fact what solipsism intends is quite correct, only it cannot be said, but it shows itself. That the world is my world shows itself in the fact that the limits of language (the language, which I alone understand) means the limits of my world."


According to http://www.jstor.org/stable/2251341 Hintikka's On Wittgenstein's `Solipsism', the parenthetical phrase is a mistranslation, and should instead read "the only language that I understand". He also argues that what Wittgenstein thought solipsism intends to say is different from what most philosophers take it to say.
1 is https://en.wikipedia.org/wiki/Fallacy_of_composition Fallacy of Composition, assuming what is true of a part is true of the whole.  

2 is https://en.wikipedia.org/wiki/Fallacy_of_division Fallacy of Division, assuming what is true of the whole (the complete list of usages of "ad-duniya") is also true of a part (the proper subset of times that "ad-duniya" is meant as meaning "world").
Al Ghazali about a century earlier had written a text called The Incoherence of the Philosophers, a critique on metaphysics (or, more bluntly, an attack on the philosophers). By philosophers he means primarily those scholars whose metaphysics is based on Aristotle and Plato, but his attacks really were focused on Avicenna, the intellectual predecessor of Averroes. Wikipedia summarizes the contents of his work as follows:



He states that Avicenna and his followers have erred in seventeen points (each one of which he addresses in detail in a chapter, for a total of 17 chapters) by committing heresy. But in three other chapters, he accuses them of being utterly irreligious. Among the charges that he leveled against the philosophers is their inability to prove the existence of God and inability to prove the impossibility of the existence of two gods.

The twenty points are as follows:

Refuting the doctrine of the world's pre-eternity.

Refuting the doctrine of the world's post-eternity.

Showing their equivocation of the following two statements: God is the creator of the world vs. the world is God's creation.

The inability of philosophers to prove the existence of the Creator.

The inability of philosophers to prove the impossibility of the existence of two gods.

The philosopher's doctrine of denying the existence of God's attributes.

Refutation of their statement: "the essence of the First is not divisible into genus and species".

Refutation of their statement: "the First is simple existent without quiddity".

Their inability to demonstrate that the First is not a body.

Discussing their materialist doctrine necessitates a denial of the maker.

Their inability to show that the First knows others.

Their inability to show that the First knows Himself.

Refuting that the First does not know the Particulars.

Refuting their doctrine that states: "the heavens are an animal that moves on its own volition".

Refuting what they say regarding the reason that the heavens move.

Refuting their doctrine that the heavens are souls that know the particulars.

Refuting their doctrine that disruption of causality is impossible.

Refuting their statement that the human soul is a self-sustaining substance that is neither a body nor an accident.

Refuting their assertion of the impossibility of the annihilation of the human soul.

Refuting their denial of bodily resurrection and the accompanying pleasures of Paradise or the pains of Hellfire.

Beyond heresy

The three irreligious ideas are as follows:

The theory of a pre-eternal world. Ghazali wrote that God created the world in time and just like everything in this world time will cease to exist as well but God will continue on existing.

God only knows the universal characteristics of particulars - namely Platonic forms.

Bodily resurrection will not take place in the hereafter only human souls are resurrected.

Ocassionalism

The Incoherence of the Philosophers is famous for proposing and defending the Asharite theory of occasionalism. Al-Ghazali wrote that when fire and cotton are placed in contact, the cotton is burned directly by God rather than by the fire, a claim which he defended using logic.



Averroes attempted to refute this work with his The Incoherence of the Incoherence, which defends Aristotlean thought. However, this work was poorly received compared to Al Ghazali's critique. The main thing was that Al Ghazali had written a fairly well reasoned attack on the philosopher's account of metaphysics, which conflicted with orthodox islamic thought, and instead presented the case for the supremacy of god and orthodox islamic consensus while showing the Aristotleans to be heretical, thus garnering the title "Proof of Islam". So deeply entrenched then was his philosophical account that Averroes account, which went against the tide of Islamic account and to the eyes of his contemporaries, against islam itself, was bound to fail. Indeed, many of Al Ghazali's ideas can be found in the works of later philosophers (Descarte presents the idea that every moment is willed from god in the meditations, for example, and Kant raised some critiques of metaphysics that are similar to Al Ghazali's). 

In Christian thought however, despite the critiques of Averroes which echo Al Ghazali, the explicit philosophical movement of ocassionalism never took hold. Hence, Aristotle's ideas were still considered useful to an extent. St. Thomas Aquinas, despite disagreeing with and attacking Averroes's metaphysical positions, still incorporates Aristotlean thought in his works such as the Summa Theologica, finding several Aristotlean concepts useful. In Judaism, a similar dynamic occurs, wherein the philosophy of ocassionalism never took hold.

In short, Averroes's metaphysics is rejected by all three Abrahamic faiths. However, the critiques of Al Ghazali against the entire Greek tradition, and the establishment of the doctrine of ocassionalism in Islamic thought, was unique to Islamic philosophy, and made it less receptive to Averroes's commentaries on Aristotle. The philosophical tides were more receptive however in Christian and Jewish philosophy, explaining why Averroes had a more profound impact on these two philosophical traditions.
As noted earlier, it is probably impossible to firmly logically prove there is no Bigfoot, in the same way as https://en.wikipedia.org/wiki/Russell%27s_teapot Russell's Teapot. However, and this come from the Mathematician in me, there are a few strategies you can try to either disprove or weaken a claim. 

Proof by Contradiction

Basically, assume that the claim is true, and show that it being true causes some logical impossibility. This is more or less the only way to actually prove something false. Things to help support this kind of argument:


Include requirements. I.E If Bigfoot is 8 feet tall and a mammal, then his heart has to be some size, food intake this, skeletal structure that...
Include other, given truths. I.E If we assume Bigfoot exists and the Sun is hot, then ..{argument}.. which is a contradiction.
Take an extreme. I.E if Bigfoots exists, there must be a smallest Bigfoot..
Generalize. I.E If Bigfoot exists, then at least one undocumented mammal exists...


Proof by Contrapositive

A => B implies not B => not A.

This one is weird, and probably not applicably outside of rigidly defined areas like math. But essentially, you take the contrapositive and prove it must be true. In a single statement case this is redundant, and becomes proving "Bigfoot does not exist", which is where we started. But apply it to a specific implication used, and it can be useful. I.E "Most photographs are not fake => Bigfoot exists" would also imply "If BigFoot does not exist => Most photographs are fake" which is just a silly implication.

I know the example for this one is weak, I'm trying to come up with a better one, but these cases are usually relatively subtle and require a lot of context

Probabilistic Proof

Instead of absolutely proving a claim wrong, you can prove that a claim is not likely to be true, therefore the inverse is more likely to be true, therefore the claim is more likely to be false than not. Not an absolute proof, but certainly weakens a claim if it's most likely to be false.
Tips:  


Look at implications. Bigfoot existing need X acres of land never surveyed, there have been Y acres surveyed out of Z, so Bigfoot's X being within those Y acres is only ~Y/Z*X
Look for correlations. I.E Number of recorded sightings of Bigfoot decreased in proportion to availability of cameras, a regression or even common sense says that's very unlikely if Bigfoot exists.

Hume is talking about concrete events. You are talking about possible events.

For example, a bus being late may be caused by 1) a car crash or 2) the driver not showing up. Suppose for the moment that it is not possible for these events to happen simultaneously.

Now let's look at bus nr. 609 which should have arrived at my station as of August 8, 2015, 11:50 CET, but failed. This is a concrete event. It is either caused by a car crash or by the driver not showing up. That is what Hume is talking about. If the cause of the bus arriving late hadn't been there, it would have been on time. 

Of course, that doesn't mean the other cause (which wasn't the cause for this bus arriving late) can never cause any other bus to arrive late. But for this concrete event, there is only one cause.
There are several remarks in the Philosophical Investigations in which Wittgenstein comments on the "business" of philosophy:


  It was correct that our considerations must not be scientific ones.
  The feeling ‘that it is possible, contrary to our preconceived ideas, to
  think this or that’ — whatever that may mean — could be of no interest to us. (The pneumatic conception of thinking.) And we may not
  advance any kind of theory. There must not be anything hypothetical
  in our considerations. All explanation must disappear, and description
  alone must take its place. And this description gets its light — that is
  to say, its purpose — from the philosophical problems. These are, of
  course, not empirical problems; but they are solved through an insight
  into the workings of our language, and that in such a way that these
  workings are recognized — despite an urge to misunderstand them. The
  problems are solved, not by coming up with new discoveries, but by
  assembling what we have long been familiar with. Philosophy is a struggle against the bewitchment of our understanding by the resources of
  our language. (§109)


And from §122 to §129:


  It is not the business of philosophy to resolve a contradiction by
  means of a mathematical or logico-mathematical discovery, but to render surveyable the state of mathematics that troubles us — the state of
  affairs before the contradiction is resolved. (§125)
  
  Philosophy just puts everything before us, and neither explains nor
  deduces anything. — Since everything lies open to view, there is nothing to explain. For whatever may be hidden is of no interest to us. (§126)

The statement holds in general.

Proof. (A_1 and ... and A_n) false 

<=> not (A_1 and ...and A_n) true 

<=> (not A_1 or ... or not A_n) true 

<=> (not A_1 or ... or not A_n-1) or not A_n true

<=> not (A_1 and ... and A_n-1) or not A_n true

<=> not ( (A_1 and ... and A_n-1) and A_n) true

<=>  (A_1 and ... and A_n-1 => not A_n) true, q.e.d. 
I do not know about any philosopher who claims that moral discussion is impossible. Probably one can ascribe such a claim to a solipzist, who can have a moral discussion only with himself.

The fact that there are many moral discussions in this blog, i.e. discussions about moral decisions as well as discussions about moral and ethics itself, show that such discussions are possible.

But it is considered controversial, whether it is possible to recognize values to guide moral decisions or to give an ultimate justification for such values.
We have to remember that :


  a ∈ {x:ψ(x)} ⇔ ψ(a) 


(this is the definition of the "set-builder" symbol { _ : __ } ).

But {x:ψ(x)} = Ø, and thus :


  for all a, a ∉ {x:ψ(x)} ⇔ for all a, ¬ψ(a).

You made a mistake in step 6. Nr. 3 is not a universal quantification, it's a negation (the ¬ is outside the ∀). Therefore, you cannot eliminate the ∀.

What you instead need to do is to eliminate the ¬. If you want to eliminate ¬ from ¬p, you assume p and then work towards a contradiction. So you get:


Assume ∀x(¬S(x))


Now you may eliminate ∀ on x0:


¬S(x0)


This in combination with 5 gives us


R(x0)


But this is in contradiction with 4. Hence, the assumption in 6 was incorrect, and therefore it must hold that ¬∀x(¬S(x)).
Let's amend the premise by taking "space" out of it:"our acquired knowledge of objects comes to us from having seen them in certain relations to other objects". Space points do not come with labels attached, even when we measure distances they are distances between markings of some sort, not points. And we can easily establish relations between objects without space, say numbers 2 and 17, or even vectors and operators in infinite-dimensional spaces. They are not physical objects of course, and space is around for those, but even physically objects are demarcated by other objects, and their characteristics are contrasted to characteristics of other objects. 

Come to think about it there is no space involved in any of it, it is merely kept, and perhaps constructed, as framing. Psychologists established that our visual perception starts with a flat and spotty impression on the retina, which is then filled in and 3-dimensionalized by the brain. Kant even believed that this imposition of flat 3D space is the source of synthetic a priori knowledge which will forever subject our physical theories to 3D Euclidean geometry. As it turned out, even the space of visual perception is slightly hyperbolic.

The idea of empty independent absolute space only came to prominence after Newton, who adopted it because it was the simplest way to express classical mechanics (modern textbooks have to go through a complication of reference frames). Before that relational theory of space was a consensus since Aristotle. Descartes identified space with matter, so that when matter moves the space moves, it does not leave some empty space behind to arrive at a new one. http://plato.stanford.edu/entries/spacetime-theories/#6.1 Leibniz gave the most comprehensive expression of the "ideality of space" in relational theory. He held that "(i) a body comes to have the ‘same place’ as another once did, when it comes to stand in the same relations to bodies we ‘suppose’ to be unchanged... (ii) That we can define ‘a place’ to be that which any such two bodies have in common... And finally that (iii) space is all such places taken together. However, he also holds that properties are particular, incapable of being instantiated by more than one individual, even at different times; hence it is impossible for the two bodies to be in literally the same relations to the unchanged bodies. Thus the thing that we take to be the same for the two bodies — the place — is something added by our minds to the situation, and only ideal. As a result, space, which is constructed from these ideal places, is itself ideal: ‘a certain order, wherein the mind conceives the application of relations’".

When working on general relativity Einstein noticed that if we know gravitational field around a region in space there is no unique way to extend it into that region, but that different extensions are physically equivalent despite assigning different field values to different spacetime points. This came to be called the https://en.wikipedia.org/wiki/Hole_argument#Einstein.27s_resolution "hole argument". Einstein concluded from it that individual spacetime points as such are physically meaningless and accepted the relational theory:"People before me believed that if all the matter in the universe were removed, only space and time would exist. My theory proves that space and time would disappear along with matter." 
The aphorism "πάντα ρει" (everything flows) which is attributed to Heraclitus 



τὰ ὄντα ἰέναι τε πάντα καὶ μένειν οὐδέν
"All entities move and nothing remains still"// Plato's Cratylus 



is related to the nature of the world and not specifically to "time" . 
The river is a metaphor and it means that "change" differentiates the being of things constantly. (γιγνεσθαι) The world is in a condition of constant change.

As Heraclitus is considered a dialectical philosopher 
his philosophical position is contrarian to the positivist /empiricist view.
https://en.wikipedia.org/wiki/Unity_of_opposites https://en.wikipedia.org/wiki/Unity_of_opposites

So no, the "moving/growing block" theory of the ontology of time is unrelated to the Heraclitian view as you can easily see from the quote at Plato's text I cited. 
Let's start lower down. By the incompleteness theorems, PA (first-order arithmetic) can't prove its own consistency. Do we have to worry that PA is inconsistent? Fortunately not: we have a stronger system (ZFC) which proves the consistency of PA. But that's not much help -- if you doubt the consistency of PA, this just means you should doubt the consistency of ZFC as well. What we really want to say is: we have very good reason to believe that PA is consistent. Namely, we've been working with PA for many years and uncovered no inconsistency. And the axioms seem to be saying pretty common-sense things about a class of objects (the natural numbers) whose existence most of us take for granted. 

Now move up to someone who doubts the consistency of ZFC. I could point to some higher system (say, ZFC + "there exists a strongly inaccessible cardinal") which proves the consistency of ZFC. But again, that wouldn't help, since if you doubt the consistency of ZFC you doubt the consistency of the second system. It's much better to say: we've been working with ZFC for a long time and uncovered no inconsistency. And ZFC seems to be saying some pretty common-sense thing about a class of objects (sets) whose existence most mathematicians now take for granted.
First part :

1) ¬∃xP(x) --- premise

2) P(x) --- assumed [a]

3) ∃xP(x) --- from 2) by ∃-intro

4) ⊥ --- contradiction, from 1) and 3)

5) ¬P(x) --- from 2) and 4) by ¬-intro (or →-intro, if we agree on the abbreviation ¬ϕ : = ϕ → ⊥), discharging [a]


  6) ∀x ¬P(x) --- from 5) by ∀-intro, where x does not occur free in any undischarged assumptions.


Thus, from 1)-6) we have : ¬∃xP(x) ⊢ ∀x ¬P(x) and with a final application of →-intro we conclude with :


  
    ⊢ ¬∃xP(x) → ∀x ¬P(x).
  




The second part is similar, derive : ∀x ¬P(x) ⊢ ¬∃xP(x) and conclude by →-intro with :


  ⊢ ∀x ¬P(x) → ¬∃xP(x).


Finally, apply ↔-intro.
It is an application of the https://proofwiki.org/wiki/Reductio_Ad_Absurdum Reductio Ad Absurdum rule.

RAA is a formulation of the principle of proof by contradiction: if one derives a contradiction from the hypothesis ¬ϕ, then one has a derivation of ϕ (without the hypothesis ¬ϕ) :


  assume ¬ ϕ --- step 3)
  
  derive a contradicition : ⊥ --- in this case steps 2) and 4)
  
  conclude with "rejecting" the assumption, i.e. with ϕ --- step 6).




If we agree on the truth-functional definition of connectives (as per classical logic), the rule is valid : if we derive a contradiction from the assumption ¬ ϕ, this implies that ¬ ϕ is false.

Then, by double Negation, ϕ must be true.

In fact, the rule is equivalent to the https://en.wikipedia.org/wiki/Double_negation#Double_negative_elimination Double negation eleimination rule :


  
    ¬¬ ϕ ⊢ ϕ.
  




http://plato.stanford.edu/entries/intuitionism/ Intuitionism rejects as "fallacoius" this type of argument : in http://plato.stanford.edu/entries/logic-intuitionistic/ Intuitionistic Logic RAA and Double Negation are not valid rules of inference. 



See also https://it.wikipedia.org/wiki/Ex_falso_sequitur_quodlibet Ex falso quodlibet (or Principle of explosion) : 


  any statement can be proven from a contradiction. 


The two premises R ∧ C and ¬ R are contradictory, while S is not present in the premises; thus, the proof shows that from contradictory premises we can derive absolutely anything as a conclusion.

With this logic law we can derive S in a simpler way :

1) and 2) : as above

3) R --- from 1)

4) R ∧ ¬ R --- from 3) and 2) by https://en.wikipedia.org/wiki/Conjunction_introduction Conjunction introduction

5) ⊢ R ∧ ¬ R → S --- Ex falso quodlibet


  6) S --- from 4) and 5) by https://en.wikipedia.org/wiki/Modus_ponens Mmodus ponens.


Note : Ex falso quodlibet is intuitionistically valid. For logics that do not allow it, see http://plato.stanford.edu/entries/logic-paraconsistent/ Paraconsistent Logic.
First, correspondence theories of truth are generally associated with realism, not idealism. The point of a correspondence theory is that there is a correspondence between mental or linguistic representations and reality. Linguistic content is not necessarily mental (not for externalists) and in any case, having a correspondence implies that there is something that is not mental/linguistic to which mental/linguistic entities correspond. Realism is the position that assumes just that, and idealism denies it. Idealism is generally associated with coherence conceptions (if everything is mental then no correspondence is needed, only coherence between different ideas in a conceptual scheme).

It seems to me that correspondence theories are dominant today. 

Coherence theories of truth seem to imply that truth is relative to a coherent conceptual scheme, and that in a sense, contradictory schemes are all equally true by their own standards. The constraints on truth are not strong enough: truth is not objective.

An alternative family of theories are pragmatic theories which claim that truth has to do with ideal utility, assertability, verifiability, ... These theories tie truth to our epistemic situation. 

A first difficulty of these conceptions is to define an epistemic agent in a non anthropocentric way: who do we include in our epistemic community? 

A second difficulty has to do with "ideal". Bare utility (or assertability) will not do because there are beliefs that are temporarily useful (or justified, or...) but not true. True beliefs should be useful in the long run, ideally that is, but the criteria of "ideal" utility are not easy to define. what do we mean by that? 

A third difficulty is that these conceptions do not respect bivalence (a proposition is either true or false) since it is easy to formulate a proposition such that neither it or its negation is ideally useful (or justified...), for example the proposition that there exists a huge golden mountain that disappears whenever someone looks at it (or attempts to observe its effects). The proposition is not obviously meaningless: we understand what it says. A fundamental indeterminacy ensues.
However bivalence is a fundamental aspect of classical logic.

Correspondence truth eschews these difficulties. It seems to capture more adequately the intuition that truth transcends our epistemic position and capacities, and concerns something independent from us.
Firstly, we have to sharpen up a little by assuming that an evil man is understood to be one who has no virtue, otherwise the argument cannot go through at all. After that, as you say, the argument is denying the consequent, which is to say it has the form "If P then Q; not Q; therefore not P". All we need to ensure that the argument correctly instantiates this form is that the second premise is contradictory to the consequent of the first. The fact that the second premise is a particular does not matter in this respect, because "some evil men possess the good" is clearly contradictory to "no evil man possesses the good". So the argument is valid. 
¬(P → Q) is equivalent to : P ∧ ¬Q.

Thus, ¬((x)Px → (x)Qx) must be : (x)Px ∧ ¬(x)Qx.
Proof in Natural Deduction, avoiding Double negation-elimination (thus, the derivation is intuitionistically valid) :

1) ¬¬(A ∨ B) -- premise

2) ¬(B ∨ A) --- assumed [a]

3) A ∨ B --- assumed [b] 

4) A --- assumed [b1] for ∨-elimination

5) B ∨ A --- from 3) by ∨-introduction

6) B --- assumed [b2] for ∨-elimination

7) B ∨ A --- from 5) by ∨-introduction

8) B ∨ A --- from 3), 4)-5) and 6)-7) by ∨-elimination, discharging assumptions [b1] and [b2]

9) ⊥ --- from 2) and 8) by ¬-elimination

10) ¬(A ∨ B) --- from 3) and 9) by ¬-introduction, discharging [b]

11) ⊥ --- from 1) and 10) by ¬-elimination


  12) ¬¬(B ∨ A) --- from 2) and 11) by ¬-introduction, discharging [a].

https://en.wikipedia.org/wiki/Non_sequitur_(logic) Non sequitur.


  Non sequitur (Latin for "it does not follow"), in formal logic, is an argument in which its conclusion does not follow from its premises.

I'll go a bit backwards, because we know what we want to arrive at, so it makes sense to work all of the rules backwards to what we'd like to be true.  Then, at some point, we'll figure out that it actually IS true.  Then we can start from the axioms, and work our way forward to the solution.

One of the rules of propositional logic is that "X->Y" is true for any Y if X is false.  Thus, if you wish to prove ~R->T, when you know nothing about T, one easy solution is to prove that ~R is false, i.e. R is true.

Now, you have one axiom about R's truthfulness, (Q->P)->R.  (Q->P) is the same as (NOT(Q) OR P).  We don't know much about P yet, but we do have some statements about Q.  If we can prove that Q is false, the NOT(Q) must be true, so (NOT(Q) OR P) must be true, which implies R must be true (like we want it to be).

The remaining steps of proving ~Q I leave up to you.
If you are allowed to divide an "OR" into two hypotheticals (discharged by showing the same result for both) this is easy at this point.  On the left, you assume R, and then you can build R V S and (R V S) V (T & Q) both by OR introduction.  On the right, assume T, and you already have Q, so you are good to go there as well.

If you don't have access that rule, your best bet is to assume the opposite of your conclusion, ~ [(R V S) V (T & Q)]

From there, yield ~(R V S)  & ~(T & Q)
Assume T
Then you have T & Q which contradicts ~(T & Q).  Discharge your assumption and yield T.  From there you can build the entire statement that directly contradicts your assumption.
The statement is correct.

We can rephrase it as: (F → G) → (¬G → ¬ F). That this holds is the idea of https://en.wikipedia.org/wiki/Modus_tollens modus tollens.

About your proposal of rewriting:
If F is sufficient for G (so F → G), then it doesn't follow that lacking F is not sufficient for G (¬(¬F → G)). For example, if G is always true, anything is a sufficient condition for G: both F and ¬F. 



There are several tools online with which you can make truth tables yourself and check the truth value of statements. This is a truth table for the correct statement:

https://i.stack.imgur.com/O3gwF.png 

And this is a truth table for your proposal:

https://i.stack.imgur.com/u2JZR.png 

These tables were made with https://demo.camilstaps.nl/CleanLogic CleanLogic. Disclaimer: I developed that.
It seems that you are taking a rather extreme form of mathematical nominalism and denying the existence of distantly mind-independent objects. There is no existent "counter" who can keep on "counting" to infinity, thus it is a mere word, or rather the fanciful negation of the word "finite." Reason exceeds its remit. 

This starts off sounding like good, hard-headed common sense, but can soon lead into the radical empiricism of Bishop Berkeley. If nothing is granted "real" existence apart from what can be "counted" or observed, how do you distinguish between your own observing mind and the so-called "existing" world, including other minds in that world? Are they all really there when you aren't looking or counting? 

Moreover, what seems most certain by direct experience is that you cannot observe absolute "finitude." Have you ever seen it, chewed it, experienced it? Where is it? As far as each of us can think in all directions and back into our past, don't we discover only a kind of infinite regress of consciousness? Never some clear boundary where "finitude" begins. In terms of strict empiricism, there seems to more evidence for an infinite continuum than for its opposite.

So now we are in the reverse antinomy. We want to kick the stone and say "things exist!" Yet we do not grant existence to all those things that cannot be observed or counted, like some infinite series of numbers or anything those numbers might refer to. Must we then assume, with Berkeley, that all these things exist because they are being observed and counted.... by some possibly infinite Accountant?      
You can't, because it isn't valid. Think about it with numbers, consider:


  a=1 
  b=2 
  c=1 


It's true that a≠b & b≠c, yet a=b.
I think Jo's answer is right on, but I just want to draw out the differences a little.

innate = from birth  <=> adventitious  = arriving from outside

a priori = without experience <=> a posteriori = derived from experience

All innate knowledge is a priori, but not all a priori knowledge is innate.

All a posteriori knowledge is adventitious and all adventitious knowledge is a posteriori.

Plato advocates that we have innate knowledge of the forms. Neo-Kantians (19th century) really liked Kant and seemed to equate the two but most contemporary Kantians thinks things like "there is an a priori form of right action" (at least that's how I read Korsgaard), but they do not think this is innate. Rather they think it's what happens when you apply the idea of action. 

Descartes also considers innate vs. adventitious ideas in part of his proof for God's existence but to my knowledge he does not use the terms a priori and a posteriori -- though many people understand him in these Kantian terms.

Taking ideas and working them together is to yield a new idea synthetic .  Learning something by breaking down a known complex idea is analytic.

Here's http://hume.ucdavis.edu/mattey/phi102f03/apriori.html an interesting page that uses these terms correctly (or at least how they are used in contemporary philosophy).
Your sentence implies its own falsity, therefore it is false. However, its being false does not imply that it is true. As such, it might be said to be half of the liar paradox. In a liar paradox ("this sentence is not true") the truth of the sentence implies its falsity, while its falsity implies its truth. There is a huge amount of literature trying to explain the liar paradox. 

I wouldn't say your example is genuinely paradoxical, it is merely strange to say something that implies its own falsehood. Another example would be "all generalisations have exceptions". This is clearly itself a generalisation, so if it is true, it has an exception, which implies that it is false. But it being false does not imply that it is true, so it is simply false. 
Kurt Gödel was already a Platonist by that time. In fact - he presumed Platonism to be true already when he wrote his dissertation where he proved his completeness theorem. His belief in Platonism only grew stronger after he came up with his famous incompleteness theorems.

See SE http://plato.stanford.edu/entries/goedel/ http://plato.stanford.edu/entries/goedel/ for general review about his work.
It poses a challenge to overly-specific notions of ontology, but there aren't many around that are that specific.

Relatively general ontological frameworks like http://plato.stanford.edu/entries/process-philosophy/ process philosophy (e.g. as promoted by Whitehead) have a sufficiently general notion of what is "actual" that a phenomenon that has both wave-like and particle-like properties (depending on context) is not a serious challenge.

But even http://plato.stanford.edu/entries/substance/ substance metaphysics has to deal with stuff like water and sand and sound and light, and so it's usually phrased in a general enough way that it doesn't have at its core the idea of involately particulate matter.  Some specific arguments may fall flat, but overall it is so general that ideas e.g. of substance being a combination of form and matter survive with only minor tweaks when one considers that the matter has characteristics best described by quantum mechanics.

In particular, with wave vs particle duality, you just say that the substance is what it is, and you will notice different aspects of its behavior in different conditions.

And, thus, all you have to do to rescue most arguments is to insert an "in the classical regime" and everything goes through as before, albeit with a little less satisfaction about having gotten to the true nature of things.

So, in general: no, not very much.  In any particular case: you need to check the details.
Yes "is" is a verb in natural language. There is a distinction, grammatically, between transitive and intransitive verbs. 


Transitive: He kicked the ball. 
Intransitive: He blushed.


"Is" is intransitive. Note too that "is" has distinct uses. 


Predicative: The ball is blue. (This fits what you say above best.)
Categorizational: A square is a rectangle. (Note, not rectangular.)
Existential: "The blue ball is." (More idiomatic in contemporary
English would be "the blue ball exists," but the existential is is
genuinely grammatical.)


As far as I know the verb for to be can be used in each of these separate ways in English, German, French, Latin, Ancient Greek and Hebrew. I can't speak to other languages, but I imagine there are similar uses.
I read your question as being about the relation between access consciousness and subjective time.

The literal notion of now has no temporal extension, so it would appear to be trivially true that "the mind only conceives of one object at a time".  However, the subjective time notion of now does have a temporal extension.  According to my reading on the subject, most cognitive scientists and philosophers place a value of between two and three seconds of subjective time on the "now".  In my experience, the subjective now can be shorter in duration depending on the circumstance.

Consider the example where one if faced with a life threatening situation.  I was once climbing a rock face without any safety equipment when I started to lose my balance. Within the subjective now - i.e., in no more that one second - I had to conceive of many objects in my environment as well as many abstract objects. I was able to make the decision to lower my centre of gravity in order to regain my footing.  After taking appropriate action, all that was left for me to do was to clean my underpants.
1) Aristotle develops his classification from the study of mesocosmic (= middle world) objects only. Further stimulation came from the discussion of Plato's theory of forms. E.g. causa formalis resembles the Platonic concept of form. 

Aristotle did no know about microcosmos. Concerning macrocosmos he had only a theoretical - and wrong - conception. 

Aristotle gives in Metaphysics a history of philosophy. He emphasizes that his forerunners detected only fractions of the different types of cause (aition). Hence he considers his doctrine of 4 causes a progress in philosophy.

2) I agree with you that causa finalis does not apply to physical objects. But to a certain degree it applies to animals and plants when considering the role of genes. Of course, the genotype defines only the domain of possibilities. Not every possibility will be realized. Most of all causa finalis describes the way that humans make their decisions. 

3) Concerning your interesting example to explain light rays - microphysics - by the means of Aristotle's doctrine, my proposal is:


causa materialis: Photon E = h*ny, m = E/c**2. 
causa formalis: Any model from a theory of light from physics: Optics, Maxwell's electrodynamics, quantum electrodynamics. But we operate only by using models, any essence remains unknown. It is even dubious whether essence is a meaningful notion in this context.
causa efficiens: Emission of light in atoms, pair annihilation.
causa finalis: not applicable. 


I think one can make a similar list for electron and black hole.
Statements in the form "Everything is ______" tend to be accepted via abduction (http://plato.stanford.edu/entries/abduction/ SEP).  In abduction, you infer that a hypothesis is true because it is the best explanation for a behavior.  Abduction, unlike deduction, does not prove the statement, but rather simply infers it.

Statements in that form can also be subjective.  It is entirely possible for someone to claim "everything is ______" and simply make themselves incapable of being aware of anything which is not part of that everything.  This is often a self fulfilling prophecy.  For example, if you believe everything in the world is made of matter (materialism), you are very unlikely to seek out and observe anything which is supernatural, such as a mind (dualism).  Your statement can be true about your world until something forces you to be aware of a counterexample.

I do believe small children accomplish this the best.  They have a miraculous ability to assume "mom and dad cannot see me while I do this" so completely that they can utterly ignore mom or dad standing right behind them.  They are aghast when they find their plans foiled, as though mom and dad somehow ruined their perfect world.

An interesting example of this occurs in the book Permutation City by Greg Egan.  In it, they create a simulated world built out of simulated automata.  Some of the characters develop a "Garden of Eden" pattern which can provably never evolve naturally.  The presence of this pattern proves that you are in a simulation.  When they attempt to use this Garden of Eden pattern to disprove the beliefs of the denizens that they are not in a simulation (an "everything is real" argument), well, I wont spoil it, but I will say there are many creative ways to avoid being exposed to such a pattern!
There are few "propositional" laws used in http://plato.stanford.edu/entries/aristotle-logic/ Aristotle's logic.

One of the methods of proof used by A. in deriving figures form the basic ones is http://plato.stanford.edu/entries/aristotle-logic/#MetProPerDedConRed reductio (anagein) through the impossible (dia to adunaton), i.e. the principle of indirect proof.

An early use of https://en.wikipedia.org/wiki/Consequentia_mirabilis Consequentia mirabilis has been found; see William Kneale, http://www.jstor.org/stable/628635?origin=crossref&seq=1#page_scan_tab_contents Aristotle and the Consequentia Mirabilis (1957), in the lost https://en.wikipedia.org/wiki/Protrepticus_(Aristotle) Protrepticus.

But it seems that an explicit use of https://en.wikipedia.org/wiki/Principle_of_explosion Ex falso (sequitur) quodlibet is not present into his logical works.

Of course, the http://plato.stanford.edu/entries/aristotle-noncontradiction/ rejection of contradictions is the "firmest" principle of A.' logic and metaphysics.

An aristotelian locus that seemingly involves "explosion", but in the context of metaphysics and not logic, is Metaph, IV.4, 1007b19–on :


  Again, if all contradictories are true of the same subject at the same time,
  evidently all things will be one. For the same thing will be a trireme, a wall, and a man, if it is equally possible to affirm and to deny anything of anything [...].




See http://plato.stanford.edu/entries/aristotle-noncontradiction/#11 Aristotle on Non-contradiction : Dialetheism and Paraconsistency for a proposed "paraconsistent" interpretation of some passages found in A.'s works.



According to https://en.wikipedia.org/wiki/Ernest_Addison_Moody Ernest Addison Moody, https://books.google.it/books?id=i0ruAAAAMAAJ Truth and Consequence in Medieval Logic (1953), page 90, in http://plato.stanford.edu/entries/buridan/ Buridan, https://books.google.it/books?id=6CbpCAAAQBAJ&pg=PA201 Consequentiae, (Book I, ch.8, rule 7) we have: 


  "Ad omnem copulativam ex duabus contradictoriis constitutam sequi quamlibet aliam, etiam consequentia formali"


that we can translate as :


  
    p ∧ ¬ p ⊢ q
  


See http://plato.stanford.edu/entries/consequence-medieval/ Medieval Theories of Consequence for an overview:


  Buridan's treatise on consequence and the treatises inspired by it, most notably Albert of Saxony's (a chapter of his Perutilis logica) and Marsilius of Inghen's treatise on consequence. There is also the interesting commentary on the Prior Analyics formerly attributed to http://plato.stanford.edu/entries/duns-scotus/#ScoWor Scotus which is thought to have been composed before or in any case independently of Buridan's treatise. [...] This tradition can be referred to as the Parisian/continental tradition on consequences.

An argument that contains an inconsistent definition is guilty of "equivocation" because it fails to use the same term with the same meaning throughout. This is a type of "informal fallacy" because disputants could in principle disagree about whether what is happening is a material equivocation or meaningless (where the change in definition between statements/premises/conclusion does not matter).

Whether this has an impact of the validity of a deductive argument, the soundness of a deductive argument, or the strength of other arguments will depend on what happens when/if you resolve the equivocation.
A premise is not valid or invalid, it is either true or false. Validity only applies to deductions.

Maybe the confusion comes from the fact that you're conflating the logical implication "->" and the deduction rule. Logical implication is a logical operator that says that either its antecedent is false or its consequence is true, but it does not say that B is deducible from A. For example if "p:=tigers are mammals" is true and "q:=it is raining" is true, "p->q" is true even though q cannot be deduced from p.

In your example, the premise is not a syllogism, but a logical statement that can be true or false depending on what you mean by A and B. From this sentence and the other premises you can deduce the conclusion. The argument is valid. Whether the premise is true or not will depend on what you mean by A and B, but the premise is neither invalid or valid: it's not a deduction, but a statement.
On page 202, the author is highlighting that statement 5 is not a standard form because of the ambiguity arising in the act of rendering the statement.  The quantifier negation rules only apply to standard form statements, so they do not apply to statement 5.

If we could unambiguously render statement 5 as "No S are P", then since this rendering is a statement in standard form we could formalise it as an instance of statement 4.2.

If we could unambiguously render statement 5 as "Some S are not P", then since this rendering is a statement in standard form we could formalise it as an instance of statement 2.2.

Of course, we cannot unambiguously render statement 5 as one or the other.

The ambiguity is in the rendering not in the formalisation, so as a non-standard form we cannot apply the quantifier negation rules to statement 5.  That is to say, the ambiguity of 5 does not pass to 1-4 since 1-4 only apply to standard forms.

EDIT

I should have mentioned that the ambiguity in the rendering arises since it is not clear how the "scope" of the quantifier "all" is to be applied.  If "all" applies just to "S" then the rendering is "No S are P", while if "all" applies to "S are not P" then the rendering is "Some S are not P".
Nancy Cartwright's definition, applied to mind-independent reality, does not reference time:


  C causes E if and only if C increases the probability of E in every situation which is otherwise causally homogeneous with respect to E. (http://people.hss.caltech.edu/~jiji/Causation-Explanation/Cartwright.pdf Causal Laws and Effective Strategies, 423)


The technical definition of "causally homogeneous with respect to E" is articulated on p423ff. I myself encountered the above definition in Cartwright's https://scholar.google.com/scholar?hl=en&q=How%20the%20Laws%20of%20Physics%20Lie How the Laws of Physics Lie, which contains a slightly expanded version of the essay I've cited.

Switching to mind-dependent reality, you might want to consider using the term 'entailment', which can be seen as more general than 'causation'. Mathematical biologist Robert Rosen looks at the issue extensively in http://rads.stackoverflow.com/amzn/click/0231075650 Life Itself, and uses the mathematics of category theory to rigorously describe the relationship between entailment in a model of reality and causation in reality. There is also some interesting discussion of whether causation/entailment exists in the model or also in reality in the beginning of Martin Hollis' http://rads.stackoverflow.com/amzn/click/1107534372 Models of Man.

One way to consider the difference between logical entailment in formal systems and causation in reality is to recognize that not all conceptions of causation in reality are http://www.iep.utm.edu/lawofnat/#H2 necessitarian. In contrast, logical entailment in a formal system is necessitarian. Given that there does not seem to be a link between necessitarianism and time, I shall say no more on this topic.
I think we have to back off from the question and see what we mean by 'water'.  If we are questioning the structural similarity of water to the substance we know scientifically, what was it before that?  

Outside of a given scientific paradigm, water is something with three states that is usually encountered as a liquid, that in that state conducts heat well, and has a certain convenient surface tension behaviors that make it wick into cloth, that expands when it solidifies... In short, it is a collection of behaviors.

But behaviors of a substance are perceptions.  It is very hard to imagine that any collection of accidents like perception can prove to have an absolute logical necessity.  So it seems down that path your answer is going to be no.

However, these perceptions are mediated by the interface between body and mind.  So there is a huge place in the system for psychology.  To some other being, constituted quite differently physiologically, the relevant aspects of water for us, might all be met by something else.  After all, what is transparent depends upon your eyes, and what flowing means depends on your sense of time, and...

We can take half a step in this direction by looking at something like Robert Forward's 'Camelot at 30 K', where he constructs a society that uses hydrogen flouride the same way we use water, since the major difference between the two substances is that the latter has a very, very low boiling point.  He extrapolates that things are slower, so they have to be smaller in order to have the same sort of rhythm of life and otherwise finds reasonable ways to make their experience of H2F be our experience of H2O.

You can imagine continuing down this path of mapping the experiences of more and more exotic beasts onto human experience to consider stranger and stranger substances to be the equivalent of water, manipulating aspects of those variant creature's natures so that their experience of, say, boiling tar, would be our experience of water.

Eventually you could reach the point where 'water' was not two of one thing and one of another, but a compound of three different elements.

We can't get there by working up from more detailed layers of science, because we got those layers by observing our macroscopic universe, and delving it.  So when this beast explored its own chemistry and physics, it would not agree with ours.

That would mean that the laws of nature are not only distant from necessary truths, but can take variant forms depending upon what sort of beast derives them.
This reminds me of the older question https://philosophy.stackexchange.com/questions/24637/was-wittgenstein-foreshadowing-godel Was Wittgenstein anticipating Gödel? There is more to it in the case of Kant than there was in the case of Wittgenstein though, at least in spirit. One could say that Kant pioneered in epistemology the stratification into levels of discourse, which Gödel later applied to formal semantics.

When the Gödel theorem first appeared many mistook it for a paradox, like Russell's, a contradiction within a system of mathematics. Many included Russell himself, Wittgenstein and Zermelo, at least according to the traditional view, see however https://philosophy.stackexchange.com/questions/29288/russells-response-to-g%C3%B6dels-incompleteness-theorems/29304#29304 What sources discuss Russell's response to Gödel's incompleteness theorems? for a different view. The issue was that the paradox only arises if one mixes the levels of language. Gödel sentence is unprovable in the object language, the proof that it is nonetheless true is done in the meta language, if one properly distinguishes between the two the paradox disappears, and we uncover an interesting property of the object language. Russell, Wittgenstein and Zermelo were presumably thinking universalistically, within an all-encompassing logical system.

What does this have to do with Kant? Kant also has a two level distinction, not in the language but in ontology, appearances and things in themselves. Like Gödel's, Kant's predecessors were in the habit of instinctively identifying the two, and antinomic reasoning was a direct result of taking this identification to its logical conclusion. What prevents Gödel sentence from being a paradox is a subtle rephrasing of "I am false" into "I am unprovable [in a language]". Kant similarly resolves the antinomies by relating them to appearances, our 'language of mind'. What creates the Liar is the language trying to handle unrestricted truth within itself, what creates the antinomies is mind trying to handle unrestricted reality within itself. 

Both paradoxes result from disregarding self-limitations, and are resolved by explicitly reinstating them. As long as we do not regard the "world" as both an appearance and a thing-in-itself there is no antinomy of it having and not having a beginning in time, as long as we do not regard true and provable as a single item there is no Liar. Taking the analogy further one could say that Kant would have regarded Frege's universalist logicism program as a logical case of transcendental illusion, reasoning about appearances as if they were things-in-themselves. 
Many have argued that logic is empirical, or as you describe it, logic's "axioms are dependent on observation". 

Quine, in his paper "Two Dogmas of Empiricism" questioned the https://en.wikipedia.org/wiki/Analytic%E2%80%93synthetic_distinction analytic-synthetic distinction, and suggested that even analytic propositions were dependent on empirical evidence. Since the rules of logic were analytic propositions par excellence, they too, were ultimately dependent on empirical data, and were not absolute laws.  

Birkhoff and Von Neuman proposed in the 1930s that the paradoxes of Quantum Mechanics can be explained if we abandoned classical logic and used some form of https://en.wikipedia.org/wiki/Quantum_logic Quantum logic instead. Such a Quantum logic would change or abandon all together some of the rules of classical logic, and would be a perfect case of logical axioms arrived at by observation. 

Hilary Putnam discussed this in depth in his paper http://www.socsci.uci.edu/~dmalamen/courses/prob-determ/Putnam.pdf "Is Logic Empirical?",  later republished as "The Logic of Quantum Mechanics.". In it he argued that, just as empirical physical results - relativity - forced us to abandon Euclidean geometry, so it is possible that the results of quantum mechanics will force us to abandon classical logic. 

Although Quantum logic is still an active field of study up to the present day, it is does not get much attention from most philosophers and had been abandoned completely by physicists. Those who do study the topic view it mainly as a mathematical tool for studying Quantum phenomena, not as some sort of fundamental logic to replace our current classical rules of logic. 

The main problem that is faced by Quantum logic (or any such radical revision of logic, empirically justified or otherwise), is that we tend to think and communicate in classical logic. It would be very difficult, or in Kantian fashion, outright impossible for us, to perceive and discuss the world in anything other than classical logic - it seems to be hardwired into our brains. Although the https://en.wikipedia.org/wiki/Logical_atomism logical atomist program failed as a metaphysical theory, it did show us just how ingrained classical logic is into our linguistic and mental structure. As Wittgenstein stated, the limits of language are the limits of the world: No one can place themselves outside of logic, and then pick among different logics to reason and argue with, even if those alternative logics are justified.        

Those non-classical logics which have been successful (fuzzy logic, modal logic, intuitionistic logic) are those that extend classical logic, as opposed to replacing it, or at least respect classical truth tables in the limiting case.  



As an after thought, one of https://en.wikipedia.org/wiki/Mimsy_Were_the_Borogoves my favorite Sci-fi short stories discusses the idea that while logic is indeed subjective, we learn classical logic at a very young age and once we grow into adults, we are incapable of unlearning it. If we were to somehow come across non-classical logics at a very young age, we would be capable of all sorts of superhuman feats. The story is of course, just sci-fi, but I do find the idea compelling.
While the answer depends a bit on context,

generally no, you cannot express ∀x(Px ≡ Qx) as simply Px ≡ Qx . The reason has to do with bound and unbound variables. (http://www.cs.odu.edu/~cs381/cs381content/logic/pred_logic/quantification/quantification.html http://www.cs.odu.edu/~cs381/cs381content/logic/pred_logic/quantification/quantification.html) 

Thus, seeing x without the ∀ means something different than (∀x) precisely because x in the former would represent a specific x whereas in the latter use it does not refer to a specific thing but rather is a variable standing over a range . 
I think Derrida means more than just actuality when he refers to "presence." Presence in the sense of formal essence stipulates a functionality as placeholder but only in the generic sense of being indexical, or pointing to. This is presence in the more localized sense, what Deleuze associates with territorial reason.

If we take logos in the sense of Heidegger's "gathering" as the intersection of sending and receiving, the mobility of presence goes beyond the purely formal sense. This is the kind of semantic structure that speaks to the hermeneutics of any possible readers and writers and shows the radical way in which presence is de-centered and, therefore, not logocentric--oddly enough.   
They are in opposition, as Quine and Kripke generally are on interpreting modal logic, and much of what is related to it. Rigid designators are defined as those picking out the same object in all possible worlds, so unsurprisingly Quine and Kripke do not see eye to eye on this issue in particular. To pick out the same object we must agree on how it is done, obviously we want some things to be different in different worlds, so what would the sameness cover? According to Kripke, there are some naturally existing (not theory dependent) "essences" that stay the same, e.g.  being human is part of Nixon's essence, but being president is not, being H20 is water's essence, but being present in comets is not. This is how the evening star becomes identical to the morning star necessarily, both designations are seen as pointing past contingent descriptions to the common essence of Venus.

How do we decide what constitutes an essence? "Consult your metaphysical intuitions", as Almog put it in https://philpapers.org/rec/ALMNWN Naming without Necessity. In Naming and Necessity Kripke introduces something called "metaphysical possibility", and argues that it is "intuitive" to many people. So it is metaphysically possible that Nixon lost the 1968 election, but not that he is an alien, possible that Earth is not overflowing with water, but not that  water is not H20, that much is a posteriori necessary (after the chemical composition was discovered). 
The idea is that essences of "natural kinds" (Mill's term resurrected by Russell in 1948) like water are discovered by science, and reflect truths about reality. This is even more controversial than Kripke's modal metaphysics for proper names, see e.g. Ben-Yami's recent critique in http://link.springer.com/article/10.1023%2FA%3A1004527227686?LI=true Semantics of Kind Terms.

Quine anticipated this approach in http://www.thatmarcusfamily.org/philosophy/Course_Websites/Readings/Quine%20-%20Reference%20and%20Modality.pdf Reference and Modality over 20 years before it was fully developed by Kripke, and dismissed it outright:


  "...the way to do quantified modal logic if at all is to accept Aristotelian essentialism. To defend Aristotelian essentialism is not however part of my plan. Such a philosophy is as unreasonable by my lights as it is by Carnap's or Lewis's. And in conclusion I say as Carnap and Lewis have not: so much the worse for quantified modal logic. By implication, so much the worse for unquantified modal logic as well..." 


Quine did not close the door on modal logic entirely. But he favored possibility and necessity understood in terms of physical (i.e. theory dependent) conditions specified explicitly, rather than metaphysically intuited. Such approaches were developed in recent decades, see https://philosophy.stackexchange.com/questions/32568/modal-logic-without-lewiss-axioms/32575#32575 Is there modal logic without possible worlds?

An engaging historical survey of Kripke's development of modal metaphysics as a response to Quine's technical and philosophical objections to modal logic is https://www.academia.edu/12217424/Quine_and_Quantified_Modal_Logic_Against_the_Received_View Tuboly's Quine and Quantified Modal Logic.
Your question seems to reference issues that arise with self-referential statements in formal languages.

Alfred Tarski spent a great deal of time exploring questions of meaning such as these.  In particular he explored the question of defining the semantics of formal languages.  This seems very related to the question you have in mind.

Two of his works which may be of interest to you are Tarskian Truth and his undefinability theorem.  Tarski's https://en.wikipedia.org/wiki/Semantic_theory_of_truth Semantic Theory of Truth explored a very particular definition of the concept of the truth of a statement: "P" is only true if and only if P.  Its power was tying statements about linguistic utterances to "reality."

https://en.wikipedia.org/wiki/Tarski's_undefinability_theorem Undefinability may be the concept you are looking for with this question.  Tarski proved that for self-referential formal languages which meet some basic criteria (such as being able to prove arithmetic statements), a language cannot define its own semantics.  This is exactly the kind of rabbit hole one goes down when exploring the meaning of words like "is."
Mathematician David Woplert proved in 2008 that no intelligent agent can fully predict the evolution of a system that it is part of. In the context of your question, an observer within the automata can never completely predict the future state of the automata.The only way to do that would be from outside the automata. This means that there would be limits on the ability of the observer to provide formal causal explanations/models of their automata universe, and some parts of it would have to appear non-causal/non-deterministic (even if the automata is completely determined when observed from the outside). 

His result is analogous in a way to Godel's "No theory can prove it's own consistency", in that he shows that no universe can be completely predicted from the inside, hence no universe can predict itself. The universe is inherently unpredictable from the inside, showing that formal causal explanations are bound to be incomplete. 

Here's the original http://ti.arc.nasa.gov/m/pub-archive/1476h/1476%20%28Wolpert%29.pdf article and a http://www.scientificamerican.com/article/limits-on-human-comprehension/?page=1 general explanation of the idea. 
We cannot derive a contradiction from P ↔ Q and P → ¬Q, because the the two formuale are simultaneously satisfiable.

It is enough to consider a truth assignment v such that:


  v(P)=v(Q)=false.

The reference you cited doesn't assert that Kant held that the natural science are a priori. Rather, "synthetic a priori judgments provide the necessary foundations for human knowledge." In the Preface to the Second Edition to the Critique of Pure Reason, Kant made it clear that although the objects of investigation are to be determined a priori, such information is insufficient for empirical science:


  "Mathematics and physics are the two theoretical sciences which have
  to determine their objects a priori. The former is purely a priori,
  the latter is partially so, but is also dependent on other sources of
  cognition." [Bix]


The transcendental argument which is the heart of the Critique is aimed at establishing that all objects of experience are subject to the categories and, consequently, to related a priori principles such as causation. Such principles provide the necessary foundation for understanding the natural sciences, but that is only the beginning of what is required. In fact, Kant gives a nice description of the scientific method in which he asserts that reason must direct us in forming hypothesis, but our understanding of physical laws is ultimately determined through experimentation:


  "[Natural philosophers] learned that reason only perceives that which
  it produces after its own design; that it must not be content to
  follow, as it were, in the leading-strings of nature, but must proceed
  in advance with principles of judgement according to unvarying laws,
  and compel nature to reply its questions. For accidental observations,
  made according to no preconceived plan, cannot be united under a
  necessary law. But it is this that reason seeks for and requires. It
  is only the principles of reason which can give to concordant
  phenomena the validity of laws, and it is only when experiment is
  directed by these rational principles that it can have any real
  utility. Reason must approach nature with the view, indeed, of
  receiving information from it, not, however, in the character of a
  pupil, who listens to all that his master chooses to tell him, but in
  that of a judge, who compels the witnesses to reply to those questions
  which he himself thinks fit to propose." [Bxi]

Here's about half the proof you need:

1. ~(A v B)   Premise
2. | A        Assumption
3. | A v B    vI 2
4. | (A v B) & ~ (A v B) &I 3,1
5. ~A         Contradiction Elimination
6. ~A v B     vI 5
7. A -> B     Material Implication 6
...
~14. A <-> B   Biconditional Introduction 7, 13




Proof with DeMorgan:

1. ~(A v B)   Premise
2. ~A & ~B    DeM 1
3. ~A         &E 2
4. ~A v B     vI 3
5. A -> B     Material Implication 4
...
10. A <-> B Biconditional Introduction 5,9

The argument of Spinoza's https://en.wikisource.org/wiki/Ethics_(Spinoza)/Part_1 Ethics is:


  Prop.XI. God, or substance, consisting of infinite attributes, of which each expresses eternal and infinite essentiality, necessarily exists.
  
  Proof. If this be denied, conceive, if possible, that God does not exist: then his essence does not involve existence. But this (Prop.VII) is absurd. Therefore God necessarily exists.


The "essence implies existence" relies on:


  Prop.VII. Existence belongs to the nature of substances.
  
  Proof. Substance cannot be produced by anything external (Corollary, Prop.VI), it must, therefore, be its own cause—that is, its essence necessarily involves existence, or existence belongs to its nature.


and this, in turn, with the definition of God:


  Def.VI. By God, I mean a being absolutely infinite — that is, a substance consisting in infinite attributes, of which each expresses eternal and infinite essentiality.


licenses the conclusion.



Comment

Seemingly, in the "proof" of Prop.IX we need also:


  Ax.VII. If a thing can be conceived as non-existing, its essence does not involve existence.


Thus: "conceive, if possible, that God does not exist [...] But this is absurd. Therefore God necessarily exists."



In conclusion, we can state at least two "debatable" points:


is it really "absurd" to conceive "that God does not exist" ?
is it really necessary that for substance "essence involves existence" ?

Some comments.

In On Vagueness (1923) Russell writes:


  In an accurate language, meaning would be a one-one relation; no word would have two meanings, and no two words would have the same meaning. In actual languages, as we have seen, meaning is one-many.


See Tractatus (1922):


  4.111 Philosophy is not one of the natural sciences. [...]
  
  4.112 Philosophy aims at the logical clarification of thoughts. Philosophy is not a body of doctrine but an activity. A philosophical work consists essentially of elucidations.
  
  Philosophy does not result in ‘philosophical propositions’, but rather in the clarification of propositions. Without philosophy thoughts are, as it were, cloudy and indistinct: its task is to make them clear and to give them sharp boundaries.


And see Ramsey's comment (1923) on it:


  Let us now pass to Mr Wittgenstein's account of philosophy. "The object of philosophy," he says, "is the logical clarification of thoughts. Philosophy is not a theory but an activity. A philosophical work consists essentially of elucidations. The result of philosophy is not a number of 'philosophical propositions', but to make propositions clear. Philosophy should make clear and 
  delimit sharply the thoughts which otherwise are, as it were, opaque and blurred" (4.112). 


The above carachterization of the role of philosophy is common to the first phase (Tractatus) and the second phase (Philosophical Investigations) of W's philosophy: the elucidations of problems.

What drastically changes is the dismissal of the "project" of a perfect language, immune from vagueness, and the acceptance of the "real life" language, with its multiplicity of meanings, to be analyzed with the concepts of https://en.wikipedia.org/wiki/Language-game_(philosophy) language game and https://en.wikipedia.org/wiki/Family_resemblance family resemblance.

  Are propositions of natural sciences to be considered propositions of natural language?


Yes, they are; they can be "symbolized" and quite often propositions of natural science are expressed using mathematical symbolism.

But we have to remind that for W the opposition is not between natural science and logic as two different "domains" of knowledge.

Logic is not a body of truths, but only the "showing" of logical form.

Thus, we have always natural language; but different "contexts" can have different levels of "perfection", i.e. the thoughts expressed by scientific propositions must be more clear and with "sharper boundaries" (4.112) then plain language.

The elucidating role of philosophy must applyies also to science:


  4.116 Everything that can be thought at all can be thought clearly. Everything that can be put into words can be put clearly.

For "A is B" the explanation is simple.

Gensler's language has two types of "basic" formulae:


  (i) formulae expressing relation between sets ("general categoris"): "All logicians are charming", translated as "all L is C"
  
  (ii) formulae expressing the fact that an individual belongs to a set: "Gensler is a logician", translated as "g is L".


In this second case, g is the name of an individual; thus, we cannot quantify it with "all" or "some".

In the previous case, instead, L and C are names for sets and we have to quantify the first one in order to correctly express the relationship between them. If we say "Logicians are charming" (i.e. L is C) we have an ambiguous expression, because we do not know if we are asserting it of all or some Logicians.



I presume that he forbid the expression "all A is not B" as "ungrammatical" (non-wff) simply because it is already expressible as: "no A is B".
In http://plato.stanford.edu/entries/logic-higher-order/ Second-order Logic, the comprehension schema (considering for simplicity only unary predicate variables) is:


  ∃X∀x [ ϕ(x) ↔ X(x) ],


where x is an individual variable, X is a 1-ary predicate variable and X may not occur free in ϕ.



What prevents form generating http://plato.stanford.edu/entries/russell-paradox/ Russell's Paradox ?

Two facts:

(i) we cannot substitute X for x.

A s-o language for sets must use individual variables x for sets and unary predicate variables for classes. So, x ∈ A must be formalized as A(x) and thus, using it as ϕ(x), what we get is:


  ∃X∀x [ ~A(x) ↔ X(x) ].


(ii) the proviso that X may not be free in ϕ prevents from using ~X(x) to get:


  ∃X∀x [ ~X(x) ↔ X(x) ].

There's two things at work here that I think help explain how Mill is not merely providing an expression of Aristotle. (You can also look at https://philosophy.stackexchange.com/questions/14597/pleasure-vs-happiness "pleasure" vs "happiness"  and https://philosophy.stackexchange.com/questions/30669/what-is-the-causal-connection-between-virtues-and-eudaimonea-in-virtue-ethics What is the causal connection between virtues and eudaimonea in virtue ethics? )

For one thing, it's not clear that Aristotle and Mill agree about the nature of happiness. One hint is that Mill seems to identify happiness with pleasure. For Aristotle,  pain and pleasure signal right and wrong but only for the phronemos (man with practical wisdom) (NE Book 2). What Aristotle means by "happiness" is eudaimonia, a concept that invokes both "happiness" and "flourishing" and other things at the same time. In other words, it should not be identified with pain and pleasure.

At the same time, Mill does distinguish between two types of pleasure in Utilitarianism. One type of pleasure is "base" and the other "noble". But at least in my view, there's little argumentation in support of this claim in Mill's account. (Looking at it historically, this is an evolution to fix a problem with Jeremy Bentham's utilitarianism where pleasure/pain are not differentiated at all -- so everyone getting drunk every night may be the best way to maximize happiness for the most people).

The second issue is that Mill is committed to a calculative approach to morality, "the moral calculus." In contrast, Aristotle is committed to a reasoned approach to morality. It seems doubtful he thinks it can be calculated (even if it involves moderations between extremes and "fit" to one's own nature).

To reword the second claim I'm making, Aristotle is committed in part to saying regular people cannot simply calculate morality. Instead, what is right is somewhat opaque to them because they act from confused signals and poor upbringing. To know what to do rightly requires being a phronemos. For Mill, right courses of action are apparent because we just need to calculate what will bring about the most pleasure (of the right kind), which he doesn't propose to be a daunting task available only to a few.

These two considerations aside, Mill might say he's just perfecting Aristotle. After all, his claim regarding Jesus is that if/since Jesus is doing good, Jesus is maximizing pleasure for the most number (I think this is in Utilitarianism chapter 4 but I could be off on the chapter).
It is a natural idea, but unfortunately the answer is no, it is not feasible. The root of incompleteness is not numbers, but the possibility of (implicit) self-reference, arithmetic is just the simplest structure that already realizes that possibility. In fact, one does not even need the Peano arithmetic, but a much weaker https://en.wikipedia.org/wiki/Robinson_arithmetic Robinson arithmetic without even induction for the proof to go through. In the end what matters is not whether the theory has numbers, or sets, or something else, or how the pieces are connected to or isolated from each other, but only the expressive power of the theory. As long as it can imitate the minimal expressive power of arithmetic the incompleteness sets in, whether we connect numbers to other objects, or whether we even have numbers at all, makes no difference. The incompleteness does not spread from numbers, it is inherent in anything that can simulate numbers. If the other objects can not they remain "healthy", but then their theory is weaker than arithmetic so we can drop them altogether for we are essentially committing to reducing all mathematics to arithmetic (this is not to say that some useful complete fragments weaker than arithmetic do not exist, elementary Boolean algebra and elementary plane geometry are examples). 

One can go a surprisingly long way with that actually, this is called http://plato.stanford.edu/entries/nominalism-mathematics/#MatFic predicative mathematics. As nominalists like Field showed, while it is weaker than classical mathematics, it is enough for all the purposes of classical physics at least. In predicative mathematics the incompleteness is essentially reduced to that of arithmetic only. Wittgenstein was willing to go even further, and reduce mathematics to primitive recursive arithmetic, which is finitist, see https://philosophy.stackexchange.com/questions/24637/was-wittgenstein-anticipating-g%C3%B6del/24641#24641 Was Wittgenstein anticipating Gödel? But if we really want to beat incompleteness without trivializing mathematics, structural manipulations won't help, we have to give up one of Gödel's other premises: either that mathematics is recursively axiomatizable (axioms are recognizable as such), or that it is consistent (or both). Again, Wittgenstein was willing to give up consistency and confine contradictions using what later developed into non-classical ("dialetheic") logic. Development of these ideas led to modern inconsistent mathematics, which produces complete inconsistent arithmetics that can prove non-triviality of their consistent parts, see https://philosophy.stackexchange.com/questions/29338/can-a-turing-machine-generate-an-inconsistent-formal-system Does Gödel's argument that minds are more powerful than computers have the inconsistency loophole? and https://philosophy.stackexchange.com/questions/26796/in-which-text-paper-was-the-concept-of-dialetheism-first-introduced-as-a-serious/26810#26810 In which text/paper was the concept of dialetheism first introduced as a serious position?

It is unlikely that predicativism, finitism or dialetheism will become mainstream positions among mathematicians however. They are viewed as too restrictive and/or artificial to support the existing mathematical practice, which does not really need either completeness or foundations.
By ideas that 'help us to get into  satisfactory relations' James means ideas that are useful to believe. James is therefore an instrumentalist about truth. A few sentences later in the work you cited he says:


  Any idea that will carry us prosperously from any one part of our experience to any other ... is ... true instrumentally.


In what sense is a belief useful? Interpreting James, Kirkham [1] suggests a few:


The belief helps us to manipulate objects in the world.
The belief allows successful communication.
The belief leads to accurate predictions.
The belief explains other phenomena.


Holding that a belief is true whenever it is useful is of course very controversial, and it differs greatly from more mainstream views that hold that things are true because of some objective and mind-independent facts. Indeed, for James, there are no mind-independent facts and he thus holds that truth and reality are relative.

In The Meaning of Truth (1909) James says:


  Truth may vary with the standpoint of the man who holds it. (p. 135)


It's quite difficult to defend such a view. But I'll leave it at that.

Reference

[1] Kirkham, Richard, 1992, Theories of Truth: A Critical Introduction, Cambridge, MA: MIT Press.
I contacted one of the authors of the textbook, Max Cresswell and he kindly suggested me a solution to the problem.

All I need to prove is that K follows from K**.

So here's a proof:


((p->q)&p)->q PC axiom.
L((p->q)&p)->Lq 1,R*.
(L(p->q)&Lp)->L((p->q)&p) K2*.
L(p->q)->(Lp->L((p->q)&p)) 3,PC.
L(p->q)->(Lp->Lq) 2,4, PC.


Where PC is axiom of propositional logic, and in 5 I used a sort of hypothetical syllogism.
Sorry for not getting back to you sooner.  The short version of my answer is something like: to me Pragmatism is more like an attitude or orientation than a claim.  If it were a claim along the lines of a definition of truth or reference in terms of practices then it would be circular as you note.  But pragmatists do not (usually) presume to define truth and reference (i.e. what they are); rather they tend to explain the way we use those terms, and then (usually) argue that looking for more than that is a waste of time.  At the extreme this means that concepts like truth and reference are vacuous; they are not properties, have no substantial role, and have no explanatory value.  Do a little searching on "deflationary truth" for more on this.

Most (all?) pragmatists are anti-foundationalists, by which I mean they do not offer a "better" foundation (or "claim") than the old ones; rather they reject very idea that we can get at such a foundation.  Not that they disprove it or deny that some such may exist.  Maybe a foundation exists, maybe not, but either way arguing about it is a waste of time.

So "how pragmatism can explain the content of its own words and standards"?  By explaining how that content is instituted by and illustrative of our practices, not by appealing to concepts like truth and reference.  The nice thing about this is that is naturalistic, and so compatible with natural science - explaining our practices, which are naturalistic, is something natural science can (in principle) do.

Note that terms like "practical bearings" or even "practices" are not philosophically mysterious, unlike semantic vocabulary like "truth".  The practices of a linguistic community are analogous to the behavior of a population of non-human creatures or of a system of non-living particles: something that can be observed and described scientifically.  So there's no circularity in using such terms to describe how we explain our practices.

BTW, the risk of circularity was/is indeed well known to praggies, and the issues are actually much more complex and subtle than I've indicated.  Plus there is more than one such risk.  For a detailed argument about a possible circularity in the way pragmatists use semantic vocabulary like "true" and "refers" ("semantic" vocab being terms that purport to get their significance from representational relations) see Huw Price's response to Horwich in http://www.cambridge.org/us/academic/subjects/philosophy/philosophy-mind-and-language/expressivism-pragmatism-and-representationalism Expressivism, Pragmatism and Representationalism.

Hope that helps.  FWIW the best intro to contemporary pragmatism that I know of is http://www.polity.co.uk/book.asp?ref=9780745646657 Pragmatism by Michael Bacon.

Edit (in response to comments): Consider Wittgenstein's famous dictum "meaning as use".  That's a slogan, but it is often offered as a kind of definition, "meaning is use".  The problem is it looks like a definition of meaning, which it is not, and the word "use" inevitably suggests instrumental use.  But the kind of practices involved are not necessarily instrumental, at least not for the individual.  E.g. "Ouch!" - there's no goal achieved by squeeling that, as far as I can see.  It's easy to imagine many less elementary cases.

Replace "meaning as use" with "practices explain signficance" and you get closer to contemporary pragmatism.  More technical:  normative practices institute conceptual content.  It's really the normativity that makes the difference.  Here the risk is infinite regress: to apply a first-level rule, you have to have a 2nd-level rule telling how to apply the first-level rule.  To apply the 2nd-level rule, you need a 3rd-level rule, and so on ad infinitum.  This is where practice comes it: it halts the regress.  At some point we say "that's just the way we do things around here."  So practice is the unexplained explainer.

The normativity of practice is instituted socially, by punishment of deviation from norms and reward of conformance.  If you use a word in a way that does not conform to community standards your interactions in the community are more likely to result in failure.  This is where Pragmatism is deeply related to evolutionary thinking.

As to circularity and the need for Pragmatism to explain its own terms I guess the question would be something like, if normative practices explain conceptual content, then what is it for normative practices to explain the conceptual content of "normative practices".  Point one: we've already rejected the idea that we need to explain some mysterious semantic relation between the term "normative practices" and something in the world that we describe using that term.  Second, it's not the term "normative practices" that does the explanatory work, its the practices themselves, which can be observed and studied scientifically, unlike whatever it is that "true" and "refers" are supposed to mean.  So in looking to our practices to explain our term "practices" there is no circularity.  Just think of it as "look at what we do when we employ the term 'practice' and that will tell you what the term 'means' (to us, at least)"; this does not require an antecedent definition of "practice" that needs explanation.  Or maybe think of "normative practices explain conceptual content" (or whatever your preferred pragmatism slogan is) as a methodological recipe rather than a definition.

Dunno how convincing that is but it's about the best this non-professional can do.  If you really want to dig into this I highly recommend Bacon's Intro or Robert Brandom's http://www.hup.harvard.edu/catalog.php?isbn=9780674006928 Articulating Reasons.  FWIW reading Brandom changed my life, no lie.
See page 247-48 (and footnote page 247):


  The idea of the proof of this theorem can be expressed in the following words: (1) a particular interpretation of the metalanguage is established in the language itself and in this way with every sentence of the metalanguage there is correlated, in one-many fashion, a sentence of the language which is equivalent to it (with reference to the axiom system adopted in the metatheory); in this way the metalanguage contains as well as every particular sentence, an individual name, if not for that sentence at least for the sentence which is correlated with it and equivalent to it. 


This is what in the proof of Gödel's Incompleteness Theorems has been called: the http://plato.stanford.edu/entries/goedel-incompleteness/#AriForLan arithmetization of syntax.

The original Gödel's proof "diagonalize" without using an explicit "general purpose" http://plato.stanford.edu/entries/goedel-incompleteness/#DiaSelRef Diagonal Lemma.


  (2) Should we succeed in constructing in the metalanguage a correct definition of truth, then the metalanguage — with reference to the above interpretation — would acquire that universal character which was the primary source of the semantical antinomies in colloquial language. It would then be possible to reconstruct the antinomy of the liar in the metalanguage, by forming in the language itself a sentence x such that the sentence of the metalanguage which is correlated with x asserts that x is not a true sentence. In doing this it would be possible, by applying the diagonal procedure [emphasis added] from the theory of sets, to avoid all terms which do not belong to the metalanguage, as well as all premises of an empirical nature which have played a part in the previous formulations of the antinomy of the liar. 


You can find the "diagonal argument" referred to by Tarski in e.g.: 


Joseph R. Shoenfield, https://books.google.it/books?id=a9zuAAAAMAAJ Mathematical Logic (1967), page 131:



  Diagonal Lemma (Cantor). Let P be a binary predicate. For each number b, we define a unary predicate P_b by P_b(a) ↔  P(a,b). 
  Let P be a binary predicate, and let Q be the unary predicate defined by Q(a) ↔ ¬ P(a,a). Then Q is distinct from all the P_b. 


A simple proof of Tarski's Theorem in "old style" can be found in: 


Roger C. Lyndon, https://books.google.it/books/about/Notes_on_logic.html?id=KLI-AAAAIAAJ&redir_esc=y Notes on Logic (1966), page 80-on.

Truth table:

A  B  A*B  ~A>~B  (A*B)>(~A>~B)
===============================
T  T   T     T         T
T  F   F     T         T
F  T   F     F         T
F  F   F     T         T


The sentence is a tautology.
See:


Anne Troelstra & Dirk van Dalen, https://books.google.it/books?id=-tc2qp0-2bsC&pg=PA17 Constructivism in mathematics: An Introduction. Volume 1 (1988), page 17.


There is no "explicit" philosophy of mathematics in Kronecker's works.

He may be regarded as a "constructivist", or perhaps as a precursor of the finitist approach. 

In his essay "Uber den Zahlbegriff" (1887) he outlined the project of "arithmetizing" Algebra and Analysis; that is, to found these disciplines on the fundamental notion of number, avoiding thus geometrical intuition. 

In his arithmetization project he considered a mathematical definition acceptable only if it could be checked in a finite number of steps, criticizing the "pure" existence proofs. He stated that an existence proof for a number could be considered correct only if it contained a method to find the number whose existence was proven.

Some of his remarks belong to mathematical folklore, like his widely reported statement that: 


  "the Lord made the natural numbers (ganze Zahlen), everything else is the work of men"; 


the same idea is reiterated in the following statement:


  "I consider mathematics only as an abstraction of the arithmetical reality". 

Arithmetization of syntax allows Gödel to show that statements about number theory are also statements in number theory. This allows him to construct self-referential statements about number theory in a simple way. This in turn allows him to show that Self-reference is inevitable, and that it is impossible to avoid self-reference when trying to construct complete formal axiomatic systems rich enough to describe arithmetic (And thus refuting Russell's logicist project). 

There are many resource, but a good informal one which accessible to mathematicians and non-mathematicians alike is Douglas Hofstadter's "I am a Strange Loop" Chapter 10 - Gödel's Quintessential Strange Loop.   
The short answer is that Quine is not a mathematical realist as intended in the question (on my reading of it). Why does he call himself a realist? Because he practices what he preaches. Indeterminacy of translation, and hence meaning, implies that words only mean as relata in a scheme, not as individual references to raw reality or mental content. Holism of verification implies that conceptual scheme is only testable as a whole, although he moderated this to "chunks" with a "critical mass" in http://www.jstor.org/stable/40231747 Two Dogmas in Retrospect (his dismissal of analyticity is somewhat moderated there as well, so it should be read in conjunction with the original Two Dogmas for a full picture):


  "Looking back on it, one thing I regret is my needlessly strong statement of holism... "no statement is immune to revision". This is true enough in a legalistic sort of way, but it diverts attention from what is more to the point: the varying degrees of proximity to observation..." 


So "2+2=4" is subject to revision only in a legalistic sort of way, in practice it is highly likely to stick around under the maxim of "minimal mutilation". But we could make it false by changing the use of symbols 2, +, = and 4, in the absence of intrinsic meanings this is hardly surprising. 

What of existence? This is clarified in http://math.boisestate.edu/~holmes/Phil209/Quine%20-%20On%20What%20There%20Is.pdf On What There Is:


  "To be assumed as an entity is, purely and simply, to be reckoned as the value of a variable", “to be is to be the value of a variable” in a scheme. "We look to bound variables in connection with ontology not in order to know what there is, but in order to know what a given remark or doctrine, ours or someone else's, says there is". 


So when Quine says that mathematical and physical objects "really exist" what it amounts to is that they can not be eliminated from our current scientific scheme by paraphrase, like "the current king of France" can be. In his own words from Theories and Things:


  "I see no way of meeting the needs of scientific theory... without admitting universals irreducibly into our ontology... Nominalism... is evidently inadequate to a modern scientific system of the world". 


This came to be known as the "indispensability argument" against nominalism, and it is in this way that Quine is a realist about universals. And this realism has no need for the analytic/synthetic distinction. 

This is clearly not the colloquial meaning of "existence" or "realism", although Quine would claim that he is using the words in the same role relative to the scientific scheme as they are used colloquially relative to the naive everyday "scheme". And if a paraphrase, known as "nominalistic reconstruction", of universals were to be found, well, then presumably they won't exist no more. Burgess, a traditional realist, gives an illuminating explanation with a bit of history in http://www.thatmarcusfamily.org/philosophy/Course_Websites/Readings/Burgess%20Why%20I%20Am%20Not%20A%20Nominalist.pdf Why I am Not a Nominalist:


  "Some antinominalists have argued  that   the  conflict   between   nominalism   and   science   is  so  strong  that nothing  like  modern  science  as  we  know  it  could  survive  if  the  nominalist  ban on  mathematical  abstractions  were  accepted.  Such  a  position  has  been  reluctantly  maintained  by  the  ex-nominalist  Quine  ever  since  the  failure  of his joint attempt  with  Goodman  at nominalistic  reconstruction.  Such  a position  was  also maintained,  under  Quine's  influence,  by  Hilary  Putnam,  during his phase  of  enthusiastic  realism... In short, Quine and   Putnam  have   maintained  that  mathematical  objects  are scientifically    indispensable... Quine and Putnam have been false friends of numbers in making the case, for their acceptance seems to depend on a claim of indispensability".


Well, where Quine and Goodman failed in 1940s Field and Chihara largely succeeded in 1980s. Predicative mathematics has been http://plato.stanford.edu/entries/nominalism-mathematics/#MatFic nominalistically  reconstructed, and it does seem to suffice at least for most of science. So presumably Quine would go back to nominalism now, we know Putnam did.

Finally, it is tangential to Quine, but traditional mathematical realists (Platonist or Aristotelian, see https://philosophy.stackexchange.com/questions/34073/for-a-mathematical-realist-is-there-a-distinction-between-real-mathematical-obj/34079#34079 For a mathematical realist, is there a distinction between real mathematical objects and constructed mathematical objects?) have no need for the analytic/synthetic distinction either. According to them mathematical knowledge derives from ideal perception just as physical knowledge derives from sense perception, so it is in a way "empirical". Of course, they have the formal/material distinction depending on the source instead, hence Husserlian division into formal/material sciences in Logical Investigations. It is empiricists like Russell or Carnap, who consider the five senses to be the sole source of knowledge (no innate ideas, rational intuition or mental a priori), but wish to carve out a privileged epistemological status for mathematics, that need analyticity to get it done.
Józef Maria Bocheński, O.P.'s 1959 https://isidore.co/calibre/browse/book/5775 A Precis of Mathematical Logic defines "mathematical logic" this way on p. 1:


  0.2. Logic and mathematics. Mathematical logic is called 'mathematical' because of its origin, since it has been developed particularly with the aim of examining the foundations of this science. There is moreover a certain external resemblance between its formulas and those of mathematics. Certain logicians also claim that mathematics is only a part of logic, although this opinion is far from receiving general approval. However, mathematical logic does not consider either numbers or quantities as such, but any objects whatsoever.

Yes, it is okay for a sentence to refer to itself.

Consider:


  This is a sentence.


There doesn't seem to be any problem in saying that it is true. Importantly, it can be given a precise form and even proven (e.g. in https://en.wikipedia.org/wiki/Peano_axioms PA). One can construct a predicate Sent(x) which says of a given x that it is a sentence. Now consider the sentence A:


  Sent(⌈A⌉)


("⌈A⌉" is used to refer to the sentence A. This can be done with https://en.wikipedia.org/wiki/G%C3%B6del_numbering Gödel numbering.)

This is a well defined self-referring sentence. As noted above, it can also be proven.

So if self-reference is okay, then what's the deal with this:


  This sentence is false.


This leads to a contradiction since it can be shown to be both true and false (and not neither of them as you suggested).

This is known as http://plato.stanford.edu/entries/liar-paradox/ the liar paradox. Some philosophers and logicians (e.g. https://en.wikipedia.org/wiki/Alfred_Tarski Tarski, https://en.wikipedia.org/wiki/Saul_Kripke Kripke) have attributed great importance to it, since it seems to say something about the notion of truth. Some of them have attempted to solve it. You can read about these attempts in the above link.

Another great source discussing these issues is http://plato.stanford.edu/entries/self-reference/ this very good SEP entry on self-reference.
You can see https://archive.org/details/arithmeticespri00peangoog Arithmetices principia: nova methodo exposita (1889), translated into:


Jean van Heijenoort (editor), https://books.google.it/books?id=v4tBTBlU05sC&pg=PA85 From Frege to Gödel: A Source Book in Mathematical Logic (1967), page 85-on:



  Questions that pertain to the foundations of mathematics, although treated by 
  many in recent times, still lack a satisfactory solution. The difficulty has its main source in the ambiguity of language. [...] My goal has been to undertake this examination, and in this paper I am presenting the results of my study, as well as some applications to arithmetic. 
  
  I have denoted by signs all ideas that occur in the principles of arithmetic, so that every proposition is stated only by means of these signs. 
  
  The signs belong either to logic or to arithmetic proper. The signs of logic that occur here are ten in number, although not all are necessary. 


Thus, it seems that logic and arithmetic - contrary to the founding fathers of logicism: Frege and Russell - are distinct.

But the signs K for classes and ∈ [est] for membership are listed as logical signs, and thus the theory of classes is part of logic, and this is common to Frege and Russell.



For a detailed discussion, see:


Hubert C. Kenendy, http://www.jstor.org/stable/186238?seq=1#page_scan_tab_contents The Mathematical Philosophy of Giuseppe Peano (1963), 


where it is argued for a "rejection of the logicist thesis of the reduction of mathematics to logic." [Also available http://hubertkennedy.angelfire.com/TwelveArticles.pdf here].
See:


  6.41 The sense of the world must lie outside the world. In the world everything is as it is, and everything happens as it does happen: in it no value exists—and if it did exist, it would have no value.


Here W speaks of ethics, but we may apply it also to theology.


  6.432 How things are in the world is a matter of complete indifference for what is higher. God does not reveal himself in the world.
  
  6.4321 The facts all contribute only to setting the problem, not to its solution.
  
  6.44 It is not how things are in the world that is mystical, but that it exists.
  
  6.5 When the answer cannot be put into words, neither can the question be put into words. The riddle does not exist. If a question can be framed at all, it is also possible to answer it.
  
  [...] 6.52 We feel that even when all possible scientific questions have been answered, the problems of life remain completely untouched. Of course there are then no questions left, and this itself is the answer.


So, it seems, the "meaningful" (i.e. scientific) questions are those related to the facts in the world. We can "speak of" them with our language and thus - in principle - we can answer them.

But we have here also a distinction between "questions" (the scientific, i.e. legitimate ones) and "problems", that remain untouched by science, like ethics and theology; here is the place for the mystical:


  6.522 There are, indeed, things that cannot be put into words. They make themselves manifest. They are what is mystical.


Frankly speaking, the link from the theory of language to the "philosopical" problems that stay outside the language, seems feeble to me.
Yes, this is Modus Tollens, a Hypothetical that reaches its conclusion by denying the consequent of a conditional statement: If X, then Y. Not Y. Therefore not X. It is deductively valid.

However, If X, then Y.  Not X.  Therefor not Y.  This is deductively invalid. It is a fallacy called "Denying the Antecedent," a hypothetical that reaches its conclusion by denying the antecedent of a conditional statement.
it's kinda the same as https://en.wikipedia.org/wiki/Karl_Popper popperian https://en.wikipedia.org/wiki/Falsifiability falsifiability in science. 

developing a new proposed theory that, in essence, says nothing different than the old existing theory does not carry new meaning.  the old existing theory is good enough and the new theory is not falsifiable and may not survive as a viable theory.  it's only if the new proposed theory makes some claim that differs tangibly from the existing theory, such that one can set up an experiment and observe the difference (or fail to observe it), only those theories are meaningful in science.
Yes.  He believed he saw indeterminacy of translation in actual languages.  Of course his ideal case of "radical translation" involves languages with no common history at all--which never happens on this earth.  English speakers can (and do, and must) draw on vast historical precedent in translating a closely related language such as French.  But many language pairs are a lot less related than that.  And all sciences build theories on non-ideal evidence.

Quine was in fact an avid language learner.  He lectured in Portuguese in Brazil, and in WW II he worked in naval intelligence decoding German messages.  I have heard that he also worked in Asian languages for intelligence but I cannot find confirmation of that now.  Anyway,Tom Tillemans article "Count nouns, mass nouns, and translatability" (in Chakrabarti et al eds. Comparative Philosophy Without Borders) quotes Quine's argument for one kind of indeterminacy, namely that of his "gavagai" example, based on actual features of Asian languages. Tillemans is skeptical of Qune's analysis of the Asian languages but he shows how Quine intended to describe actual situations in translating between languages (naturally with more evident indeterminacy for languages that are less related to each other historically).

Apart from Tillemans' article, and my avowedly undocumented claim about working n Asian languages for intelligence, this can all be sourced from Wikipedia and Mactutor.
This isn't a fallacy, but rather an example of what Sartre and deBeauvoir identified as "bad faith," which is when a human being denies his or her freely willed agency in the world.  That agency manifests in the inescapable necessity of making choices.


  Bad faith (from French mauvaise foi) is a philosophical concept used by existentialist philosophers Jean-Paul Sartre and Simone de Beauvoir to describe the phenomenon where human beings under pressure from social forces adopt false values and disown their innate freedom hence acting inauthentically. It is closely related to the concepts of self-deception and ressentiment.
  https://en.wikipedia.org/wiki/Bad_faith_%28existentialism%29 https://en.wikipedia.org/wiki/Bad_faith_%28existentialism%29

I doubt that Schelling is getting his inspiration on this point from Hegel. More likely, they are both depending on the conception of science at the time.

A few pieces of background on what's going on in the Hegel passage that makes this unlikely:


Most of Hegel's texts are divided between a terse version and Zusatze ("Additions"). The quote is from an addition which means it's meant (at least in part) to interpret the terser bit rather than to make a novel point.
The original part is section (basically like a paragraph) 4 of the Philosophy of Right which states



  The basis [Boden] of right is the realm of spirit in general and its precise location and point of departure is the will. The will is free, so that freedom constitutes its substance and destiny [Bestimmung] and the system of right is the realm of actualized freedom, the world of spirit produced from within itself as a second nature. 



Within the addition, the part you're quoting appears as the analogy to argue from -- not the claim to argue to. The Hegel quote is not trying to make a point about matter but about freedom and the will. The quote continues:



  ... Heaviness constitutes the body and is the body. It is just the same with freedom and the will, for that which is free is the will. Will without freedom is an empty word, just as freedom is actual only as will or as subject. But as for the connection between the will and thought ...



Hegel and Schelling were not on good terms at the time the book was written, so if it was a novel development by Hegel, then Schelling would have wanted to distinguish his position from it. None other than Charles Taylor notes the following about Hegel's failure to predict evolution:



  This is another example of how Hegel's philosophy of nature was dependent on (his understanding of) the science of his time as well; other writers in the same field like Schelling; while his philosophies of man and history struck out beyond all contemporaries (Charles Taylor, Hegel, Cambridge University Press: 1975, 91). 

This paragraph is false. This sentence is false.  
Let's assume contrary to @shane that you have correctly expressed your real motivations.

If your motivation is to be 'avant-garde' or to be contrarian, then you have a problem.  Not everyone can be contrarian, and purposely flout the opinions of the majority for sport.  Nor can everyone simultaneously 'push the boundaries' on purpose by constantly undermining popular opinion.  We value those actions in a modern competitive society because they keep things evolving.  But it seems clear that Kant would not.

To play with expectations, with those motivations, is not universalizable.  Knowingly disturbing people only for the sake of your own ego, or to improve some abstract process, is treating them as mere means.  Especially because the people you are most likely to disturb are those who will not value the disruption, even if it does, in the end, improve their lives, or at least those of their progeny.

If this is not a chosen motivation, and you like what you like in all honesty, there is no problem with being out of step by simply being yourself.  Sentiments are not moral, according to Kant, so you are free to have what sentiments you have, as long as your motives and considerations are right.

--- Here is a whole different framing for this argument, focussed on the questions raised in the comments.  It is the same argument, just much, much pickier.

It is called the categorical imperative to rule out arguments tied to contingencies. Theoretically, for Kant, every truly moral argument should be understood by any adequate intelligence, not just other humans, and he tried to analyze out the kinds of bases that an intelligence would have to have. If the intention cannot be put in terms of a certain sort, it is not 'categorical' and cannot describe a worthy maxim because it could not be translated into a usable form for a different kind of intelligence that recognized a different range of contingency.

There are huge holes in his analysis of what is and is not a category, or how exactly categories should be combined into a maxim if other intelligences do not necessarily have languages.  It is one of the primary weaknesses of the resulting ethics.  But there is an idea that we should be able to develop an intuition for what is and is not adequately general, even if Kant's attempts to do so are either too hard to understand or too incompletely expressed for most of us to use them.

It is clear that sentiments don't qualify, so yearnings, insecurities or other motivations for winning can't be an important part of a moral argument. Neither can natural human desire for a given outcome if it is just an emotional position contingent upon our humanity.

But 'beauty' does qualify. Doing something with others well is a form of beauty, even if that thing is competition. So it is probably universalizable to say that if you join in a communal endeavor, you should intend to do as well as possible. Everyone can apply that to their own behavior.  So to the degree that choosing beauty is done competitively, we are all still fine with most forms of competition.

Cultivating and recognizing originality could also be an important aspect of beauty, especially if variety multiplies the opportunities for beauty itself.  But in this case, the approach to finding originality stems from rebelling against the status quo.  Were it universalized, the status quo against which one is rebelling would cease to exist.  Like 'truth' in the classic Kantian argument against lying, defying standards dissolves the opportunity for standards to exist long enough to be defied.  So we would lose the ability to follow our maxim if we agreed upon it.
The question is slightly out of focus: it is conventional to say that the material conditional is defined by the truth table that it has. One does not need to argue why it has this truth table, though it is relevant to ask why such a truth function is useful and important. 

In the propositional calculus, all connectives are truth functions, and it is a principle of compositional semantics to try to account for the meaning of a compound sentence as a function of the meanings its parts, as far as possible. One of the truth functions we would like to have is one that minimally captures certain relationships in deductive logic, such as modus ponens and modus tollens. If we posit a two place truth function A * B, we want a relation that satisfies A * B; A therefore B (modus ponens) and A * B; ¬B therefore ¬A (modus tollens). These constraints are not enough on their own, because they are consistent both with the material conditional A→B and the material biconditional A↔B. A further constraint is that affirming the consequent is invalid, i.e. A * B; B does not entail A. The material conditional is the only truth function that satisfies all three. Consequently, it serves a useful role within the propositional calculus for expressing implication relationships. 

An important feature of the material conditional is that it is purely a truth function and cannot express the full richness of what is meant by if/then sentences in natural languages. This is not a problem, since we can define all kinds of other conditionals if we wish, and the literature is replete with examples. For example, there are strict conditionals, which are just material conditionals under the scope of a modal operator. There are analogs of material implication in non-classical logics that have different rules of inference, e.g. in intuitionistic logic, where the → conditional is naturally interpreted as "I can prove that a proof of A can be transformed into a proof of B". There are conditionals in non-classical logics with different definitions, e.g. those of Kleene and Łukasiewicz. There are semantically defined conditionals, such as those given by Frank Jackson, Robert Stalnaker and Hartry Field. Also, the conditional probability P(B|A) is a kind of conditional that is defined within probability theory and has its own logic. 

All of these are tools in the logician's toolbox and it is part of the logician's art to know which tool to use for which job. It is important not to suppose that the material conditional is the only way to represent a conditional relationship. It is one tool in the box, and usually the first conditional you are introduced to when learning logic, but not the only one. 
The term "material implication" was coined by Russell, who made a distinction between formal and material implication.

Here's a quote from the Principia:


  [W]herever [...] one particular proposition is deduced from another, material implication is involved, though as a rule the material implication may be regarded as a particular instance of some formal implication, obtained by giving some constant value to the variable or variables involved in the said formal implication.


So material implication concerns implication between particular propositions, whereas formal implication is supposed to be more general. It doesn't have much to do with matter as in physical stuff, it is material only in the sense of being a particular instance of something.

Nowadays the term "material conditional" just means the familiar conditional with its familiar truth conditions. I don't think that "formal conditional" or "formal implication" is still used though, but maybe others could elaborate.

Also note that there are other conditionals which are not material, like the subjunctive conditional (e.g. "If Oswald had not shot Kennedy, then someone else would have").

  Does Aristotle mean that wonder is the beginning of philosophy in the history of philosophy and hence not repeatable; or does he mean that it is the beginning of philosophy in every man or woman who is struck by the wonder of the world, and then takes hold of this orientation?


Well, both. Aristotle says this near the beginning of the Metaphysics. He asserts that philosophy was, and continues to be, born from wonder. That is, from an acute awareness of ignorance, together with an accompanying desire to escape ignorance. A consideration that Aristotle invokes to support his theory that philosophy was historically a fruit of wonder, is that philosophy appeared at a time "when more or less all the necessary sciences existed". So that philosophy itself was not a "necessary" science - but a science for the sake of science.


  But it is clear that this science [=philosophy] is not productive [=aimed at a practical purpose] also from the early history of philosophy. For it was because of wonder that men both now and originally began to philosophize. To begin with, they wondered at those puzzles that were to hand, such as about the affections of the moon and events connected with the sun and the stars and about the origins of the universe. And the man who is puzzled and amazed is thought to be ignorant (hence the lover of stories is, in a way, a lover of wisdom, since a story is composed of wonders). And so, if men indeed began to philosophize to escape ignorance, it is clear that they pursued science for the sake of knowledge and not for any utility. And events bear this out. For when more or less all the necessary sciences existed, and also those connected with leisure and lifestyle, this kind of understanding began to be sought after. So it is clear that we seek it for no other use but rather, as we say, as a free man is for himself and not for another, so is this science the only one of the sciences that is free. For it alone exists for its own sake. (Metaphysics Book Alpha Ch 2)

Everything is derivable from first principles because there are no constraints on the choice of first principles. 

The more interesting question is  whether fascism is derivable from plausibly true first principles. Here again though I think the answer is No. I'm more sure that it is wrong to murder my political rivals than I could ever be of any of the abstract principles from which someone might attempt to derive this claim.
The vaccuous truth of universal quantification over an empty set strikes again.

A >--> B because B, being blind, has no accessible worlds, and therefore lacks access to any recourse with which to defend itself.  It can supply no information that would keep any proposition in A from being verified.
Something logically possible is something that is not logically impossible, and logical impossibility need not be defined circularly. Much of our modern ways of thinking about modal logic can be traced back to Leibnizian thought, and Leibniz associated impossibility with contradiction. For him, contradictions were impossible combinations such that, for any ontology, the only necessarily false statements were contradictory statements. To be more specific, necessitatem absolutum, in Leibnizian philosophy, entails truth in all possible worlds by virtue of principium contradictionis. “Les vérités nécessaires sont fondées sur le principe de contradiction” (Leibniz 1686). Likewise, impossibility entails falsity in every possible world, which justifies principium exclusi tertii sive medii inter duo contradictori. “In like manner as […] an assertion cannot be both true and false, so […] an assertion must be either true or false” (Mill 1843).

[Note: Necessitatem absolutum ought not to be confused with necessitatem ex hypothesi. “Necessity […] consists either in the constant conjunction of like objects, or in the inference of the understating from one object to another” (Hume 1748). Necessitatem ex hypothesi is the truth/falsity of an apodosis as being contingently necessary for the truth/falsity of any hypothesis to which that apodosis belongs (irrespective of any protasis in particular). On the other hand, necessitatem absolutum may invoke a Parmenideanistic mundus intelligibilis (perhaps evocative of Platonic-Pythagorean εἶδοη). “Indépendamment de la preuve qu'on appelle apodictique [il y a donc] une certitude que nous avons souvent [...] qualifier de philosophique ou de rationelle, parce qu'elle résulte d'un jugement de la raison” (Cournot 1851).]
In the strongest sense, an ideology is a specific way of looking at ideas in terms of a chosen motive.  It differs from a mere theory in that it makes ideas fit into it, or rather makes them fit together in terms of itself, instead of trying to fit together with other theories as though they are on par with it.

So prescriptive religions are ideologies, and to a certain degree so are non-prescriptive religions, (the earlier answer's choice of Buddhism is excellent.)  But so are other overarching philosophical constructs like Marxism, psychoanalysis, pacifism, statism, or paradigmatic Science.  If you accept them as presented, they propose a given way of manipulating all other ideas and forming your overall worldview into a structure subsidiary to themselves.

So to your main question -- the question of whether you are following a given ideology is most easily answered by asking what would cause you to consider it irrelevant.  For a follower, the ideology is relevant to any fact or idea even distantly logically connected to it, and the new idea must always be first vetted in terms of the ideology, not the other way around.

To the second point -- yes.  It is easy to take things that are intended to be ideologies and treat them as simple theories, to consider whether they provide a good way of looking at things under certain conditions, but never using them as a guide to your overall process of learning.  This is especially relevant if a number of ideologies impact your life at the same time.

Orthodox religious people throughout history have always done this to sciences; theist scientists often do this to religion; old-fashioned feminists and pacifists subordinate Marxism to their own ideology, but can make extensive use of it as a motivating explanation, and may still do this within a more strictly scientific framing based upon statistical observation.  Things can get as layered as you choose, but if there is intellectual bedrock, you are following an ideology.
There's no paradox here; the answer is simply yes, it is possible for something to be impossible. The appearance of paradox stems from confusing this:


  It is possible that it is impossible that X


for this:


  It is possible that X


which are obviously different.

Example

Consider the following statement:


  There are married bachelors.


Call that 'X'. We know that X is impossible. Formally: ~◊X.

In standard accounts of http://plato.stanford.edu/entries/logic-modal/ modal logic, anything that is true is also possible. That is, if A, then ◊A.

By the above we get: ◊~◊X. That is, it is possible that it is impossible that there are married bachelors
No, existence does not seem to me analytic.

Your argument for the analyticity of existence relies on extraction from context, like this:


  a exists "in some sense" => a exists


If this were a valid inference, we would have also, by analogy:


  a doesn't exist "in some sense" => a doesn't exists


Now, for any controversial object, e.g. Batman, there will be a sense (s1) in which it exists, and another sense (s2) in which it doesn't exist. So we have:


  Batman exists in sense s1 => Batman exists


But also


  Batman doesn't exist in sense s2 => Batman doesn't exist


And from these two, we have


  Batman both exists and doesn't exist


Which is absurd.

To prevent such absurd conclusions, we need to disallow the extraction from context. So that, for example


  That Batman exists "in some sense" does not imply that Batman (simply) exists


By the way, Kant, who inserted the analytic/synthetic distinction into the philosophical dictionary, was also of the clear opinion that judgments about existence were synthetic.


  For the real object ... is not analytically contained in my conception, but forms a synthetical addition to my conception (which is merely a determination of my mental state) ... Whatever be the content of our conception of an object, it is necessary to go beyond it, if we wish to predicate existence of the object.
  (Critique of Pure Reason, "Of the Impossibility of an Ontological Proof of the Existence of God")

The wikipedia definition you quote strikes me as being not particularly helpful for grasping the distinction (though not particularly wrong).

A better way to put it is that formal fallacies means fallacious inferences in formal logic. Here, we'd put things like affirming the consequent:


A -> B
B
Therefore , A


For a formal fallacy, no rule of inference enables the leap from these premises to the conclusion and the truth table shows that it's possible to have the opposite conclusion while both premises are still true. 

Maybe to reword that, under normal sentential logic, any argument that is invalid (= able to have a false conclusion with all true premises) is committing a formal fallacy whether or not that fallacy has a name.

Conversely, any other error in reasoning can be called "fallacious" but this would be an informal fallacy. You're right that these can also be about the structure of the argument. 

Thinking about it a bit, several things that are informal fallacies also contain a formal fallacy as well. For instance, argument from ignorance when symbolized would be formally fallacious. But argument from ignorance is not a formal fallacy. The explanation for this might sound like a sleight of hand, but you can only commit a formal fallacy when you make a formal (i.e. deductive, modal, truth-functional, propositional and their ilk) argument.

Conversely, you can commit an informal fallacy in any sort of argumentation. Thus, slippery slope formalizes to the chain rule / a series of hypothetical syllogisms, but we still consider it an informal fallacy. Presumably, this is because we take the series of hypotheticals in question to have dubious veracity.

A second feature that happens because informal fallacies apply to informal argument is that there can be a large amount of disagreement as to whether or not a given informal fallacy has been committed. (In contrast, if we receive the already symbolized deductive argument, there's no question as to whether or not it yields its conclusion in a truth-preserving fashion). Thus, we can argue endlessly about whether something is "ad hominem" or relevant to the case in question.  

Or as we've seen several times on this site, there can be disagreement about whether a particular claim is "question-begging." I recall a question that contained an argument roughly of this form:


Eighteen year olds are mature enough to drink
Anyone who is mature enough to drink should be legally allowed to drink
Therefore, the drinking age should be lowered to 18.


The OP didn't see how this was question-begging but myself and several others pointed out that the premise 1 captures the entire argument. If we formalize the argument, it turns out to be valid, but that does not mean the argument as supplied isn't question begging).

tl;dr

formal fallacy (1) applies to formal argument (2) is objectively clear

informal fallacy (1) applies to any type of argument (2) requires a judgment as to whether it is fallacious (3) can produce an argument that would be valid if symbolized (or can fail to)

  The sentence "If Ron went to the store, Ron would be home by now." does not have truth value, how do we further determine the validity of the argument?


When you say this, it seems to me that you are conflating two different levels of logic: sentences being true and arguments being valid.

So first of all, let's make our definitions of arguments' being valid, sentences' being true, and sentences' having truth-values:


"An argument is valid" means:


If its premises are true, then its conclusion must be true also. 
To be precise with the scope of modality, the condition above is equivalent to this: it is necessary that if the premises are true then the conclusion also true. 
Your definition of validity is just another way to put the same definition above, namely: it is not possible that the premises are true and at the same time the conclusion is false.

"A sentence is true" means:


We assign a truth-value of "True" to the sentence. 
What we precisely mean by "assignment" and "truth-value" depends on which https://en.wikipedia.org/wiki/Interpretation_(logic) interpretation or https://en.wikipedia.org/wiki/Semantics_of_logic semantics of logic we adopt. There are many kinds of semantics, i.e., different ways to interpret and use logical symbols like '->', 'and', 'or', 'p', 'q', etc. (In fact this description also conflates some details, but let me pass over at this point.)

"A sentence has a truth-value" means:


Under some interpretation of logical symbols (based on some semantics), the sentence has some particular value assigned.
We usually assign "True" or "False" to a sentence. This is an intuitive and natural semantics, but it is not the only one available. See https://en.wikipedia.org/wiki/Many-valued_logic many valued logic for example.





I think you are right to say that the first premise (conditional) does not by itself have a truth-value; it really does not have a truth-value, until we assign some particular value to it. 

Yet notice: to check whether the argument is valid, we start by assuming that the premises are true -- assigned a truth-value of 'True' -- and then see whether the conclusion necessarily follows. If the conclusion necessarily follows, then the argument is valid: the form of the argument, if the premises are really true, guarantees that the desired conclusion follows.

You might also be interested in the notion of arguments' being sound. An argument is sound if and only if the argument is valid and the argument's premises are actually true.

Hope it helps. 

(In fact, I should add: we need not adopt any 'interpretation' or semantics to show that a given argument is valid; we may manipulate logical symbols formally (that is, without considering their "meanings" we assign to them) and show that the desired conclusion does or does not follow from the premises.)
I'd suggest two things.

First, what you're offering is similar to an argument that Descartes offers elliptically in the Meditations. In Med. 1, Descartes points out that he has at times been mistaken. And as a consequence should doubt all of his beliefs, but that checking the beliefs would take an infinite amount of time. His solution is to instead suspend belief until he can come up with a firm foundation. Ostensibly in his argument, this is the cogito, but more accurately it's a circle of (a) the cogito (Med 2), his argument for a good God (Med 3), and "clear and distinct ideas" (scattered throughout and not well defined).

In the process, Descartes highlights a problem for the sort of system you're suggesting. Namely, there's going to be a negative infinite regress. In his case, it's that we have to keep doubting our judgments -- including our judgments about our judgments. In your case, it's a continuous loss of probability.

This leads to the second issue. Maybe both you and Descartes are wrong about what it means to know something? A lot of recent work has suggested that knowing is an act. This research is spear headed by Ernest Sosa and John Greco. On their view, to know something is more similar to successfully baking a cake than justified true belief. You either end up with cake or you don't. 

On such a model whether you arrive at knowledge depends on the techniques you're using and virtues of the knower, and your confidence levels don't really enter in. Moreover, once the knowing is accomplished, it's over, so there's no room for the sort of compounding probably you and Descartes face.

Maybe to state it more simply, knowing may not be the sort of thing subjectable to an infinite set of regresses about our confidence in each act of knowing. Instead, it might just be something that succeeds or fails. 
There is a theory of arguments, but I am afraid that the OP conception of argument is too idealized, and the notion of effective debate too narrow, to apply to most of them. If people argued from sets of established axioms and the only issue was whether those sets are equivalent they'd be proving mathematical theorems and meta-theorems of mathematical logic instead of having debates. 

The crux of real life debates is not disagreement over axioms, but vagueness and ambiguity of  translating available real life evidence into generalities, and even finding the right terms and classifications for expressing them adequately. To one person history indicates that ends justify means, to another this is a hasty generalization; to one person Napoleon is a great leader, to another he is a mass murderer; to one person soul can clearly exist apart from a body, to another this is a fanciful nonsense, etc. It is eliciting intuitions, affecting judgements, bringing out "facts", and deciding what is or is not a "fact", i.e. generating fruitful concepts and defensible "axioms", which can plausibly withstand factual objections and criticisms, that make effective debates effective. Often effective for both sides even if in the end they still do not come to an agreement. This will not be captured by a scheme that presupposes fixed concepts and axioms. 

Wikipedia has a long entry on arguments, including https://en.wikipedia.org/wiki/Argumentation_theory#Theories theories of argumentation. The study of debates goes back at least to sophists and Socrates, and was known as the https://books.google.com/books?id=56Rfs72S5bcC&pg=PA121&lpg=PA121&dq=dialectic+debates+sophists+antiquity&source=bl&ots=KVs11FIcyg&sig=KfQySffYeWnhi6WTsnK62pcVo5s&hl=en&sa=X&ved=0ahUKEwjMxID7387PAhUHxoMKHXfQA-QQ6AEINTAE#v=onepage&q=dialectic%20debates%20sophists%20antiquity&f=false art of dialectic in antiquity. In recent times https://en.wikipedia.org/wiki/Stephen_Toulmin#The_Toulmin_Model_of_Argument Toulmin's model of argumentation, developed in his book https://books.google.com/books?id=8UYgegaB1S0C Uses of Argument, has become very influential. Here is from the abstract:


  "Starting from an examination of the actual procedures in different fields of argument - the practice, as opposed to the theory, of logic - he discloses a richer variety than is allowed for by any available system. He argues that jurisprudence rather than mathematics should be the logician's model in analysing rational procedures, and that logic should be a comparative and not a purely formal study."


Toulmin models arguments based on six elements: Claim (Conclusion), 
Ground (Fact, Evidence, Data), Warrant (movement from the ground to the claim), Backing (credentials certifying the ground), Rebuttal (restrictions to the claim) and Qualifier (degree of certainty for the claim). This model shifts the focus where it belongs, to inspection and genesis of claims and evaluating evidence for them rather than on piecing together deductive chains, which is often a triviality and always an afterthought.
Hint

We need LEM : ∀zP(z) ∨ ¬∀zP(z)

Proof by cases (or ∨-elim) :

1) ∀zP(z) --- assumed [a]

2) P(x) --- by ∀-elim

3) ¬P(y) ∨ P(x) --- by ∨-intro


  4) ∃y∀x (¬P(y) ∨ P(x)) --- by ∀-intro followed by ∃-intro


5) ¬∀zP(z) --- assumed [b]

6) ∃z¬P(z)  --- equivalent

7) ¬P(y) --- assumed [c] for ∃-elim

8) ¬P(y) ∨ P(x) --- by ∨-intro


  9) ∃y∀x (¬P(y) ∨ P(x)) --- now we may discharge [c] by ∃-elim from 6)


From 4) and 9) we conclude with :


  
    ∃y∀x (¬P(y) ∨ P(x))
  


by ∨-elim with LEM.



We may "verify" it through some equivalences. Consider :


  ∀x (A ∨ P(x)) ↔ (A ∨ ∀x P(x));


with it, we may rewrite the original formula : ∃x ∀y (¬P (y) ∨ P (x)) as :


  ∃x (∀y ¬P (y) ∨ P (x)).


Then we need the equivalence :


  ∃x (A ∨ P (x)) ↔ (A ∨ ∃x P(x))


and we rewtite the last formula as :


  ∀y ¬P (y) ∨ ∃x P (x))


which in turn is equivalent to :


  
    ¬ ∃y P (y) ∨ ∃x P (x) --- LEM.
  

If we stay with the definition of http://plato.stanford.edu/entries/analytic-synthetic/ Analytic according to which :


  “Analytic” sentences are those whose truth seems to be knowable by knowing the meanings of the constituent words alone, unlike the more usual “synthetic” ones whose truth is knowable by both knowing the meaning of the words and something about the world,


we have that from : 2+2=5 and the other axioms for arithmetic, by purely logical transformations, we can derive : ¬(2+2=4), i.e. the negation of an analytical sentences.

If - according to the above definition - we can know the truth-value of 2+2=4 without recurring to "information" about the world, this holds also for 2+2=5.
given that the two main things you need to prove are A -> C and C -> A. As a general strategy, it is often the easiest to do so with conditional proofs. Given the three assumptions you've been given, it's also a successful strategy.

There's quite a few different syntaxes and allowed procedures (it'd be better to give a link or spell out what you can and cannot use rather than just saying "20-rule proof system").

Here's how I'd do it:


A -> ~ B   A
~C -> B    A
~A -> ~C   A
| A        A
| ~B       MP 1,4
| ~~C      MT 5,2
| C        DN 6
A -> C     CP 4-7
| C        A
| ~~C     DN 9
| ~~A     MT 10
| A       DN 11
C -> A    CP 10-12
(A -> C) & (C -> A) &I 9,13
A <--> C            BiCond. Int 14


In the above A = assumption, | means we are in a subproof, DN = double negation, MT = modus tollens, MP = modus ponens, CP = conditional proof, &I = conjunction introduction, and BiCond Int = biconditional introduction. For some proof systems, you need to use R to repeat things to use them in subproofs (omitted).

The largest dependency in the above is MT. If you need to avoid it, then the basic pattern is:


P -> Q A
~Q      A
| P     A
| Q     MP 1,3
| Q & ~Q  &I 2,4
~P      RAA 3-5


which you would need to substitute for each use of MT.
While the intension/extension distinction is ancient and unproblematic, the details of sense/reference (denotation) distinction are controversial, and are still evolving. I will suggest here a simplified version.

The sense/denotation distinction is similar to the intension/extension distinction, just that the intension/extension distinction applies to general names, predicates, expressions that purport to represent general types and properties.


  For instance, the intension of “ship” as a substantive is “vehicle for conveyance on water,” whereas its extension embraces such things as cargo ships, passenger ships, battleships, and sailing ships. (https://www.britannica.com/topic/intension Britannica)


While the sense/denotation distinction applies to proper names and definite descriptions, expressions that purport to represent particular objects.


  The reference of 'evening star' would be the same as that of 'morning star', but not the sense. (Frege, http://en.wikisource.org/wiki/On_Sense_and_Reference On Sense and Reference)


In Frege's classical example, the two descriptions "the evening star" and "the morning star" were once held to apply to two different planets. In time, it turned out that the two descriptions applied to the same planet, Venus. One reference, two senses.


  In an Intensional definition, we describe a class of object by means of a property satisfied by all elements of the class.
  In an Extensional definition, we describe it by enumering every elements. The extension is determined by the intension.


can be put


  In an Intensional definition, we describe a property (or a general type).
  Its Extension is the class of objects that exemplify that property, exemplify the intension. The extension is determined by the intension.


We begin with the intension, which is a property, or type, and derive from it the class, the extension.


  The Denotation of an expression is its reference, the object in the reality to which it refers to, what is interpreted. In formal logic, the denotation of "(True /\ False)" is False.
  The Sense of an expression is how we interpret it, a way to obtain the denotation. In formal logic, the sense of "(A /\ B)" is A or B.


can be put


  The Denotation of an expression is its reference, the object to which it refers.
  The Sense of an expression is the aspect, or the description, under which the speaker refers to the object. It is something that the speaker knows, and that serves the speaker to fix the reference of the stated expression.


I would not use the term "interpretation" here, because it can imply that we hold the object in thought, as it were, and then interpret it. But the sense is the very way in which we "hold" the object in thought. We do not have a reference before we have sense.

I would also leave formal logic aside. Formal logic abstracts from all references, to concentrate on the form of expressions. There is no referring going on in formal logic.
I agree with Mr. Allegranza that both are adequate but the second seems awkward. The first is to the point, but if you want to break up famous and mathematician you could also:  

there exists a person that is ((German and famous) and (German and a mathemetician))  

or,  

there exists a person that is (German and (famous and a mathematician))  

I'm not crazy about lots of parentheticals but I like the latter. The former seems overstated.

Hope that helps.
If Γ is maximally consistent set, then there is a unique valuation v such that


  v(ψ) = 1 for all ψ ∈ Γ.


Thus, if P → ¬P ∈ Γ, we must have v(P → ¬P) = 1.

This means : v(P)=0 and thus : v(P → Q)=1 i.e. P → Q ∈ Γ.
The link between the two is strong.

The "dialectical method" (ἡ διαλεκτικὴ μέθοδος)was used by http://plato.stanford.edu/entries/plato/ Plato in his dialogues as the central tool for philosophical inquiry; see http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0168%3Abook%3D7%3Asection%3D533c Rep, 533c and see e.g. :


Richard Robinson, https://books.google.it/books/about/Plato_s_Earlier_Dialectic.html?id=B10wAAAAYAAJ&redir_esc=y Plato’s Earlier Dialectic, Clarendon Press (1941).


For Aristotle, http://plato.stanford.edu/entries/aristotle-logic/#DiaArgArtDia Dialectical arguments are different from demonstrations in :


  the character of their premises, not in their logical structure: whether an argument is a sullogismos [a demonstration] is only a matter of whether its conclusion results of necessity from its premises. The premises of demonstrations must be true and primary, that is, not only true but also prior to their conclusions in the way explained in the Posterior Analytics. The premises of dialectical deductions, by contrast, must be accepted (endoxos).


We can see also the http://plato.stanford.edu/entries/dialectical-school/ Dialectical School :


  a group of early Hellenistic philosophers that were loosely connected by philosophizing in the — Socratic — tradition of https://en.wikipedia.org/wiki/Eubulides Eubulides of Miletus and by their interest in logical paradoxes, propositional logic and dialectical expertise. Its two best-known members, http://plato.stanford.edu/entries/diodorus-cronus/ Diodorus Cronus and https://en.wikipedia.org/wiki/Philo_the_Dialectician Philo the Logician (or Dialectician), made groundbreaking contributions to the development of theories of conditionals and modal logic.


Thus, we have here three different approaches :


Plato : dialectic is the (philosophical) method
Aristotle : dialectic is (sort of) formal logic
Megarians : dialectic is logic.


Then we have to consider at least http://plato.stanford.edu/entries/hegel-dialectics/ Hegel's dilectic; see :


Ermanno Bencivenga, https://books.google.it/books?id=HyjCZfTamBEC&printsec=frontcover Hegel’s Dialectical Logic, Oxford University Press (2000).

For one direction :

1) ∀x¬Px

2) ∃xPx --- assumed [a]

3) Pa --- assumed [b] from 2) for ∃-elim

4) ¬Pa --- from 1) 

5) contradiction --- closing ∃-elim and discharging [b]


  6) ¬∃xPx --- from 2) and 5) discharging [a].


Thus :


  
    ∀x¬Px → ¬∃xPx --- from 1) and 6) by →-intro.
  




For the other direction, we need Double Negation :

1) ¬∃xPx 

2) ¬∀x¬Px --- assumed [a]

3) Px --- assumed [b]

4) ∃xPx --- from 3) by ∃-intro

5) contradiction 

6) ¬Px --- from 3) and 5), discharging [b]

7) ∀x¬Px --- from 6) by ∀-intro

8) contradiction


  9) ∀x¬Px --- from 2) and 8) by Double Negation, discharging [a]
  
  
    ¬∃xPx → ∀x¬Px --- from 1) and 9) by →-intro.
  

You can prove it by assuming a negated instance and proving a contradiction:


{1}      1.   ~((Fa → Ga) ∨ (Ga → Fa))            Assum.
{1}      2.   ~(Fa → Ga) & ~(Ga → Fa)             1 DM
{1}      3.   ~(Fa → Ga)                          2 &E
{1}      4.   ~(~Fa ∨ Ga)                         3 MI
{1}      5.   Fa & ~Ga                            4 DM
{1}      6.   Fa                                  5 &E
{1}      7.   ~(Ga → Fa)                          2 &E
{1}      8.   ~(~Ga ∨ Fa)                         7 MI                        
{1}      9.   Ga & ~Fa                            8 DM
{1}      10.  ~Fa                                 9 &E
{1}      11.  Fa & ~Fa                            6,10 &I
-        12.  ~~((Fa → Ga) ∨ (Ga → Fa))           1,11 RAA
-        13.  (Fa → Ga) ∨ (Ga → Fa)               12 DNE
-        14.  ∀x[(Fx → Gx) ∨ (Gx → Fx)]           13 UI


Here's another version that doesn't rely on identities such as DeMorgan's law:


{1}      1.   ~((Fa → Ga) ∨ (Ga → Fa))            Assum.
{2}      2.   Fa → Ga                             Assum.
{2}      3.   (Fa → Ga) ∨ (Ga → Fa)               2 ∨I
{1,2}    4.   ⊥                                   1,3 &I
{1}      5.   ~(Fa → Ga)                          2,4 RAA
{6}      6.   Ga → Fa                             Assum.
{6}      7.   (Fa → Ga) ∨ (Ga → Fa)               6 ∨I
{1,6}    8.   ⊥                                   1,7 &I
{1}      9.   ~(Ga → Fa)                          6,8 RAA
{10}     10.  Fa                                  Assum.
{11}     11.  Ga                                  Assum.
{10,11}  12.  Fa & Ga                             Assum.
{10,11}  13.  Ga                                  12 &E
{11}     14.  Fa → Ga                             10,13 CP
{1,11}   15.  ⊥                                   5,14 RAA
{1}      16.  ~Ga                                 10,15 RAA
{17}     17.  ~Fa                                 Assum.
{1,17}   18.  ~Fa & ~Ga                           16,17 &I
{1,17}   19.  ~Ga                                 18 &E
{1}      20.  ~Fa → ~Ga                           17,19 CP
{11}     21.  ~~Ga                                11 DNI
{1,11}   22.  ~~Fa                                20,21 MT
{1,11}   23.  Fa                                  22 DNE
{1}      24.  Ga → Fa                             11,23 CP
{1}      25.  ⊥                                   9,24 &I
-        26.  ~~((Fa → Ga) ∨ (Ga → Fa))           1,25 RAA
-        27.  (Fa → Ga) ∨ (Ga → Fa)               26 DNE
-        28.  ∀x[(Fx → Gx) ∨ (Gx → Fx)]           27 UI

In short, Gettier illustrated a problem with the JTB definition of knowledge by identifying cases where someone "gets the right answer for the wrong reasons".  These are cases where the hypothetical person had a justified belief, and that belief was true, but are not cases where the term "knowledge" applies (basically because there is a disconnect between the person's justification and what actually happened in the real world).

Gettier cases do not say anything about whether "one cannot always have knowledge that it is true".  Indeed, the Gettier cases have little to do with the truth component of JTB (it is still the case that for all knowledge, the belief is true), instead they all relate to the justification.
Echoing what has been said in the comments on your question, the idea of "fundamental" seems a little bit ill-defined. However, what it seems like you are asking is something about the strength of https://plato.stanford.edu/entries/logic-higher-order/ second order logic and its ability to be the foundational system that can solve everything (well, at any rate everything that is solvable). We have, via our tools of metalogic, ideas such as http://www3.cs.stonybrook.edu/~pfodor/courses/CSE371/slides07/7slides.pdf completeness and https://www.cs.rice.edu/~vardi/comp409/lec23.pdf compactness that can define a logic, and we would be well within our rights to say that second order does have some strong properties. However, second order logic is no where as strong as first order logic, in terms of these properties. First order logic itself isn't even able to be used as a completely whole foundation for logic! This is because of Gödel's https://plato.stanford.edu/entries/goedel-incompleteness/ incompleteness theorems. Even if we accept the results of the first and argue away that it doesn't make a difference if there are unprovable truths, since we can look and see that they are true, we still cannot prove the consistency of first order logic within itself! The second incompleteness theorem also applies to second order logic so we would be in the same boat. If first order logic, which is stronger, isn't enough to form the foundation for logic then how could second order success when it is much less strong?

https://en.wikipedia.org/wiki/Lindstr%C3%B6m%27s_theorem Lindström's theorem is a result in metalogic that showed first order logic is the "strongest" logic, due to possessing the https://en.wikipedia.org/wiki/L%C3%B6wenheim%E2%80%93Skolem_theorem Löwenheim-Skolem theorem as well as the https://en.wikipedia.org/wiki/Compactness_theorem compactness theorem. The https://en.wikipedia.org/wiki/Second-order_logic#Metalogical_results metalogical results of second order logic show that second order logic (with full semantics) show that it does not possess these properties. Even worse, as https://plato.stanford.edu/entries/quine/ Quine (http://rads.stackoverflow.com/amzn/click/0674665635 1970) pointed out, second order logic doesn't even have a complete proof system! This is very bad news for second order logic. Ultimately, no axiomatic will be strong enough to be used as a "fundamental" logic and this is brought to us with special thanks in part by https://en.wikipedia.org/wiki/Tarski%27s_undefinability_theorem Tarski and Gödel. That being said, first order logic is a much better pick than second order logic for us to make due with. 

I am sure that the deadline for your essay has passed but I hope this has provided a (somewhat) useful answer for anyone who has a similar question. 
Problem 4:


-        1.   ∀x[∀y[Rxy ↔ x=y]]                Definition of Identity
{2}      2.   Rab & Rac                         Assum.
{2}      3.   Rab                               2 &E
-        4.   ∀y[Ray → a=y]                    1 UE
-        5.   Rab → a=b                        4 UE
{2}      6.   a=b                              3,5 MP
{2}      7.   Rac                              2 &E
{2}      8.   Rbc                              6,7 =E
-        9.   (Rab & Rac) → Rbc                4,8 CP
-        10.  ∀z[(Rab & Raz) → Rbz]            9 UI
-        11.  ∀y[∀z[(Ray & Raz) → Ryz]]        10 UI
-        12.  ∀x[∀y[∀z[(Rxy & Rxz) → Ryz]]]    12 UI


Here's another version:


{1}      1.   a=b & a=c                        Assum.
{1}      2.   a=b                              1 &E
{1}      3.   a=c                              1 &E
{1}      4.   b=c                              2,3 =E
-        5.   (a=b & a=c) → b=c                1,4 CP
-        6.   ∀z[(a=b & a=z) → b=z]            5 UI
-        7.   ∀y[∀z[(a=y & a=z) → y=z]]        6 UI
-        8.   ∀x[∀y[∀z[(x=y & x=z) → y=z]]]    7 UI


Problem 5:


{1}      1.   ~(Fa ↔ Fb)                       Prem.
{2}      2.   a=b                              Assum.
{3}      3.   Fa                               Assum.
-        4.   Fa → Fa                          3,3 CP
{2}      5.   Fa → Fb                          2,4 =E
{2}      6.   Fb → Fa                          2,4 =E
{2}      7.   (Fa → Fb) & (Fb → Fa)            5,6 &I
{2}      8.   Fa ↔ Fb                          7 ↔I
{1,2}    9.   ~(Fa ↔ Fb) & (Fa ↔ Fb)           1,8 &I
{1}      10.  a≠b                              2,9 RAA

I suggest you to use some modern commentary, like :


Beth Lord, https://books.google.it/books?id=QwCrBgAAQBAJ&printsec=frontcover Spinoza's Ethics (2010)


or :


Genevieve Lloyd, https://books.google.it/books?id=1Dreiuw1p4sC&pg=PR4 Spinoza and the Ethics (1996).


For a more detailed study, see :


Edwin Curley, https://books.google.it/books?id=Cl4xvLjY0lYC&printsec=frontcover Behind the Geometrical Method : A Reading of Spinoza's Ethics (1988).




Substance is in itself; this means that a substance is self-subsistent: it depends on itself alone. 

Thus, the concept of a substance (our conceiving it) is not formed from the concept of another thing. A substance requires nothing beyond itself to exist and to be understood.

With his definition of attribute ("what the intellect perceives of a substance, as constituting its essence") Spinoza does not adopt a "subjective" point of view; he means that attributes are the different ways in which a substance can be perceived. The intellect perceives a substance not as "pure" being but through one of its attributes. 

An attribute is the substance itself, as perceived in a certain way.

See https://books.google.it/books?id=Cl4xvLjY0lYC&pg=PA24 Curley, page 24, for the example regarding omniscience. Omniscience is not an attribute of God because it is not a "fundamental" property: it presuppose thought. An omniscient being must be thinking, but a thinking being must not be omniscient.


  So omniscience [...] is only a mode of the thinking thing, rather than an attribute of substance.


We perceive being as either as physical bodies or as minds. We perceive substances as extended and as thinking. 

According to Descartes, extension and thinking are the fundamental properties of substances. 

For Spinoza, extension and thinking are not properties of a substance, but rather two different "ways" that a substance can be perceived.

Extension and thinking are two expressions of the essence of substance.

According to P.II, two substances having different attributes have nothing in common with one another: they are two separate beings that are perceived in two separate ways. 

Thus, each substance exists independently of the other, and is conceived independently, of the other. 

According to Descartes, there could be multiple substances of the same attribute (many distict bodies all endowed with extension).

For Spinoza, this is not possible (P.V): two substances sharing the same attribute exist and are perceived as the same thing. Two substances with the same "essential characteristic" cannot be distinguished; therefore they are the same thing. There cannot be multiple substances sharing the same attribute.

Thus, there cannot be multiple substances sharing the attribute "thinking" or multiple substances sharing the attribute "extension". Since there cannot be two or more substances of the same attribute, there can be only one thinking substance and one extended substance.

Since distinct substances (if any) have nothing in common (different attributes implies different essences) they cannot cause or produce one another.
A substance must therefore be "cause of itself" (P.VI). This implies that:


  its essence necessarily involves existence, or existence belongs to its nature (P.VII).


Substance may have more than one attribute: there may be two, three or more ways of perceiving what it is. A substance could have infinite attributes, infinite ways of express its being. But, according to D.VI, a substance of infinite attributes is God.



Assuming this simplified reading of Part I (titled : Concerning God) of Spinoza's Ethics, what is the meaning of :


  Prop.X. Each particular attribute of the one substance must be conceived through itself.
  
  Proof. An attribute is that which the intellect perceives of substance, as constituting its essence (D.IV), and, therefore, must be conceived through itself (D.III).


Spinoza is clearly alluding to Descrtes' metaphysics, with his dualism of mind (the thinking thing) and body (the extended thing).

We conceive thinking and extension as distinct and independent attributes; but from this - according to Spinoza - we cannot infer that they pertain to two different and independent substances.

In the Note Spinoza says :


  It is, then, far from an absurdity to ascribe several attributes to one substance : for nothing in nature is more clear than that each and every entity must be conceived under some attribute, and that its reality or being is in proportion to the number of its attributes expressing necessity or eternity and infinity. 
  
  Consequently it is abundantly clear [???], that an absolutely infinite being must necessarily be defined as consisting in infinite attributes, each of which expresses a certain eternal and infinite essence.


Is this really consequence of the proof of P.X ?
Such is the "https://en.wikipedia.org/wiki/Principle_of_sufficient_reason principle of sufficient reason" and "...many philosophers of the period...did not carefully distinguish between the two."

Per https://plato.stanford.edu/entries/sufficient-reason/#Spin https://plato.stanford.edu/entries/sufficient-reason/#Spin we can see how Spinoza uses the terms interchangeably: "Nothing exists of which it cannot be asked, what is the cause (or reason) [causa (sive ratio)], why it exists." A translation of "causa (sive ratio)" is either "the cause (or cause)" or "the cause (or reason)", even "the cause (or system)". Related to "ratio" note also that "https://glosbe.com/la/en/rati rati" and "ratus" translate to "thinking" (compare to http://latindictionary.wikidot.com/printer--friendly/portable:latin-to-english cogitare) - as well as by some accounts "http://www.wordhippo.com/what-is/the-meaning-of/latin-word-7740eeaa2bd84bb323cca1036372b8c3a7cddbda.html thoughtlessly".

The terms are, however, either synonymous or antonymous depending on https://plato.stanford.edu/entries/wittgenstein/#MeanUse use of term. For example,

Q1: "Why are you shivering?" &
Q2: "Why are you putting on your sweater?"

are both answered by

A: "Because I am cold"

In the former instance, the answer cites (brute, physical, non-volitional) cause, in the latter, reason (and arguably, volition or "logical cause"). To discern meaning and intention with use of term, you might appreciate Grice's, "http://williamstarr.net/teaching/speech_acts/Grice-1969-Utterers_Meaning%E2%80%93and_Intentions.pdf Utterer's Meaning and Intention." Lastly, as for how the "mental" is causal, you might want to investigate http://web.mit.edu/abyrne/www/intentionality.html intentionality and I would recommend Searle's "https://www.scribd.com/doc/97933716/3b-Searle-1983-Intentionality Intentionality".
Hegel is not known for making "formal logical correlations", in fact he despised formal logic, on time specifically see http://www.dlib.si/stream/URN:NBN:SI:DOC-UR6P0ITX/68ccb934-f10b-49f3-81e2-408568f197a9/PDF Safatle's Hegel Against a Formal Concept of Time. As for Heidegger's grounds for his interpretations, in this case he is very explicit about them on the pages preceding the quote. The parts on Time come from Philosophy of Nature, and the parts on Spirit from Phenomenology of the Spirit, he even gives page numbers. Here is Philosophy of Nature §260 (quoted from http://www.gwfhegel.org/Nature/kf.html Foldes' Hegel’s Deduction Of Matter): 


  "...time is the immediate collapse into indifference, into undifferentiated asunderness or space, because its opposed moments which are held together in unity, immediately sublate themselves. In this way, the negative determination in space, the exclusive point, no longer only implicitly [in itself] conforms to the Concept, but is posited and concrete within itself."


It helps to remember Hegel's ubiquitous https://en.wikipedia.org/wiki/The_Phenomenology_of_Spirit#Hegelian_dialectic dialectic triad of abstract-negative-concrete, and how the concrete (or the more concrete) appears as sublation (negation of a negation) of the abstract and its direct negation. Thus, everything appears as a negation of a negation in its maximal concretion, which does not preclude it however from entering the next triad as the next "abstract". Here is how the Spirit appears as a negation of a negation in Heidegger's words:


  "Since grasping the non-I presents a differentiation, there lies in the pure concept, as the grasping of this differentiation, a differentiation of the difference. Thus Hegel can define the essence of Spirit formally and apophantically as the negation of a negation... Since the restlessness of the development of Spirit bringing itself to its concept is the negation of a negation, it is in accordance with its selfactualization to fall "into time" as the immediate negation of a negation." 


As a reminder, Being-Essence-Concept is the overarching triad of the Spirit's dialectic that the Science of Logic is devoted to, see e.g. https://www.academia.edu/1605534/Peirce_and_Hegel Kaag's Hegel, Peirce, and Royce on the Concept of Essence. Or, as earlier in Phenomenology, Reason becomes Spirit when it achieves the full consciousness of itself. As for the Spirit "falling into" time, it is perhaps best to quote the last chapter of Phenomenology of the Spirit directly:


  "Time is the Concept itself that exists there and is represented to consciousness as empty intuition. Consequently, Spirit necessarily appears in time, and it appears in time as long as it does not grasp its pure concept, which is to say, as long as it does not annul time. Time is the pure self externally intuited by the self but not grasped by the self; time is the merely intuited concept. Since this concept grasps itself, it sublates its temporal form, comprehends the act of intuiting, and is intuition which has been conceptually grasped and is itself intuition which is comprehending... Thus Time appears as the very fate and necessity of Spirit when it is not in itself complete - the necessity of its giving self-consciousness a richer share in consciousness, of setting in motion the immediacy of the in-itself...".


In other words, the Spirit has to "fall into time" not because of some formal analogy (which applies to everything in Hegel) but because of how Hegel set it up to "grasp" itself. As Heidegger puts it, "Spirit necessarily appears in time, and it appears in time as long as it has not grasped its pure concept, that is, has not annulled time."

One of Heidegger's problems with all of this is the most people's problem with Hegel in general, that he spins concrete out of abstract, the world out of nothing, and randomly at that, that he reaches "each category from the last preceding by virtually calling 'next'!", http://www.unav.es/gep/SeminarioJensen.pdf as Peirce put it. Heidegger's approach is the opposite (as one would expect from existentialist vs. essentialist), it "begins with the "concretion" of factically thrown existence, and reveals temporality as what makes such existence primordially possible. "Spirit" does not first fall into time, but exists as the primordial temporalizing of temporality", i.e. time is not encountered externally but is a primordial aspect of "self"'s (Dasein's) existence.
So your friend's second argument has the fault of being a Referential fallacy;


  assuming all words refer to existing things and that the meaning of
  words reside within the things they refer to, as opposed to words
  possibly referring to no real object or that the meaning of words
  often comes from how we use them.


He argues that his original point is not "post hoc" simply because he does not explicitly state "I make this conclusion based on this correlation", but the correlation is implied and presented in how he delivers his statement grammatically. 

From your example "Julian Assange broke the law, and we all lost" the comma or pause when verbally relaying the argument does imply the explicit connection that "because Julian broke the law we all lost".

So, to summarise, he does explicitly imply the correlation but argues that the phrasing he used doesn't mean what it actually does which is the second fallacy he commits.
There are approaches to logic that would give an affirmative answer to your question. Keynes and Carnap, in particular, developed the concept of logical probability, under which the premises of an argument support the conclusion, or provide a partial entailment of it, expressed as the conditional probability of the conclusion given the premises. Deductive entailment is then the limiting case in which the support is total, and the conditional probability goes to unity. 

Carnap's theory is purely syntactical in nature and runs into the standard objection, expressed by Nelson Goodman among others, that the degree of support of some premises for a conclusion depends on the choice of predicates with which the argument is expressed. Today there is little support for Carnap's approach, though there is still strong support for the closely related Bayesian approach to confirmation theory. 

As to inductive 'logic' being fundamental, this could be argued, though in practice it is contentious. There has been a lot of discussion around the issue of the epistemology of logic, i.e. how do we know that logical truths are true, or that logical arguments are valid? A number of different positions on this issue have been defended, which broadly fall into 'internalist' and 'externalist' positions. One form of internalism is that we judge logical truths to be true because we cannot coherently conceive them being otherwise. One common form of externalism is that logical truths are those that are completely reliable and give structure to the way we organize and systematize our knowledge. 

Sometimes you will see people criticize inductive justifications for inductive reasoning on the grounds that this is circular, but exactly the same objection could be made about deduction. One cannot justify deductive reasoning without using deduction in the process. One possible position you might adopt is to argue that deductive reasoning is justified inductively, because we observe that it works reliably, though again, this is only one of many positions that have been defended. 
There are some references that you might find useful in my answer to this question: 
https://philosophy.stackexchange.com/questions/39956/references-for-the-justification-of-the-use-of-logic References for the justification of the use of Logic
its a simple enthymeme, a syllogism with an unstated premise.  the missing premise is sth like "anybody who works for Big Name Company is good".  She worked there; therefore she's good. (drop "must be").
The axiom of reflexivity, specifying that equality is reflexive, simply helps define one aspect of the '=' symbol. This helps make a connection between well-formed formal strings that use that symbol and our personal conception of what equality should act like. It only seems boring and tautological because the concept is obvious, but the axiom is needed to operate on the symbols usefully. The axiom contains quite a bit of information ('usefulness' might be more appropriate) because without it, all sorts of theorems about natural numbers could not be proved.

I don't know what you could mean by 'empty of information in their own frame of reference'. You're using words that don't have any technical relevance to axioms and proof theory. If you're using them non-technically, you'd really have to explain what you mean by 'information' and 'frame of reference' as they refer to the mathematical system.

But does 'mathematics then provide truth only within an ontologically relational framework'? Whatever you think about those words together, a yes or no answer to it will only justify some hidden personal definitions to make the answer right.

But to jump from the axiom that equality is reflexive and that it feels tautological all the way to a statement about all of mathematics and reality (and ontology) is just perverse. The axiom is included to make proofs work (if you do some examples you'll see where the axiom comes in useful). If you want to draw the earth-shattering consequence that there is no absolute truth, the seeming tautology of reflexivity in arithmetic is not the place. (on the other hand, https://en.wikipedia.org/wiki/Tarski's_undefinability_theorem truth is undefinable inside arithmetic, but that's not the truth you're thinking of)

To help with this, is there anything special about reflexivity axiom? Aren't symmetricity and transitivity also tautological? What about the other axioms? Axioms are often very boring because they should be intuitively non-questionable (in order to trust that the proof system is proving things that fit with your intuition).
The fallacy of https://yourlogicalfallacyis.com/ambiguity ambiguity, or https://en.wikipedia.org/wiki/Polysemy polysemy.

This fallacy includes the http://www.conservapedia.com/Logical_fallacy#Fallacies_of_Ambiguity fallacies of https://en.wikipedia.org/wiki/Equivocation equivocation, https://en.wikipedia.org/wiki/Conflation conflation, https://en.wikipedia.org/wiki/Fallacy_of_composition composition and https://en.wikipedia.org/wiki/List_of_fallacies such as well as your case of mixing technical terminology and ordinary usage. Conflation can also be considered a http://rationalwiki.org/wiki/Continuum_fallacy continuum or http://rationalwiki.org/wiki/Equivocation equivocation fallacy, but in particular conflation is the merging of two different things. http://rationalwiki.org/wiki/Fallacy_of_ambiguity Ambiguity, being open to more than one interpretation, more accurately describes the general fallacy.

Depending on the argument, the mis-use of technical jargon can also be an argument from false authority, or an example of a rhetorical device of "http://philosophy.lander.edu/logic/ignorance.html argumentum ad ignorantiam" which relies upon the audience's ignorance. Stating "gravity is just a theory" is not, however, an argument.

An argument that  


gravity is a theory, and,  
all theories are opinions, therefore  
gravity is an opinion  


...is an example of an http://rationalwiki.org/wiki/Informal_fallacy informal fallacy, i.e. a http://www.iep.utm.edu/val-snd/ valid and unsound argument predicated upon a fallacy of ambiguity that https://yourlogicalfallacyis.com/ambiguity misrepresents the truth.
For analytic statements we cannot conceive the possibility that they can turn out to be false. The statements that Kant characterizes as synthetic aprioi are (mostly, if we put mathematics aside) not of this nature. Take for example the law of causality: every change has a cause. We can apparently conceive that an event (say, a sudden rain in a summer day) will happen without any prior cause. This was indeed part of David Hume's theory of causality, which deeply influenced Kant. Nevertheless, Kant claimed that every event does have a cause, and necessarily so. And since the opposite can be conceived, that necessary statement is synthetic. 
Start with:

(1) ~(◻◇◻p → ◇p)  (ass., for RAA)      
(2) ◻◇◻p (from 1)
(3) ~◇p  (from 1)


You should be able to derive ~◇◻p from (3) in a few steps, and get a contradiction by deriving ◇◻p from (2).

Regarding your second question, doesn't ◻◇◻p "hold" in the single-world model w1: {p = True}?
The symmetry between introduction and elimination rules for the logical constants is called harmony. The idea of logical harmony has been defended by several logicians, including Gentzen and Prawitz, as being a requirement for a proof-theoretic justification of logic. Harmony guarantees that the introduction of a logical constant is conservative with respect to implication. Michael Dummett has taken the argument further and claimed that any language, including natural languages like English, should have harmonious and stable rules for its terms. Dummett proceeds on this basis to argue that since classical negation is not harmonious, it has no defensible meaning, and he takes this to be an argument for adopting intuitionism and a verification based semantics for language. 

These claims are disputed. Ian Rumfitt argues that harmony is overkill as a condition of admissability, and that defective logical constants such as Arthur Prior's 'tonk' can be ruled out because they lack truth conditions. 

There is quite a good explanation of this issue in Nils Kurbis "Proof-Theoretic Semantics, a Problem with Negation and Prospects for Modality" Journal of Philosophical Logic 44 (6):713-727 (2015) which can be found on PhilPapers.org at https://philpapers.org/rec/KRBPSA https://philpapers.org/rec/KRBPSA

Other useful references are: 
Steinberger, F. (2011) “What harmony could and could not be”. Australasian Journal of Philosophy 89: 617-639; and 
Rumfitt, Ian (2016) “Against Harmony”. Forthcoming in Robert Hale, Crispin Wright, and Alexander Miller, eds., The Blackwell Companion to the Philosophy of Language, 2nd edition. Oxford: Blackwell.
If A, B, and D are true and C is unknown, to resolve the statement (DvC)&(Av~B) you'll want to plug in the truth values for the corresponding symbols and evaluate the expressions.


(DvC)&(Av~B) can be read as (D or C) and (A or not-B).


...replacing the truth values leaves us with:


(true or unknown) and (true or not true).


Given the https://en.wikipedia.org/wiki/Law_of_thought#The_three_traditional_laws principles of non-contradiction and the excluded middle, I am going to presume that "not true" is rendered into "false" instead of "unknown" - because we do know something about the truth value of "~B" inasmuch as we know that B is true and ~B is not true, hence we can positively say ~B is false instead of negatively resolving that though "not B" is "not true", what "~B" evaluates to is unknown. So, we have:


(true or unknown) and (true or false)


Next, you'll want to evaluate the or statements into a truth value before comparing the conjunction (the "and" expression). Here I am also going to presume that you are using https://stackoverflow.com/a/3246264/5225057 an inclusive or and not an exclusive or, and we can use the following truth table to evaluate the possible results of the two "inclusive or" statements (the column under "a V b" is the inclusive or, and, the column under "a V̲ b" is exclusive or):

https://i.stack.imgur.com/FMqfl.gif 

Given an inclusive or, you can see that if at least one of the two values is true, then the entire expressions evaluates to true. So for (DvC) (i.e. "true inclusive or unknown") we end up with true. Likewise for (Av~B) (i.e. true inclusive or false) we end up with true. Now the statement can be analyzed according to the conjunction:


true and true


As we can see from working out the truth table for the conjunction of two terms (note that this image uses the ∧ notation for conjunction instead of the ampersand &):

https://i.stack.imgur.com/Isbex.gif 

...(DvC)&(Av~B) is analyzed as "true and true" which evaluates to "true."

http://www.mathopenref.com/qed.html Q.E.D.



PhilosophySE isn't here to do your homework for you, but to resolve the remaining expressions and presuming the ⊃ symbol is used for "If ..., then ..." conditionals, this might help:
https://i.stack.imgur.com/5FPwp.gif 

Lastly, I am assuming that the equals sign "=" is being used such that "T=F" and "F=T" are both false, and, "T=T" and "F=F" are both true. Also, I'm presuming the tilda "~" is being used for "not" such that ~T is false, ~F is true, ~~T is true and ~~F is false.
In classical and intuitionistic logic, the https://en.wikipedia.org/wiki/Principle_of_explosion Principle of Explosion is often a basic law of inference.

https://en.wikipedia.org/wiki/Principle_of_explosion Wiki's entry deduces it from https://en.wikipedia.org/wiki/Disjunctive_syllogism Disjunctive syllogism:


  Assume P as true; then (by https://en.wikipedia.org/wiki/Disjunction_introduction Disjunction introduction) we have: P ∨ Q, with Q whatever.
  
  But we have also ¬P. Thus, we may conclude with Q.


This is what happens in classical and intuitionsitic logic when we assume as true a contradiction.

Conclusion: contradictions are never true. 

We have to be clear about the difference regarding: assuming something as true, and proving something. 

But it is allowed to assume something (not known to be true or false) as true and see "what happens". 

If a contradiction follows by way of the correct "inference procedure" (i.e. having applied correctly the inference rules) then, because contradictions can never be true, this mean that our starting assumption is wrong e we have to reject it.
Several answers have been given above about what a law of physics is or does or whatever.

One of them claims that laws of physics are about prediction, another claims they are a 'provisional statement of an observed regularity in Nature'. 

From these statements, it follows that anything that can't be observed is not part of physics. It may be impossible to put a measuring instrument in the core of the sun, and so by this standard, physics has nothing to say about the core of the sun. We also can't measure the past because we can't travel into the past and stick measuring instruments into it, and so the whole past history of the universe is not part of physics. Anything that is spacelike separated from us also can't be measured and so is not part of the laws of physics.

There is another extremely serious problem. All of the limitations I pointed out are a consequence of the laws of physics. The laws of physics strictly forbid observations of a lot of the stuff they refer to.

Physics is not about prediction. Science in general is not about prediction. Rather, science in general is about explanation: giving an account of what is happening in reality and why it is happening. A prediction itself is such an explanation. If you can't explain what's going on in an experiment, which includes stuff you can't observe, then you can't do the experiment. Why? Because the idea that the laws of physics is about observations, if taken seriously, destroys the distinction between correct and incorrect measurements and so destroys the whole subject of measurement theory and all experimental science. This is a matter of immediate practical experimental relevance. It is not a theoretical quibble or anything like that. The dominance of the idea that science is about prediction is a serious threat to theoretical and experimental progress. Witness the pathetic, floundering confusion in physics about explaining even the simplest quantum mechanical experiments.

One of the comments claims that physics is about dynamical laws. This is not true either. The second law of thermodynamics is not a dynamical law.

For more explanations of these points see 'The Fabric of Reality' and 'The Beginning of Infinity' by David Deutsch, and also https://arxiv.org/abs/1210.7439 https://arxiv.org/abs/1210.7439.
I will add here the most common usages of the term κατηγορώ.

An accuser in a court is a κατήγορος.

A sentence is in general a κατηγόρημα applied via the verb to the subject. 

"Socrates is sleeping" Socrates the subject is called that he is sleeping, his being is categorized "accused- called" with the accidental of sleep. 

"Socrates is beautiful" Beautiful is called in Greek grammar κατηγορούμενο. (an adjective called on the subject)  

The "κατηγορίες", genres, species, αυτό το φυτό ανήκει στην κατηγορία των οπωροφόρων, this plant belongs to the taxonomy (category) of orchards.

Ταξινομία means to puts something in its τάξη, i.e. order. Category means to have called something as different and distinguishable from others. To be able to call something on its own, to accuse it, means to differentiate it. 

κατά + αγορεύω = "for this" "I talk in public" , plead 

a. η απόδοση μομφής σε κάποιον, καταγγελία = complaint, denunciation

b. η ομάδα στην οποία ανήκει ένα είδος, το σύνολο ομοίων πραγμάτων ή ιδεών

ομάδα, τάξη, σύνολο/ group, taxis- order, aggregation, summation
The term https://el.wiktionary.org/wiki/%CE%BF%E1%BD%90%CF%83%CE%AF%CE%B1 Ουσία derives from a form of the verb εἰμί - I am, but is a different word. 

οὐσία < from οὖσα - ἐοῦσα (ὤν < female participle of the present tense of the verb εἰμί - I am) 

Ουσία in Ancient Greek meant: a. Property, wealth, b. philosophical essence, substance c. chemical elements

Εστί means to exist, 3rd person present tense of verb εἰμί / I am. 

Είναι -> infinitive present tense of verb εἰμί / I am 

being qua being -> όντως ον (ὤν < participle of the present tense of the verb εἰμί (I am) )

Τί ην είναι ~ being qua being ("essential being"), -> https://en.wikipedia.org/wiki/Quiddity quidditas

Τόδε τι -> ουσία/ "what is there in a thing" / basic characteristic /what is the essential component of a being, what is crucial component/property/attribute to differentiate one being from another and for a being to stay the same

επιστήμη (science) -> επίσταμαι, I know in depth, I am able, επ + ίσταμαι = to stand above something 

Now the "being/είναι and ουσία may seem interchangeable and this because the form "είναι" means what is there indefinitely. (and we can still use the term with this notion synonymous with essence like "the being of man" "To είναι του ανθρώπου"). But there is also an alternative way of usage: the term είναι has taken the form of "what is there now" or what it seems a thing to be, what is its immediate existence and general formation, and this in contrast with ουσία/essence that is the substrate that stays the same regardless the accidentals applied to the being. 
So the being of a chair is this chair in front of me which may appear black, wooden but the essence of the chair is the piece of furniture we use to sit on regardless the differences (accidentals - συμβεβηκότα) between different forms of these.

Synonymous with ουσία in some contexts is also the term physis (φύση)

In Hegelian logic, the movement of the Being takes three steps: The immediate: Being (Είναι), the mediated (from other thoughts and reflective thinking): Essence (Ουσία), and the speculative-dialectical synthesis of the Being and Essence which is the Notion/Idea.

https://el.wiktionary.org/wiki/%E1%BD%A4%CE%BD https://el.wiktionary.org/wiki/%E1%BD%A4%CE%BD

https://el.wiktionary.org/wiki/%CE%B5%E1%BC%B0%CE%BC%CE%AF https://el.wiktionary.org/wiki/%CE%B5%E1%BC%B0%CE%BC%CE%AF

https://en.wikipedia.org/wiki/Accident_(philosophy) https://en.wikipedia.org/wiki/Accident_(philosophy)

https://en.wikipedia.org/wiki/Ousia https://en.wikipedia.org/wiki/Ousia

https://en.wikipedia.org/wiki/Nature_(philosophy) https://en.wikipedia.org/wiki/Nature_(philosophy)
In short: The so-called definition of natural numbers as those that can be obtained from 0 by adding 1 repeatedly is circular, but there is no viable alternative, which already makes it impossible to uniquely pin down natural numbers mathematically. Worse still, there does not seem to be ontological reason for believing in the existence of a perfect physical representation of any collection that satisfies PA under a suitable interpretation.



Why the so-called definition is circular

It is circular because "repeatedly" cannot be defined without essentially knowing natural numbers. You cannot use the natural numbers to do counting because you have not defined them yet! You are stuck; you must already know what are natural numbers before you can talk about iteration. This is why in mathematical logic the meta-system must already have the collection of natural numbers to be able to define what it means for a formal system to be arithmetically sound (prove only arithmetical sentences that are true in the 'true natural numbers'). And for a slightly less brief account of various assumptions needed in the foundations of mathematics, see http://math.stackexchange.com/a/1808558/21820 this post.

Why there is no viable alternative

The problem comes right at the beginning even before you can talk about arithmetic. Think about just pure propositional logic. What is a well-formed formula in propositional logic? To even define that, you necessarily must assume the existence of finite strings in the real world, otherwise you cannot even have a precise syntactic form for sentences, not to even say a full logic system. Furthermore, it is not enough to have a constructivistic view of finite strings in the sense that you can recognize them while not necessarily generating them. This is because propositional logic has deduction rules that permit deducing "A ⇒ A ∨ A" for any propositional formula "A", which clearly implies that one must be able to generate arbitrarily long strings. Effectively, to even describe the deductive system for propositional logic one already must accept the existence of the collection of finite strings.

One might naively think that perhaps we do not need formal systems at all. But the only way we can precisely and objectively describe something to another person is by a finite sequence of symbols in a common language. Pictures do not work because they are subject to interpretation unless they are in an agreed fixed format, in which case they could easily be encoded by symbol strings anyway. And if we want to have logical reasoning, the mere notion of mathematical proof involves finite sequences of symbols, hence by accepting any formal system whatsoever as being meaningful, we already must accept the basic properties of string manipulation, which amount to accepting https://www.impan.pl/~kz/files/AGKZ_Con.pdf TC (the theory of concatenation). But TC (despite having just the concatenation operation and no arithmetic operations) is essentially incomplete, so we cannot pin down even the finite strings!

So we do not even have hope of giving to anyone a description that uniquely identifies a collection of finite strings, which naturally precludes doing the same for natural numbers. This fact holds under very weak assumptions, such as those required to prove Godel's incompleteness theorems. If one rejects those... Well one reason to reject them is the following...

Why there is no apparent physical model of PA

As far as we know in modern physics, one cannot store finite strings in any physical medium with high fidelity beyond a certain length, for which I can safely give an upper bound of 2^10000 bits. This is not only because the observable universe is finite, but also because a physical storage device with extremely large capacity (on the order of the size of the observable universe) will degrade faster than you can use it.

So description aside, we do not have any reason to even believe that finite strings have actual physical representation in the real world. This problem cannot be escaped by using conceptual strings, such as iterations of some particular process, because we have no basis to assume the existence of a process that can be iterated indefinitely, pretty much due to the finiteness of the observable universe, again.

Therefore we are stuck with the physical inability to even generate all finite strings, or to generate all natural numbers in a physical representation, even if we define them using circular natural-language definitions!



Further facts

There are two curious facts related to this. Firstly, despite the fact that PA (Peano arithmetic) is based on the assumption of an infinite collection of natural numbers, which as explained above cannot have a perfect physical representation, PA still generates theorems that seem to be true at least at human scales. My favourite example is HTTPS, whose decryption process relies crucially on the correctness of Fermat's little theorem applied to natural numbers with length on the order of thousands of bits. So there is some truth in PA at human scales.

This may even suggest one way to escape the incompleteness theorems, because they only apply to deterministic formal systems that roughly speaking have certain unbounded closure properties (see https://pdfs.semanticscholar.org/c278/147b7a68385836a90939a175a9959cabbf0b.pdf this paper about self-verifying theories for sharp results regarding the incompleteness phenomenon). Perhaps the physical 'fuzziness' due to quantum mechanics or the spacetime limitations may permit the real world to be governed by some kind of system that does is syntactically complete, but anyway such systems will not have arithmetic in full as we know it!

Secondly, any meta-system MS that can reason about finite strings and sets of finite strings can prove the incompleteness theorems about itself, which immediately implies that if MS is consistent then MS' = MS + ¬ Con(MS) is consistent but Σ1-unsound (from the perspective of MS). But think about it: How do you know that MS is not already Σ1-unsound? The unsoundness could be buried deeper as well. https://mathoverflow.net/a/267837 MS'' = MS + ¬ ω-Con(MS) is ω-consistent but of course not arithmetically sound.

The thing is that we don't have any way to decide whether or not our preferred meta-system (whether ZFC or something nice and predicative like ACA) is in fact arithmetically sound, until we actually find a proof of say "⬜..⬜(0=1)" for some number (possibly zero) of "⬜"s. We cannot just say that if nobody has found such a proof then it is good enough evidence that they do not exist, since https://en.wikipedia.org/wiki/G%C3%B6del%27s_speed-up_theorem Godel's speed-up theorem and elegant examples by Harvey Friedman show that it is possible for the shortest such proof to be so long as to be impossible for humans to discover by trial and error.
Yes, the axioms do trivially prove themselves. Your last derivation, however, is not valid: "A=A" can not be substituted for A because the latter is a symbol in a formal system, while the former is an object of it. You are free to postulate identity law as applied to symbols or laws, of course, in addition to just the identity law for objects, but that is a separate meta-law. I think OP's intuition might be of something like the https://plato.stanford.edu/entries/logicism universalist conception of logic that logicists envisioned, where everything has to be justified from within the system, and hence some axioms must be at the foundation and "unprovable". This, of course, is very different from the https://en.wikipedia.org/wiki/Formal_system#Proof_system modern theory of formal systems, where (following Hilbert) meta-linguistic manipulations are "silently granted" without axiomatizing them in the system itself. If need be, a separate meta-language can be introduced for that purpose.

The confusion is understandable since, in Hilbertian terms, the universalist conception mixes the object language of the formal system with the meta-language used to talk about statements of that language, see https://en.wikipedia.org/wiki/Semantic_theory_of_truth semantic theory of truth. These were separated by Tarski exactly to avoid the https://plato.stanford.edu/entries/liar-paradox paradoxes of self-reference, after Gödel's incompleteness results made the universalist logicism unattractive. Tarski proved that a formal first order theory that contains Peano arithmetic and is capable of expressing claims about the truth of its sentences, is necessarily inconsistent. This is his https://en.wikipedia.org/wiki/Tarski%27s_undefinability_theorem Undefinability of Truth theorem, and it might be the "one of the implications of Gödel" mentioned in the OP:


  "This implies a major limitation on the scope of "self-representation." It is possible to define a formula True(n) whose extension is T but only by drawing on a metalanguage whose expressive power goes beyond that of L. For example, a truth predicate for first-order arithmetic can be defined in second-order arithmetic. However, this formula would only be able to define a truth predicate for sentences in the original language L. To define a truth predicate for the metalanguage would require a still higher "metametalanguage", and so on."


The requisite tower of meta-languages is known as the https://plato.stanford.edu/entries/liar-paradox/#TarsHierLang Tarski hierarchy. It is this hierarchy that prevents things like the Liar sentence "I am not true" from being formally expressible. 

Now to another "implication of Gödel", Löb's theorem, which deriving A from A "can be shown to violate", according to the https://philosophy.stackexchange.com/questions/42095/are-axioms-tautologies OP's earlier question. Of course, no such thing can be shown, but it does not help that Wikipedia's "formulation" of the https://en.wikipedia.org/wiki/L%C3%B6b%27s_theorem Löb's theorem is highly misleading:


  "in any formal system F with Peano arithmetic (PA), for any formula P, if it is provable in F that "if P is provable in F then P is true", then P is provable in F". 


It can not be "provable in F" that "if P is provable in F then P is true", because such a thing would require F to contain its own truth predicate, and hence be inconsistent by the Undefinability of Truth. https://stanford.library.sydney.edu.au/archives/win2013/entries/goedel-incompleteness/#RefPriLbsThe SEP gives a precise formulation of the Löb's theorem, and explains why Wikipedia's "lame terms" version of it is, well, lame:


  "In order to understand Löb's theorem properly it is useful to first consider the so-called “reflection principles”. Above, the focus has been on expressing, inside a formal system, that the system is consistent, i.e., on Cons(F). But naturally the theory should not merely be consistent but also sound, i.e., prove only true sentences. How should the soundness of a system, i.e., the claim that everything derivable in the system is true, be expressed? If one wants to express this in the language of the system itself, it cannot be done by a single statement saying this, because there is, by the undefinability of truth, no suitable truth predicate available in the language. Various restricted and unrestricted soundness claims can, however, be expressed in the form of a scheme, the so-called Reflection Principles...
  
  Exactly which instances of the reflection scheme are actually provable in the system? Löb's Theorem gives a precise answer to this question (assuming that Prov_F(x) satisfies the derivability conditions). [...] the instances of soundness (reflection principle) provable in a system are exactly the ones which concern sentences which are themselves provable in the system."

There are two possibilities that we can consider here. The first, as you brought up, is the False Dichotomy Fallacy. Our person assumes there is only two possibilities, A and B, so after eliminating A she believes she has proven B. 

The second fallacy is referred to as the Blind Loyalty Fallacy.  Our person assumes their source cannot be misinformed/mistaken, so after eliminating the possiblility that he is lying, she concludes that what he says must be true.  

We can agree that we probably started with a trichotomy (or some other finite number of possibilities).


Our source is lying
Our source is mistaken
Our source is correct


The first logical misstep that the person makes is assuming that our source cannot be mistaken and thus eliminating option 2, a clear example of Blind Loyalty Fallacy.  This fallacy then causes what we could call a false dichotomy down the line, but the argument was clearly already ruined before this point.

Fallacies are incorrect lines of reasoning. At the point in the argument where we have eliminated the possibility that our source may be mistaken, it is in fact "logically sound" to assume the he must be correct since he isn't lying.  Because of this, it would really be incorrect to call this a false dichotomy fallacy.  The only real mistake in this person's argument was when he eliminated option 2 above, and thus it this is an example of a Blind Loyalty fallacy.
My expertise lies more towards the ethics side of things (especially with Aristotle), but I think Aristotle's point here generally makes sense  so I will see if I can spell it out better. One confusing and important point is that "change" in English is broader in meaning than what Aristotle means, which could be called "alteration" instead.

On Aristotle's account, change has two dimensions:


a thing that continues to exist (it is there at points in time A and B).
an alteration such that something about this thing is different at points in time A and B.


If you don't have 1, then you're looking at the destruction of one thing and the generation of something new rather than a change.

If you don't have 2, then there's been alteration in the thing between point in time A and B, thus no change.

Aristotle articulates this conjunction as saying the thing "must be divisible." What this means is that, there's got to be some sort of continuous thing, but that it must have aspects that can change while it's the same thing.

An example or two might help.

A red car undergoes change when you paint it blue. The car still remains (reaching condition 1), but the car's color has changed (reaching condition 2). The color of the car is thus one divisible aspect of the car.

A car does not undergo change when it is incinerated (imagining that it is completely turned into its elements at 50000 K). Once burned, there's no car (breaking condition 1). Thus, there's no aspect that changes -- the thing ceases to exist. And nothing divisible has happened.  (Aristotle calls this "destruction" -- not "alteration")

A car also does not undergo change when it is made in the factory. It becomes a car and wasn't one before (= "generation" or "creation" for Aristotle).



I think you're losing Aristotle's track when you suggest "two items" but maybe I am misreading you.

(n.b., I'm not necessarily endorsing this as a true and complete model of change).



Further Reading


Paul Studtman "Aristotle's Categories" in https://plato.stanford.edu/entries/aristotle-categories/?level=1 SEP.
Soren Kierkegaard [Johannes Climacus] Philosophical Fragments p. 70-73 or so is a consideration of alloioisis and the problem of coming into being and out being.

This is how http://classics.mit.edu//Aristotle/metaphysics.1.i.html Ross's translation of Metaphysics bk. 1 §5 reads:


  the Pythagoreans … thought that finitude [= odd numbers?] and infinity [= even numbers?] were not attributes of certain other things…, but that infinity itself and unity itself were the substance of the things of which they are predicated. This is why number was the substance of all things.


St. Thomas Aquinas explains this in https://isidore.co/aquinas/Metaphysics1.htm#9 Sententia Metaphysicæ, lib. 1 l. 9 n. 14-17 [147-150]:


  
  Here he summarizes the opinions expressed by the Pythagoreans, both what they held in common with the foregoing philosophers, and what was peculiar to themselves. Now the opinion common to some of the foregoing philosophers and to the Pythagoreans was this that they posited, in a sense, two principles in the same way as the foregoing philosophers did. For Empedocles held that there are two contrary principles, one being the principle of good things, and the other the principle of evil things, and the Pythagoreans did the same thing, as is clear from the co-ordination of contrary principles which they posited.
  However, they did not do this in the same way; because Empedocles placed these contrary principles in the class of material cause, as was stated above (111), whereas the Pythagoreans added their own opinion to that of the other thinkers. The first thing that they added is this: they said that what I call the one, the limited and the unlimited are not (~) accidents of any other natures, such as fire or earth or the like, but claimed that what I call the one, the limited and the unlimited constitute the (+) substance of the same things of which they are predicated. From this they concluded that number, which is constituted of units, is the substance of all things. But while the other philosophers of nature posited the one, the limited and the unlimited, they nevertheless attributed these to another nature, as accidents are attributed to a subject, for example, to fire or water or something of this kind.
  The second addition which they made to the views of the other philosophers is this: they began to discuss and to define “the whatness itself,” i.e., the substance and quiddity of things, although they treated this far too simply by defining things superficially. For in giving definitions they paid attention only to one thing; because they said that, if any given definition were to apply primarily to some thing, this would be the substance of that thing; just as if one were to suppose that the ratio “double” is the substance of the number two, because such a ratio is found first in the number two. And since being was found first in the one rather than in the many (for the many is composed of ones), they therefore said that being is the substance itself of the one.But this conclusion of theirs is not acceptable; for although the number two is double, the essence of twoness is not the same as that of the double in such a way that they are the same conceptually, as the definition and the thing defined. But even if their statements were true, it would follow that the many would be one. For some plurality can belong primarily to something one; for example, evenness and the ratio double belong first to the number two. Hence [according to them] it would follow that the even and the double are the same. And it would likewise follow that that to which the double belongs is the same as the number two, so long as the double is the substance of the number two. This, indeed, is also the conclusion which the Pythagoreans drew; for they attributed plurality and diversity to things as if they were one, just as they said that the properties of numbers are the same as the properties of natural beings. 
  Hence, Aristotle concludes that it is possible to learn this much from the early philosophers, who posited only one material principle, and from the later philosophers, who posited many principles. 
  

Körner is referring to the http://faculty.fordham.edu/davenport/texts/REFIDEAL.HTM Refutation of Idealism argument (B274–279), directed against the skepticism about the external world attributed to Descartes and Berkeley. The idealism in question is the "dogmatic" idealism concerning the empirical, hence the "empirical realism". The choice of words is unfortunate, however, since "empirical realism" is also one of the two names given by Kant to his own philosophy. The other one came to be much better known, and became its canonical label, the transcendental idealism. But Kant considered "empirical realism" to be equally valid, except that his meaning for it has little to do with what the term means today, which is closer to what is argued for in the Refutation, see http://staffweb.hkbu.edu.hk/ppp/ksp1/KSP6.html Palmquist's Two Perspectives on the Object of Knowledge. 

Here is Dicker's reconstruction of the Refutation argument:


  1) I am conscious of my own existence in time; that is, I am aware, and can be aware, that I have experiences that occur in a specific temporal order. (premise)
  
  2) I can be aware of having experiences that occur in a specific temporal order only if I perceive something permanent by reference to which I can determine their temporal order. (premise)
  
  3) No conscious state of my own can serve as the permanent entity by reference to which I can determine the temporal order of my experiences. (premise)
  
  4) Time itself cannot serve as this permanent entity by reference to which I can determine the temporal order of my experiences. (premise) 
  
  5) If (2), (3), and (4), are true, then I can be aware of having experiences that occur in a specific temporal order only if I perceive persisting objects in space outside me by reference to which I can determine the temporal order of my experiences. (premise)
  
  6) Therefore, I perceive persisting objects in space outside me by reference to which I can determine the temporal order of my experiences. (1–5)


Permanence is required as a baseline for establishing temporal order among past experiences. Conscious states can not play this role, "this permanent something cannot be something in me, for the very reason that my existence in time is itself determined by this permanent something". The empirical "soul", or self, is a collection of appearances arranged in time, the metaphysical soul, along with the immateriality arguments, is dispatched in the Second Paralogism, see https://philosophy.stackexchange.com/questions/31312/what-are-the-problems-with-the-argument-for-the-mind-body-dualism-from-immateria/31340#31340 What are the problems with the argument for the mind-body dualism from immateriality of thoughts? The noumenal self is moot to the kind of "metaphysical" idealism that Kant is dealing with here. The entire argument concerns the realm of appearances, in space and time, we are talking about realism vs idealism about appearances, the noumena are out of the picture. Time itself can not supply the permanence either, for "time by itself is not perceived... Hence it follows that consciousness in time is necessarily connected also with the existence of things without me". 

There are three common objections. One might suggest that the memory is unreliable, and hence reject premise 1). One can argue, contra premise 3), that conscious states (empirical ones, we are not talking about the "soul" here) can function as the permanent baseline in lieu of the external things. Kant could reply, I suppose, that this is highly implausible due to general obscurity of introspection (and Wittgenstein would agree). But on Berkeley's view, the "external things", including clocks, are subject's mental states too, esse est percipi. So Kant's refutation falls short of its intended target. Finally, the "permanence", if established at all, is only of relative quality, flashing mental time stamps would suffice for establishing the temporal order too. See https://plato.stanford.edu/entries/kant-transcendental/#RefIde SEP discussion for more details. A recent defense of the argument is in https://www.cambridge.org/core/books/cambridge-companion-to-kants-critique-of-pure-reason/refutation-of-idealism-and-the-distinction-between-phenomena-and-noumena/FCFD4DB6FD894726D75930D45C2F2C31 Refutation of Idealism and the Distinction between Phenomena and Noumena by Edmundts in the Cambridge Companion to Kant's Critique of Pure Reason.
1) ¬∀xFx --- premise

2) ¬∃x¬Fx --- assumed [a]

3) ¬Fy --- assumed [b]

4) ∃x¬Fx --- from 3)

5) ⊥ --- contradiction: from 2) and 3)

6) Fy --- from 3) and 5) by Double Negation, discharging [b]

7) ∀xFx --- from 7): no y free in "open" assumptions (i.e. [a])

8) ⊥ --- contradiction: from 1) and 7)


  9) ∃x¬Fx --- from 2) and 8) by Double Negation, discharging [a]

The problem is that you have defined in 1) that there is good and evil and in 2) you have defined the characteristics of each, but you have not specified that either of these things are the case in all possible worlds. If you do, then such a situation as you describe in 3) exists in all possible worlds, so your conclusion at 4) is wrong, we do not live in the best (superlative) of all possible worlds, we live in one of an infinite number of possible worlds all of which are identical in this one respect.

If, on the other hand, you are suggesting that Good and Evil might not exist in some possible world, or they might not have the characteristics you've specified, then your definition at 3) does not apply to the other possible worlds and so again your conclusion at 4) is wrong because it is not possible to judge the value of other worlds by the measure on which ours is "the best".
It depends on how you want to ‘slice your cake’, as it were. Normally, a formal system or ‘logic’ consists of two parts: syntax and semantics. The syntax is further sub-divided into a formal language, and a deductive calculus (i.e. rules (or axioms) that govern the proofs).

Note that the language itself has at least two aspects: Firstly, there are the expressions that are part of the language – the sentences and formulae, the predicates and constants, the quantifiers and variables, the connectives and auxiliary symbols, etc. Secondly, there are the rules that tell you how to construct complex expressions from simpler ones – e.g. ‘If φ and χ are sentences, ‘φ v χ’ is also a sentence.’

Now, which term one uses for which part is mostly a matter of convention. However, I wouldn’t use ‘predicate calculus’ to refer to just the language of the formal system. Rather, I’d use that term to refer to (a version of) the deductive calculus. Perhaps I’d also use the term to mean predicate logic as a whole, but with a focus on the deductive part. If you think that the language itself already constitutes the ‘logic’ – e.g. because it already encodes logical form –, you might say ‘predicate logic’ and mean just the language.

Conceptually, what’s more important to realise is that there is no such thing as the language of predicate logic: each textbook has its own dialect. As far as I know, they all include at least one quantifier, matching variables, connectives, and at least some (n-place) predicate letters. Which connectives they include, already varies from one source to the next. Commonly, individual constants are also included, and brackets might be used. Additionally, you might have function symbols, identity, smaller-than and/or larger-than, etc. etc. The point is that there really isn’t one language that would deserve to be called the language of predicate logic. (This becomes relevant once you turn to meta-logic! Not all meta-theorems apply to all dialects.)

So, the best practice is to give the particular dialect you are working with a name of its own – such as ‘L-prop’. Often, ‘L=’ is used to indicate that the dialect in question includes identity. (In more mathematical contexts, people often start with a particular theory, and then name the language after the theory. E.g., one might use ‘L-PA’ to mean the language in which Peano Arithmetic is written.)

That’s how it is strictly speaking. More casually, it is not uncommon to just speak of ‘the language of predicate logic’, and leave it to the reader/audience/interlocutor to figure out the details.
Quine was very pragmatic about the issue. 

These examples come from a book by Prof. Gibson, Quine scholar. 

J.S. Mill said mathematics was empirical. 

A.J. Ayer, who was an empiricist,  says the necessity of logic and mathematics is real, and necessary because analytic, but does not provide truth about the world. 

Quine rejects both of these men, supposedly:   "The justification of logic and mathematics is on a par with the justification of theoretical physics. Logic, mathematics, physics are needed in the construction of our overall best theory of the world, and all three are justified to the extent that they make that theory come out right, .ie maximize true predictions." Prof. Roger Gibson Jr. on Quine. 

Huh?  Circular somehow?  

Now to be fair to Quine, his philosophy fits within a system. A pragmatic system. He sweated over this pretty hard. (Edit: like I'm doing)  It hangs together as a system, and it would require me to set out his entire pragmatic system to make Quine's statement above make sense. 

EDIT. After reading what Not-here wrote above, I think Gibson might have skipped over a point. It seems that Quine accepted Ayer-like position (analytic) essentially, but Ayer (and Ayer is not the only one) said such mathematical reasoning does not provide truth about the the world, therefore the use of math and logic and theoretical physics would need a justification if used to advance our overall theory of the world,  and all three are justified to the extent they make our theory of the world come out right, that is to say, that they maximize true predictions. 

(The paragraph above is still not right because it does not state what not-here was referring to, and the question appears to have already been answered, which is a good thing because I am certainly not making any progress with Quine's thought.)

Einstein's position and the position of Ayer seem to be closer to what mainstream philosophy has to say today. But I'm not up to date on it.  Quine's position is subtle and pragmatic, but will not satisfy the mathematicians. None of the three philosophers mentioned is probably able to satisfy a mathematician. 

All of this is apt to drive the mathematicians insane. This lack of clarity and the looseness of it is because we are still confused about just exactly what mathematics is. We may never figure it out. 

Do what the mathematicians do and forget about it. Keep doing mathematics. 

Sure you can do proofs. Just follow the rules of the game. Deduction. These truths were never true in the real sense of the world (see Ayer).  However, oddly, strangely sometimes when math touches the world it seems to be a match made in heaven!  When this happens, mine it for all it's worth. 

See SEP on Quine as not-here suggests. You can also read about philosophy of math in MacMillan Encl Phil. 10 vols 
Physical systems, more precisely the state of a physical system, is also defined "in terms of what is true about it". In this case, the "true statements" are yes/no propositions that can be operationally tested by constructible apparatus (very much a logical-positivist-like approach, but arising from strictly physical considerations).

Then, measuring the yes/no-outcomes for a maximal set of consistent propositions completely determines the measured system's state. Here, "consistent" means simultaneously measurable, i.e., no measurement disturbs the already-determined outcome of any other. And "maximal" means that any additional measurement necessarily distrurbs one or more already-determined outcomes.

And, in fact, there's no other way to define a physical system's state. So, not only "can you define...", but you've got no choice "but to define...".

Edit... 
... too-long-for-a-comment answer to @Gordon's comment below

The remarks above are just a standard result. It's typically derived as a quantum result since classical propositions are always consistent (also called "compatible"). But the same idea holds for everyday measurements of macroscopic systems, as follows.

Firstly, as I parenthetically remarked above vis-a-vis logical positivism, particularly the verifiability theory of meaning, a proposition is associated with the procedure or apparatus used to verify/measure it. And then the general idea here is that whenever two or more propositions are compatible, you can construct a single apparatus that simultaneously measures them both (or them all).

You can see that exactly stated in https://books.google.com/books?id=dVs8PcZ0Hd8C&pg=PA115 https://books.google.com/books?id=dVs8PcZ0Hd8C&pg=PA115 (just above the commutator, Eq.6.25). I'm not sure why Isham calls that "trivially compatible" (I'm not sure how they'd commute otherwise). So let's just overlook that wrinkle (unless somebody can followup with a counterexample).

The ultimate upshot of this is that any observable can be decomposed into a maximal set of yes/no-propositions (which are just observables with exactly two possible outcomes/eigenvalues). And that's precisely stated and derived somewhere in Chapter 5 or Chapter 6 of https://books.google.com/books/about/Foundations_of_quantum_mechanics.html?id=FwpRAAAAMAAJ https://books.google.com/books/about/Foundations_of_quantum_mechanics.html?id=FwpRAAAAMAAJ But I'm not immediately finding it in my printed copy, which I studied carefully some years (or decades) ago. And google seems reluctant to display entire pages, anyway (though I did notice an easy-to-find pirated pdf you can download, but won't explicitly give out the url).

Perhaps the easiest-to-follow discussion comprises the first six pages (right, just six) of https://books.google.com/books?id=WWYbAQAAIAAJ https://books.google.com/books?id=WWYbAQAAIAAJ at which point Schwinger concludes "...the symbol of this compound measurement is...[math elided]...which then describes a complete measurement, such that the system possesses definite values for the maximum number of attributes; any attempt to determine the value of still another independent physical quantity will produce uncontrollable changes in one or more of the previously assigned values." (he didn't seem to have much use for periods)

Anyway, the above quote's from pages 5-6 (and you might likely want to read through page 12), but google also seems reluctant to display entire pages of this book, and I'm not seeing any pdf's. Schwinger's subsequent https://books.google.com/books?id=fDX6CAAAQBAJ&pg=PA001 https://books.google.com/books?id=fDX6CAAAQBAJ&pg=PA001 develops that discussion much more formally, and is a correspondingly harder read, but google seems willing to display pages.

Finally, re Boltzmann, no. These are complete measurements determining pure states, not density matrices determining canonical ensembles. That wouldn't really address the OP's question regarding "things" (I'm taking his "something" to explicitly mean thing rather than ensemble). Whatever philosophical argument you want to have about the definition of "thing", a pure state is as close to "thing" as you're going to physically get.
Causal set theory, https://sites.google.com/site/lisaglaserphysics/research/causal-set-theory https://sites.google.com/site/lisaglaserphysics/research/causal-set-theory or https://en.wikipedia.org/wiki/Causal_sets https://en.wikipedia.org/wiki/Causal_sets or just google "causal sets", imposes a poset ordering on events corresponding to "causal connection" (your words). The logical analogy is just implication, and I'd think that's all there is to it. Although maybe substructural logical connectives better fit your intuition, which you don't really elaborate in sufficient detail for a sharp answer. For example https://arxiv.org/abs/gr-qc/0109053 https://arxiv.org/abs/gr-qc/0109053 which involves resource-aware linear implication in this case.
The point raised in the quote is not the same as the question that you are asking.

In the quote: It is a difference whether we define what one is, and then we define what two is, and so on, or whether we define the abstract concept of (natural) number (as pointed out by Mauro Allegranza in the comments). Of course we can say that natural number refers to the totality of the numbers we have defined, and that is a separate definition, and we may or may not be happy with it. (If you want to learn more about problems with this, look for non-standard models of Peano arithmetic).

Your question: There are many concepts of numbers of some kind that differ from the natural numbers. Examples are integers (including negative numbers), rational numbers, real numbers, complex numbers, (transfinite) ordinal numbers, (transfinite) cardinal numbers, surreal numbers. They have all nothing to do with the point raised in that quote, though.
Logicism's original goal certainly was not to diffuse Platonist impulses, although it was later adapted to that end, Frege was a devout Platonist. It was an epistemological reduction programme (aside from the more technical mathematical project): show that mathematics reduces to logic, which is more "secure" on any conception, and deal with the justificatory status of logic later. Frege, Russell, early Wittgenstein and Carnap had different ideas how, from platonism to positivism, and how much can be reduced, from primitive recursive arithmetic to all of mathematics. What attracted positivists to logicism was the hope of eliminating platonist metaphysics by reducing mathematics to conventions, because it seemed initially plausible that logic could be so reduced (this hope was later dashed by http://www.hist-analytic.com/QuineTruthbyConvention.pdf Quine's Truth by Convention). See https://conservancy.umn.edu/handle/11299/185663 Friedman's Logical Truth and Analyticity on the evolution of logicism from Frege to Carnap until the final blow to it was dealt by Gödel's incompleteness.

As for "logical objects", like connectives and predicates, they are eliminable. Wittgenstein in the Tractatus came up with the idea of combinatorial interpretation of connectives, they then stand for no objects at all, "logic takes care of itself" (of course, late Wittgenstein concluded that all of mathematics is not "about" anything, and is normative grammar in disguise, but that was hardly logicism). But according to Friedman, the Tractarian logicism recovers, at most, only primitive recursive arithmetic.

Quine famously adapted Russell's paraphrase to deny ontological existence to predicates. His https://plato.stanford.edu/entries/ontological-commitment criterion of ontological commitment is "to be is to be a value of the bound variable", i.e. to be is to be in the range of existential quantifier of scientific theories after paraphrasing out dispensable fictions, like Pegasus (sets and numbers can not be plausibly paraphrased, according to Quine, they are indispensable). He then replaced Frege's second order logic by first order logic with "semantic ascent" that does not quantify over predicates. Instead, we move to the meta-language and "paraphrase" second order quantification over predicates with schemata that contain placeholders fillable by definable predicates only. This is a formal mirror reflection of medieval nominalism, "real" common natures (objective predicates) are replaced with nomina ("words" for symbolic predicates). Of course, Quine was no logicist, but one could say that he picked up the pieces from Carnap's version of it that could be salvaged by embedding logic and mathematics into his holist web. In https://iweb.langara.bc.ca/rjohns/files/2015/03/Quine_selection.pdf Epistemology Naturalized he admits that the original promises of logicism ring hollow:


  "Mathematics reduces only to set theory and not to logic proper. Such reduction still enhances clarity, but only because of the interrelations that emerge and not because the end terms of the analysis are clearer than others. As for the end truths, the axioms of set theory, these have less obviousness and certainty to recommend them than do most of the mathematical theorems that we would derive from them. Moreover, we know from Gödel's work that no consistent axiom system can cover mathematics even when we renounce self-evidence. Reduction in the foundations of mathematics remains mathematically and philosophically fascinating, but it does not do what the epistemologist
  would like of it: it does not reveal the ground of mathematical knowledge, it does not show how mathematical certainty is possible."


Friedman, for his part, argued that Quine's "web of belief" is not sufficiently stratified, and that mathematics and logic enjoy certain autonomy (his own student Parsons and later Maddy expressed similar sentiments). This opens up a possibility that the reduction to logic and set theory may be more meaningful epistemologically than mere pragmatic convenience. And more recently there is a movement that is even closer aligned with the original logicism, the https://plato.stanford.edu/entries/logicism/#NeoFre neologicism or neo-Fregeanism:


  "The neo-Fregean movement seeks to reveal that a significant amount of mathematics is analytic. This is a stronger claim than that it is a priori and derives no part of its justification from empirical science, or even from successful applications within the empirical sciences. For that would hold of mathematics (or indeed any other branch of knowledge) conceived of as synthetic a priori. The neo-Fregean maintains in addition that significant parts of mathematics flow logically from principles that are analytic of (or definitional of) their central concepts or predicates, such as ‘natural number’ or ‘real number’. That is, they flow from the very meanings of those central predicates."


Of course, neo-Fregeans learned Frege's lesson, and admit that logic does not express indubitable "laws of thought", and that analytic "truths" can be discarded on pragmatic grounds, albeit not empirical, but that is still much closer to Frege and Carnap than to Quine. Not all neo-Fregeans are prepared to go as far as the above SEP quote, however. Heck in http://rgheck.frege.org/pdf/published/JuliusCaesarObjection.pdf The Julius Caesar Objection states more modest goals:


  "Not even the claim that numbers are objects is required for Frege’s proofs of the axioms of arithmetic. What is required is that expressions of the form
  “the number of numbers less than 5” should be of the same logical Sort as those of the form “the number of Roman emperors”... The attractions of the genetic story told at the beginning of this section do not depend upon the claim that the various instances of Hume’s Principle are logical truths, analytic truths, or any such thing. Frege’s most fundamental thought—that our knowledge of the truths of arithmetic derives, in some sense, from our knowledge of Hume’s Principle—could well be true, even if it does not have the epistemological implications he had hoped it would."

I. I do agree with Barcan and Kripke: if two things are actually one and the same, then they are necessarily one and the same. As far as my judgment can discern, this is just the statement that, for all possible worlds or scenarios, x exists if and only if x exists.

So when we say that "Hesperus is Phosphorus," or that "the morning star is the evening star," the state of affairs we are expressing is the same as when we say "Venus is Venus."

On the comments above, jobermark read the second of these sentences as de dicto, when the intended reading of you and Kripke is de re. To give a classic example of that distinction, consider what someone is stating when they say "Necessarily, the 44th president of the United States is Obama." The de re reading is that Obama is necessarily Obama: the object referred to by "the 44th PotUS" is what is being asserted some property (i.e. being necessarily identical to Obama). The de dicto reading is that whoever occupies the place of the 44th PotUS is necessarily identical with Obama: the description provided by "the 44th PotUS" is being asserted to always refer to something identical to Obama, which is false.

—

II. Unlike you asserted, that A = A entails N(A = A) is not true for all of its instances because merely because it is a theorem of (some version of) modal logic, I think, but rather because the world (necessarily) is that way and that logic's axioms capture the way the world (necessarily) is.

Unlike Quine would have to assert if faced to your objection, A = A or N(A = A) is not true in all its instances because we have defined so, but rather because things in reality are such that something couldn't have been numerically distinct to itself. (I'm reading '=' as asserting numerical identity, of course. Reading '=' as asserting qualitative identity yields a contingent statement: the morning star now differs from what it itself was some time ago.)

Something makes it so Barcan's assertion is universally and necessarily true, and it's not our definitions or linguistic practices (I think — many would beg to differ), but rather because of something in reality, namely, everything, for everything is identical to itself. (Funnily enough, it was Quine that said that "A = A" is true because of the way reality is. I suppose he denied it to be necessary for any of its instances.)

—

P.S. I don't see the difference between «N(P)» and «"P" is necessarily true». Perhaps the difference is that, in the second sentence, one is asserting that in all possible worlds the sentence-type "P" asserts something true. But this is false. "All female foxes are vixens" certainly expresses a necessary truth in our context of linguistic usage, because it expresses that all female foxes are vixens, which is necessarily true. But in some other context of usage, in the actual world or elsewhere, that very same sentence-type may express that some female foxes are NOT vixens, which is false. To fix that problem, we may instead state that «Necessarily, whenever "P" expresses that P, "P" is true» — which is very close to «N(P)», but pretty far away from our original sentence, for it does not involve what Quine dubs a semantic predicate.
Contrary to some of the comments given above, quantum theory does involve a theory of time. Time is often used as a parameter labeling the set of states of a quantum system, but this is problematic because time is measured using clocks which are physical systems. So quantum theory is incomplete without a theory of time and clocks.

A quantum theory of clocks was proposed by Don Page and William Wootters in 1983 and has been improved since then:

https://arxiv.org/abs/1610.04773 https://arxiv.org/abs/1610.04773.

The short version is that quantum mechanics implies the existence of other universes and any given time is a special cases in the set of universes universes that contains records of previous times but not future times. See the paper above and chapter 11 of "The Fabric of Reality" by David Deutsch. As such, all times exist. They don't go in and out of existence. This sounds somewhat like the b theory of time.
For the logical analysis of definitions, we can see https://plato.stanford.edu/entries/definitions/#LogDef The logic of definitions.

According to a view, definitions are a sort of "typographical conventions". We introduce in the meta-theory a new symbol that stands for a "complex" expression formed with the primitive symbols of the language.

For example, in first order arithmetic, we define the binary predicate n < m
as an abbreviation for the formula ∃k (m=n+(k+1)).

According to a more subtle analysis, a new symbol (name, predicate) is introduced through an expansion of the language L of the theory.

This expansion L+ must be conservative, in the sense that no new formulas expressed in the original language L must be provable by the introduction of the new symbol.

In addition, we want that the new symbol (the definiendum) can be eliminable replacing in a context its occurrence with the "original" expression (the definiens).

The Conservativeness criterion can be made precise as follows.


  Any formula of L that is provable in L+ is provable in L.


That is, the definition does not enable us to prove anything new in L. 

The Eliminability criterion can be made precise thus:


  For any formula A of L+, there is a formula of L that is provably equivalent in L+ to A.


A simple example can help: we can introduce a new "name" (an individual constant) a through the definition:


  Def: a=x ≡ ψ(x)


provided that we can prove that there exists a unique ψ(x).

This amounts to proving in the theory that 


  ∃x ψ(x) ∧ ∀y (ψ(y) → y=x).


To say that the above formula is provable in the theory means that it is true in every model of the theory (i.e. in every interpretation that satisfies the axioms of the theory).

Thus, the said formula guarantee that in every model of the theory there is "a unique object such that ψ holds of it".

Adding the stipulation expressed by the definition, we call that object a.

In this sense, we can say that "the existence of a is true by definition".
Language isn't just symbols.  Language is your history.  (This is why it is a crime to assimilate people into your culture.)

From that history, there are generationally-encoded values of both semantics and syntax.  We pass these former unconsciously, each generation, through culture.  

Now it is very meaningful to ask:  can we really understand a language from a different culture?  And I argue no, not without being connected to its history or assimilating to it.  The white man never understood the Red Man, for example (with few exceptions, some involving ingesting psycho-active substances), because they share NO history AT ALL.  The evolutionist would argue otherwise, but the Biblical narrative is the only way to understand this complete failure of Western culture to understand the Native American.

Another example is Ancient Greece.  It is not clear at all that we share any genetic connections from Greece, so these texts must be carefully translated.

So now, in philosophy, we have two different systems of semantics (not just syntax):  first-order calculus consisting of many non-alphbetic symbols and predicate sentential logic using some form of the verb "IS".  The latter we can understand intuitively, the other we have to be trained and only when we have completely embodied this training can we be said to understand their propositions.

As another example, you understand what I'm saying, even though these ASCII characters givce you get NO inflection data (besides the use of caps, etc.), no tonation, and no rhthym of interaction, right?  You propably 50-80% of what I typed despite what I just said because we share a lot of common culture.  (I can tell by YOUR use of language).  The rest you have to extrapolate or ask for clarification if sufficient interest is present.

{1}     1.   ƎxƎy[Cx & Cy & ~x=y & ∀z[Cz → (z=x ∨ z=y)]]                       Prem
{2}     2.   Ǝy[Ca & Cy & ~a=y & ∀z[Cz → (z=a ∨ z=y)]]                         Assum. TD(a)
{3}     3.   Ca & Cb & ~a=b & ∀z[Cz → (z=a ∨ z=b)]                             Assum. TD(b)
{3}     4.   Ca & Cb & ~a=b                                                    3 &E
{3}     5.   ∀z[Cz → (z=a ∨ z=b)]                                              3 &E
{6}     6.   Cc & Cd & Ce                                                      Assum.
{6}     7.   Cc                                                                6 &E
{3}     8 .  Cc → (e=a ∨ e=b)                                                  5 UE
{3,6}   9 .  c=a ∨ c=b                                                         7,8 MP
{6}     10.  Cd                                                                6 &E
{3}     11.  Cd → (d=a ∨ d=b)                                                  5 UE
{3,6}   12.  d=a ∨ d=b                                                         10,11 MP
{6}     13.  Ce                                                                6 &E
{3}     14.  Ce → (e=a ∨ e=b)                                                  5 UE
{3,6}   15.  e=a ∨ e=b                                                         13,14 MP
{16}    16.  c=a                                                               Assum. (9 1st Disj.)
{17}    17.  d=a                                                               Assum. (12 1st Disj.)
{16,17} 18.  c=d                                                               16,17 =E
{16,17} 19.  c=d ∨ c=e ∨ d=e                                                   18 ∨I (12 1st Conc.)
{20}    20.  d=b                                                               Assum. (12 2nd Disj.)
{21}    21.  e=a                                                               Assum. (15 1st Disj.)
{16,21} 22.  c=e                                                               16,21 =E
{16,21} 23.  c=d ∨ c=e ∨ d=e                                                   22 ∨I (15 1st Conc.)
{24}    24.  e=b                                                               Assum. (15 2nd Disj.)
{20,24} 25.  d=e                                                               20,24 =E
{20,24} 26.  c=d ∨ c=e ∨ d=e                                                   25 ∨I (15 2nd Conc.)
{16,20} 27.  c=d ∨ c=e ∨ d=e                                                   15,21,23,24,26 ∨E (12 2nd Conc.)
{16}    28.  c=d ∨ c=e ∨ d=e                                                   12,17,18,19,27 ∨E (9 1st Conc.)
{29}    29.  c=b                                                               Assum. (9 2nd Disj.)
{30}    30.  d=a                                                               Assum. (12 1st Disj.)
{31}    31.  e=a                                                               Assum. (15 1st Disj.)
{30,31} 32.  d=e                                                               30,31 =E
{30,31} 33.  c=d ∨ c=e ∨ d=e                                                   32 ∨I (15 1st Conc.)
{34}    34.  e=b                                                               Assum. (15 2nd Disj.)
{29,34} 35.  c=e                                                               29,34 =E
{29,34} 36.  c=d ∨ c=e ∨ d=e                                                   35 ∨I (15 2nd Conc.)
{29,30} 37.  c=d ∨ c=e ∨ d=e                                                   15,31,33,34,36 ∨E (12 1st Conc.)
{38}    38.  d=b                                                               Assum. (12 2nd Disj.)
{29,38} 39.  c=d                                                               29,38 =E
{29,38} 40.  c=d ∨ c=e ∨ d=e                                                   39 ∨I (12 2nd Conc.)
{29}    41.  c=d ∨ c=e ∨ d=e                                                   12,30,37,38,40 ∨E (9 2nd Conc.)
{3,6}   42.  c=d ∨ c=e ∨ d=e                                                   9,16,28,29,41 ∨E
{3}     43.  (Cc & Cd & Ce) → (c=d ∨ c=e ∨ d=e)                                6,43 CP
{3}     44.  ∀z[(Cc & Cd & Cz) → (c=d ∨ c=z ∨ d=z)]                            43 UI
{3}     45.  ∀y∀z[(Cc & Cy & Cz) → (c=y ∨ c=z ∨ y=z)]                          44 UI
{3}     46.  ∀x∀y∀z[(Cx & Cy & Cz) → (x=y ∨ x=z ∨ y=z)]                        45 UI
{3}     47.  Ǝy[Ca & Cy & ~a=y]                                                 4 EI
{3}     48.  Ǝxy[Cx & Cy & ~x=y]                                                47 EI
{3}     49.  Ǝxy[Cx & Cy & ~x=y] & ∀x∀y∀z[(Cx & Cy & Cz) → (x=y ∨ x=z ∨ y=z)]  46,48 &I
{2}     50.  Ǝxy[Cx & Cy & ~x=y] & ∀x∀y∀z[(Cx & Cy & Cz) → (x=y ∨ x=z ∨ y=z)]  2,3,49 EI
{1}     51.  Ǝxy[Cx & Cy & ~x=y] & ∀x∀y∀z[(Cx & Cy & Cz) → (x=y ∨ x=z ∨ y=z)]  1,2,50 EI

Of course there are objects like an electric field or a phonograph that were inconceivable to primitive humans of Stone Age. And there are modern devices like micro chips or lasers or cell phones that were inconceivable to people 200 years ago because the effects they are based upon were far out of reach even for advanced physicists of that time. It is inconceivable that this series will stop. (And if it would then this very fact is inconceivable today.) 
Interpretation of Peirce's realism which grew out of combining Kantian epistemology with scholastic ontology of Duns Scotus (Peirce calls himself "a scholastic realist of a somewhat extreme stripe") is indeed difficult. What makes it even more difficult is that Peirce went through several major reworkings of his "architectonic" without clearly indicating what was kept and what was revised, so it is easy to find statements in his works that appear to be contradictory. It is fair to say, however, that he flatly denies the intelligibility of "things in themselves", as did many others starting with Fichte, "there can be no concept of absolutely incognizable since nothing of this sort occurs in experience... Hence a term can have no such meaning". 

But Peirce also rejected Kant's identification of things in themselves with noumena. https://www.jstor.org/stable/40320700 Pihlström in Peircean Scholastic Realism and Transcendental Arguments argues that Peirce can be understood as naturalizing and pragmatizing Kantian transcendental arguments, and his noumena are projections of "pure reason" downgraded to fallible human knowledge. Due to Kantian scruples Peirce can not adopt a metaphysical realism with already conceptualized world "grasped" by the mind (he rejects what is now called https://plato.stanford.edu/entries/sellars the myth of the Given). On the other hand, he wants a robust notion of truth answerable to independent and demystified reality. The result is a dual picture of reality, first as non-conceptual and encountered through action and reaction (pragmatism), and second as conceptualized through such encounters but only "at the end of inquiry". The inquiry may involve multiple human generations, and potentially even non-human beings, and may never be achieved in actuality, Peirce defends reality of possibilia. The truth is identified with this "final opinion", hence it is not a correspondent truth in any traditional sense. And it is this final opinion that hosts the noumena:  


  "There is a definite opinion to which the mind of man is, on the whole and in the long run tending. On many questions the final agreement is already reached, on all it will be reached if time enough is given... This final opinion, then, is independent, not indeed of thought, in general, but of all that is arbitrary and individual in thought; is quite independent of how you, or I or any number of men think. Everything, therefore, which will be thought to exist in the final opinion is real, and nothing else...
  
  This theory of reality is instantly fatal to the idea of a thing in itself, - a thing existing independent of all relation to the mind's conception of it. Yet it would by no means forbid, but rather encourage us, to regard the appearances of sense as only signs of the realities. Only, the realities which they represent, would not be the unknowable cause of sensation, but noumena or intelligible conceptions which are the last products of the mental action which is set in motion by sensation". [CP 8.12-13, emphasis Peirce's]


https://www.jstor.org/stable/40320422 Margolis in The Passing of Peirce's Realism rightly points out that in the absence of metaphysical guarantees it is unclear why the inquiry should converge on any "final opinion". Peirce himself moved from certainty about this to merely "hope" in late years. Aside from being written off as "19th century optimism", the usual defense is pragmatic, this convergence is merely a hypothesis, as the entire corpus of science is hypothetical, albeit confirmed by practice. This is not far from Kant's regulative ideas or neo-Kantian "limit concepts".

Pragmatism even allows Peirce to offer hypothetical metaphysics, which connects his remote noumena to the acting/reacting reality of everyday encounters, see https://www.jstor.org/stable/40320422?seq=1#page_scan_tab_contents Haack's "Extreme Scholastic Realism:" Its Relevance to Philosophy of Science Today. It posits actions/processes at the core of experienced reality (Secondness), objects are viewed as their conceptual derivatives, and it construes "generals", relations and laws (Thirdness), as manifested in the patterns of actions/reactions. It is they that are the fundamental reality for Peirce:"All that Hume attacked I defend, namely, law as a reality", this is why he calls his realism "of extreme stripe". This relational ontology might raise concerns familiar from dealing with Platonic froms, infinite regress on relations of relations, etc. But for Peirce only some generals are real, which ones is for empirical science to hypothesize and confirm in action, it is not a business of a priori Platonic contemplation with its pure mathematical largesse.

Pihlström suggests that https://plato.stanford.edu/entries/kant-transcendental-idealism/#DualAspeView one world/two perspectives (a.k.a. dual aspect) interpretation of Kant is more favorable to finding common ground with Peirce than the traditional phenomenalist interpretation that Peirce accepted. Indeed, this reading is more hospitable to realism since appearances are already real and things-in-themselves are just another aspect of them. Two key differences still remain however. First, Kant's picture is static, we have a fixed transcendental subject confronting a fixed world dressed into a straightjacket of reason's own fixed a priori categories and schemata. There is little epistemic movement in this world beyond clarification of the obscure. Kant can not therefore make the use Peirce has for noumena as happy limits of epistemic evolution, they are static empty posits.

Second, Kant's view of knowledge is absolutist, nothing short of apodictically certain passes the muster, not metaphysics, not chemistry, not psychology. On this standard the chasm between phenomena and noumena is indeed unbridgeable. But on Peirce's pragmatic view even mathematical certainty is fallible, and its difference with the empirically scientific one is of a degree, not kind. Even the "a priori" that frame phenomena are neither a priori nor immutable, they are merely high level hypotheses to be revised eventually. So what Kant cast out as less-than-knowledge aided by practical reason and reflective judgment, is now admitted in full and paves the road from phenomena to noumena. On Peirce's optimistic "hope" the two perspectives merge in the "final opinion".
For simplicity, rewrite axiom (A3) as follows:


  (¬A → ¬D) → ((¬A → D) → A),


where (see page 3) A and D are statement forms (i.e. place-holders that stand for formulas whatever; see https://plato.stanford.edu/entries/schema/#UsesSche Uses of schema in Logic).

Il Line 1, we perform the substitution of B in place of A and of ¬B in place of D, getting:


  
    (¬B → ¬¬B) → ((¬B → ¬B) → B).
  

If you take "fitting a definition" to mean meeting the criteria associated with this definition, then it's a fact that something fits a definition.

So for example, as you say, it can be a fact that an object O falls down. Now imagine that you define a new term "fallingobject" with the criteria that something is a fallingobject if it falls down.  Then it's a fact that O is a fallingobject, i.e. that it fits this definition, because what you mean by "O is a fallingobject" is just that O falls down, and by stipulation, this is a fact.
Transposing the premises does not change the validity of the syllogism. This action does alter the normal format of a syllogism, where the predicate of the conclusion appears in the first premise (the major premise). But validity is unaffected.

Compare:

(1) All emeralds are green.This gemstone is an emerald. Therefore this gemstone is green.

(2) This gemstone is an emerald. All emeralds are green. Therefore this gemstone is green.

Example (1) shows the usual order of the premises. But both are valid.
The answer to your question is "it depends" Different axiomatic systems can lead to different conclusions. The truth of a statement S can only be said to be true or false, within a given model. 

This issue is not really a problem if you are talking about reaching a conclusion in two different axiomatic systems. We usually only work in one axiomatic system at a time and there is usually some kind of justification for using that system. And in this case, H1 and H2, in OP's question would not be independent, as canifold points out. 

However, it is also possible for a single axiomatic system to lead to contradicting results. Such a system is said to be "inconsistent." We generally do not work with such systems and it is usually assumed that reality itself is logically consistent, but we do not have to make that assumption. There is a whole field of study involving http://www.iep.utm.edu/math-inc/ inconsistent logical systems. They do have their benefits. For instance, in a consistent logical system, there are theorems which can never be proven true or false, while in an inconsistent logical system, we can always determine whether a theorem is true or false. 

Additionally, canifold's argument does not hold in a logically inconsistent system. Reductio ad absurdum, or proof by contradiction, is a form of valid proof, specifically because we have assumed that our logical system results in only true or false statements and that a statement cannot be both true and false.

The link I provided discusses this concept in more detail and also explains some reasons why we might not be certain that our reality is logically consistent and also how we can take a current axiomatic system and add to it in such a way as to make it useful but not consistent. 
When it comes to justification there is indeed a symmetric http://homepages.wmich.edu/~mcgrew/Deduction problem of deduction. But forming general opinions or laws is not part of deduction, it is https://en.wikipedia.org/wiki/Abductive_reasoning abductive (or in older terminology inductive), when it comes to science it is the "hypothetical" part of the hypothetico-deductive method, see https://philosophy.stackexchange.com/a/46120/9148 Are “if smoke then fire” arguments deductive or inductive? for more on abduction in science. 

The advantage of (formal) deduction is that at least we know that it is always truth preserving, even if we can not justify it. Induction, on the other hand, sometimes works and sometimes does not. So the problem of justifying it is much more substantive because the hope is that if we have an explicit justification at hand we will be able to better tell when is which. The problem of deduction is a particular case of the so-called https://en.wikipedia.org/wiki/M%C3%BCnchhausen_trilemma Agrippa's trilemma/justificatory regress arguments, and is discussed by Dummett in https://rads.stackoverflow.com/amzn/click/0856720909 Justification of Deduction: 


  "We should probably make use either of those very forms of inference which we were supposed to be justifying, or else of ones which we had already justified by reduction to our primitive rules. And, even if we did neither of these things, so that our proof was not strictly speaking circular, we should have used some principles of inference or other, and the question could then be raised what justified them: we should therefore either eventually be involved in circularity, or have embarked upon an infinite regress."


Dummett argues for a different kind of "justification", semantic justification, where interpretations (truth tables in simple cases) are used to show that deductive rules are indeed truth preserving. Of course, the same rules are implicitly used to reason about the truth tables, but this does not erase their explanatory (as opposed to justificatory) value, in place of a vicious circle we get a virtuous https://en.wikipedia.org/wiki/Hermeneutic_circle hermeneutic circle. An alternative, syntactic, approach to "justifying" deduction is https://social.stoa.usp.br/articles/0016/4213/Haack_1976_.pdf Haack's Justification of Deduction. Here is Dummett on how deduction compares to induction on semantic approach:


  "The situation is thus the reverse of what seems to be the case with
  induction. In the case of induction, we appear to have a quite unconvincing
  argument that there could not in principle be a justification, but we lack any
  candidate for a justification. In that of deduction, we have excellent candidates, in the soundness and completeness proofs, for arguments justifying
  particular logical systems; in the face of an apparently convincing argument
  that no such justification can exist.
  
  The circularity that is alleged against any attempt to justify deduction,
  viz. to justify a whole system of deductive inference, is not of the usual kind.
  The validity of a particular form of inference is not a premiss for the
  semantic proof of its soundness; at worst, that form of inference is employed
  in the course of the proof. Now, clearly, a circularity of this form would be
  fatal if our task were to convince someone, who hesitates to accept inferences of this form, that it is in order to do so. But to conceive the problem of
  justification in this way is to misrepresent the position that we are in. Our
  problem is not to persuade anyone, not even ourselves, to employ deductive
  arguments: it is to find a satisfactory explanation of the role of such
  arguments in our use of language.


There is a second aspect to the problem of deduction that is specific to it, what Hintikka called the https://dialnet.unirioja.es/descarga/articulo/4729757.pdf scandal of deduction (paraphrasing Hume's "scandal of induction"). The paradox, over which already Mill and Peirce puzzled, is that, on the one hand, deduction merely reveals what is already "contained" in the premises (as classically held by Locke and Kant) and hence produces no new information, but on the other hand, when mathematicians prove non-trivial theorems it seems like they do learn something new. The scandal of deduction is an active research topic in modern epistemic logic, see e.g. http://uhra.herts.ac.uk/handle/2299/2995 The Enduring Scandal of Deduction by Floridi and D'Agostino.
I would say the answer is yes. Art is, well, an art and not a science. The precision of a deductive conclusion is not possible, nor is it possible to conduct the repeated experiments that support induction.

But it is possible for the observer to assemble whatever data is available and draw an inference to the best explanation. A painter might have worked several centuries ago. Deduction and induction are out of the question. But a lot of information still exists, such as: when the painter lived, the cultural customs of the day, materials available to an artist, and the work done by other contemporary painters.

Such data data allows an observer today to form some interpretation of what the painter was likely trying to say. Scientific it is not, but abduction is the best method available and its conclusion can still be meaningful.
18 - ∀z (Cube(z) → (z = c ∨ z = f))

19 - (Cube(c) ∧ Cube(y) ∧ c ≠ y) ∧ ∀z (Cube(z) → (z = c ∨ z = f)) --- from 4 and 18 by ∧-intro

20 - ∃x ∃y [(Cube(x) ∧ Cube(y) ∧ x ≠ y) ∧ ∀z (Cube(z) → (z = x ∨ z = y))] --- from 19 by ∃-intro twice

20 is derived under the two assumptions 3 and 4 made for two ∃-elim's with terms c and f. They are not present in 20; thus, we can safely conclude with 20 by ∃-elim twice, discharging temporary assumptions 3 and 4.

Conclusion:


  1, 2 ⊢ ∃x ∃y [(Cube(x) ∧ Cube(y) ∧ x ≠ y) ∧ ∀z (Cube(z) → (z = x ∨ z = y))] 

The paper http://killingbuddha.altervista.org/FILOSOFIA/GToNe.pdf General Theory of Natural Equivalences by Eilenberg and Mac Lane (1945), where the terminology is first introduced, mentions neither Aristotle, nor Kant, nor even Carnap, who was still alive. The motivation given was:


  "In  a  metamathematical  sense  our  theory  provides  general  concepts
  applicable to all branches of mathematics, and so contributes to the current
  trend towards uniform treatment of different mathematical disciplines. In
  particular it provides opportunities for the comparison of constructions
  and of the isomorphism occurring in different branches of mathematics;
  in this way it may occasionally suggest new results by analogy."


This roughly corresponds to Aristotelian view of categories as the most general forms of being, or to Kantian view of them as most general forms of thinking. Then again, it also corresponds to their colloquial meaning, which derives from legal Latin. But Mac Lane is a philosopher-mathematician like Hilbert, see https://case.edu/artsci/phil/BJPSMacLane.pdf McLarty's The Last Mathematician from Hilbert’s Göttingen: Saunders Mac Lane as Philosopher of Mathematics, so philosophical inspiration is not unexpected. Almost 30 years later Mac Lane recalled in Notes to Chapter I of http://www.maths.ed.ac.uk/~aar/papers/maclanecat.pdf Categories for Working Mathematicians: 


  "A direct treatment of categories in their own right appeared in Eilenberg-Mac Lane [1945]. Now the discovery of ideas as general as these is chiefly the willingness to make a brash or speculative abstraction, in this case supported by the pleasure of purloining words from the philosophers: "Category" from Aristotle and Kant, "Functor" from Carnap (Logische Syntax der Sprache), and "natural ransformation" from then current informal parlance. Initially,
  categories were used chiefly as a language, notably and effectively in the Eilenberg-Steenrod axioms for homology and cohomology theories."


However, the idea that objects are merely relational placeholders in a structure was certainly alien to both Aristotle and Kant. But it was common in French structuralism of the time derived from Saussure, and the notion of structure was taken up by Bourbaki, albeit still tied to the set-theoretic base. https://arxiv.org/abs/math/0507203 Kutateladze in Saunders Mac Lane, the Knight of Mathematics claims that he and Eilenberg were into Kant at the time, but I could not confirm this from a first hand source. Still, he tells an interesting background story which sounds plausible:


  "In Mac Lane’s opinion, the conceptions of category theory were close to the
  methodological principles of the project of Nicholas Bourbaki. Mac Lane was sym
  pathetic with the project and was very close to joining in but this never happened (the main obstacles were in linguistic facilities).  However, even the later membership of Eilenberg in the Bourbaki group could not overcome a shade of slight disinclination and repulsion. It turned out impossible to “
  categorize Bourbaki” with a theory of non-French origin as Mac Lane had once phrased the matter shrewdly and elegantly. It is worth noting in this respect that the term “category theory” had roots in the mutual interest of its authors in philosophy and, in particular, in the works of Immanuel Kant".


The connection to Carnap also does not seem to go beyond general similarities. Here is from http://www.pitt.edu/~belnap/140undercarnapslamp.pdf Belnap's paper Under Carnap’s Lamp: Flat Pre-semantics on Carnap's usage:


  "At the abstract level that is relevant to our concerns, we think of a grammar as involving the following. Categorematic expressions, such as sentences or terms, with the idea that a semantics will then give a “value” of some kind to each categorematic expression. Syncategorematic expressions, such as “∼” or “&” or “(”, which play a role in some grammatical operation. Grammatical operations, or modes of combination or functors, each of which is a (grammatical) function taking categorematic expressions as input, and producing a categorematic expression as output. Example: the operation which, given two sentential inputs A1 and A2, produces an appropriate “conjunction” of those two sentences, perhaps having the appearance “(A1 & A2)”."


Corfield, quoting Belnap on the https://golem.ph.utexas.edu/category/2007/02/in_the_footsteps_of_rudolf_car_1.html n-Category Cafe blog, is ambivalent:"I guess that gives us an idea of why Mac Lane chose it, although I don’t see that the notion of a functor taking arrows as arguments is present." One can see that MacLane could have borrowed not just functors but also categories directly from Carnap. Of course, Carnap himself, like other positivists, was heavily indebted to Kant, see Friedman's Parting of the Ways.
It depends. Popper's http://plato.stanford.edu/entries/popper/ falsifiability clearly discusses a way to define the nature of a fact, rather than the reduction of any logical argument. 

We can claim that logical arguments that are non-falsafiable are non-scientific from an objective perspective:


  These factors [Marxism, astrology] combined to make Popper take falsifiability as his criterion for demarcating science from non-science: if a theory is incompatible with possible empirical observations it is scientific; conversely, a theory which is compatible with all such observations, either because, as in the case of Marxism, it has been modified solely to accommodate such observations, or because, as in the case of psychoanalytic theories, it is consistent with all possible observations, is unscientific. For Popper, however, to assert that a theory is unscientific, is not necessarily to hold that it is unenlightening, still less that it is meaningless, for it sometimes happens that a theory which is unscientific (because it is unfalsifiable) at a given time may become falsifiable, and thus scientific, with the development of technology, or with the further articulation and refinement of the theory. Further, even purely mythogenic explanations have performed a valuable function in the past in expediting our understanding of the nature of reality.


...


  As Popper represents it, the central problem in the philosophy of science is that of demarcation, i.e., of distinguishing between science and what he terms ‘non-science’, under which heading he ranks, amongst others, logic, metaphysics, psychoanalysis, and Adler's individual psychology. Popper is unusual amongst contemporary philosophers in that he accepts the validity of the Humean critique of induction, and indeed, goes beyond it in arguing that induction is never actually used by the scientist. However, he does not concede that this entails the scepticism which is associated with Hume, and argues that the Baconian/Newtonian insistence on the primacy of ‘pure’ observation, as the initial step in the formation of theories, is completely misguided: all observation is selective and theory-laden—there are no pure or theory-free observations. In this way he destabilises the traditional view that science can be distinguished from non-science on the basis of its inductive methodology; in contradistinction to this, Popper holds that there is no unique methodology specific to science. Science, like virtually every other human, and indeed organic, activity, Popper believes, consists largely of problem-solving.
  
  Popper, then, repudiates induction, and rejects the view that it is the characteristic method of scientific investigation and inference, and substitutes falsifiability in its place. It is easy, he argues, to obtain evidence in favour of virtually any theory, and he consequently holds that such ‘corroboration’, as he terms it, should count scientifically only if it is the positive result of a genuinely ‘risky’ prediction, which might conceivably have been false. For Popper, a theory is scientific only if it is refutable by a conceivable event. Every genuine test of a scientific theory, then, is logically an attempt to refute or to falsify it, and one genuine counter-instance falsifies the whole theory. In a critical sense, Popper's theory of demarcation is based upon his perception of the logical asymmetry which holds between verification and falsification: it is logically impossible to conclusively verify a universal proposition by reference to experience (as Hume saw clearly), but a single counter-instance conclusively falsifies the corresponding universal law. In a word, an exception, far from ‘proving’ a rule, conclusively refutes it.
  
  Every genuine scientific theory then, in Popper's view, is prohibitive, in the sense that it forbids, by implication, particular events or occurrences. As such it can be tested and falsified, but never logically verified. Thus Popper stresses that it should not be inferred from the fact that a theory has withstood the most rigorous testing, for however long a period of time, that it has been verified; rather we should recognise that such a theory has received a high measure of corroboration. and may be provisionally retained as the best available theory until it is finally falsified (if indeed it is ever falsified), and/or is superseded by a better theory.


However, using this philosophy, we can only claim that certain logical arguments are non-scientific, rather than non-useful. Expanding the debate to cover other philosophers of science, the nature of falsifablilty and the objective reality behind it may be a fuction of one's paradigm (http://plato.stanford.edu/entries/thomas-kuhn/ Kuhn) or Research Programme (http://www2.lse.ac.uk/philosophy/about/lakatos/scienceAndPseudoscienceTranscript.aspx Lakatos). 

It is not appropriate to apply the requirements of science to any logical argument, as not all logical arguments presume predictive power of the "real" world as part of their givens. When considering constructed realities (like computer protocols) it is even worthwhile to explicitly forbid considerations of the real, as they do not apply within the subjective and constructed (but logical) arguments therein. 
"God exists" is similar to "an electron exists", because neither claim is completely falsifiable nor is its negation.  For example, imagine the sum total of all scientific data that has ever been collected to support the existence of an electron.  Now suppose every single experiment was affected by an improbably large (but still possible) amount of experimental error.  So  it is possible that all future replications of those experiments will fail to establish the electron's existence.  But it is also possible that at some point the experiments will start working again, and it will then be believed instead that it was the experiments disproving the electron's existence that were affected by experimental error.  Neither "an electron exists" nor "no electron exists" is truly falsifiable.  The existence of the electron is ultimately dependent on shared intuition, experience, and belief.  Likewise, so is the existence of God.  If God came to Earth and demonstrated his existence to every person in a way that all persons could understand and communicate effectively, then there would be no difficulty accepting "God exists" as a scientific statement.  And if God then left Earth, and many persons were no longer able to be convinced of his existence, then "God exists" would cease to be considered a scientific statement.

For a claim to be a valid scientific claim, it has to have precise and agreed-upon definitions, and there must be a convincing argument that it is extremely likely to be true.
Kripke has some examples in his book Naming and Necessity. The proposition Hesperus is Phosphorus (the evening star is the morning star, both being what we call Venus) is one of them. Kripke finds this to be analytic a posteriori because there once was a time in which people thought of Hesperus and Phosphorus as two different stars, later on they found out that they we're actually the same planet. In this way they necessarily point to the same object but this has been found out through the empirical evidence.

You can read the passage here: http://books.google.com/books?id=04CSCh06t0MC&lpg=PP1&pg=PA102#v=onepage&q=hesperus&f=false http://books.google.com/books?id=04CSCh06t0MC&lpg=PP1&pg=PA102#v=onepage&q=hesperus&f=false
I would suggest that one way to distinguish cause from effect when the two are simultaneous is  through material implication.

That is, if at some time two events A and B occur, the cause is that one which implies the other. So, if A being true means B must be true, then A is in some sense the cause of B (I realize that the whole correlation versus causation problem comes up here, but I am only trying to provide a rough model). 

For example, suppose that at some time t we have a big spherical mass M (event A), and from a distance r away you (mass m) experience a real force equal to GMm/r^2 (event B). In this case, A implies B by Newtonian gravity. However, B does not imply A: there could instead be a number of different masses that happen to produce that force.

For your example specifically, I think it is important to flesh out the definitions. We say that real force causes acceleration inversely proportional to mass. Fictitious force, on the other hand, is caused by acceleration on a frame (which is caused by a real force or another fictitious force). So, if for that example we have a real force on an object and an acceleration in that object simultaneously, the real force implies an acceleration, but the acceleration does not imply a real force: it implies either a real force or a fictitious force. If instead we have a fictitious force on an object and an acceleration on that object's frame simultaneously, the fictitious force does not imply an acceleration on that specific frame (it could be any frame that happens to contain the object), while the acceleration on the frame does imply a fictitious force on the object. This may seem a bit picky, but pickyness can make all the difference when you're talking about simultaneous events.

I think this is the crucial difference between cause and effect: not temporal separation, but material implication. After all, that's why it's called "cause" and "effect", not "before" and "after".
Metaphysics might feasibly be presented as incorporating a formal study of essence and accident. However, metaphysics is much broader than just asking about essences. I guess what I'm trying to express is that arguments and theories involving essences should probably be classified within the category of metaphysics (though of course these questions do not exhaust its range of inquiry.)

In passing, a good starting point on the question of essence, at least in ancient philosophy, might be to look at Aristotle's Metaphysics; in particular, note that what in the standard translation is rendered as essence is in fact the curious phrase http://plato.stanford.edu/entries/aristotle-metaphysics/#SubEss "what it was to be".
Axioms by definition cannot be logically defended: logic is the process of applying reasoning to a set of postulates.  So, in mathematics, Euclid took as axiomatic that parallel lines never meet: that cannot be mathematically proved or disproved, but without it geometry cannot get off the ground. Other geometers took as axiomatic that parallel lines meet (e.g exactly twice, as on the surface of a sphere), and came up with workable, non-Euclidean geometries.

Ethics is similar: you take what you consider to be ethical imperatives, and deduce what actions are moral.  Ethical imperatives include "act for the greatest good of the greatest number", "act in accordance with the wishes of the Creator [defined in some way]" , and "act so as to ensure the survival and growth of the human species". All of these are valid bases for systems of morality (you may even recognize them), but intellectual honesty only comes into it when you claim to believe in one imperative, but dislike or refuse to accept that this action is the logical consequence.  

And yes, in a sense this is entirely subjective; but it is also subjective that you choose to speak English (not Latin or your own invented language), and say that 2+2=4, not 17. 
Since we can never think outside of our own minds, anything we ever think or perceive will inherently be subjective. "Objectivity" is and has only ever been just a form of collective subjectivity. The "way things truly are" may very well be vastly different than our human minds and senses can possibly perceive or comprehend. You should note, however, that Kant did think we could discern some objective truths about the nature of the universe, but only a very limited few (the Pure Categories of understanding, the reality of space/time/causation, etc). He is often seen as a bridge between rationalism and empiricism in this regard.

Update: I wanted to add to my answer to also provide another refutation to the main concern that you seem to be having regarding existence as a predicate. You write:


  To be is to be something instead of another thing. i.e. To be is to have attributes and characteristics. Noumena is that which is not accessible to observation. Kant claims to know that attribute about Noumena. If it is not accessible to observation how can he claim the attribute that it is not accessible to observation?


This is not actually true. The way you word it in this particular passage it's not entirely clear, but based on your other responses to me in more than one question on this site, it seems that you are insisting that existence is an attribute or characteristic, when in fact is it not. If I am to say that ice cream exists, I am not adding anything more to the concept of ice cream; it is not a "characteristic" of the ice cream. It is only a relation of the object (ice cream) to the subject (me).

See "Existence" http://en.wikipedia.org/wiki/Existence http://en.wikipedia.org/wiki/Existence , although you will probably find more thorough literature on it under the arguments against the Ontological argument for the existence of God.  For example:
http://www.existence-of-god.com/ontological-objections.html http://www.existence-of-god.com/ontological-objections.html (section labelled "Kant’s Objection to the Ontological Argument: Existence is not a Predicate")
http://en.wikipedia.org/wiki/Ontological_argument#Kant:_existence_is_not_a_predicate http://en.wikipedia.org/wiki/Ontological_argument#Kant:_existence_is_not_a_predicate
A definition (in the mathematical context) is simply the granting of a new name.  When we define pi to be the ratio of the circumference to the diameter of a circle, we are not making a claim of any kind; we're simply agreeing to use a given greek letter to substitute for a given notion.  

A proposition, on the other hand, attempts to add new information (by predication, etc.)

EDIT: 

Since the question was updated to mention Kant's analytic/synthetic distinction, I thought I'd update my answer as well.

Note that I said above "(in the mathematical context)"-- in mathematics, it is customary to draw a distinction between definitions and propositions.  This distinction is not maintained the same way in Kantian (and post-Kantian) metaphysics; there, as you point out, a definition is a type of analytic proposition.

Certain words are "terms of art" in more than one domain-- it is important to take these in the appropriate context, and avoid slippages between them.
First of all, complex numbers (and imaginary numbers) do appear in real-world phenomena; they have lots of practical applications.

But now, on to the philosophical portion of the problem.  

Numbers are abstractions.  They don't exist in the same way that, say, physical objects exist.  You can give me two apples, but you can't just give me a two.

As abstractions, they follow certain conceptual patterns.  For positive integers, these abstractions are fairly intuitive; for other types of numbers (such as negative numbers, or rational numbers, or irrational numbers) they are less so.

Your question mentions the square root of -1, but let's take the square root of 2.  Does that number "exist in the world" in any meaningful way?  Can you give me the square root of 2 apples?  

Fortunately, this doesn't hamper our ability to use the square root of 2; from a philosophical perspective, we can do this by adopting a position known as http://plato.stanford.edu/entries/fictionalism/ Fictionalism-- in short, we can treat numbers a fictional objects, and substituted them into formulas without making any ontological commitments as to their existence.  As long as the substitution satisfies the constraints (which is to say, in a broader context, is adequate to the phenomena) we're golden.

So, to answer your question: we don't have to rationalize that complex numbers exist. It doesn't matter if they exist or not.

EDIT: I found an SEP article that specifically treats of http://plato.stanford.edu/entries/fictionalism-mathematics/ Fictionalism in Mathematics; it is a nice reference for the more specific case.
Congratulations, you've found one of the major problems with Anselm's Ontological argument.

We can easily say "Imagine a perfect circle", as we have a clear notion of what the essential properties of a circle are, and can recognize the actual circles we come across daily as approximations of some imputed ideal.

If, on the other hand, we say "Imagine a maximally powerful being", we run into trouble, as "power" is a diffuse concept, and our daily interactions with powerful beings tend to diverge, not converge towards some ideal.  Thus the conundrum of whether an omnipotent being can create a rock so large he cannot lift it.

If we then extend this to "imagine a perfect being", we're lost.  We can't even begin to list all of the perfections such a being would have, much less be able to coherently conceive of the interactions between them.

I don't know of anywhere this idea (of imagining a perfect being) comes up outside of Anselm's argument, and I don't know of anybody who takes Anselm's argument seriously.
Let me preface this by saying that I am not at all a Spinoza scholar, and stand to be corrected on this.

My reading is that existence is certainly a power, so that the first Axiom means (or implies) that all existent things either exist by virtue of themselves (i.e., are self-caused primitives) or by virtue of another (where "exist by virtue of" can be read as "draw their power of existence from").

This is a fairly standard place for philosophical texts to begin; about 1500 years before Spinoza, and a continent away, Nāgārjuna began his Mūlamadhyamakakārikā with the verse: "Neither from itself, nor from another, nor from both, nor without a cause, does anything whatever, anywhere arise."  Among the views he was rejecting in that list is Spinoza's axiom, which was already considered classical in 2nd century India.
There are a number of different opinions on that; a brief overview can be found http://en.wikipedia.org/wiki/Philosophy_of_mathematics here.
The whole idea of logical validity is to divorce form from content; I think what the poster was highlighting is that in formal logic, context is irrelevant. If P, then P; this is true irregardless of context. Since that is true, then "If P, then P or Q" must also be logically true, because we already know P is true, thus it doesn't matter if Q is or not. 

What you seem to be getting at, however, is the notion known as http://en.wikipedia.org/wiki/Contextualism philosophical contextualism.


  Contextualism describes a collection of views in philosophy which
  emphasize the context in which an action, utterance, or expression
  occurs, and argues that, in some important respect, the action,
  utterance, or expression can only be understood relative to that
  context. Contextualist views hold that philosophically controversial
  concepts, such as "meaning P," "knowing that P," "having a reason to
  A," and possibly even "being true" or "being right" only have meaning
  relative to a specified context. Some philosophers hold that
  context-dependence may lead to relativism; nevertheless, contextualist
  views are increasingly popular within philosophy.

"http://plato.stanford.edu/entries/closure-epistemic/ Epistemic closure" is a term used in epistemology. An agent satisfies closure when she satisfies the following conditional:


If the agent knows P and knows that P implies Q then the agent knows Q.


Here's an example of an agent failing to satisfy closure:


Sally knows that it is Tuesday. She also knows that "If it's Tuesday, then it isn't the weekend". However, Sally fails to know it is not the weekend.


It seems a pretty obvious principle in some sense. But there are two reasons to deny it.

First, epistemic closure is an important part of http://plato.stanford.edu/entries/brain-vat/ sceptical arguments.

Second, satisfying epistemic closure means knowing all the logical truths: knowing all the truths of mathematics. So it is clearly too strong a principle in general.

More generally, "closure" in this sense means something like a kind of "completeness". So in logic a set of sentences is "closed under entailment" if the following conditional holds:


If P is in the set and P implies Q then Q is in the set.


In mathematics one sees people talking about sets being "closed under an operation". So a set of numbers is "closed under addition" if a+b is in the set whenever a and b are.
They are both simple variations on http://en.wikipedia.org/wiki/Modus_ponens Modus Ponens, and I don't think that anybody questions that the inferences are valid; what is at stake is whether or not they are sound.

EDIT:

The general consensus is that the arguments are not sound; there is no particular reason to believe that "necessary existence is a perfection", or that "perfections" are even an appropriate concept here: does it make sense to conceive of a perfectly large (i.e., largest) integer?
Generally speaking, there is more of a burden to supply a reason for doing (or believing) something than not doing (or believing) something, but this is not always the case; for example, it is generally accepted that I would need a very good reason for not believing that the cup I see in front of me is actually there.  
You seem to be puzzled by the fact that Newtonian mechanics was retained in a successive theory and that this somehow precludes incommensurability. It seems that you would think of incommensurable theories as "not relating to each other" somehow and that retaining a former theory is incompatible with this requirement.

If this is your view, the point to be clarified here is that the possibility of including a former theory within a successive theory does not preclude incommensurability between those theories. 

Both Kuhn and Feyerabend - who put forward the concept of incommensurability between scientific paradigms (Kuhn) viz. universal theories (Feyerabend) - were trained physicists, so they knew very well that you can derive Newtonian mechanics as a special case of STR.

So, what was Kuhn's point? He writes:


  … the physical referents of these Einsteinian concepts are by no means identical with those of the Newtonian concepts that bear the same name. (Newtonian mass is conserved; Einsteinian is convertible with energy. Only at low relative velocities may the two be measured in the same way, and even then they must not be conceived to be the same.) (SSR, p. 102)


His point is that key concepts in both theories, while retaining the same name - like "mass" - have not only different meaning, but that the meanings of Newtonian mass and Einsteinian mass exclude each other. According to Kuhn, there is no concept of mass, in this case, which can consistently unite both meanings - thus they are "incommensurable" concepts, i.e. concepts "without a common measure". (How this gets us to incommensurable theories and paradigms is a lot more complicated, but you get the picture.)

Kuhn uses this point against what is known today as convergent realism, the view that science shows improving approximation to the truth. Why? Because, according to Kuhn, the concept of mass as devised by Einstein does not extend, but replaces, the concept of Newtonian mass. So, if we want to understand in which way science progresses through these shifts, Kuhn argues, the model of successive extension or piecemeal revision of a concept is not a good candidate.

Did Kuhn argue from this - as it is sometimes assumed - that one couldn't derive Newtonian mechanics as a special case of STR? Certainly not. He argued, however, that the special case derived is not actually Newtonian mechanics, but a substitute of it, a numerical approximation within STR. This qualification is of no interest to the working physicist, but it is of interest to someone - philosopher or physicist alike - who wants to argue that the progress of science consists in (old-style) convergent realism.
A statement being uprovable is very different from it being an inconsistency of the theory. 

If a statement is shown to lead to an inconsistency, then you can use it to prove statements and their converse statement, which cast a bad shadow on all statements in the theory. Even then, you don't want to drop the theory but examine what led to the bad apple statement being provable, and maybe delete the bad axioms. 

If a statement is unprovable in some theory, but a lot of other statements are provable, then some people might think "that sucks", but this doesn't imply that there will be an incosistency. Otherwise we'd know arithmetic will be inconsistent. So arrival at this point doesn't imply at all you should stop caring about that model. 


  Do Godel's incompleteness theorems support the idea that the examination of a 'system' should only be undertaken to arrive at the inconsistency?


The theory of differential equations (with all its facets and connections to other topics such as topology and pure existence questions) is one which clearly incorporates arithmetic and also much bigger mathematical problems inherited from its set theoretical basis. There is still a lot to find out about differential equations and the solutions are needed in physics to build your iPod. Should the 'system' only be examined to find inconsistencies in it? Not if you want a new iPod! Why should "examination of a 'system' ... only be undertaken to arrive at the inconsistency"? Mathematics is not done for mathematics sake, by the majority of people who do it.

On that note, I find it problematic that you ask for puropose of a human activity here. The consequence of the discovery of an inconsitency is just another statement about the particular model or similar ones. "Invalid" as commonly used is a strange word in this context (not to be confused with the technical term valid in logic). Because there are several similar theories, with which you do arithmetic and arithmetic like operations. If something in a 'system' is true, say the statement "2+3=5", and the theory as a whole turns out to be invalid, I'd still use "2+3=5". Similarly, something being wrong in a particular logic or theory doesn't imply something about the validity of "the same" statement in other theories, where "validity" here really only has operational meaning. 
I think the answer you are looking for might be this: There is no syntactic way to identify theorems across different formal systems. If, that is, you look only at the symbols that comprise the theorem then the only "content" is their relation to the axioms of your formal system via the deductive system that allows you to prove it (the theorem) from them (the axioms). And this "content", being dependent upon the formal system you are considering, must differ across formal systems.

On the other hand there certainly are semantic ways to identify theorems across different systems. Of course here you are not merely identifying a proposition P (NB: it now has a truth value so it is a proposition) in formal system m with one in formal system n. What you are doing is identifying P in the model M of m with P' in the model N of n. And here, as Begoner's answer suggests, given the right models, you can of course identify P and P' because you can see that you are talking about the same 'stuff', e.g. natural numbers and natural numbers within ZFC. In short, semantic identification is dependent not simply on your formal system but also on the models you choose - and you can therefore choose models in which your propositions can be identified.

(I have used "identification" informally throughout this answer.) 
Objectivity can be considered as some sort of "pure information", in the sense that different people would have the same interpretation of it and would apply it in the same way.
Information is rather different than communication, communication cannot be reduced to it. For a more extense debate on this subject, you may find this interesting:
https://linguistics.stackexchange.com/questions/2144/which-languages-are-used-for-purposes-other-than-facilitating-communication https://linguistics.stackexchange.com/questions/2144/which-languages-are-used-for-purposes-other-than-facilitating-communication

Subjectivity implies that there are some sorts os "holes" in speech (missing information), which will be completed by each person with their own values.
Objectivity means that there's nothing missing, all elements needed for interpretation are provided. Maybe total objectivity is not possible, as we cannot erase ourselves completely in the linguistic process of elaborating knowledge.

If I say to 2 persons "Go to the garden and pick up a beautiful flower", unless you are a philosopher meaning to discuss with me the nature of what is beauty, you'll go to the garden and pick up whichever flower appeals to you as beautiful. The flower you picked may be very different from the flower the other person picked.

If I say to you "Go the the garden and pick up a red flower with 5 petals", this is much more objective; considering you know what "flower", "red", "petal" and "5" means in general, the flower you pick possibly will be very similar to the one the other person would pick. As you see.. objectivity implies specification, and the more objective I want to be, the more elements I need to provide, to make sure that you and I have the same references and values. Objectivity requires a common background, and making sure that mine is the same as yours, or at least, the closest to being the same; we must share a vocabulary and values for it to work.

Not only do I have to say what I think/want, I need to specify what it is that I mean with those terms. Usually we are unaware of how tricky this can be, even for simple things like "red" ("oh.. so this is red to you? I consider it to be orange"). The more we have a common ground of values, more likely it is that we will be able to share information.

I consider objectivity and subjectivity to be qualities in a relationship within a linguistic set. In this sense, there is no way a person can be objective in itself, whithout considering how it would be perceived by another.
It was an old anecdote "It depends on a definition what is is", but it seems to answer your question.

There are several formal meanings to is:


A is B stands for "any object x that belongs to a category A, also always belongs to a category B". In this case, A is a subset of B. One may say, any odd number is a number, and it's valid. Obviously, the opposite is not valid: any number is odd number;
A is B stands for "objects A and B are equal". Likewise "My Cat and Cat Living In My House are the same terms". Equality is symmetric;
Similar to above, but A is B applies to categories: if category A equals to category B, that means that any object x belonging to A also belongs to B and, vice versa, any y belonging to B, also must belong to A.


So, answering your question, variant (1) perfectly allows A is B and B is not A
The answer to this will depend on the metaphysical commitments you have. It is hard to see how a physicalist, or signatory to any form of metaphysical monism, i.e. to the belief that reality at root consists of one kind of stuff, can find room for numbers to be revealed to us through experience without being experienced themselves.

If everything is reducible to physics, then so is the number we count to when we see two apples; if this is physical in nature, then we experience this just as we experience their redness or acidic taste. Someone with this belief might further want to say that numbers are not things that exist apart from the physical entities that they bear on, rather that they supervene on top of these 'bona fide' physical things. The twoness of two apples is just a relation that obtains between these physical things, rather than something separately existing.

Just some thoughts to get started with.
It strongly depends what you mean by 'logical', but from philosophical point of view the most important is the ability to express logical topics, to speak logical etc.

The modern linguistic has refused the theory, that some languages are 'primitive' and it is not impossible to express some ideas in them. I've found this statement in very many scientific books about Africa, where in introduction about the African languages, there was a statement, that linguistics haven't found any example of modern human language, in which some ideas couldn't be expressed. Of course, languages have various vocabulary and isolated languages spoken by a few people tends to have limited vocabulary, but even among English speakers, most of them have limited vocabulary as single people, only the language as the whole has so wide vocabulary. 

But note how little words in English are native English words. Most of them are from Latin, Greek and Latin languages. The same process can be applied to any other of human languages. So, if the whole philosophical library can be translated into any language, none of them can be considered less applicable for discussing logic issues as well. Therefore, no language is more logical than the other.
A tautology is a conclusion which requires no hypotheses. Then, in particular, it doesn't have any hypotheses which are false. Therefore, as we say in the business, all of its hypotheses (vacuously) are true. Then the tautology is a true conclusion.

You should perhaps think of logical validity not as "truth-preserving", but more accurately as "not increasing falsity". (This depends on being rather staunchly Boolean in one's view of logic, so that you would for instance regard 0 = 1 to be no larger a falsehood than 0 = 0.0001; but it is certainly not an extremely controversial view.) Then, if you have no premisses, you have no falsehood, and anything you can derive from no premisses can therefore contain no falsehood, i.e. is necessarily true.
Your understanding of transcendental deduction (TD) is much broader than how Kant actually employs the term in his Critique of Pure Reason (CPR). TD is a much smaller step in Kant's outline of his transcendental philosophy. 

Briefly, in http://en.wikipedia.org/wiki/Critique_of_Pure_Reason#The_Transcendental_Deduction The Transcendental Deduction Kant wants to show that the "pure concepts of the understanding" (aka categories), which he derived from the logical forms of judgement in the previous chapter (http://en.wikipedia.org/wiki/Critique_of_Pure_Reason#The_Metaphysical_Deduction The Metaphysical Deduction), actually apply to our experience. 

The argument in TD runs as follows: 


A unified sensory experience can only possibly be accounted for if the categories apply to that sensory experience. 
[Insert complex/puzzling justification of claim (1) here…] 
Since we do perceive that kind of unified experience, the categories do in fact apply to our sensory experience.


With Hume against Hume

In order to understand Kant's philosophical move, TD must be set against Hume's empiricist psychological theory. Hume denied that apriori concepts could correctly apply to objects featured in experience, because only concepts (Hume: ideas) deriving from sensory experiences (Hume: impressions) can actually apply to the objects in our experience. Apriori concepts lack such a connection and therefore, according to Hume, fail to apply to the objects of experience - they lack objective validity.

Against this position, Kant employes TD to prove that certain apriori concepts do correctly apply to objects featured in our experience, i.e. that such apriori concepts have objective validity after all. In doing so, Kant doesn't counter Hume's point, he actually concedes that apriori concepts cannot be derived from sensory experiences. He does, however, object to Hume's conclusion that a priori concepts therefore cannot apply to the objects of experience at all. To block this conclusion, Kant needs to show that there's another, non-empirical way in which apriori concepts can correctly apply to experience and achieve objective validity. This alternative way is through a http://plato.stanford.edu/entries/transcendental-arguments transcendental argument, see the outline above.

This is why Kant employes a deduction. Kant's "deduction" is not used in the modern sense, but in a way used in German legal language at the time. Here Deduktion meant a proof of the quid iuris (the matter of the law) in opposition to the quid facti (the matter of the fact). As Kant's argument does not involve the rule of law in a literal sense, the analogy that he tries to convey is about a legitimate source of justification not concerned with establishing matters of fact.

Please note

Because you write that


  the role of cognition and sense is to verify whether these categories apply to objects


it is important to stress that this is exactly not Kant's point. The opposite is the case: TD itself is the verification! – The "proof" that his categories apply to the objects of experience is exactly what Kant tries to achieve through TD.

Kant's argument is not about cognition in any "naturalized" sense of the term. Kant is not interested in asserting an empirical hypothesis that might be somehow verified by empirical means (that would have been an "empirical deduction", as Kant calls it, which Hume already showed to fail for apriori concepts).



To get an overview of the actual argument deployed in TD and also get a better grasp on how TD fits into Kant's broader project, you may read  http://hume.ucdavis.edu/mattey/phi175/trdedlechead.html a very good summary from a philosophy course at UC Davis.
What do you mean by "infinitely powerful", and what does it mean to be infinitely powerful in an intelligible universe?

Suppose you mean an omnipotent being. Is it possible to have two omnipotent beings? As I've noted elsewhere (see my remarks https://philosophy.stackexchange.com/a/4452/757 on the notion of 'unmakability'), this depends on what you intend "omnipotent" to mean — what the range of possible powers there is in the world.

For instance, an omnipotent being could presumably destroy any object, including another omnipotent being. Is this a contradiction? Not really, though it depends on the details. If the omnipotent beings both have the power to make each other cease to exist instantaneously without reaction, then it would simply happen as soon as one (or both) of them willed it. But can't either of them make themselves impervious? That depends on whether there's any particular mechanism to destroying objects — resistance against destruction only means that it can prevent that which brings about destruction from succeeding. So how one of the beings could possibly resist destruction now requires us to consider that there be a mechanism for the destruction: that is, that the world is one of mechanical principles, and the omnipotence of these beings consists in unfettered access to the resources (such as energy) by which the mechanism operates. Pretty much any impasse that can arise from two omnipotent beings can be characterized in this way: if one wants to make X happen and the other wants to prevent X from happening, then we must consider the mechanism by which X is possible — or posit a world in which the notion of "something happening" is much more complicated and nebulous than it is in our world.

Let us continue to consider the possibility of being A destroying being B. With normal physical objects, to see if one would be able to destroy (for instance, penetrate into) another, we would have to consider a balancing of forces: to see which force is greater, and consider the net effect, which consists of the difference between one force and another. With this sort of situation, involving what amounts to an immovable object and an unstoppable object, the corresponding difference of forces would be of the form ∞ − ∞ , which is a classic indeterminate form. When this occurs in mathematics or in physics, the message that one obtains is either that not enough information has been provided to obtain an answer, or there is no well-defined answer. 


What extra information could we be missing? Well, to be omnipotent means that the being could presumably marshal an unbounded amount of power at any given point in time — but that does not mean that it always does marshal an infinite amount of power. If one of A or B does exert more power than the other — either to destroy or to preserve itself — then it will prevail, if only because the other being is holding itself back. This is concievably true even if both of the beings are using an infinite amount of power: depending on how we model the infinite amounts of energy they have at their disposal (either by http://en.wikipedia.org/wiki/Asymptotic_analysis asymptotic analysis or by http://en.wikipedia.org/wiki/Transfinite_number transfinite cardinals, to give just two examples of mathematical models of the infinite), both A and B could still use an infinite amount of power, in such a way that one is using a more infinite amount of power than the other.
If both A and B are "omnipotent", this presumably means that neither is more powerful than the other. So such restraint as I describe here is only possible if either A or B deliberately, or through its particular mood, expends less energy in its destructive or defensive efforts.
If the two beings A and B expend exactly the same energy as one another, then again what happens depends on whether they are actually using an infinite amount of power at that moment, on the initial conditions of the mechanism of destruction, and (if they are using an infinite amount of power) again on the model of infinity that we consider. Suppose, for the sake of definiteness, that A and B have taken human form momentarily at a whim. To destroy B, the being A must choose some particular mechanism: for instance, perhaps A chooses to fire an fast and hard bullet at B. To defend themself, B has to use some mechanism to stop the bullet: for instance, erecting a  strong barrier between them and the bullet. (We ignore the fact that these conditions violate special relativity and known material science.) What happens?


If the bullet and wall have the same finite strength, and are fired/erected with the same finite speed, it depends on whether or not B erects the wall in time to prevent the bullet from passing completely through the wall. If not, the bullet makes it through, and without further action B dies; otherwise the bullet is blocked.
If the bullet and wall have the same infinite strength, and are fired/erected with the same instantaneous speed — but the power used by the two beings can somehow be described in terms such that (despite being infinite) they cancel to give a definite positive or negative outcome — then the same observation applies: that definite value implies either than A succeeds, or that B succeeds.
Suppose that we require that the puzzle take the classic form of the unstoppable force and the unmovable object: that we require both of the infinite efforts of A and B to be absolute and unreconcilable, in order to force a paradox. In this case, there can be no outcome: by the very fact that the two forces cannot be evaluated relative to one another, to see whether either A or B prevails, there is simply no outcome. What does this mean? Essentially the end of time; reality ceases to exist, because there can be no events afterwards to determine whether A or B prevail. At the very least, everything which would ever possibly interact with the future causal cone of the battle between A and B must cease to exist. — But can't we consider the possibility that A and B co-operate to make sure that the universe continues to exist, while trying to destroy one another? That comes down again to what mechanism there could be to prevent the universe from ceasing to exist when they are doing something which would cause the universe to cease to exist. Again it comes down to how these efforts cancel with one another; and if there is no way to reconcile the efforts, then there is no outcome and no future.



In the above, I have made one key assumption: that whatever powers we assume that these omnipotent beings have, that these powers are somehow intelligible. In essence, that despite the existence of these beings, that the universe is a world of laws, and that the power of these beings to do what they please stems from complete mastery within those laws. I see no way to answer this question  — or indeed any other question whatsoever — if we assume that the universe is unintelligible even in principle; so I think this assumption is quite reasonable if we're going to entertain the question at all. But an important consequence of this assumption is that the question attains a definiteness which is not typical when such problems have been considered historically, and how precisely we choose to make the question definite enough to be answerable — what specific conditions we suppose in terms of how A and B act — will change the nature of the answer.
What is modal logic?

Modal logic is an extension of classic propositional and predicate logic that allows the use of modal operators. In others words, modal logic is everything classic logic is + modal operators. Modal operators express modality, such as:


Necessity (denoted by □)
Possibility (denoted by ◇)


The above possibilities are the only operators used in modal logic in the narrow sense. However, the term modal logic is often used to include other extensions, for instance temporal logic, that allows for the expression of past or future truths. 

Why is modal logic useful?

Classic logic is great for mathematics, but for the analysis of daily language and arguments, it lacks certain operators. There are many sentences that you can't express in classic logic that can be expressed in modal logic. 
Example: "I may get burned if I lie in the sun for too long". In classic logic, you can say: "I get burned if I lie in the sun for too long", but you can't express the possibility of getting burned. In classic logic, it's either true or false. In modal logic, you can also express the possibility or impossibility of a proposition being true or false.

Further reading: 


http://plato.stanford.edu/entries/logic-modal/ SEP
http://mally.stanford.edu/notes.pdf Lecture notes of a Stanford course on modal logic
http://www.youtube.com/watch?v=vTE29cFurYw Youtube course on modal logic

The difference is simple, non-existent objects don't exist whereas abstract objects might.

For example, round squares are non-existent because they are self-contradictory. The golden mountain is non-existent because it is not actual. Fictional entities are non-existent because fiction isn't literal truth.

Non-existent objects are not (at least, needn't be) abstract. The golden mountain really is made of gold, it is concrete and not abstract--- it just doesn't exist. Likewise, there is a detective who really lived at 221b Baker street, he just didn't exist.

Meinongianism and the theory of non-existent objects has not gotten wide acceptance. Most philosophers who advance some sort of Meinongianism these days try to accomplish the same as the distinction between exist/not-exist with some other distinction(s) like abstract/concrete or actual/possible/impossible.

See http://plato.stanford.edu/entries/nonexistent-objects/#ProDisAboPasFut this article, especially section 5.4 for more discussion of this.
The Stoics were continuum physicists of a sort. They believed that matter was infinitely divisible and never bottomed out in atoms.

Daniel Nolan has a good paper on http://www.jstor.org/discover/10.2307/4182801?uid=3739696&uid=2129&uid=2&uid=70&uid=4&uid=3739256&sid=21101660280133 Stoic Gunk.

You can also read about Stoic Physics http://plato.stanford.edu/entries/stoicism/#Phys here.

I'm afraid that I don't know of ancient physicists that explicitly talked of waves, but I'm not too knowledgeable in the area.
This varies depending on which logical system you're working in.

If the system is both sound and complete you can move between semantic "proofs" (I prefer the term "verifications") and syntactic proofs freely (since we know there is a perfect match between syntactic and semantic validity).

When systems lack soundness or completeness (generally completeness, people are usually reluctant to put forward proof systems whose acceptable inferences don't preserve truth) you won't be able to assume this perfect match.

Some systems have both sorts of proofs in a single style of representation. For instance, in http://www.cambridge.org/gb/knowledge/isbn/item1162310/?site_locale=en_GB An Introduction To Non-Classical Logic, Graham Priest gives a tableaux system of proof where a single proof-tree/truth-tree is both a semantic and a syntactic proof of the proposition in question. A downside of this is that you have to accept that you'll have some proofs which branch infinitely (so, you don't have a decidable proof system). This can generally be recognized because you'll just have to keep reapplying a rule that creates a new branch, and you can see that you'll just get stuck in this sort of "infinite loop" but that none of these branches will serve to "close" the tree (i.e., derive a contradiction on that branch). It is hell on automated theorem provers, though.

So this brings me to the final answer to your question "surely a proof should have some semantic status too?". Yes, generally people think that it should. This is why soundness and completeness are desirable properties for a system to have (though there are trade-offs to be made, as evidenced by the fact that most of the more interesting mathematical systems will be incomplete). But ultimately these are meta-theoretic properties and proofs are carried out in the meta-language. I think we keep these notions (syntactic and semantic validity) separate so we can modify our system along two dimensions to allow for theories where the syntactically and semantically valid propositions are not the same.
(This is more in the nature of a comment to your question, than an answer; but its a little too long to be one).

a. To complicate your reading of standard & non-standard models, one could consider the real line. This seems eminently like a natural object; however it has no natural notion of infinitesimals. This makes it clumsy for the purposes of calculus & analysis. One can introduce a non-standard real line that actually does have them, and this was done originally by utilising http://en.wikipedia.org/wiki/Model-theoretic_approach non-standard models by Robinson. 

So here we have a non-standard model to introduce natural features to the real line. Certainly it seems that which model is standard depends on epistemic, pragmatic or aesthetic reasons.

b. That there can be standard & non-standard models for some theory is indisputible, but can we be more precise about this? We can, there is the property of http://en.wikipedia.org/wiki/Model_theory#Categoricity categoricity in Model Theory. If it is satisfied it simply means that even when there are more than one model of theory, they are in fact all isomorphic - so in a sense we have only one model. For a first-order theory this is fact not possible. But when we consider this along with the size of the model more can be said. It turns out (when the language of the theory is countable that) all models of each cardinality are isomorphic.

Then we can see at least that the standard model occupies a privileged place - it is at the beginning of this transfinite count, this ladder of models.

The other interesting thing that can be done is to turn this ladder into a line by topologising it. This makes it look a lot like the real line (except of course its still very different), and then the standard model is at the beginning of a line of models.

edit

Badiou explores the metaphysics of models in his concept of the model. I think he sees a model as representing a mathematical concept. I'm not able to say much more than that.
There certainly has been a large amount of development of modal logic interpretations of Quantum Mechanics.  Once you have a Kripkean accessibility relationship (induced from nonorthogonality), the development is simple.  Early work here is Goldblatt - "Semantic analysis of orthologic" Journal of Philosophical Logic (1974) and "The Stone space of an ortholattice" Bulletin of the London Mathematical Society (1975), and Dalla Chiara - "Quantum logic and physical modalities" Journal of Philosophical Logic (1977) and "Physical implications in a Kripkean semantical approach to physical theories" Logic in the 20th Century (1983).  The basic idea to obtain the accessibility relations uses work from Foulis and Randall on lexicographic orthogonality, as it's not so simple to avoid what would become "extended probabilities" in a counterfactual view of separated events to build the modal relationship.

It is important to make some distinctions with the work described on the page you linked.  That research programme is linked with modality in the view of possibilities as a means of regaining realist foundations.  That's an involved program that delves into operationalist interpretations and can indeed be used to build modal operators as one is familiar with in modal logic.  In fact, there are S4 interpretations on the surface of most operationalist approaches.  However, that is not quite the same things as the modal operators of the quantum events themselves, based on the standard orthomodular logical foundation.  My first paragraph deals with the latter program.
I can only speculate that it is because we typically view causation as a finite process (in which case there must be a first one)--but this begs the question--or that when doing induction we normally use the natural numbers not the integers:

Prove P for case 0
Prove that P(n) => P(n+1)
Therefore P is true for all natural numbers


instead of

Prove P for case m
Prove that P(n) => P(n+1)
Prove that P(n) => P(n-1)
Therefore P is true for all integers


which is equally valid.  (You can also get it with P(n) <=> P(n+1).)

It's also slightly distasteful to be left without an ultimate reason because it's reasons all the way down.

Disquieting though it may be, it's not logically invalid.
I think there's an even simpler reason: it's hard to preserve truth when all you start out with are falsehoods!

I take it that you've been studying axiomatic systems of logic (I'll restrict my focus to propositional logic). Typically these systems will have a number of axioms (which will be tautologies) and an inference rule or two--- typically something like modus ponens.

The concept of a valid inference is one that preserves designated values. In classical propositional logic the designated value is "truth", and so a valid inference is one that preserves truth. How do you prove this? Well you start off by assuming you have true propositions to reason from and then show that repeated applications of modus ponens will never take you from a truth to a falsity.

If you start with only contradictions then you will never get from a truth to a falsity, but only because all of your "axioms" were false to begin with! Such a logic might make falsity its designated value and gerrymander a notion of valid inference where a valid inference is one that preserves falsehood. But the standard notion of validity will do no work in the setting you've described (assuming I understand correctly that you want to replace all tautological axioms with contradictory ones).

Also, depending on what inference rules you have, your system could get messy very quickly. If you still allow ex falso quodlibet (literally "out of a falsehood anything (you) please", sometimes called the rule of "explosion": from a contradiction anything follows) style inferences then since you have let in contradictions as axioms you will have every single sentence expressible in your language as a theorem of your system--- surely an undesired result.

So, why don't we base our logic on contradictions? Because contradictions are bad!

(NOTE: Paraconsistent logics like Graham Priest's "Logic of Paradox" typically invalidate ex falso quodlibet. In Priest's case it is because he is a "dialetheist" and believes there are true contradictions. Removing the explosion rule is a way to handle "true contradictions" (which, itself, reads as a contradiction to me) without having them "infect" your entire system and trivialize inference.)
Formal ethics comprises more than just deontic logic. Deontic logic is primarily concerned with a logical analysis of notions such as 'ought', 'must', 'may', or 'being permitted', whereas formal ethics comprises all ethical and moral reasoning that makes use of formal methods. 

Apart from deontic logic, formal ethics is also concerned with the modelling of formal systems of norms and rules (which are usually conflicting with each other, sometimes also defeasible and context-dependent), the formal modelling of the permissiveness of moral rules as 'soft constraints', dealing with vagueness of norms, laws, and moral rules, game-theoretical explanations of moral norms, formal models of distributional justice, the formal modelling of values including value incommensurability, moral issues of decision making under uncertainty (e.g. the Precautionary Principle), problems of value and preference aggregation, and utilitarian ethics in general. 

Some of these topics are also addressed in AI research, formal epistemology, game theory, and economics. Research in this area is fairly interdisciplinary.
While there is some confusion at work in how the question is asked, I think there's a good question in there somewhere. I first want to suggest the following opening version of the question: What is similar in Plato and Kant's respective accounts of where the truth is found?

I take on a basic level their main point of agreement is that truth is not out there in the world. Thus, for Kant, our apparatuses -- the categories of the understanding and the forms of sensibility are more important things to look at than things to discover truth. For Plato, truth is the Forms/Ideas. Again, these are not out there in the world. Instead, what we have in the world are Demiurge's poorly made copies.

With this rough sketch in mind, we can then think about what is different in the two views. In the Kantian view, the things themselves are there but we cannot access them as such. We turn them into objects and perceptions as we use our minds on them. So the categories apply concepts and the forms of sensibility render them in space/time as things we can perceive. N.b., I am using Kant's language here so if it sounds crazy don't blame me! So the important bits that make knowledge possible are in our heads.

In the Platonic account, what we have in our head are forms. What we see in the world are the shoddy copies that are, per the Cave, like shadows. Depending on where we are reading in Plato, the degree to which we remember the forms/know the forms differs. So in Apology and Euthyphro, we are unable to really get to knowledge. We're stuck with our inadequacy. In Meno and several later dialogues, we know the forms in our souls (use minds if you prefer) and just need to jog our memories. In a weird way, that means for Plato we are already knowers of what is most real.

This finally enables us to look at the question as you worded it:


  did Plato ever speak about this impossibility of knowing things "as themselves"? ... whether Plato had any views about the impossibility of knowing the 'noumenon'. 


Part of your question is a misinterpretation of Kant. noumenon are not things in themselves, but we can address each piece separately. The misunderstanding happens because of a confusion about how knowledge works Kant. While we cannot know the things themselves, this is because of how we know. http://www.degruyter.com/view/j/kant.1968.59.issue-1-4/kant.1968.59.1-4.118/kant.1968.59.1-4.118.xml [God, in fact, can know them.] because God does not filter things through forms and categories.

The answer is that Plato does think we can know what Kant might call "noumenon" but that's because the closest thing is the forms which is in soul. On the other hand, the things behind the phenomenon are on Plato's view either the forms or just the shifting shadows of the world we live in which is inadequate to contain forms.

What the two have in common then is a belief that the real goods are in the soul from before birth / in the mind of a rational being as categories and that the world is shadow. The neo-Kantians of the 19th Century were also Platonists after a sort, so you're not the first to note the similarity. 
You're interpreting the term too loosely in the second part of your question. Your first definition "the connection between words and meaning of those words" is acceptable for logic if you add "described in a precise, rule-governed way". Logicians speak of terms, formulas, operators, connectives, and quantifiers rather than mere words, though.

Unfortunately, when you step away from such informal definitions and take a closer look things become less clear. I've seen the term being used in at least three slightly different senses. The first one is the general case, whereas (2) and (3) are imprecise. However, when someone talks about "the semantics" of a particular formal system, he may mean any of (1)-(3):

(1) In a first, primordial sense, "semantics" is used for "the definition of possible interpretations" and "the definition of models for a formal language". The semantics of a logical system is a set of rules for interpreting well-defined expressions of a formal language, which is in turn defined by syntax rules (a grammar). A logical semantics usually leaves it open how extralogical expressions are interpreted and focuses on a particular subset of expressions of a language called logical expressions. These are for example terms (how to deal with them in general), operators, connectives, quantifiers, and formulas. For the extralogical expressions such as particular interpretations of terms, you may then provide different models within the range circumscribed by the general semantic rules (aka interpretation, evaluation rules).

(2) In a second, narrower sense based on (1), "semantics" sometimes abbreviates denotational semantics which is also sometimes called model-theoretic semantics. This involves the explicit mapping of expressions of the language to elements in sets and rules for counting and combining these elements expressed in the language of set theory or, in the simplest case, by providing truth tables. So here "semantics" is understood as (1)+a particular method of achieving (1).

(3) In a third sense closely related to (1), "semantics" is meant in various ways that may be precisely described  as  algebraic semantics, combinatorial semantics, category-theoretic semantics, and so forth, where the methods used aren't based on set theory and are generally more syntactic in nature. ("syntactic" is meant as symbol manipulation by means of rules here.) Again, this sense of "semantics" is (1)+ = a particular method of achieving (1). These kinds of semantics are not very far from proof-theoretic methods such as using axiom systems + deduction rules, natural deduction, tableaux which are commonly considered syntactic methods. As confusing as this might be, speaking of "semantics" despite a certain nearness to proof theory may be well-justified in such cases provided that the method is provably more powerful than any calculus could be.  

The third use of "semantics" often conveys an overall proof-theoretic attitude of a logician, the view that the main business of logic (as opposed to mathematics) is to provide the meaning of expressions of logical languages by a well-behaved proof theory only. Logicians in this tradition will sometimes even speak of proof-theoretical semantics.

Things get really messy when you go beyond first-order logic to second-order logic with standard models, because second- and higher-order logics with standard models have no complete proof theory (axioms+symbol manipulation rules), and in that case which kind of semantics you choose can make a huge difference both conceptually and formally. 
So there are at least two interesting Dialetheist responses to your question.  The first is to pull apart propositional negation from negation in an Assertoric Context.  The second is to say that even propositional negation has a modal sense that classical predicate logic doesn't capture effectively.

Priest on Asserting Negations

Firstly, a couple of different ideas get caught up in the idea of talking about a proposition and "its negation".  Here's how we do things classically.  An atomic sentence of the form P(c) is true (/assigned the value T/1 /deemed assertable/ etc.) if, and only if, the object in the domain that our model interprets as being the referent of c is a member of the subset of the domain that our model interprets as being the extension of P.  If this is not the case, the sentence is not deemed true.  Now, the conditions for ¬P(c) are that it is true (...) if, and only if, P(c) is not true.  So in effect, if the referent of c is not in the extension of P, then ¬P(c).  Set theory is classical, being a member of P is a wholly extensional matter, and so it seems negation is entirely determinate.

Nonetheless, the classical view might be read as collapsing a distinction of value to the process of interpreting what negations are.  The negation function, ¬, is a regular propositional operator - you "negate" a proposition A (i.e. ¬(A) is an instruction, rather than a term), and in the process get another that constitutes "the negative" of A, ~A.  This is a process that is well defined for logical complexes, no matter what your basic logic is.  But for an atomic sentence, is it always obvious what its negation amounts to?  When you assert that something is not true, you presumably have to have some particular positive content in mind that backs this up - saying that the banana is not yellow ought to mean you have good reason to think it is some other colour that conflicts with its being yellow.

In this, Graham Priest in both his In Contradiction (1978) and Doubt Truth to be a Liar (2005) affirms that when it comes to Truth, we need to understand its propositional content as tied to the act of Asserting something.  Specifically, Truth is supposed to be the ideal form of assertion - when you say something, you do it in the aim of its being true, and that's what being true consists in. Assertion might, we think, have some kind of deontic component - what people can and/or ought to assert is subject to some conventional norms, with the above suggestion that you can back up your claims with good evidence being one possibility.

Truth is exactly the ideal of asserting, and theories of truth are thus supposed to be theories that capture these norms.  As such, the semantics of Truth are given in the norms of assertion, rather than in the ontological structure of the world.  (In Maths, for instance, the semantics of our language is determined by what you can prove, and the proofs you can construct, rather than the objects out there in the world)

This move allows for so many divergent factors in an account of what negations, negating, contradiction, assertion and truth have to do with one another.  We might think that for a number of cases, asserting a negation and not asserting come apart; we might say that the negating function in an assertoric context doesn't always correctly grasp a proposition's "actual" negation, we might say that you can have prima facie and ultima facie contradictions depending on whether we have an internal or external algebra of negation, we might say that you can assert a statement without failing to assert its negation etc.

I have good evidence to suggest that your banana is yellow.  I also have good evidence to suggest that it's actually brown (see, look at all of those splodges and bruise marks).  So, if our norms of assertion commit me to stating everything that I have good evidence to suggest, rather than hiding some of them for the sake of personal convenience or coherence with a preferred theory, I ought to really say that both of them are true, even when we might say that the banana being brown actually satisfies the assertive conditions for it not being yellow.  This doesn't mean I'm committed to asserting that the moon is flat and that pigs can fly; so I should accept that my assertions here are driven by a dialethic logic that can process some contradictions without falling into trivialism.

The problem with this line as I see it is that it deliberately seems to avoid the challenge of the hardcore realist.  Fine, assertions might be internally contradictory and this doesn't necessarily mean you're committed to asserting anything and everything.  Dialethic logic as a way of representing and processing how people talk (or their commitments to talk) has some interesting applied use.  But you only get there by talking about contrary states of affairs, that seem to have a certain amount of tension between one another (represented in assertions by the negating function).

There's nothing actually contradictory about the state of the banana: it's just both brown and yellow in certain respects.  You might say in your assertions "A & ¬A".  But when you say "¬A", your negating operator isn't returning the state of affairs that the realist will call ~A - it's hitting a proposition B and mislabeling it "~A".

This might be cute as a linguistic or behavioural theory, but it's not really logic with respect to the classification of the structures of facts.  There aren't really any true contradictions.  (I'm being deliberately harsh here for the sake of answering your question; there's merit to the thought that logic should be considered the theory of warranted psychological inference rather than ontological structure, that the semantics of logic should be captured in human and/or social language use, and that Priest's logic has a much better grasp than his classical counterparts)

Routley/Meyer Negation

To go for a more radical, metaphysical claim about the existence of true contradictions, all Priest needs to do is to say that there are determinate and ultimately dialethic principles of assertion in our basic metaphysics that correctly latch on to the world.  Although I don't think he makes such an assertion explicit (he does, for instance, defend the possibility of a foundational logic, but never specifically asserts what that foundational logic is supposed to theoretically cover) he does present arguments for dialetheism in the empirical sciences, which is good news if you're a naturalist. Not, though, if you're a classical set-theoretic structuralist. You're probably just going to accept that the rules of assertion in set theory are those of classical logic.

An alternative, direct account of negation in a semantic dialethic setting is presented by J. C. Beall in his Spandrels of Truth (2008), where he explicitly invokes ideas found in Relevance logics.  Beall borrows from Routley/Meyer's semantics, arguing that negation is centrally modal and that in order to understand the negation of a particular proposition at a possible world, we must look at some other well-specified possible world that represents opposing states of affairs.

The Routley-Meyer semantics is a generalization of Kripke frames.  Routley's Star operator takes each world in the domain to a dual world, such that w = w**.  The semantics for negation are defined in terms of this star operator:


  w ⊨ ¬A if, and only if, w* !⊨ A


The actual substance of the dual worlds aspect is not really fully fleshed out by Beall - he thinks the value of the semantics is mostly to give an account of a properly deflationary truth predicate, and interpreted semantics is the job of theory builders rather than logicians.  But he hints at possible ways of reading it, and thinks the idea of a Truthmaker is a valuable starting point.

The Truthmaker line is to say, as we did above, that there needs to be something in virtue of which a given proposition is true.  This is true of negative propositions as much as it is of positive atomic basics - it can't be the absence of a truthmaker that is responsible for a negated atomic sentence being true, unless we want to reify the notion of truth-making gaps.  Beall's suggestion is that we might read the negation operator intensionally, because we understand what it means for a negation of an atomic proposition to be true by considering the scope of possibilities in which that proposition would not be true.

In his semantics, then, the duals of worlds are related in terms of the facts that they posit.  We say that in Normal worlds, we have consistency by noting that the only things that there are are positive states of affairs.  Their dual worlds, too, are consistent, but where Normal worlds consist of fully compatible states of affairs, their duals contain richer collections of states of affairs that are in some sense incompatible with one another from our perspective in a Normal world.

A speculative reading of the construction might be this: if you take Tarski's point about the impossibility of consistently defining a theory of Truth without a stronger metatheory at its face value, and you suggest that the accessible distance between our world and a world at which a full notion of truth is definable can be realized as a modal claim, you get what Beall is trying to do.  Negation is essentially modal, to understand negation we move to our world's metatheoretically richer clone (whose own clone, apparently, is our non-enriched version), and when we do that, we wind up with some basic semantic contradictions, but nothing that overrules the consistency of truth at the level of the states of affairs we posit to constitute our actual world.

It's a bit of a hack, I reckon.  But it does seem to match with what a lot of the maths is pointing at!  If a Tarski-like axiomatic theory is how mathematical practitioners operate, and we take the essential richness of something like proper class theory to be strictly necessary for practice but ontologically undesirable, the suggestion that it exists as an extra-theoretical posit that we appeal to when considering the interpretation of our theories has definite advantages.
A short poem by Blake goes:


  Never pain to tell the love,
  
  Love that never told can be;
  
  For the gentle wind does move
  
  Silently, invisibly.
  
  I told my love, I told my love,
  
  I told her all my heart;
  
  Trembling, cold, in ghastly fears,
  
  Ah! she doth depart.
  
  Soon as she was gone from me,
  
  A traveller came by,
  
  Silently, invisibly;
  
  Oh was no deny.


There are no 'logical' connectives here. Having said this, 'logical' connectives  are not traditionally part of the vocabulary of linguists. It is part of the vocabulary of computer science - and specifically that of the design of computer languages. 

Babies learn speech on small fragments of conversation which one suspects do not have the 'logical' connectives that you mention. Surely communication is happening here.

In a much wider context, semiotics, the science of signs and signifiers, communication happens without 'logical' connectives. For example, the display of a clock. Perhaps one could imagine signage in an airport offering options so are implicitly offering the logical connective 'or' - but then could not one imagine that a brick on the floor offering implicitly a range of options - as in pick it up and put it in on a nearby table OR pick it up and chuck it at a window?  

Telling someone to F*** O** brutally conveys a message, as does hitting someone. People communicate by facial expression, by their posture and motion.

In short - effective communication does happen without 'logical connectives.
Yes, there are such systems, but they are rare. Most logicians agree with you. They want the https://en.wikipedia.org/wiki/Deduction_theorem deduction theorem to hold. 

Among the logicians who disagree are German logician H. Wessel and some of his scholars. In his theory of strict logical consequence A|-B only holds if and only if

1.) A->B is a tautology
2.) B contains only variables that A also contains
3.) A is not a contradiction and B not a tautology


In other words, the so-called paradoxes of the material conditional are accepted, as the conditional is only a truth function, but their analogues for logical consequence are avoided. Confusing logical consequence with the material conditional is considered a grave conceptual error in this view. This work is only available in German, see https://de.wikipedia.org/wiki/Horst_Wessel_%28Philosoph%29 this German Wikipedia link.

http://plato.stanford.edu/entries/logic-relevance/ Relevance logics have been developed in the Anglo-American tradition to alleviate the problems of the material conditional. Since they change the conditional itself, as far as I know adequate versions of the deduction theorem hold in them, so they do not speak against your point of view.
Yes, you can do this!  The case you're thinking of is very similar to Tarski's original idea of defining a Truth predicate that was Materially Adequate, and a paper by Henryk Kotlarski, Stanislav Krajewski and Alistair Lachlan in 1981 showed that we can conservatively define a Truth predicate over the sentences of Peano Arithmetic.  Their trick is to explicitly include as a condition on each of the Truth axioms they specify that our theory's objects of interest concerning Truth are codes of PA sentences, thus excluding any proper PAT sentences.  (You can see the basics of this on the SEP article for http://plato.stanford.edu/entries/truth-axiomatic/#3.1 Axiomatic Truth Theories)

Because the predicate only covers PA sentences, it doesn't include instances of the induction schema that themselves feature the Truth predicate.  If we were to try to add these to the theory, we'd lose the definability result for 2nd Godel reasons; that's probably why you thought it would be impossible to define a predicate satisfying the T-schema.

You're right, though, to think the predicate is incomplete, because when you show you can define Truth as a predicate, you also make it so that this predicate can be used in PA inductions!  So the language as a whole seems to say true arithmetic things that are outside of its account of what it thinks is arithmetically True.  But I think that's okay, since there's a sense in which it doesn't miss out anything that it didn't bring into the language by adding the truth predicate.
I can only see three conditions analytically. I'd say one of these is 'neccessary', but really what it is doing is elaborating what is meant by eternal.


It is outside of time - so that the notion of time doesn't apply to it. 
It is wholly in time, begins with the beginning with time, and ends with the ending of time. 
It is overlaps with time: 'begins' before the beginning of time, and 'ends' after the ending of time.


An example of condition 1: In Islam (I don't know enough about is theology to say which school exactly) & also the Christian Philosopher & Theologian Aquinas, say that Allah/God being eternal means He is outside of time.  Also, mathematical concepts, if one accepts they belong to a platonic realm also lie outside time - as do Platos theory of forms.

Of 2: There is one thing that is eternal by definition, and that is time itself. A second thing that is eternal is the universe itself. Now, even if time was to last 20 billion years only, what could it mean for something to last longer, for there is no more time for it to last longer. This is how Stephen Hawking for example argues that there is nothing beyond the beginning or end of our time.

Of 3: But are we limited by our imagination? Can there not be more than one kind of time? There is nothing to logically insist that there must be, all we have is the evidence of our eyes and our inner sense of time. It is in this sense that Spinoza will say that nature is one of the modes of God, that the character of eternity in nature derives from God, but God is the only true eternal substance. 

It must be indestructible, but that doesn't mean that it can't undergo change or be composite. (Even an ordinary cat which lives and breathes is permanent & unchanging in some way whilst it obviously changes).

To be honest, all of these ideas are rehearsed & explicated in the notion of substance which was originally articulated by Aristotle, for his metaphysics, picked up then by Islamic & Christian theologians and then by modern rationalist science & philosophy. One can consider to have roots before that, for example the idea of apeiron (the boundless) by the Milesian philosopher Anaximander who based his cosmology on it.
Another insightful reference for the semantics of dual-intuitionistic logics is http://kdpu.edu.ua/shramko/yse.htm Yaroslav Shramko's paper on the "logic of scientific research" (Studia Logica 2005).

Just to add a brief comment on @TMF's answer, for the benefit of the author of the original question, it is worth noting that Priest's "Da Costa Logic" was indeed proposed as a fragment of Rauszer's "Brouwer-Heyting logic", extending the conjunction-disjunction fragment of classical/intuitionistic logic by adding an intuitionistic implication and a co-intuitionistic negation, but not the duals of the latter two connectives.
I don't necessarily agree with the premise that the insights that categories bring us are surprising and new. The framework is just formal and efficient. 

In the 40's and later, mathematicans in fields between algebra and geometry were faced with a zoo of magical equivalences between abstract spaces and it was time for a clean up. With categories, mathematicans axiomize a fairly minimalist idea, namely arrow composition, and see where this alone takes one. 



Now, for those who are interested, and to shine some more light on the statement you use to motivate your question (and to argue why "addition becomes multiplication" is not vodoo magic of the 21st century but combinatorically simple):



The diagram is associated with the http://en.wikipedia.org/wiki/Coproduct coproduct, the abstract categorical motivation being the following: You start with two objects X1 and X2. You consider any third object Y at which X1 and X2 are both pointing at. The coproduct "X1 cop X2" in the middle is to be constructed by using two inclusion arrows i1 and i2, which lets you replace f1 and f2 by a single arrow f.

An application to mathematics: X1 and X2 are finite sets, like X1={a,b,c} and X2={y,z}, which contain 3 and 2 things, respectively. The arrows f1 and f2 are taken to be functions and they aim at a set Y as target. The introduced general object "X1 cop X2" must hence store the information what f1 and f2 might require, respectively: This is a set {i1(a), i1(b), i1(c), i2(y), i2(z)} containing arguments. The target to which these get sent to via f is Y. The list has 5 things. 

"The cardinality of the coproduct is the addition of the cardinalities of the intial objects X1 and X2."



Here we have the http://en.wikipedia.org/wiki/Product_%28category_theory%29 product, the abstract categorical motivation being: You start with two objects X1 and X2. You consider any third object Y which points at X1 and X2 (notice arrow reversal). The product "X1 p X2" in the middle is to be constructed by using one arrow f (necessarily by the diagram, you have only one arrow!!), which lets you replace f1 and f2 by two arrows pi1 and pi2, the projections.

An application to mathematics: Again, X1 and X2 are the finte sets from above. The arrows f1 and f2 are now functions away from Y and one and the same thing from Y might get processed differently, depending on what f1 and f2 do. Here the introduced general object "X1 p X2" must be capable of storing the information from f, which express what arrows f1 and f2 can do for a Y arguments: This is a set which I'll write as {[fa,fy], [fb,fy], [fc,fy], [fa,fz], [fb,fz], [fc,fz]}. The list must store all the possibilities how targets of the arrows f1 and f2 could be combined. The list has 6 things. 

"The cardinality of the coproduct is the multiplication of the cardinalities of the intial objects X1 and X2."

Notice that the difference merely comes from how many arrow heads end at the object which is to be defined: Can you use several lists which have index 1,2,... or do you have to make a grid? It's nice that plus turns to times by arrow dualizing, but the why is combinatorics which could have been explained to Pythagoras.



Now the formal defintion of the product and the coproduct, "the objects "X1 blubb X2" which is unique up to invertible arrow and which has the property that for any object Y, there are arrows..." looks terrible for any innocent person coming from outside. For the people working in a math jungle, it was a great tool for abstracting combinatorical principles away from the complicated spaces they were working with (not just finite sets like here). 

You provide objects, you demand objects, and the relations (arrow, or and also functors, natural transformations, enriched cateogires, 2-categories, n-categories, adjunctions,...) represent the possibilies which then emerge. To make the point clear again: addition becoming multiplication by "arrow reversal" isn't a mystery. Here it's the difference between provinging one or two slots for saving information. 

Don't be afraid of cats. The just capture everything and don't let you know.



What then happend is history. You can, similar to the construction for addition and multipliation above, capture whatever concept you like by "universal properties" (here an http://en.wikipedia.org/wiki/Pullback_%28category_theory%29#Universal_property example of a diagram related to the product). People realized that categories are (somewhat clearly) the perfect tool for capturing order and operations on the whole construction. Subset relations (topos theory, see also http://en.wikipedia.org/wiki/Subobject_classifier#Introductory_example this diagram), syntax and processes (logic, computation). Switching and matching them onto each other (functors, natural transformations). 

As a sidenote: I find it interesting and also funny to keep track of names of mathematical subjects, which merge into each other or get rewritten. In relation to this answer, look how Wikipedians speak of http://en.wikipedia.org/wiki/Combinatorial_topology this and http://en.wikipedia.org/wiki/Topological_combinatorics that in the past, and http://en.wikipedia.org/wiki/Combinatorial_commutative_algebra this or http://en.wikipedia.org/wiki/Algebraic_combinatorics this sound new and fresh - while the names of the subjects are not creatively very different.



Then, and this is the philosophical aspect of category theory*, you can replace equality be arrows too. "a=b" becomes "a isomorphic to b, and so b isomorphic to a". This way, you don't equate things, which are though of as different objects. The classical example being (I read it in http://arxiv.org/abs/math/0004133 a paper by J. Baez) that comparing two flocks of sheep can be done by comparing the sheeps one for one - or the two flocks can be compared to the sets of words "one, two, three, ...", which are invented for this purpose. Counting! The pro category argument is that the sheep get lost in the abstraction, and we should rather put the sheep in a category and not equate some cardinalities of flocks but construct the arrow putting the two flocks in bijection. 

*apart from categories being able to capture logical syntax - which isn't surprising, as they just seem to be able to capture anything they want

I personally am not a fan of the numbers. The numbers as sets that is - I think the process of counting is the important thing. To answer your question though, I think here we're discussing just Platonism in sheep's clothing.
Let's assume that classical logic refers mainly to the law of excluded middle, more precisely a bivalent logic. Let's assume that Modal logic refers mainly to the situation were there is a (loosely specified) universe with worlds and a reachability relation between these worlds, and that we are in one specific (well specified) world of this universe and talk about propositions in our own world and the (relatively well specified) worlds that are reachable from our own world (and perhaps also the worlds reachable from worlds reachable from our own world, and finite iterations of this construction).

Let's focus on propositional logic to simplify things. In classical logic, some natural language sentences correspond to propositions, and every proposition is either true or false. In classical logic, you better resist the temptation to assign a proposition to every natural language sentence, because the negation of an ambiguous sentence might still be ambiguous, and hence the functoriality of logic might fail.

One of the problems with a classical logic account of Modal logic is that there can be natural language sentences which correspond to propositions in one world, but fail to correspond to propositions in another world. One idea to remedy this situation is to only consider natural language sentences which correspond to propositions in every world of the universe. But how much will you restrict the expressive power and the useful applications of Modal logic by doing so?
V-omega, the set of hereditarily finite sets is a model of the theory you get by taking ZFC and replacing the axiom of infinity with its negation, and it is bi-interpretable with Peano Arithmetic (so indeed, Gödel's incompleteness theorem still applies).

For more, see e.g. https://math.stackexchange.com/questions/107639/what-are-the-consequences-if-axiom-of-infinity-is-negated https://math.stackexchange.com/questions/107639/what-are-the-consequences-if-axiom-of-infinity-is-negated
This is more of a linguistics question than a philosophical question.

The short answer is that a Desire may be a motivation, but a motivation is not necessarily a desire. In more concrete terms, A desire is one kind of motivation, but there are others, like a sense of duty, fear, etc.

This should probably be put to the guys over at the https://english.stackexchange.com/?as=1 English language exchange.
From looking at the meaning of deliberately, to do something deliberately would be to do something with intent and reason. Now looking at your three sentences, there is a question of who thinks S ought to do A and how S ought to do A.

(1): The one asserting the sentence is claiming that any agent S ought to do A deliberately. Here, S does not necessarily already believe that he/she/it ought to do A deliberately, but rather the assert-er does. There is an additional condition in (1) regarding the manner in which S does A. If S were to do A unintentionally, then S messed up. However, this is interpreting "deliberately speaking" as "with deliberateness." If such is not the case, then "deliberately speaking" refers to the manner in which the assert-er is asserting. The addition then says nothing regarding S or A but is a rhetorical appeal to the methodology in which the assert-er is working.

(2): Unlike (1), this sentence does not say anything about the assert-er. We are assuming S's beliefs and what S knows to be true and deducting a normative claim from that set.

(3): This sentence differs from (1) and (2) in that it is, prima facie, a nonethical statement. If we equivocate an "instrumentally rational" action with an action that ought to be done, then we can say that (3) is no different than (2). However, this assumes a lot about S and A that we don't know. Let's also look at how "instrumentally rational" could be true, while "ought" would not. Suppose that for S, following every instruction in the Torah is an ethical necessity that ought to be done. Yet, S cannot find any instrumentally rational deduction as to why S ought to live a kosher life. This does presuppose that S does not consider any deduction including a faith-based claim to be rational, but we can easily allow this to occur since (3) does not mandate S to be infallible. 
Thanks George, my original answer was wrong. Here's a less wrong, and this time negative
 answer.


  (1) says that: for all agents a, a knows that P
  (2) says that: for some agent a, a knows that P


The meaning of (1) follows from the fact that epistemically necessary propositions are such that they are true in all epistemically accessible worlds of all agents (this second "all" is the key, because each agent has her own epistemic accessibility relation). The meaning of (2) is straightforward: "P is known" simply means that there exists someone who knows P to be true.

In case it's not already obvious why (1) and (2) are not equivalent, let's fully unpack the meanings of the two sentences. Each agent α has an associated accessibility relation R(α). Using this, we observe that:


  (1) says that: for all agents α, for all R(α)–accessible worlds w, P is true at w
  (2) says that: for some agent α, for all R(α)–accessible worlds w, P is true at w 


Based on these observations, we can present a counterexample to the equivalence of (1) & (2) by having a world where some agent α knows P, but some other agent β does not know P. This situation would show that P is known (by someone), but nevertheless not epistemically necessary (because at least one agent, viz. β, doesn't know P).
You say:

"But I would like to have some understanding on why CH has to be true if it cannot be proven."

I don't think that there is a consensus that CH has to be true at all. Goedel famously thought it was false (despite showing its consistency with the axioms of ZFC using his inner model construction). Hugh Woodin has conducted two programmes to try and determine the truth value of CH; one (which he has moved away from) using a device called Ω-logic to prove ¬CH and another using his `Ultimate-L' construction with which he hopes to prove CH. Still others (such as Joel Hamkins) think that the plethora of set theoretic models show CH to have no truth value.

One argument that has been advanced for CH having a determinate truth value (we just do not know which) is based on the quasi-categoricity result for second-order ZFC. Attributed to Zermelo, it (roughly) states that given any two models of second-order ZFC either the two are isomorphic or one is isomorphic to an proper initial segment of the other. The key thing to note about these models is that they must be of inaccessible cardinality (which is way bigger than the set of real numbers). Hence the reals must be the same in any two models of ZFC2, and thus CH has a determinate truth value (we just don't know which). Of course, one should be aware that in that argument second-order resources play a key role, which are massively controversial in the context of set-theory.

As for a statement that is obviously true but independent from an axiom system, one example is the Goedel sentence for Peano Arithmetic. Very informally the Goedel sentence (G) for PA holds iff there is no proof of (G) in PA (in the actual proof this uses some recursion theory and is stated by coding the syntax of the formal system). Hence if PA is consistent then (G) is not provable in PA, if it were then there  would be no (code of a) proof of (G) in PA, and hence (G) would be unprovable. If on the other hand, its negation is provable, then it's not the case that there is no (code of a) proof of (G), hence there is a proof of (G), thereby contradicting the consistency of PA. Thus neither (G) nor ¬(G) is provable. However, if we look at what the Goedel sentence for PA informally says, then we can see that (assuming the consistency of PA) it must be true; (G) says that (G) is not provable,unprovable and further it is the case that (G) is not provable. One should be aware that, with this argument, in order to show the Goedel sentence to be true one has to assume the consistency of PA. Interestingly this turns out (in a formalised setting) to be equivalent to the consistency statement for PA, and hence there are questions as to actually how much the Goedel sentence line of argument shows.
The idea of a quantized space sounds great for one dimensional spaces. However, it has strange side effects for two and three dimensional spaces.


Because the square root of two is irrational, the diagonal and the side of a square are not multiples of a common smallest unit.
Let's try to model three dimensional space by a lattice with a non-zero unit spacing. This model won't be rotational invariant, because some directions are distinguished. But there is no reason to expect that some directions of the space around us should be distinguished.


Quantum mechanics has to cope with worse side effects than that. How is it possible that light is both a wave and a particle? This seemingly contradictory state of affairs can be modeled mathematically by incorporating randomness into the model. I guess that modeling a quantized space will also require randomness in some form. Maybe the result of comparing the length of two objects is slightly random, if they differ only on the order of the Planck scale. Or we have some random lattice structure instead of a normal lattice. Perhaps there even exists (mathematically) "universal random lattice" structures, similar to the http://en.wikipedia.org/wiki/Universal_graph universal random graph? Can we define some Planck scale for such structures? The investigation of such structures might be interesting from a mathematical point of view, independent of whether these structures model any physical reality or not.
Definition 1 and Axiom 3 in the Wiki article together seem to have something of a Diagonalization flair to them.  G(x) is defined to be a property of x such that for every positive property ɸ, ɸ(x).  G is then affirmed axiomatically to itself be a positive property.  This is rather similar to the argument invoked in the first incompleteness theorem that the numerical predicates representable in numerical syntax (e.g. ɸ(x)) can themselves be used to define a representable self-application sentence, γ iff ɸ([γ]).

I think this might be where the similarities end, but it actually looks quite promising that the argument will eventually fall into contradiction for reasons more like the Liar paradox than the Incompleteness theorem, if it ends up reasoning that there are properties that God does not have, that he must therefore have their negations as properties and, using the necessary implication hook, that God must have an impossible property.
I think what you meant is "If your absence doesn't change anything then your presence doesn't matter."  We can further express that as "If a change in state from presence to absence does not represent a significant change, then your presence is not significant," which is very nearly a tautology, a sentence that is true based just of the definition of terms.  Under this interpretation, your example would be incorrect, since the first part doesn't correctly capture the contrast between presence and absence.
It is easier to prove things false than to prove them true.

Compare http://en.wikipedia.org/wiki/Verificationism Verificationism with http://en.wikipedia.org/wiki/Falsificationism#Falsificationism Falsificationism. Basically, do we (a) verify that statements are true by making observations, or (b) make ingenious attempts to falsify statements and gain confidence when these attempts fail? Some folks who went with verificationism were the http://en.wikipedia.org/wiki/Logical_positivism logical positivists; they wanted to say that only 'scientific' statements were meaningful. https://philosophy.stackexchange.com/questions/1175/what-are-were-the-main-criticisms-of-logical-positivism They failed. Just think about it: the very statement about what is meaningful isn't science, itself! Karl Popper argued that we should instead think in terms of coming up with ideas that say that lots of things can't happen (F = ma does this by e.g. eliminating F = ma2), and then trying all sorts of way to make the ideas fail. If we can't make them fail, we gain confidence that they're trustworthy. At least until you e.g. try to understand the http://en.wikipedia.org/wiki/Perihelion_precession_of_Mercury#Perihelion_precession_of_Mercury perihelion procession of Mercury.


  "absolute truth does not exist"


This statement clearly cannot be absolutely true. It is easy to falsify. Your error is to assume that we can therefore quickly jump to:


  "absolute truth exists"


How do we verify this? It's not so clear that we can. How can we falsify it? That would be https://philosophy.stackexchange.com/questions/3680/can-one-prove-a-negative-existential-claim proving a negative, which is difficult if not impossible, except within some formal system.



P.S. You might enjoy the http://en.wikipedia.org/wiki/M%FCnchhausen_trilemma Münchhausen trilemma (https://philosophy.stackexchange.com/questions/7325/why-is-the-munchhausen-trilemma-an-unsolved-problem Phil.SE question).
To maybe put the interrelation between these three rules into perspective, it might be worthwhile to notice that from the point of view of http://ncatlab.org/nlab/show/logic#ReferencesCategoricalLogic categorical logic, the http://ncatlab.org/nlab/show/categorical%20semantics categorical semantics of a http://ncatlab.org/nlab/show/modality modality (with all three rules N, K and M) is a http://ncatlab.org/nlab/show/closure%20operator closure operator, hence a http://ncatlab.org/nlab/show/monad%20%28in%20computer%20science%29 (co-)monad on the system of propositions/of types. Rule M then interprets as the http://ncatlab.org/nlab/show/unit%20of%20a%20monad (co-)unit of the (co-)monad. This gives a usefully "global" picture of what the interrelation between these three rules is, or maybe what it should most naturally be taken to be. In particular it means that □(□A→A).

This monadic perspective on modal logic has proven quite fruitful as of late. Generalized from propositions to http://ncatlab.org/nlab/show/type%20theory type theory it leads to http://ncatlab.org/nlab/show/modal%20type%20theory modal type theory which is sometimes called http://ncatlab.org/nlab/show/computational+type+theory computational type theory due to the intimate and practically relevant application of modalities in type theories to encode computational effects in functional programming. One might view the wealth of these applications as evidence that regarding the rules N, K and M in their correct interaction as the encoding of a monad is their natural raison d'être.

Lawvere had pointed out that considering modalities in this sense related to each other such as to make for systems of "http://ncatlab.org/nlab/show/adjoint%20modality adjoint modalities" has profound implications on the re-reading of parts of philosophy to which application of modal logic had never been dreamed of, namely Lawvere http://ncatlab.org/nlab/show/adjoint+cylinder#references points out that adding axioms for systems of http://ncatlab.org/nlab/show/adjoint%20modality adjoint modalities to the intuitionistic type system serves to usefully(!) formalize the "unities of opposites" that infamously govern Hegelian metaphysics.

Recently with the advent of http://ncatlab.org/nlab/show/homotopy%20type%20theory homotopy type theory this monadic perspective on modalities has brought about some developments that seem fairly dramatic compared to the developments in modal logic (to which this still reduces on http://ncatlab.org/nlab/show/propositions%20as%20types (-1)-truncated types). In http://ncatlab.org/nlab/show/propositions%20as%20types cohesive homotopy type theory we add a triple of adjoint modalities to the logic/the type system and find that this provides a formal context in which a maybe surprisingly large bit of modern higher geometry may be usefully axiomatized. 

All this is only possible with reading the rules for N,M,K in the "natural way" (which hopefully is the way that the Wikipedia article describes).
In general, contemporary philosophers don't accept Kant's account of understanding. There are at least two different strains at work in rejecting Kant's categories of the understanding.

Stated in the simplest and roughest terms, Kant believes that we understand objects through the application of the categories of the understanding to perceptions shaped by through the forms of sensibility as applied to things. For Kant, things are in themselves in accessible to our understanding and imperceptible. To access these things through our senses we must subject them to the forms of sensibility and to understand them we must apply the categories to them.

There's a question about the relationship of mind and reality that the Kantian picture assumes, and this creates two directions of objections. First, you can believe that we have more direct access to things (and reject this three-level distinction of what we are encountering).  This sort of account is prevalent in contemporary analytic philosophy. Understanding is seen as not occurring in such clear distinction to perception. Thus, we deny the need for a Transcendental Unity of Apperception to glue everything together.

Second, you can argue that what we do is not grounded in the thing-sensible-object system. In other words, we have objects but they need not be so tightly linked to reality. Part of the reason is the recognition that Kant's 12 categories are kind of arbitrary (despite their systematic brilliance). On these accounts, offered say for instance by Rorty but having their genesis in Hegel, our access to reality is seen as limited by our modes of understanding. Depending on the species this can be pernicious to the point that there's knowledge -- just interpretation or softened such that we still have access to things through the objects but according to different lists of categories.
Actually, You Can Prove A Negative Sometimes

In general, sure, it can be difficult to disprove the existence of something on a universal level, because theoretically you would need total, infallible awareness of the entire universe to prove it with complete certainty. The problem here lies in that the scope of the concern and requirement of proof is essentially unbounded (extends to the entire continuum), and we lack the ability to search the entire universe.

However, if you are simply referring to existence in a local continuum (trying to prove the existence of something within a defined and searchable area and which itself is capable of being observed by humans), then the problem becomes possible to solve. Simply searching the problem space and finding the no existence of X would prove X does not exist (deductively).

Proof by Deduction vs Induction

The problem you are touching upon is whether you can deductively arrive at the conclusion that something does not exist, and you can in a searchable problem space. Only when the problem space becomes unsearchable in some way do we have to rely on inference (https://en.wikipedia.org/wiki/Inductive_reasoning induction) rather than https://en.wikipedia.org/wiki/Deductive_reasoning deduction; for example, if the search area is too big to possibly search, or our methods of observation [i.e., eyes, radar, etc.] lack 100% reliability [i.e., they could be used and but still potentially not see whatever you are looking for, maybe because the human blinked his eye and missed the sea monster, or the machine cannot see the monster when it is ghost form, whatever. The point is, if there is any possibility that the observation method could fail, however unlikely or odd, you move from deduction to induction, with the certainty of your conclusion directly related to the reliability of your method(s) of observation, the size of your search space, and your strategy for conducting that search (presuming your eyes/radar cannot see the entire space at once you will need to devise a plan to systematically investigate the entire area — and some strategies are of course more effective than others.).].
Your sceptic must understand what the symbols 1+1 means otherwise he is not justified in claiming that 1+1 is two. For example there are number systems in which there isn't a 1, or certain operations are undefined, or 1+1=0. But one could also imagine that the symbol '1' means a drop of water, and '+' means physical addition, so that 1+1 means add one drop of water to another drop of water, so we get a another (larger) drop of water, so in this case 1+1=1.

Assuming it has the traditional sense, then if you start from the Peano axioms, which are roughly that there is a zero and that you can always add one to a number, then you can prove that 1+1=2.

But this is not the whole truth. Had these axioms shown us that 1+1 is not in fact 2, then Peano would simply have thrown his axioms away. 

What he was attempting to do was find a set of axioms that accurately captures our intuition about how the integers act; and obviously 1+1=2 is an act of the integers that is true by intuition/observation which he has to incorporate for his axioms to meaningfully model the integers.

Now given Peanos formalisation and the mathematical logic introduced by Boole & Frege, Bertrand Russell attempted to derive Peanos axioms from logic. This is why it took him several hundred pages to reach the point of saying that 1+1=2. 

Perhaps the most practical set of axioms in the sense that it mirrors our intuition is that it is a well-ordered ring. This means that it is a set with two operations called addition & multiplication and they are commutative, associative and have an identity; that multiplication distributes over addition; that there is an order relation on the set such that every non-empty set has a minimal element.

These three sets of axioms are connected by a simple dependency of deduction: Logic -> Peano Axioms -> Ring Axioms; but one should retain in mind that the other direction holds too as a process of historical reflection and label it as thus: Logic <- Formal (Peano) <- Intuition (Ring).

The mythical Amazonian tribe that can't do or understand arithmetic will also not understand what you mean by proof; but this is simply because they have no perception that arithmetic in the right context can be important; and this conceals the important point that the long development of arithmetic & measurement in Ancient Mesopotamia is to understand its use & importance; it is this acquaintenance that was bequeathed to Greece and by which Euclid first outlined a complete axiomatic system. It's hardly creditable that he was the first to conceive of an axiomatic one but he was the first to achieve something like a complete system. In India, and roughly contemporareus Panini developed a complete formalisation of Sanskrit Grammar. 

Formalisation as a concept in mathematics only occurred in the early 20th Century after the revitalisation of mathematical logic. This differs from axiomatic systems in that the idea of truth is absent - self-consistency is the only requirement, whereas an axiom should be self-evidently true. That is in a formal system, you may 'prove' something, but because the 'axioms' are contigent rather than self-evident, one could argue that in fact nothing has been proved. In this case, proof has been reduced to syntax. 

And this in fact, is the difference between the two earliest attempts at formalisation. After all, there is only one system of integers; whereas there are many languages other than Sanskrit. 
They are different. As a quick example, in non-standard analysis we use "infinitesmals", e.g., dx, dy, as somethings. Also, I'm not sure that there was NOTHING before the big bang. Lawrence Krauss has a http://rads.stackoverflow.com/amzn/click/1451624468 new book on just this issue.
Truth is indefinable & ineffable. Nobody bothers to define it for that reason. Those that do are aiming at limited objectives, or are engaging in chasing circular definitions which has its own amusement.

After Godel published his famous incompleteness theorems in formal logic, Tarski, a logician, showed that truth was undefinable using the same formal apparatus.

Euclid, a  mathematician, based geometry on the first fully and aesthetically pleasing axiomatic system. Axioms are traditionally held to be self-evident  truths. But sometimes the truths are so self-evident, or that the purpose of an axiomatic system is so inevident that no-one bothers to form axioms for them - for example arithmetic wasn't put into this form until the 20C by Peano.

Wittgenstein might say that we're using the word 'truth' in many different and significant ways, and each one should be labelled ie truth-1, truth-2 etc. As truth-for-me is different from truth-for-you in subtle ways for the same concept, one should label these truths by individuals which multiplies them endlessly. Truth as language in this sense ramifies spectacularly. Despite this, one might suppose that he did think truth was one but chose to remain silent about it.

Mathematicians learn about mathematical truth by learning their tradition, and it being verified by their peers - in this perspective truth appears to be socially conditioned. 


  There is an old saying: "Truth is eternal"; would Mathematicians agree with that saying?


Plato would have said yes - as a form of the Good. Some modern day philosophers of Number suppose it all might be some kind of fiction, but not a hoax. Others suppose both. 
Not sure this will be an answer per se, more of a long comment.

Re (1) Zourabichvilli (whose book is excellent) ties the concept of event back to Hegel and Heidegger (whom Deleuze mentions in conjunction with DR). Some of what Deleuze is picking up on can be found in Heidegger's "The Question Concerning Technology," i.e., the event as epoch-defining. A key version of what the event is Nietzsche's mad man who declares the death of God (in which connection aside from the obvious Deleuzian texts, one might also look at Heideggers "Nietzsche's Word: God is Dead" as well as §125 of Nietzsche's The Gay Science). This filters through to Deleuze's "third synthesis" in the "Repetition for Itself" chapter of Difference and Repetition, i.e., the event is not something that happens, but has already happened.

Re (2) "possibility" derives from Aristotle's works on logic (collectively known as the Organon), in particular, his Prior Analytics and On Interpretation. Since you're referencing the SEP article, it may be helpful to look at Smith's "The Concept of the New," which was published in Deleuze Studies and which looks at, inter alia, what possibility is and how Deleuze changes it.

Note that virtuality (since that doesn't seem to have been addressed by this question or the other linked one) is a term Deleuze's picks up from Bergson, in particular from Bergson's Matter and Memory. (Bergson is quite readable on his own, but you could also look at Deleuze's Bergsonism)
If time exists separate from the course of increasing entropy, as imagined by Boltzmann, how could we know?  Our memory is an exothermic chemical reaction, completely dependent on increasing entropy to store information.  And the rest of our sense of time is an extrapolation of our experience of accumulating memory.

So, I would contend that sheer thermodynamics need not prove the direction of time, our physiology ties us to a single arrow, instead.  But that is the arrow of increasing entropy, which may not align perfectly with any specific dimension of spacetime.

From this point of view, we must inhabit a part of the history of the universe where entropy is relatively low, and entropy must decrease fairly continuously as you move away from points of low entropy and toward points of high entropy in all dimensions, including any 'time' dimensions.  But heat does this in space, and we can assume it would have to do so in time.

Indeterminacy can be accounted for in this model by the fact that the progression of entropy would be erratic, so time is not strictly unidirectional, only macroscopically so.  The very high level of order in our local space would mean that time never moves backward very fast or for very long, upstream through the sort of 'osmotic pressure' of entropy along the temporal direction.  As the 'fluctuation theorem' proves, this pressure remains very, very high until entropy becomes nearly maximal.

The proposed gedankenexperiment would not make sense in this kind of world.  An empty space is automatically both minimal and maximal in entropy, and there would be no reason for time to 'move' forward from there.  Only in a universe with enough complexity for accumulating entropy to appear continuous could there be something like time as we know it.  So space might not have been 'initially' empty, and attempts to project time too far backward may lack logical content.  Whatever the underlying structure of time is, against which entropy moves, it could act quite differently in a much simpler place.
Don't you think you also tend to build continue representation of obviously discrete objects? Think how films are working: 27 pictures per seconds and there you think it's a continuous movement. Your mind even don't bother to inform your conciseness  it's an illusion.

Both discrete and continue approach are used in our everyday representations of the world, as well as in abstract theories.
For the first question, interpret c as the number 0, and interpret P(x,y) as y = x+1. Since for every natural number x the number y=x+1 is not zero, the first clause holds in your desired statement. The second clause holds because if P(x,z) and P(y,z), then z=x+1=y+1, and so x=y. So the second clause also holds, and the statement is satisfied by this interpretation in the natural numbers.

The statement is not satisfiable in any finite domain, because the second part of the clause is exactly saying that the relation P is one-to-one, in the sense that there are never two different objects x and y both related to a single object z. And the first clause is saying that the relation P is not surjective, in the sense that no object x is related to the object c. If we had a model of the statements on a finite set, then we could define a function f(x)= some (any) y such that P(x,y) holds. This would be a one-to-one function on that finite domain, by the second clause. But any one-to-one function on a finite domain must be onto, by the pigeon-hole principle, and this will violate the first clause.  (The issue of non-empty domain is moot because the language has a constant symbol c, which must be interpreted in the domain, so the domain is nonempty). 

Your last question is totally different and should have been asked as a separate question.
You can make a model of your theory by defining S(x)=x+1 on the natural numbers, but defining + by x+0=x, and x+y=x+1 if y is not 0. (That is, we redefine addition to be this weird thing that ignores y in x+y unless y is 0, and otherwise only adds 1 rather than adding y.) Now you get all your axioms being true, since P4 only says every x has some y for which it works, and you can take y=0 to satisfy P4. But x+y=y+x will not generally be true, for example 0+2 will be just 1, but 2+0 will be 2 on this interpretation.
Kant insists the perceived thing outside me (on which my awareness of the temporality of things in me depends) is ‘not…the mere representation of a thing outside me.’ (B275)  So he means to deny that what Dicker calls ‘objects in space’ are representations, but as Kemp Smith observes, in the first edition Kant had said the opposite:  ‘Outer objects (bodies) are mere appearances, and are therefore nothing but a species of my representations, the objects of which are something only through these representations.’ (A370, quoted in A Commentary to Kant’s Critique of Pure Reason, p. 313).    Reinforcing this is that the only alternative referent for ‘thing outside me’ would be a thing in itself, but that won’t work here because the Refutation needs the thing outside to be perceived, i.e., be an object of experience.  

Berkeley could accept the so-called Refutation of Idealism in B275, unless the perceived permanent thing Kant is appealing to is the thing in itself.  Kant says Kant's view is an ‘empirical realism’ that distinguishes it from something he calls ‘idealism,’ but he hasn’t refuted anything just by claiming a label for himself.  In Kant's theory, to be empirical is to be the product of a mind’s synthesizing, or some component of a mental synthesizer; a body is ‘real’ only in the way being a product or component of mental synthesizing lets a thing be real—which is compatible with Berkeley’s account of body.  Berkeley never said anything that conflicts with Kant about the empirical reality of body.   

I’m aware your question mentioned Descartes, not Berkeley, but part of the point is to challenge Kant’s positioning himself in opposition to both of them, whereas the real contrast is between Descartes on one side and Berkeley/Kant on the other.  Kant distorts language when he calls Descartes an idealist; something even stranger is going on when he continues to call his own view ‘idealism’ after inserting a refutation of idealism.   You’re right to be unsure what the point of his proof is.
You are right.

An argument is valid when, if the premisses are true, then the conculsion is true.

So the definition simply exploit the property of the propositional connective "if ..., then ...".

Reminding of truth-functional properties of the above connective, we have that a sentence of the form "if P, then Q" is false only when P is true and Q is false.

Therefore, we have that an argument is invalid only when from true premisses concludes a false conclusion.
I suspect that you will not find an interesting mathematical theory about finite arithmetic.

Ultrafinitism is a consistent and interesting philosophical theory (I mean : philosophy of mathematics), but what kind of arithmetical "interesting" facts do you expect to find in such a theory ?

If we simply delete the Peano axiom which states that for every number there is a successor, we have the "finitist" Peano arithmetic.

But we still want all other properties of numbers ? If we work with "little" numbers, their sum and product will be still "little"; so, no problem.

But what will happen if we add two "big" numbers, i.e.two finite numbers that are quite at the "border" of the "biggest" thinkable or computable number ? We will step outside the "domain" ? or we have to enlarge it ?

We have no support for the infinitude of our physical world; thus, the infinity of the natural numbers may be only a fiction. But it is still a very very useful and interesting fiction...
This is old, but in case anybody finds it again: 5 + 7 = 12 is an example used in Plato's Theaetetus. I believe others referred to it as well.
It sounds like the quote is referring to charity of interpretation which is a general expectation in philosophy. This concept means that when I read someone's argument or position, I should try to understand it in the most charitable light and then find flaws with that. In other words, ideally I am not attacking them for misspelling words or for failing to include very obvious premises.

Of course, charity of interpretation is a virtue -- which means there's no manual on how to do it perfectly. So the question in the case of what Hume does is whether he's really presenting a position anyone holds. As in, do people hold a stupid view that expects the sun to rise tomorrow merely because it rose yesterday and today OR is there an implied premise taken to be so trivial that its inclusion is pedantic. In other words, is the argument Hume must really deal with one that includes a premise that "the world is uniform in its operation"? But the problem with seeing that as the charitable interpretation per the quote is that there are many different premises that could give validity to the argument.

Formalized, it's something like this:


  (1) S-1 [sun rose yesterday]
  
  (2) S0 [sun rose  today]
  
  Therefore
  
  (C) S+1 [sun will rise to tomorrow]


On a direct reading, this argument is invalid... to make it valid we can add several different entities:

strategy I: assert that anything that happens both yesterday and today is bound to happen tomorrow.

strategy II: expand the set we are looking at further into the past and add a premise that things with a regularity above 1000 instances will happen again.

strategy III: expand the set we are looking at further into the past 1000x and assert that things which happen every time out of a 1000 times will happen again in the 1001st instance.

strategy IV: strategy III + the restriction of this to natural phenomenon

...

And the problem is that with so many different strategies it is not clear what the most charitable route to go is. So he's asserting that the route Stove et al. use would make Hume guilty of a simple logic error. Presumably, he will defend Hume against this.
Boolean algebras have complementation laws which are equivalent to the Law of the Excluded Middle. A multi-valent logic would have to abandon or modify those. One of the reasons classical logic is useful is that it expresses rules of inference and valid argumentation. If a multi-valent logic fails to do so, it has limited utility as a logic.
It is correct that in http://ncatlab.org/nlab/show/homotopy+type+theory homotopy type theory two http://ncatlab.org/nlab/show/term terms x1, x2 : X of http://ncatlab.org/nlab/show/type type X may be different, while being http://ncatlab.org/nlab/show/equivalence equivalent, as witnessed by an equivalence or homotopy between them. Since under http://ncatlab.org/nlab/show/propositions+as+types BHK we may think of x1 and x2 as being proofs of a proposition -- namely the proposition represented by the  http://ncatlab.org/nlab/show/n-truncation+modality (-1)-truncation  http://ncatlab.org/nlab/show/bracket+type isInhab(X) of X, this means that these two proofs are thereby equivalent.

Regarding the discussion in the comments above, notice that one needs to distinguish here the type X from its http://ncatlab.org/nlab/show/n-truncation+modality (-1)-truncation  http://ncatlab.org/nlab/show/bracket+type isInhab(X). Not every two terms of the former need to be connected by a homotopy, this will only be the case if X is a http://ncatlab.org/nlab/show/connected+homotopy+type connected type. On the other hand, by construction any two terms of isInhab(X) are equivalent, reflecting the fact that this is really a http://nlab.mathforge.org/nlab/show/mere+proposition mere proposition.

Finally regarding the statement that homotopy is "analytic, in that the paths are built out of points": this is really only true in the particular presentation of http://ncatlab.org/nlab/show/homotopy+theory homotopy theory by http://ncatlab.org/nlab/show/Ho%28Top%29 topological spaces up to weak homotopy equivalence. Arguably more "direct" is the presentation by http://ncatlab.org/nlab/show/infinity-groupoid infinity-groupoids which, specifically when thought of as http://ncatlab.org/nlab/show/Kan%20complex Kan complexes. In this presentation homotopies are "elementary" and not "built out of points".
So, I'm not sure what motivates the position you're talking about. But off the cuff, my response would be: 

X's property p is whatever makes the sentence "x is p" true. 

A quality is just a particular kind of intrinsic property. Color is a quality. So are texture and temperature. 

So, all qualities are properties, but not vice versa. Here's a property that isn't a quality--being a father. It is true of me that i'm a father, but that isn't an intrinsic fact about what I am like, it's true in virtue of relationship I have to something outside of me.
In the context of dynamic epistemic logic, the binary operator 'M | φ' takes a model M and a formula φ and 'updates' M, by removing from M all the worlds where φ is false ([Holliday], Lecture 13, Slide 7; [van Benthem], Chapter 15, Definition 15.2.1):


  M | φ = {v ∈ |M| : M, v |= φ}.


In the context of dynamic epistemic logic, this represents the process of learning, and can be used to give explications of belief-revision and so on. Here are some references to the relevant literature:



van Benthem, J. (2010) http://rads.stackoverflow.com/amzn/click/157586598X Modal Logic for Open Minds, Stanford, CSLI Lecture Notes #199.
Holliday, W.H. (2012) http://philosophy.berkeley.edu/people/page/123 Modal Reasoning, Lecture Course (Spring), UC Berkeley.
I think you are just redefining things,  the problem still exists

in your example, you seem to have a problem that you can't solve, but the problem itself is solvable ( just not by you )

its like having the problem 2+2 = ??

and you can't solve it, so you say, pffff, math is inconvenient.   The problem still exists, and has a solution, but its just you have decided not to care about it / reclassify it.

Now this is different from a problem that provably has no solution.  Like "the halting problem" and leads into Gödel's incompleteness theorems.  You may regarded it as inconvenience, but in essence it defines a 'limitation' of what one can know.
Paradoxes are indeed invalid arguments, but what makes them special is that they rest on seemingly unproblematic assumptions. We know, for example, that Achilles will in fact outrun the tortoise (http://en.wikipedia.org/wiki/Zeno's_paradoxes Zeno's Paradoxes), we know that the surprise exam will take place (http://en.wikipedia.org/wiki/Unexpected_hanging_paradox Surprise Exam Paradox), and so on. Because we know the conclusions of those paradoxes are false, we know that something is wrong with the arguments. The task then is to identify the assumptions that lead to the false conclusion.

Paradoxes can be called 'invalid' and ignored, but if taken seriously they can help us diagnose and fix problems with existing logico-mathematical frameworks. Axiomatic set theory and type theory owe much to http://plato.stanford.edu/entries/russell-paradox/ Russell's Paradox, for example. The above mentioned Surprise Exam Paradox has led to lots of interesting developments in epistemic, dynamic, and public announcement logics. There are, of course, the classical ones, like the http://plato.stanford.edu/entries/sorites-paradox/ Sorites, the http://plato.stanford.edu/entries/liar-paradox/ Liar, and so on. Each has opened some interesting door.

I claimed that paradoxes are invalid arguments. https://philosophy.stackexchange.com/users/5877/sequitur Sequitur's contribution inspired me to add that someone might ask: "invalid according to which logic?" I'd say classical bivalent first-order, but there are possibilities for significant 'paradox-preserving' deviations from that. What's important here is to realize that: you can't simply change the logic and claim that the paradox is resolved. Suppose classical logic C gives rise to paradox Π, but intuitionistic logic I does not. You cannot simply dispose of classical logic, adopt an intuitionistic one, and claim that you have handled the paradox. Even after doing that, fact will remain that Π is a paradox for C!, so one needs, if interested, to address why C enables the paradox.

Take paradoxes seriously, because they indicate that (at least) some thing is not as true as it seems.
Some logics have a nullary truth-functional connective (⊤) that evaluates to true under all valuations. You can call it 'A', you can call it 'atomic', if you so desire. Here are some facts about ⊤:


It's special in that its interpretation is fixed, i.e., ⊤ is a http://plato.stanford.edu/entries/logical-constants/ logical constant. That means that truth-assignments (like the rows of truth-tables) cannot alter its truth value.
It's general in that it can be used to represent the equivalence class of all tautologies in a given logic (the set of all φ, ψ s.t. φ ⇆ ψ). Because if φ and ψ are tautologies, then φ ≡ ψ ≡ ⊤.


I'm not sure if that's what you're looking for. I have to admit that the question wasn't very clear.
This is far from an actual answer, but it suggests support for the idea.

If a construction were to proceed here, a few first steps would be:


to find a meaningful example of a paraconsistent logic that allows us to think clearly about it and judge the success of our project as it proceeds
to answer Niel de Beaudrap's concern as to whether there is a compelling topology on the space of its decisions
to think about what kinds of mappings from that topology onto set-theory or measures seems to capture our 'degree of consistency'.


It seems to me that the primary model of paraconsistency that we use on a daily basis is morality.  People who reason about morality in one very conventional way admit contradictions in their moral axioms all the time.  Then they work outward on difficult issues from a highly conflicted position to one which achieves an acceptable level of consistency by taking in more and more perspective.

So a good model, in my head, would be a modal logic on the mode of "ought", with a common set of conflicting axioms, and degrees of consistency.

If you don't agree that morality works that way, Common Law explicitly tries to.  Laws depend upon past application, and precedent matters more the closer it is or the more settled it is and the whole point of each judgement is to maintain or increase the internal consistency of the system, hoping that consistent and not offensive eventually devolves on just.

So I think there probably is a clearly articulated topology here, with open sets being something like 'must be considered to decide'.  If you start from a very clear prescriptive cult doctrine and some Bayesian version of the 'ought' logic, or a rigidly statutory version of Common Law with an 'expert system' notion of an AI judge, you could probably present an approximation to that topology as simply as a graph.

It would not be a simple graph, and it would have to allow for degrees, regressions, etc., it may need probabilistic or bi-measured edges, etc. (to capture recency and settledness?)  But, in concept, each statute or tenet applies to bodies of other decisions in a vaguely hierarchical way.

A second potential example might be causal flow or 'collapse' in a quantum environment limited by relativity or some other localizing principle.  [Out comes the harp /] If you adopt my favorite notion of time as accumulated entropy, I can imagine making this a nice clean topology like a metric space on a manifold, by considering the merger of the local balances on entropy as the wavefronts sweep outward from separate decisions to find a global consensus of the direction in which entropy increases.  The measure on consistency could be driven by "How long do I have before I am globally observable?"
It's certainly valid. Your derivation doesn't seem exactly right, but it can be modified to become correct. 


I'm sure it is a syntheic proposition because when he says 'I think', it is not a tautology. If you negate either word, it could still be true or false; 'I don't think' could still be true. I don't think that thinking is a predicate of 'I'. 

Nevertheless, Descartes is trying to say that thinking IS a predicate of 'I' so 'I don't think'makes no sense because an 'I' (a human) has to think be an 'I'. 
Possible-worlds models are relational structures (an underlying set equipped with a bunch of relations). E.g., a possible-worlds model M may consist of the following components: W, →, @, where W is a set of possible-worlds, → is an accessibility relation on W2, and @ is a distinguished world of W. A standard application of these models is the explication of modalities, but they can also be thought of as models of non-modal classical propositional calculus if thought of in the following way. Starting with the language:


  Definition 1. (Language) Given a propositional letter p, the language of propositional logic is defined by the following grammar:   φ   :=   p   |   φ′   |   ¬φ   |   (φ ∧ φ),


we can continue our logical pursuits in one of two ways: (1) we can equip this language with a proof system (a set of axioms and rules of inference) and start deriving truths of propositional logic, and/or (2) we can equip this language with a semantics (an interpretation of its formulas in some recognized structure) and start reasoning semantically. The question under consideration is about the semantics for (Definition 1), so we will ignore proof systems of classical propositional logic and consider its semantics:


  Definition 2. (Semantics) Models of classical propositional logic are truth-assignments.


Truth-assignments are functions that take propositional letters of the language of propositional logic to truth-values (which in the classical case means {0, 1}). A formula φ of  the language of propositional logic is said to be true with respect to a truth-assignment v (symbolically: v ⊧ φ) just in case φ becomes true whenever the propositional letters occurring in φ are assigned truth-values according to v. For example, under assignment v = {p → 1, q → 0}, formula (p → q) becomes false. 

Now, the question is whether this semantics has anything to do with possible-worlds, and the answer is that it does. The key is to observe that functions (including the truth-assignments) are also relations, and therefore truth-assignment functions are also relations, namely, relations that associate propositional letters to (unique) truth-values. The set of all truth-assignments is a relational structure, and we use it all the time when talking about tautologies and contradictions in classical propositional logic:


  Definition 3. (Tautologies & Contradictions) Formula φ is a tautology ( ⊧ φ ) iff every truth-assignment makes φ true. Similarly, φ is a contradiction iff ¬φ is a tautology.


Truth-assignments can be thought of as possible worlds: all we have to do is extract from them those propositional letters that they map to 1 and we have a set of propositional letters that can be said to be true at that 'world'. For example, formula (p ∨ q) has 22 possible truth-assignment, 4 possible worlds: 00, 01, 10, and a 11 world. The first can be described as the world where neither p nor q hold; the second as the world where p holds but q doesn't, and so on. The formula will be neither a tautology, nor a contradiction (so a contingency), because there are worlds (namely: 01, 10, and 11) where it holds, and there is a world (namely: 00), where it doesn't. 

That's the basic idea. For a standard treatment of truth-assignments look at:

Enderton, H. (1972) http://rads.stackoverflow.com/amzn/click/0122384520 A Mathematical Introduction to Logic, 2nd Edition, § 1.2.
I agree with the above answer, but I'll attack the issue from another point ov view.


  Let S be a finite set of cardinality = n;


clearly, with n "little, we have no problem to "hold epistemically" (to imagine ? to visualize ?) it.

I'm not sure about a set S with n = 3567 elements.

For sure, a set S with n = 6665734529976967438675338965633321266643790584532111111 elements is quite far from beeing "easy to hold".

Consider nowe the set N wich include an "initial" element, call it 0, and for each element n, it includes also its "successor", call it S(n).

We have "specified" in a easy and understandable way the set N of natural numbers, which has infinite (exactly : countable many) elements.

The basic concept of an unlimited possibility of iteration, which is the "core" of the potential infinite is much more easy to grasp than a "very very big" finite namber.

What about aleph-one ? 

Assuming that it is equal to c, here we have to start from the so-called real line; what is the role of a "continuous" magnitude in our thinking ? We need it "only" inside mathematics ?
See http://plato.stanford.edu/entries/modality-epistemology/ The Epistemology of Modality. We can sey that the first one are (logical or metaphysical) impossible properties, while the second one are possible.
At minimum, composition is the specific composite parts plus their configuration, i.e. their position relative to each other and/or the way they are joined.

Additionally (and much more interestingly), you can have https://en.wikipedia.org/wiki/Emergentism emergence of qualities that the composition exhibits. 
This is a 2D variation on Sequitur's already complete answer. The question is whether:


  1) ▢♢φ |= L Aφ


holds for modal logic L. The answer depends on which logic this L is. Confining our attention to normal modal logics, we choose the strongest (S5) among them and see whether (1) holds with respect to S5. If (1) does not hold with respect to S5, that is, if we can invalidate it, then we can invalidate it in all normal modal logics weaker than S5 (i.e. logics B, S4, T, D, and even K)! Before we get to the counterexample, let's understand the meaning of (1). We start, as usual, with the underlying language:


  Definition 1. (Language) Given a propositional letter 'p', the language of modal propositional logic is generated by the following grammar:
  
                                                φ   :=   p   |   φ′   |   ¬φ   |   (φ ∧ φ)   |   □φ   |   Aφ.


The addition to our usual setup here is the actuality operator A, which intuitively tells us whether a formula is true in the world of utterance; to see how it works pay attention to the semantics below.


  Definition 2. (Models) A modal model is a triple M = (W, R, V), where W is a set of 'possible worlds', R is a binary accessibility relation on W, V is a function from propositional letters and worlds to {0,1}.


We've got the language and the models, so we just need to interpret the language in those models:


  Definition 3. (2D Semantics) The truth of a formula φ of the modal propositional language in a modal model M = (W, R, V) at a pair of worlds (w,v) ∈ W2 (hence the name '2D') is defined as follows:
  
  
  M, w, v |= p               iff         V(p, w) = 1;
  M, w, v |= ¬φ            iff         ¬(M, w, v |= φ);
  M, w, v |= φ ∧ ψ        iff         (M, w, v |= φ) and (M, w, v |= ψ);
  M, w, v |= □φ            iff         ∀w′ ∈ W : wRw′ → M, w′, v |= φ;
  M, w, v |= Aφ            iff         M, v, v |= φ.
  


The novel thing here is clause (5), which says that φ is true in model M at a world of evaluation w with respect to a world of utterance v just in case φ is true in M if we evaluate it at world v. For the usual applications in philosophy of language we'd define operators to shift the world of utterance to make the most out of v, but for our purposes here the above will be sufficient. Recall that S5 are those modal models that have an R that is an equivalence relation, which means that all worlds can access each other, effectively rendering the notion of accessibility useless. This allows us to simplify clause (4):


  Clause (4) simplification: M, w, v |= S5 □φ iff  ∀w′ ∈ W : M, w′, v |= S5 φ.


The box and the diamond are duals in this logic, so we introduce the abbreviation: ♢φ ≡ ¬▢¬φ. Now we're ready to invalidate (1) in (S5) in this 2D framework:


  Fact 4. For all L ≤ S5, ▢♢φ |= L Aφ is false.


Proof. [Sequitur] Let consequence be S5-consequence. We want to consider an S5 model M and a pair of worlds (w,v) s.t. M, w, v |= S5 ▢♢φ but ¬(M, w, v |= S5 Aφ). Unpacking the meanings of the fancy symbols according to (Definition 3) we understand that (i) to make the antecedent true it will suffice to make M such that all of its worlds see a φ-world, and (ii) to make the consequent false we have to make sure the context of utterance (or actual) world is not a φ-world. Let φ be p and consider the model M = (W, V), where W = {a, b} and V = {(a,p)}. Since p is true at a according to V, and since every world can see every world in W, the antecedent is satisfied, i.e., M, a, b |= S5 ▢♢p. But since V doesn't assign (b,p) the value 1, M, b, b |= S5 p ≡ V(p, b) = 1 is falsified. Therefore, (1) is not valid with respect to S5 and by extension with respect to all logics between K and S5. ■

                                                               References (2D Semantics)

Davies, M. & L. Humberstone (1980) "Two Notions of Necessity", Philosophical Studies 38:1–30.
Kamp, H. (1971) "Formal Properties of 'Now'", Theoria 37: 237–273.
Kaplan, D. (1989) "Demonstratives", in Themes from Kaplan.
Neither 'house' nor 'shed' nor 'and' refer to any physical things. Your house (that is, the referent of the expression "my house" as uttered by you; if you have a unique house) is a physical thing, which may have a shed in it, which is also a physical thing. But the expression/term 'house' refers to an abstract entity, which we take to be the plurality/class/set of all houses. Similarly for 'shed', 'dog', and so on. A particular house, a particular shed, a particular dog, may be referred to by 'house', 'shed', and 'dog', but only if the context of utterance is such that it unambiguously determines the connection between the general term and the particular individual in the domain of discourse. That much about terms.

                                                §1 And1 : Sentence × Sentence → Sentence

The word 'and' is usually introduced as a sentential connective: a function on the sentences of some underlying language. E.g. "p and q" is taken to be the sentence r which is true just in case p and q are true. This means that 'and' can be taken to be referring, with respect to some language L, to the set consisting of all triples (P, Q, R) s.t. R is true iff P and Q both are. From the category-theoretic point of view this explication of functions is certainly problematic, but that's an issue for another occasion. 

The conclusion then is that 'and', like 'house' and 'shed', both refer to sets of entities: in the case of 'house' and 'shed' we're dealing with a set of physical entities; in the case of 'and' we're dealing with a set of linguistic and thus abstract entities. I.e., those terms are similar in that they both refer to abstract entities, but different in that those abstract entities they refer to contain entities of different kinds.

                                                       §2 And2 : Term × Term → Term

It's worth noting that in some logico-mathematical settings 'and' is overloaded: 'and1' is the usual and that connects two sentences, and 'and2' is a function between not sentences but terms. For example, "a and b" is taken to be the plurality that consists of individual a and individual b. That's the logical explication of the grouping idea Mozibur mentioned. To illustrate my earlier point about 'and' referring to an object, let's look at the referent of 'and2'. It's a function from a pair of terms to another term, so set-theoretically, 'and2' refers to the set of triples (a, b, c) s.t. c is the unique term s.t. for all properties φ, we have φ(c) iff φ(a) and φ(b). For example, if a is a red rose and b is a red lobster, then 'a and b' is their 'group term' of which we can truly predicate 'is red'. That's the general, informal, idea of this second, grouping sense of 'and'. If I find any appropriate readings on that I'll append to this post later.
I'd argue against your thesis. We don't always formalise conception; simply because many concepts are not amenable to formalisation. Kant for example, made a heroic effort in trying to formalise ethics - his axiom being the categorical imperative.

What is true is that many concepts are studied, and their study turns into a scientia - that is a domain of knowledge. I use the word scientia delberately to differentiate this from modern science, which is one form of scientia.

Even in mathematics, where formalisation is important; in fact, so important in the contemporary situation that there is a philosophy of mathematics that is associated with it; formalisation hasn't always been important. For example, arithmetic as opposed to plane geometry wasn't axiomatised in Ancient Greece, but in the late 19C, over two millenia later; an interval of over 2 millenia hardly speaks of always and must.
I wouldn't call it a bold hypothesis; its been generally affirmed through out history despite suffering that being human means, and of humanity in general.

Of course, the absolute worth of humanity in Europe had been under-pinned by Christian Theology; and whilst this continues, a break in morality has occurred as acknowledged by http://en.wikipedia.org/wiki/Hannah_Arendt Arendt and others; and this moment is usually indexed by Nietzsche, though of course it had led something of an underground life till then. 

Existentialism was one of the first fruit of this break - Sartre being its most prominent exponent; and also defending it from the charge that this philosophy would only, on the whole, lead to nihilism.

This is the question that http://en.wikipedia.org/wiki/Albert_Camus Camus, for example, addresses this in one of his early writings: http://en.wikipedia.org/wiki/The_Myth_of_Sisyphus The myth of Sisphus. He starts by asking the question, does the absurd (the world without being grounded in God) require suicide? He answers:


  "No. It requires revolt."


He acknowledges the life of the body as well as the thought that animates a man:


  In a man’s attachment to life there is something stronger than all the ills in the world. The body’s judgment is as good as the mind’s and the body shrinks from annihilation 


He then compares the absurdity of man's life with the situation of Sisyphus, a figure of Greek mythology who was condemned to repeat forever the same meaningless task of pushing a boulder up a mountain, only to see it roll down again. 

He ends by:


  "The struggle itself [...] is enough to fill a man's heart. One must imagine Sisyphus happy.


It is for this reason that Camus is usually taken as an Existentialist - though he himself repudiated the label; one need only recall, for example that his university dissertation was on St Augustine or his http://maverickphilosopher.typepad.com/maverick_philosopher/2009/11/czeslaw-milosz-on-simone-weil-and-albert-camus.html affinity to http://en.wikipedia.org/wiki/Simone_Weil Simone Weil, a Christian/Jewish Mystic (and so exemplary in her suffering as to cause http://www.nybooks.com/articles/archives/1963/feb/01/simone-weil/ Susan Sontag to look away) to realise the simplicity of such judgements.

In the context of another tradition, Islam; http://en.wikipedia.org/wiki/Jihad jihad aligns itself with revolt in Camus sense; of course now it has a different sense of this word is much more publicly prominent; but then so does 'revolt' or rather 'revolution' in the Marxist tradition.
Aristotle  invented the statement "There will be a sea battle tomorrow" to question his own assumption that all propositions have a definite, time independent truthvalue (De interpretatione IX). According to propositional logic the following disjunction is true:

“There will be a sea battle tomorrow or there will be no sea battle tomorrow.”

If each proposition has a well-determined, time independent  – but possibly unknown – truthvalue , then already today is determined what will happen tomorrow. 
The example shows that propositional logic supports determinism.

If you want to avoid such determinism you can employ modal logic and introduce the new  operator "possibly". The refined statements are "Possibly there is a sea battle tomorrow" and “Possibly there is no sea battle tomorrow”. The refined disjunction now reads

“Possibly there will be a sea battle tomorrow or possibly there will be no sea battle tomorrow.”

This is a weak statement which does not support determinism.

Anyhow, I do not see how this example from logic relates to the concepts of vagueness or indeterminacy.
I'm not sure about this specific topic, i.e. the fallacy consisting in :


  "interchanging" 'Nec(p -> q)' with 'p -> Nec q.' 


But both modal logic and analysis of http://www.iep.utm.edu/fallacy/#Modal fallacies dates back to Aristotle : the second one in his De Sophisticis Elenchis (Sophistical Refutations), an appendix to the Topics. 

The huge modern literature on Aristotle's syllogistics include :


  
  Jan Lukasiewicz, http://rads.stackoverflow.com/amzn/click/B006GPIZ50 Aristotle Syllogistic From the Standpoint of Modern Formal Logic (1957).
  Paul Thom, http://rads.stackoverflow.com/amzn/click/0792339878 The Logic of Essentialism An Interpretation of Aristotle's Modal Syllogism (1996).
  Richard Patterson, http://rads.stackoverflow.com/amzn/click/0521522331 Aristotle's Modal Logic: Essence and Entailment in the Organon (1995).
  Adiane Rini, http://rads.stackoverflow.com/amzn/click/9400700490 Aristotle's Modal Proofs: Prior Analytics A8-22 in Predicate Logic (2010).
  Marko Malink, http://rads.stackoverflow.com/amzn/click/0674724542 Aristotle's Modal Syllogistic (2013).
  


A good discussion of Modal Theorem in Aristotle [if 'Nec(p)', and 'p -> q', then 'Nec(q)'] is in Jonathan Barnes, http://rads.stackoverflow.com/amzn/click/0199568170 Truth etc Six Lectures on Ancient Logic, (2007), page 463-on.

A lot of useful material is discussed into Jonathan Barnes, http://rads.stackoverflow.com/amzn/click/0199577528 Logical Matters Essays in Ancient Philosophy II (2012), Ch.3 : Logical form and logical matter (page 43-on), in particular the section on Necessity and syllogism (page 87-on) where we can find the material on the ancient debate regarding the relation between 'necessity' (the modal operator) and consequence (what is expresse in a valid argument by the word : 'therefore').
There are a few areas where "meta-mathematics" can take on political overtones:


Political word problems. See http://www.radicalmath.org/main.php?id=SocialJusticeMath Radical Math as an example of an organization that promotes this sort of thing. (Personally, I'm skeptical that this is a good idea.)
Acceptance into the Mathematics profession. See http://www.nctm.org/resources/nea/mt2000-12-782a.pdf this paper for one take. Women such as http://en.wikipedia.org/wiki/Sophie_Germain Sophie Germain are a rarity in math history, and she was only able to have success by bucking societal expectations for women. (I think understanding this history and changing the gender-imbalance situation that persists is a noble endeavor.)
How to do math "correctly." Not political in the sense of government politics, but political in the sense of it being a person-to-person issue more than a did-I-do-my-addition-properly issue. This comes up more often in statistics where the "right answer" is harder to verify. One example is the debate between Bayesian and non-Bayesian approaches to statistics.


There are, of course, the "social sciences," which try to make use of mathematical models in a similar way to how physics did. Since the social sciences are themselves political, mathematics can be politicized in this way as well.
I think you need to distinguish between "concepts" in general and the specific type of concept you raise here, models. Models are useful simplification of mechanisms. They are not "false," they just simply don't tell the whole story. Good models tell enough of the story, where what we mean by enough depends on the specific practical problem at hand. There's a well-known quote, "All models are wrong, some are useful," I think that's what you're getting at.

Note that trying to measure how close to "substance" a concept is would necessarily be wrong in some cases, since how close to "substance" the model is depends on the question. Some models for population might, for example, be very accurate in the next year or so but be very wrong in the long term.
Consider two points on the surface described by https://www.google.com/search?q=1%2Fx*1%2Fy z = 1/x * 1/y.  Is there a line on that surface that connects them?

Imagine two airplanes flying in the same direction 500 miles apart.  If they both go straight, will their flight paths be parallel?

If I have a drop of water, and I add a drop of water to it, how many drops of water do I have?

Our "natural intuitive interpretation" of mathematical concepts is highly context-dependent.   If you're talking about apples, then 1+1=2, but if you're talking about drops of water 1+1=2 doesn't make any sense.  If you're talking about flat surfaces, Euclidean geometry follows naturally, but not all surfaces are flat--and I don't just mean that we live in three dimensions instead of two.  3D space itself http://en.wikipedia.org/wiki/General_relativity can be curved!

Euclidean geometry, like Newtonian physics, seems intuitive and natural to us because it's simple, and it's a good approximation of how the world actually behaves most of the time.  However, that behavior is not a necessary fact.  In a universe where physics worked quite differently, I doubt the inhabitants would find such things as obvious as we do.
Pragmatism, Empiricism and Nominalism are different positions, that are each best understood as responding to quite different kinds of questions.


Nominalism is a metaphysical view about what kinds of things exist. According to nominalists the only things that exist are particular individuals, like Socrates. On this view although we can talk about abstract, universal ideas like "humanity" it is really only particular humans, for instance, that exist.
Empiricism is an epistemological view about how we know things. Specifically empiricists believe that all of our knowledge is acquired through sensation. 
Although this is somewhat controversial, I think it's best to understand pragmatism as a view about truth. Namely, a pragmatist is a person who believes a sentence is true if and only if it is what we would all agree to, at the ultimate end of inquiry. That is, the truths are the things we'll believe when all the evidence is in, as it were.


Now how are these three positions related? Well, to my knowledge every single pragmatist is also an empiricist, although the converse doesn't hold. Nominalists tend to be empiricists and vice versa, because one of the reasons to be suspicious of universals, as nominalists are, is that it's hard to make sense of how we could ever come to know them, given that we don't experience them. Pragmatists, I would suppose, would tend to be nominalists too, since they're all empiricists, but I wouldn't be surprised if some pragmatists denied nominalism. (All you have to think is that all the evidence isn't in yet about whether we need numbers to be real for science to work, for instance.)
I've never done much modal logic, but I think I can outline a proof for you assuming I'm reading this correctly.

(□(a>b)&◊(a&c))>◊(b&c) 

meaning 

((NecessaryThat(a IMPLIES b) AND PossibleThat(a AND c))
IMPLIES
PossibleThat(b AND c)

So, proceed as follows:


Assume N(a IMPLIES b) AND P(a AND c)
P(a AND c)
P(a) AND P(c)
P(a)
P(c)
N(a IMPLIES b)
P(a IMPLIES b)
P(a) IMPLIES P(b)
P(b) (via 4 and 8)
P(b) AND P(c) (via 9 and 5)
P(b AND c)
THEREFORE...  1 IMPLIES 11


You might have to tweak this to fit the exact axioms open to you, but I think the basic approach is sound.

EDIT: Try this kind of approach


a -> P(a)
b&c -> P(b&c)
((a->b)&c) ->P(b&c)
(N(a->b) & c) -> P(b&c) 
etc...

I think that this explanation of Stephen Cole Kleene, http://rads.stackoverflow.com/amzn/click/0486425339 Mathematical Logic (1967 - Dover reprint) [pag.10 - footnote 12] is a good short elucidation of the "formalization" of conditional in a truth-functional setting : 


  The ordinary usage certainly requires that "If A, then B" to be true when A and B are both true, and to be false when A is true but B is false. So only our choice for T in the third and fourth lines [of the truth-table entered for A and B, i.e. the lines F-T and F-F] can be questioned. But if we changed T to F in both these lines, we would simply get a synonym for ∧ ["and"]; in the third line only, for ↔ [i.e. the bi-conditional]. If we changed T to F in the fourth line only, we would loose the useful property of our implication that "If A, then B" and "If not B, then not A" are true under the same circumstances [...].


The truth-functional definition of propositional connectives is a "model" that in some cases "fit" quite well with our usage in natural language (negation, disjunction, conjunction) and not so well in other cases (conditional).

When we assert a sentence A we are expressing the fact that we "judge it" to be true.

Thus, asserting the conditional A → B means to "judge" it true. 

When mathematicians (like Frege) introduced the truth-functional conncetive , they have in  mind one characteristic property of the connective, viz., the rule of modus ponens. With this rule, we assert A → B and A; in this case, the first assertion "exclude" the case when A is true and B false, while the second assertion "exclude" the two cases where A is false.

Thus, we have only one possibility left : B true, and this is what we expected. 

In our "ordinary" use of the language we seldom assert a conditional "if ..., then ___" when we know the antecedent to be false; but the "modelling" of mathematical logic fit quite well with the use in ordinary mathematics.

The very important "context" in mathematics is the following :


  
    Σ⊨φ;
  


in this case we say that Σ entails φ. The condition validating the relation of "entailment" is that : every interpretation that satisfy (all the sentences in) Σ will also satisfy φ; or, equivalently, there is no interpretation such that all of Σ are true and φ is false.

This "context" is commonly used when we assert that some thorem (φ) follows from a set Σ of sentences, e.g.the axioms of a theory.

When Σ={σ}, from σ⊨φ we have that : ⊨σ→φ.

This result establish a strict connextion between the conditional (→) and the relation of entailments (⊨). The two are different relations, but the above link between them is so useful that we "accept" the "not perfect" fit of the conditional with our natural language habits.

  Is the universe just a mathematical construct?


Its probably worth beginning by saying that the question as constituted is one that can really only be asked in the tradition of logical positivism which takes ontology as a matter of convention; whilst this may be a useful manouever, or have useful ramifications I don't think it can in fact be true.

You can set up such a simulation of a universe on a computer and watch it evolve, and even perhaps evolve life. However, this is a simulation, we have no procedure to actually change the laws of physics (even if in the recently fashionable multiverse view, at least in Smolins take on it, there could be different universes with different laws).

The point is that the world is phenomenological; that is it acts on us; it is not simply structure; though that is a neccessary part of its constitution.


  The "real universe" seems no different from a hypothetical one, apart from the fact that we exist as part of the consequences of the laws acting on the initial conditions.


Existence is a primary notion; however in medieval philosophy (Averroism) it was asked whether essence preceded existence (this then was religously framed, from the rationalist falsafa tradition in Islam); this in a sense is your question. Is the essence of the universe mathematical, and does essence precede existence?


  What evidence, if any, suggests the existence of this universe is different than mathematical existence?


As the reformulation of the question shows essence is different from existence; for one to actually have a universe, one cannot merely posit a possible one (though people have). Sartre later turned this around, saying that essence suceeded existence; this is the attitude of contemporary physics: we live in a universe, let us see if we can extract its (mathematical) essence.

Following Sartre, and in another direction, this question is simply asking can there be other universes? In contemporary physics this option is fore-closed by Occams Razor - even though it is a respectable one - and has been source of highly speculative (physically based) cosmologies - the multiverse again.
You're correct that moving from the integers to the rationals does not fit, because the generalisation that inductive reasoning refers to is a generalisation of statements (or predicates) not of the objects themselves. 

For example, you could generalise from the statement "all the even numbers above 3 we ever tried can be written as the sum of two primes" to "all of them can" - and that's an example of inductive reasoning. (There's there's no known deductive proof of this conjecture.)

Even numbers can be "generalised" to all numbers, but that's different to inductive reasoning. We don't move by inductive reasoning to "all whole numbers above 3 are the sum of two primes" because we find that 11 doesn't work. 

Generalising generally vs inductive reasoning

"Generalising" the integers to the rationals is a superset relationship, which I can write very simply in maths notation, because it's like 

The generalisation that inductive reasoning makes is:




hence we believe that 



I've not generalised A to B, I've generalised P from being true in all of A to being true throughout B.

Better abstractions

In fact, you could say that the rationals are an example of a field, whereas the integers are only an example of a ring - a generalisation of a field. Mathematicians "generalised" (not inductive reasoning) from the number systems, matrices and other examples to make the abstractions of groups, rings and fields. Some theorems about fields can be generalised to rings, which would be an example of inductive reasoning were it not for the fact that mathematicians don't admit inductive reasoning as proof, and proved these generalisations deductively! 

(Proof by induction is, perhaps confusingly, an example of deductive reasoning, but is often something you do after having used inductive reasoning to conclude that you might want to prove your statement.)

A deep understanding of inductive reasoning isn't necessary for good abstractions, but good abstractions result from deep understanding of diverse examples.

Summary

Abstraction is a form of generalisation, and supersets are a form of generalisation, but inductive reasoning specifically means generalising statements from examples to every example.

All inductive reasoning is generalisation, but not all generalisation is inductive reasoning.
I think Lewis's main argument is relatively clear. It's just a modus ponens:


If it is a fact that moral code M1 is better than moral code M2, then there must be some absolute standard A in the light of which M1 is judged better.
It is a fact that moral code M1 is better than moral code M2.
Therefore, there must be some absolute standard A in the light of which M1 is judged better.


(1)-(3) is clearly a valid modus ponens argument, so if the premises (1) and (2) are true, (3) has got to be true too. The more interesting question, as usual, is why we should believe (1) or (2) true.

It is hard to deny (2). It's hard to imagine somebody giving us a plausible reason to think there are no instances of one moral code being superior to another. (It might be hard to tell which of two codes is superior. For instance, M1 might be superior to M2 in one respect, but worse in another. But that won't detract from Lewis's point.)

(1) is the odder looking claim to me. Burning paper is hot; molten lead is hotter, but I can't see why that would imply that there is some absolute standard of hotness. (1) looks like a more plausible claim about a property like grey. If one thing is bluer than another, it looks more likely that there has to be some canonical instance of grey. Still though. I'm not sure that's even right. Suppose we have a spectrum of fifty shades of grey. The ones at the edges will look "whiter" or "blacker" but that might not mean that there are a couple of different shades in the middle none of which is definitively grayer than any other. 

So my vote is that Lewis's argument here is valid, but of questionable soundness, because (1) is false, or at least in need of serious qualification and defense.
If the keyword "Kant" is particularly significant here, then what is meant by "intuition" is likely radically different from what we ordinarily mean by that term.

The most prominent place that Kant writes about intuitions is in his second critique--the Critique of Pure Reason. What he means by 'intuitions' are things immediately given. We have empirical intuitions, such as the experience I am now having of this particular blue. Such intuitions are given in experience. There are also a priori intuitions, such as time and space. These are "Pure Intuitions" because they are not derivative--we do not experience time after or based on our sense experience. Sense experience is only possible in time. So time is a necessary prerequisite for there to be any experience whatsoever. The Pure Intuitions of space and time are forms that must be a priori present for perception or experience to occur.

Intuitions in Kant's sense are not fallible, largely because intuitions are not judgments. My experience of this particular blue cannot be said to be true or false, except perhaps metaphorically or in comparison with some other experience. My judgment that the blue I experience actually derives from specialized interactions between my retinas and an object outside of me that is really blue, this can be true or false. That is to say, judgments are fallible.
Ad hominem is the name of type of informal fallacy. I say that because the problem is that informal fallacies are more "rules of thumb" than hard fast strictly-defined philosophical terms. So often many different names will apply to the same fallacy or many different fallacies will occur in the same fallacious statement.

That said, this looks more to me like the genetic fallacy.



To illustrate:

(1) I say P.
(2) You say, "you suck!"
(3) You say "I won't listen to you because no one from New Jersey is believable."


Then (2) is ad hominem
and (3) is genetic fallacy.

You might say (3) is a type of (2), but the basic rule is when a more specific fallacy is known, we use that name. The main difference is that genetic fallacy says I don't need to listen to you because you're a ... [white supremacist / homophobe / liberal / conservative / black man / non-native speaker of English] whereas ad hominem responds with the slur rather than claims it is a reason not to address the argument.

So in this case, the attack seems to be motivated in part by the use of youtube as a source. But then using the use of youtube as a source to impugn the claims made.
Classically, this is procedural or behavioral knowledge.  Both propositional and procedural knowledge are forms of 'alethic' knowledge about what does and does not happen in reality.  I would contrast it with moral or 'deontic' knowledge about what is expected by other people (including conscience which is about what we expect of ourselves), 'doxastic' knowledge about how we communicate and decide (including 'hedonic' knowledge about what does and does not affect your own homeostasis in your own terms).

The upside of propositional logic is that via modal inflection it can encode most of the other forms of knowledge in ways that allow them to be referenced and evaluated.  In the case of procedural knowledge, we can write algorithms and say 'It is effective when faced with this arrangement to invoke this algorithm' as a propositional statement.  Likewise most of the other forms of knowledge can be encoded as complex references within a propositional language.
The task is to prove the correspondence between the so-called T axiom (φ → ♢φ) and the reflexivity of the accessibility relation. To do so we must be very explicit about the following important distinction:


  Definition 1a. (Frame) A frame F = (W, R) is a directed graph.
  
  Definition 1b. (Model) A model M = (F, V) is a labelled directed graph.


Directed graphs are simply a bunch of connected points (in this context, the points are called 'worlds', the connections form the 'accessibility relation'). Labelled directed graphs are directed graphs but with each point having a label that tells us which propositions are true at that point. We want to show that:


  Fact 2. Formula (φ → ♢φ) is valid  iff R is reflexive.


It turns out, however, that "[t]ruth in models is not appropriate for bringing out such correspondences" between formulas and properties of the accessibility relation, "as special valuations [in our terminology: labelings] may validate axioms even though the underlying frame has no nice behavior at all" (van Benthem 2010, p. 101). So what we really want to show is that:


  Fact 2. Formula (φ → ♢φ) is valid  on frame F = (W, R) iff R is reflexive.


The definition of frame validity is simple to formulate:


  Definition 3. (Frame Validity) Formula φ is valid on a frame F = (W, R) iff for all models M = (F, V) based on that frame and worlds w ∈ W, we have (M, w) |= φ.


The "for all models" is the key. Let's now tackle the task by splitting it into its two directions:


  Fact 2a. (Only if direction) Formula (φ → ♢φ) is valid  on frame F = (W, R) only if R is reflexive.


Proof. We prove by contraposition. Assume that F = (W, R) is not reflexive, with the goal of showing that (φ → ♢φ) is not valid on F. To show that (φ → ♢φ) is not valid on F it will suffice to present a counter-model M = (F, V) based on F where for some w ∈ W, (M, w) |= φ ∧ ¬♢φ. Consider M = (W, R, V), where W = {w}, R is irreflexive and V(p) = W. Informally, we have a single node called 'w' with the label 'p' meaning that proposition p is true at world w; we have no other worlds. Now, since p is true at w, we have (M, w) |= p. But since there are no other worlds and (by irreflexivity of R) w cannot access itself, (M, w) |= ♢p is false. That means that we have a pointed world (M, w) s.t. (M, w) |= p ∧ ¬♢p. That means that (φ → ♢φ) cannot be valid on frame F.                                                                                 ■


  Fact 2b. (If direction) Formula (φ → ♢φ) is valid  on frame F = (W, R) if R is reflexive.


Proof. We assume that R is reflexive, with the goal of showing that (φ → ♢φ) is valid on reflexive frames. Let M = (F, V) be any model based on such a frame. For an arbitrary point w ∈ W, we want to show that (M, w) |= φ → ♢φ. Suppose, for contradiction, that (M, w) |= φ → ♢φ is false. That will be the case iff (M, w) |= φ and (M, w) |= ¬♢φ are both true. By the box-diamond duality, this second conjunct is true just in case (M, w) |= ▢¬φ is true. But by the first conjunct, world w satisfies φ and since R is reflexive, it means that w sees a world (namely itself) that satisfies φ, so (M, w) |= ▢¬φ cannot be true. Since the assumption that (M, w) |= φ → ♢φ is false led to a contradiction, we know that (M, w) |= φ → ♢φ is true. Since (M, w) was arbitrary, we know that (φ → ♢φ) is valid on reflexive frames.                ■

Those two facts allow us to justify the desired claim, namely that:


  Fact 2. Formula (φ → ♢φ) is valid  on frame F = (W, R) iff R is reflexive.


Proof. Combine proofs of Fact (2a) and (2b).                                                                                          ■

                                                                   References

van Benthem, J. (2010) http://rads.stackoverflow.com/amzn/click/157586598X Modal Logic for Open Minds, Stanford, CSLI Lecture Notes #199; §9.1.
Holliday, W.H. (2012) http://philosophy.berkeley.edu/people/page/123 Modal Reasoning, Lecture Course (Spring), UC Berkeley; Lecture 5.
Pacuit, E. (2009) http://web.pacuit.org/classes/logicai-cmu/ml-notes.pdf "Notes on Modal Logic"; §3.1.
Gödel's work was successful. See Christoph Benzmüller, Bruno Woltzenlogel Paleo: "Formalization, mechanization and automation of Gödel's proof of God's existence", arXiv (2013). Gödel's logical deduction has passed an automated proof checking procedure. An overview including the philosophical environment (Anselm) is given https://en.wikipedia.org/wiki/G%C3%B6del%27s_ontological_proof here.
There appears to be an issue of Referential Opacity going on here.  Which is of course rather apt, given your choice of subject.  First off, though, let's get the technical point out of the way: you need to use predicate logic rather than purely propositional logic in order to make progress here.  What you seem to be doing is using a constant term Quine and having it feature in two modal claims.  Ideally, your two premises will look something like the following:


□ is_Human(Quine).
◊ ¬ E x . x = Quine


In his http://comet.lehman.cuny.edu/fitting/forclasses/phil76600fall2013/Readings/3grades.pdf "Three Grades of Modal Involvement", Willard V.O. Quine (I'll call the author Willard to avoid ambiguity with your named item Quine) discusses three ways in which we might discuss modal statements.  The first is as an operator on sentences.  An intuitive interpretation of your first premise using the first grade of modal involvement would be to say that the statement "Quine is Human" is necessarily true (or true in all possible worlds).  The second way is as an operator on propositions,  We would use the second grade of involvement to say that in all worlds, Quine is human.  And the third is as a sub-propositional operator, which allows for modal operators for themselves to play a part in the composition of more complex propositions - e.g. there is someone called Quine, such that it is necessary that that person is human.

Now Willard thinks that the first analysis of necessity is okay and the second can be phrased in such a way as to reduce it to the first.  But his problem with the third comes from the question of what object, if any, we should take as the reference of some term within the confines of the necessity operator.  Necessity does strange things to our sense of how we ought to interpret descriptive singular terms; for example, while it is true that the number of planets in our solar system (8, for the sake of argument) is such that it is greater than 5, and while it is also necessary that the number 8 is greater than the number 5, it does not thereby follow that it is necessary that the number of planets in our solar system is greater than 5 (that it is necessary that there are at least 5 planets).


□ 8 > 5
number_of_planets = 8
□ number_of_planets > 5 ???


The lesson Willard wants us to learn from this is that we can lose track of the reference of a particular individual term whenever we move it across a modal operator.  So the problem with your argument, on this account, is that you are using a constant, Quine, to track your intended reference across possibilities, when in fact it is not at all clear what, if anything, you are tracking when doing so!

Now one way around this objection against certain kinds of modal technology is to move from thinking about Quine as a referring constant term (and hence involved in the third grade) to what it means to have the essential properties of being Quine - to rephrase your concerns in terms of a Trans-world Identification predicate (and the safe forms of the second grade).  Let's assume that we have some property of what it takes for something to be Quine, and say that that thing Quineizes.  Now your two premises take a very different form indeed!


□ V x . Quineizes(x) -> is_Human(x)
◊ ¬ E x . Quineizes(x)


And these two coexist perfectly happily with one another, because the implication can still obtain even if nothing does in fact Quineize.  This field of ideas is explored in the concept of http://plato.stanford.edu/entries/identity-transworld/ Trans-world Identities - it too has quite a few problems, but if treated formally rigorously, it promises to avoid a lot of the sources of Willard's strictures against modal involvement.
If your point truly was about the soundness of his argument, as others have aptly answered, you are correct.

It's worth noting, in addition, that limited to the details you've provided, your interlocutor could not legitimately accuse you of committing a "denying the antecedent" fallacy, because you did not deliver an argument, but a proposition.  

You merely "pointed out that p was false," which is a proposition - it has a truth value.

A challenge of interpreting informal debate (and all forms of informal argumentation) is inferring formal arguments (or, for that matter, whether arguments are even being implied).

We are to assume that your interlocutor's implicit argument was (if p, then q; p therefore q) and that your response was (if p, then q; not p; therefore (if p, then q; p therefore q) is unsound).  However, your interlocutor may have inferred your argument to be (if p, then q; not p, therefore not q) in which case he would of course have been correct is his accusation.  
It doesn't. Consider the space of propositions Prop = {p, q}. These propositions are the things that are possible/necessary/actual. Consider the model M = (W, R, V, @) with possible world space W = {w1, w2}, a distinguished world @ = w1, an accessibility relation R = ∅, and a valuation V(q) = {w1} and V(p) = {w2}.  Intuitively, we have there two worlds w1 (which is the actual world) and w2. We have p true at w2 and q true at w1. The R is such that no world sees anything. For this model we have the following fact:


  Fact 1. For any proposition φ ∈ Prop, there exists a world w ∈ |M| s.t. M, w |= φ.


Proof. Let φ be an arbitrary proposition in Prop = {p, q}. If φ = p, since V(p) = {w2}, we have M, w2 |= p. If φ = q, since V(q) = {w1}, we have M, w1 |= q.  Since φ was arbitrary, we have established the fact.

The conclusion that you want to draw, however, does not hold for M:


  Claim 2. For any proposition φ ∈ Prop, if φ is actual, then φ is necessary.


Disproof. Let φ = q. Consider the model M described above. In M, φ is actual at w1, since V(φ) = V(q) = {w1} = {@}. But is φ necessary in M, that is, is it true that for any world w ∈ |M|, we have M, w |= φ? Since w2 ∉ V(q) = V(φ), we have M, w2 |/= φ. Therefore φ is not necessary, so Claim 2 is false.

Therefore, (Fact 1) doesn't entail (Claim 1).
I got a reply from a professor acquaintance: □◊A → A is simply illegitimate, comments were right.
Some comments.

1) Well before Cantor, it was already known that we may "have troubles" in comparing infinite collections of numbers ; see at lest http://en.wikipedia.org/wiki/Galileo's_paradox Galileo's paradox.

2) Of course, the purported proof : "that there are equally many reals and natural numbers, because we can make a list of natural numbers and assign a real number to each natural number" is simply wrong. We may pair, e.g. the natural number 2 with the real number 2.0, of course, but we will have in any case "unpaired" numbers like sqrt(2).

3) The history of mathematics is full of "wrong" proofs : they have been corrected. See for example http://en.wikipedia.org/wiki/Giovanni_Girolamo_Saccheri Girolamo Saccheri's proof of http://en.wikipedia.org/wiki/Parallel_postulate Euclid's fifth postulate in his Euclides ab omni naevo vindicatus (Euclid Freed of Every Flaw - 1733). 

4) Also if it is a "minority" part in the mathematics community, there are some mathematicians which do not agree on the validity of some methods of proof commonly used by the "mainstream" mathematicians; see http://plato.stanford.edu/entries/intuitionism/ Intuitionism in the Philosophy of Mathematics and http://plato.stanford.edu/entries/logic-intuitionistic/ Intuitionistic Logic.
I'm not an expert on modal logic, so take my answer with a grain of salt. I don't recall reading any discussion of a formal system where a quantified modal formula was an axiom of the system. (There may be some that I haven't come across, of course.) The point of adding quantification, and other devices that belong to first-order modal logical such as the property abstraction operator, is to increase the expressive power of the modal logic in order to use modal logic to formalize a broader range of natural language arguments that turn upon possibility and necessity.

Consider, for instance, the following "argument":


Necessarily, the President of the United States is the President of the United States.
Barack Obama is the President of the United States.
Therefore, necessarily Barack Obama is the President of the United States.


3 would appear to follow from 1 and 2 by a substitution rule. But this is surely incorrect. It turns out that the problem is a kind of ambiguity in (1). It appears that we need a way to distinguish two different ways to ascribe modal properties, we need to distinguish, in other words: 

1*. It is necessarily true of the President, that he is the President. 

from

1**. It is true of the President, that he is necessarily President.

1* is a harmless conceptual truth, but 1** is a patent falsehood. But (3) would only follow only from 1** and 2, not from 1* and 2. So clearly, in order to avoid making fallacious modal arguments we are going to have to have some formal machinery to distinguish between cases like 1* and 1**. 

Notice now that there isn't really any way to do this using just propositional modal logic. We're going to need some quantifiers, and we're going to need "property abstraction", which is the modal analogue to the definite descriptor in ordinary first-order logic. (For more details, I would recommend looking at the very nice exposition of this in the relevant chapter of Fitting and Mendelsohn's First-Order Modal Logic textbook.) In other words, the addition of the machinery isn't so much just to create new logical systems, but to give us the needed expressive power to avoid some paradoxes.
Semantics is to do with meaning, and syntax is to do with form. Its quite possible to set up a deductive system and then formally prove something that is wrong; this because the semantics of the situation hasn't been taken into account. 

To give a crude example one  could formally develop a theory of happiness by scoring peoples happiness out of ten; and then you can ask questions about the total amount of happiness, or its maximum and so on; but the semantics of this situation, when thought about, is can you measure a qualitive thing such as happiness quantitively? If this is even possible, how to take account of people lying, or not being aware of the true state of their happiness: it isn't for nothing that 'know thyself' was inscribed in the forecourt of the Temple of Apollo at Delphi. All these are common objection to theory of Utilitarianism as developed by Bentham.
You are correct that until we know there are numbers, we have a formal theory but not yet any domain where the theory can be interpreted.

There are two ways to go with this. 

1) It's clear that we all know what the natural numbers are. Even if we don't have a formal theory about them, it's clear that they satisfy PA and are therefore a domain in which we can interpret the axioms and theorems of PA. 

2) We can formalize the axioms of set theory, say Zermelo-Fraenkel (ZF). Then the concept "exists" means "can be shown to exist from the axioms of ZF." Then we can say that the empty set stands for zero, and the set containing the empty set is 1, and so forth. The objects of this construction are known as the Von Neumann ordinals. 

http://en.wikipedia.org/wiki/Ordinal_number#Von_Neumann_definition_of_ordinals http://en.wikipedia.org/wiki/Ordinal_number#Von_Neumann_definition_of_ordinals

Now we can verify that the numbers 0, 1, 2, ... as defined by Von Neumann, satisfy PA; and therefore we have a set in ZF that can serve as a domain of interpretation for PA. 

The same thing is often done pedagogically with the real numbers. The usual procedure is to list the properties of the real numbers such as the commutativity of addition, the distributivity of multiplication over addition, and so forth. That's good enough for calculus class and lets you do everything you need to.

In a math major class called Real Analysis, they'll show that you can start with the rational numbers, and construct a model of the real numbers using Dedekind cuts or Cauchy sequences. They show that to the students once, and from then on everyone forgets about it and just uses the properties of real numbers. 

In practice it turns out that most of the time nobody cares about the ontology of mathematical objects. All you need is their properties. 

But ultimately you're right, if we're going to be logically comprehensive it's not enough to write down the axioms of PA; we have to show that we can formalize something that satisfies them; and that's where the Von Neumann construction comes in. 

  We cannot really prove that there is reality


What would it mean to prove reality? Simone Weil, philosopher and younger sister to the famous mathematician Andre Weil (who solved the local Riemann hypothesis) wrote in her Lectures on Philosophy - which are in fact notes compiled by one of her students:


  One can never really give a proof of the reality of anything; reality is not something open to proof, it is something established. It is established just because proof is not enough. It is this characteristic of language, at once indispensable and inadequate, which shows the reality of the external world. Most people hardly ever realize this, because it is rare that the very same man thinks and puts his thought into action ...


The right question to ask is how can we justify our knowledge of the world; this brings us to epistemology: where the notions of inate ideas, inference, deduction, analogy and authority are explicated.
Early analytic philosophy did reject "metaphysics." But it's important to understand why they would be motivated to do so. The answer for English-speaking ones is the Oxford Hegelians. In other words, Hegel (or at least an interpretation of it) was the dominant philosophy in the English-speaking world in the 19th century. One of the humorous details of how this all works out is that http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199239108.001.0001/acprof-9780199239108-chapter-5 McTaggart, now remembered for A-theory and B-theory of time was actually a Hegelian who rejected both in favor of C-theory: that time is an illusion.

More generally, these philosophers were arguing against a background of highly convoluted philosophy in general. The project of metaphysics and epistemology in their modern forms (here modern as opposed to contemporary) was largely getting out of hand to the point where there are dogmatic competing systems that seem broken. Thus, two main responses evolved: positivism, etc., that rejected metaphysics and phenomenology that side-stepped epistemological questions.

The rejection of this sort of idea of metaphysics by Russell is equally forceful with the later rejections by the logical positivists (http://www.iep.utm.edu/russ-met/ though it must be admitted he accepts metaphysics under certain definitions). I don't have specific data on Moore and Frege. But my sense is that Moore's project in ethics is compatible with a denial of metaphysics but does not require it.
You can find several equivalent sets of axioms http://www.cs.unm.edu/~mccune/papers/basax/v12.pdf here.
I am not sure that saving phenomena can be used to argue that Plato and Aristotle admitted or did not admit that different suppositions might be consistent with them. At the time Plato posed the problem of reconciling apparent motions of planets with the Pythagorean ideal of uniform circular motions not only wasn't Ptolemy's system around, but no such theory existed at all. It wasn't clear that it could be done, and there definitely was no notion of mathematized physical theories, or of the scientific method for verifying them. It took the genius of Eudoxus to produce a model that accounted for backward planetary motions at least qualitatively, in response to Plato's challenge. It was extremely clever and counterintuitive, this cleverness might have even suggested to many that it was the only right track.

This doesn't tell us that Plato believed that multiple suppositions could be used for the task. It's likely that he believed it possible on philosophical grounds, but also believed that there was a unique "true" way to do it, and that discovering it would assert the glory of the ideal. The same goes for Aristotle, at his time the only offer on the table was still a Eudoxian type model, only with many more spheres than originally, to account for more details. Aristarchus's heliocentrism couldn't account for the apparent absence of the parallax, and so didn't "save the phenomena". Who knows what Aristotle would have thought if he was presented with Ptolemy's system as an alternative. Church theologists presented by Copernicus with a similar situation chose to admit multiple suppositions rather than give up their preferred cosmology. 

Also, there is a difference between abstraction, which he may have believed was unique, and trying to guess at the unseen from the visible, which by common daily experiences can't be done uniquely. As Aristotle says in http://www.perseus.tufts.edu/hopper/text?doc=Perseus%3Atext%3A1999.01.0052%3Abook%3D4%3Asection%3D1010b Metsaphysics 1010b:"And as concerning reality, that not every appearance is real, we shall say, first, that indeed the perception, at least of the proper object of a sense, is not false, but the impression we get of it is not the same as the perception". When Aristotle was inferring his theory of natural and forced motions from pulled carts and falling rock and feather he was abstracting, but when Eudoxus and Calippus were attaching planets to homocentric spheres they were just speculating about a mechanism behind the visible motions. It's unlikely that Aristotle believed that specific arrangements of inclination angles and rotation speeds they came up with, which were still http://www.jstor.org/stable/41134047 refined and contested in his time, were uniquely suitable. Aristotle made his own additions to the arrangement to connect spheres for different planets into a single chain driven by his unmoved mover, which required adding counter-spheres in between to prevent planet specific motions from being transferred. In other words, he was aware that mathematics can be altered to fit a theoretical goal without affecting the phenomena.

We know that soon after Aristotle's time Epicurus criticized Eudoxean models exactly by pointing out underdetermination of intrinsic properties by observable ones, see Sedley's https://www.academia.edu/3051043/Epicurus_and_the_mathematicians_of_Cyzicus Epicurus and the Mathematicians of Cyzicus, presumably such criticisms were familiar to him already. Here is Epicurus arguing that Eudoxean planetaria, built based on "mathematical suppositions", reflected the wrong properties:


  "All that this leaves is a pretence and a diehard dogma that the indications on the instrument create an analogy that corresponds with what we see in the heavens. For our friend must, it seems to me, make the distinction: (a) that when he argues about the cosmos and what we see in the cosmos he is arguing about a certain image arising from certain accidental properties of things passed through the medium of vision into a thought-process or into a memory-process permanently preserved by the mind itself, quantities, qualities; but (b) that when he argues about the indications on his instrument he is arguing about the intrinsic properties of an object".


In other words, intrinsic properties of planetaria reflected only accidental properties of the cosmos, so according to Epicurus not only are "mathematical suppositions" for saving astronomical phenomena not unique, but they do not even stand a chance of matching the reality. It is likely that Aristotle would not go that far.
Here is the short mathematical answer :


An ordinal is compact if and only if it is a successor ordinal.
A cardinal is compact if and only if it is finite.


Here we are assuming the natural "order topology" is used.  The result for cardinals follows from the result for ordinals since an infinite cardinal must be a limit ordinal.

If λ is a limit ordinal, then λ+1 is a "one-point compactification" of λ which we might loosely express as :


λ = [ 0, λ )
λ+1 = [ 0, λ ]


borrowing our notation from that of intervals in the real line.

The notion of compactification you describe in your question is strongly Euclidean and so does not readily apply to the ordinals in general.  However, because it is tempting to view the ordinals as a line in the plane, there are number of nifty pictorial representations of infinite ordinals, drawn in a compact region of the Euclidean plane. Here is one I found via google images of the limit ordinal ωω as a spiral :



Intuitively, this type of Euclidean pseudo compactification would only work for countable ordinals such as ωω, pictured here. One can (almost) imagine extending this picture to a higher countable ordinal by replacing each ordinal in this depiction with a copy of ωω to obtain something like ωω2 perhaps(?) (I'm not sure).  Imagining further substitutions beyond that, things become less clear.  The first uncountable ordinal ( ω1 ) would be at the "limit of the limit of the limit ..." of this process - if that makes any sense.

This sort of doodling is fun, but unfortunately it does not say anything about the formal, topological view of compactness among the ordinals.



EDIT : When one considers the class of all ordinals, compactification seems wholly inappropriate.  Re-reading this post, I think that most of what I've said, after the initial mathematical statements, is waffle.  Probably just a shameless excuse to squeeze in a pretty picture.  The whole idea of closure is contrary to Cantor's formulation, as suggested by @Conifold 's comment below, even if it is well-defined from a topological point of view.
In general, you can understand the term proposition as:


  1. The string of symbols that forms a truth-bearer sentence, a declarative sentence.
  
  2. The "meaning" (or the content) of a declarative sentence.


Different authors used the same term in different sense of words, so it can be confusing.

To avoid confusion, we can use the term statement (or just sentence) to refer (1), as it is done in mathematical logic, and the term proposition to refer (2).

See Strawson's article: http://www.eecoppock.info/Presupposition/Readings/strawson50.pdf On referring (1950, this is a critique of Russell's Theory of descriptions)
Propositions are moves in a language game that intends to describe things.  They have truth value, in the sense that what they propose is meant to be considered as an assertion describing reality.  But there are other language-games, which are not primarily concerned with this task.

Statements of possibility are generally not informative.  They are moves in the game of thinking itself, or in some sub-game about thinking, like auditing understanding.  They inject a parameter over the range of possible worlds, and request that one's listener consider whether the parameter with possibility has been considered adequately.  In doing so, they generally put you farther from certainty (correct or otherwise) rather than closer to it.

They are not even informative statements about alternative worlds.  When would one say "It might be dark."?  Not when one thinks this is information the listener does not have.  If he did not know of this possibility, he would need a lot more information about the nature of light, in order to care.  And any of that information would be more useful to contribute.  Instead one says this when one thinks the plans so far do not adequately consider that dimension.

Since they do not cover descriptive ground, but instead open new ground, they do not contribute directly to understanding in the same way.  So such statements do not play the role propositions are meant to fulfill.  It is possible to interpret them as propositions, but doing so does not allow them to perform their intended function.  If someone contributes "It might be dark." and your only response is to affirm the possibility, he has failed to communicate.

(Answering Pacerier's silly question, if Mary asks whether it is dark outside, and John replies that it might be, he is agreeing that he had not to that point considered whether or not it was, and whether it should have any effects.  He might only be so agreeing in excessive politeness, if he is fairly sure it does not matter.)
Your articulation:


  "An argument is valid IFF the premises are false or the conclusion is true".


misses an important feature in the textbook's definition. Namely, you've lost the must, but the must is crucial. 

The validity of an argument does not hinge on the truth or falisty of its premises or the truth of its conclusion. Instead, validity looks at the sum of all of the operations and rules of inference in an argument and evaluates it in light of every possible condition of the truth and falsity of every premise and the conclusion.

E.g., consider the following two arguments:

Argument 1
(1) If the moon is made of cheese, Kaguyahime lives there.
(2) The moon is made of cheese.
Therefore Kaguyahime lives there.


This argument is valid on your definition (at least one false premise). And valid on the must definition -- if the premises are true, then the conclusion must be true.

Argument 2
(1) The moon is smaller than the sun
(2) The moon is not made of cheese
Therefore, Apollo 11 went to the moon


This argument is valid according to your definition -- it has a true conclusion. But it is not a valid argument on the textbook definition. Why? Because there is an imaginable set of the variables where the premises are true and the conclusion is not (it is not the case that the conclusion must be true if the premises are true).

WHY?

Argument 2 can be rewritten:
(1) S
(2) C
(3) A

Per the fundamental rules of logic, S can be either T or F, C can be T or F, A can be T or F. This gives us 2^3 (8) possible arrangements of these variables. And one of these is this: S is true, C is true, and A is false. This breaks the must

Thus, your sentence is not an accurate articulation of validity because you've lost the modal consideration.
Quine's Philosophy of Logic, chapter 5 "The Scope of Logic" distinguishes logic from set theory in that logic gives no ontology and set theory gives one.  He even says logic can "simulate" ontology by using schematic variables which look as if they took classes as values, but they actually do not.  In contrast "When we take on set theory forthrightly and without simulation, we take on both vocabulary and ontology."   That is page 72 but the thought recurs in slight variants throughout the chapter.  

The essay on Whitehead discusses the logical issues at length, but so far as I can see it does not specify that the difference between logic and set theory is that set theory has an ontology.  Albeit numerous web sites claim Quine in this essay calls second order logic "set theory in disguise," the essay does not seem to discuss second order logic at all and does not seem to contain the word "disguise."  I suspect that a reasonable interpretation of what Quine is saying, somehow morphed over time into a factitious quote.  

The logical issues are discussed again in Quine's book Set Theory and its Logic but I do not find concise quotable expression of the philosophic claim that this is difference between logic and set theory .
The general view amongst Cosmologists is that the fate of the Universe depends on its overall shape or geometry.

The shape of the universe depends on the density of matter contained therein and is usually expressed using a parameter called the density parameter, which is the ratio of the observed density to the critical density. (Naively, one might say the more matter the more gravitational pull, while a smaller density of matter means there is not enough gravity to pull it all back together.)

The critical density is currently estimated to be approximately 5 atoms (of hydrogen) per cubic meter.

The observed density of ordinary matter is estimated at 0.2 atoms per cubic meters.  This leaves us having to estimate the density of the exotic dark energy.

If the ratio is greater than 1 (high density, positive curvature), then the geometry of space is closed, meaning that it has a finite future.  This case corresponds to our universe having a spherical shape.

If the ratio is less than 1 (low density, negative curvature), then the geometry of space is open, meaning that it has an infinite future. This case corresponds to a saddle shaped universe.

Finally, if the ratio equals 1 (the "Goldilocks" scenario, no curvature), then the geometry of space is flat, as in the Euclidean geometry we learned in school.  Here, the universe expands forever, but the rate of expansion decreases and asymptotically (tends to) zero (over time).

When dark energy estimates are included, our current best guess is that our universe is within 0.4% of being flat, implying an infinite future.

Regarding the past, our science and observation strongly suggest that our universe has a finite past.



In recent years, a multiverse view of reality has become mainstream.  Here, our universe is just one instance of many universes that make up a multiverse.  In this vision, the multiverse is infinite in both directions.
The best example of mathematics that involves no numbers comes from philosophy.  Propositional logic is mathematics.  How is it somehow "really" about numbers?

But modern mathematics consists mainly of things that are not numeric, but are made up of sets of rules.

As an extreme case consider topology. The easiest form of topology to describe is graph theory.  This discipline is largely about how complex connections between things can be, and still have relatively simple descriptions.  The ordinary representation of a graph is a set of points that can be moved around arbitrarily, and lines connecting some of them to one another.

An early basic result determines the conditions one must put on a graph in order to draw it in the plane.  Geometry is involved in an abstract way, but no measurement.  So this is a rather pure example.  The only number or measurement relevant to the statement of the problem is "two", and then only as the dimensionality of a plane.

Sure, the graphs have nodes, and you could count them.  The descriptions often contain numbers and the most basic ones amount to things like "draw three points on the left and two on the right and connect each point on one side to all of those on the other."  But even here, arithmetic is just used as part of language, not as the main actor.  In general graph-theoretic computations are seldom numeric -- they are about handling symbols that represent nodes and edges.  (In this way it is kind of like propositional logic.  They are both parts of the general field of "Symbolic Logic and Combinatorics".)

Important results are, for instance, about whether we can find instances of a graph with one compact description in another network with an unrelated compact description.  Applications are to things like computer networking, or telephone line maintenance.  The products are not numbers, but sequences of operations like a computer program.

Numbers usually enter in only after a problem is solved, to compare the efficiency of different solutions.
There is a point worth to be stressed in support of your concern; in logic we say that an argument is valid when we have a relation between premises and conclusion called : logical consequence.


  We say that a sentence A is logical consequence of a set Γ of sentences, and we write : Γ ⊨ A, precisely when it is not possible that all the sentences in Γ are true and A is false.


In a Proof by Contradiction, we assume the negation of A : ¬A, and derive a contradiction; this shows that it is not possible that all the sentences in Γ and ¬A are simultaneously true.

But this implies that in every "situation" where all of Γ's are true (we say : all of Γ's are satisfied), ¬A is false; by properties of truth, if ¬A is false, then A is true; thus, we have established the relation of logical consequnce.



Bute there is a particular circumstance: when all of Γ's are already contradictory (i.e.unsatisfiable).

In this case it is not possible to find a "situation"  where all of Γ's are simultaneously true (i.e.satisfied).

You can think at your example with an additional premise "on top" of (1)-(3) : (E ∧ ¬E).

What happen in this case ?

Well, applying the above definition of logical consequence, we have that :


  if there is no "situation" that satisfies every member of Γ, then for any sentence A, it is http://en.wikipedia.org/wiki/Vacuous_truth vacuously true that Γ ⊨ A.


Thus, in presence of a contradcitory set of premises, like (1)-(3) with the additional (E ∧ ¬E), it is still true that they implies (C v D), simply because they, being contradictory, impliers everything.
See W.V.Quine, http://www.uvm.edu/~lderosse/courses/lang/Quine(1956).pdf QUANTIFIERS AND PROPOSITIONAL ATTITUDES, in THE JOURNAL OF PHILOSOPHY (1956).

"Belief" contexts are "intensional" ones, and Quine does not "like" them because they have no satisfactory analysis in term of first-order logic.

A phrase like :


  "Ralph believes that someone is a spy"


is ambiguous (I don not think that he says : "meaningless") because it can be analyzed in two ways; as :


  (∃x) (Ralph believes that x is a spy), 


and as :


  Ralph believes that (∃x) (x is a spy).

For A ⊃ (B ⊃ A), we have the following derivation :

1) A --- assumed

2) B ⊃ A --- from (1) by ⊃-introduction

3) A ⊃ (B ⊃ A) --- from (1)-(2) by ⊃-introduction.



For ∼(A ∨ ∼(A & B)) the simplest thing is to prove that it is a contardiction (i.e. identically false) via truth table.

In alternative, you can build a proof of (A ∨ ∼(A & B)) showing that it is a tautology (by soundness); thus its negation : ∼(A ∨ ∼(A & B)) must be a contradiction.
All birds are green,
My dog is a bird
Therefore my dog is green

This is a valid argument because the conclusion follows from the premises. Yet the premises are false.
I would try to answer this in terms of computation (nothing fancy)

Let's do some substitutions:

S = S is not true. #This says we can replace 'S' with 'S is not true'.

S = S is not true is not true.

S = S is not true is not true is not true.


We can carry on this process until there is no further substitutions to make. Well, that condition won't ever come in this case as this is plain old infinite recursion due to self reference. If you can't reach a condition where no further substitutions can't be made you simply cannot reach a conclusion. In this particular case you cannot say S is true or false.

Similarly if you email a person about some information and he never replied (in terms of whether he knows about it or not) for whatever reason, would you say that the person knows the information or he doesn't.


  Does this automatically imply that we have made an error in logic,
  reasoning and/or the structure of the original statement?


I wouldn't call infinite recursion an error, it is more about practical purpose and infinite recursion is of no practical use to us mortals as we cannot use it to reach any conclusion. But in terms of computation an infinite recursion/loop is a bug :)
We do not usually call a statement »valid« or »fallacious«. A statement is true or false. An argument on the other hand is valid, if it complies with the rules of logic. But this is not enough: »All humans are stupid. Fred is a human. Therefore Fred is stupid.« is a valid argument. But we might doubt the premise that all humans are stupid. If an argument is valid and its premises are true, we call the argument sound.

Note that a sound argument does not require the premises to be proven. This is in fact often impossible. Instead they should be plausible. But deciding on the plausibility of a premise is often subjective. That's where (informal) fallacies come in. They are rhetorical devices employed to make a premise sound plausible. Therefore they are problematic. But the argument itself can still be valid or even perfectly sound. Even the conclusion can be true. Fallacies are just a warning sign that you should double check the premises.
No, there is no difference, because :


  if x does not occur free in α, then (α → ∃xβ) ↔ ∃x(α → β).


In your example, we have :


  ∀x(F(x) → ∃y(x  > y)) 


and thus, being y not free in F(x), it is equivalent to :


  ∀x∃y(F(x) → (x  > y)).

I think you are asking about this sentence: ~(∀x)(∀y)(Ixy → Rxy)

I think you are asking which sentence, logically equivalent to that one, includes ~(Ixy → Rxy). The answer is (∃x)(∃y)~(Ixy → Rxy). 

As we move the quantifier in past the (∀x), it becomes (∃x). That is because, to take a simpler example, “~(∀x)Px” means “not everything is P,” while “(∃x)~Px” means that “something is not P,” which amounts to the same thing.

So, equally logically equivalent is the intermediate sentence (∃x)~(∀y)(Ixy → Rxy). Move the negation inward once more and you get (∃x)(∃y)~(Ixy → Rxy).

For your second example, ~(∀x)(∃y)(Ixy & Rxy) is logically equivalent to (∃x)(∀y)~(Ixy & Rxy).
There is a long tradition of understanding probability to mean a degree of partial entailment, or a degree of justification. This began with John Maynard Keynes in his "Treatise on Probability", and was taken further by Rudolf Carnap in his "Logical Foundations of Probability" and by Richard Cox in "The Algebra of Probable Inference". It is now referred to as the logical concept of probability and it is a close cousin to the bayesian interpretation, because both conceive probability as something epistemic, i.e. concerned with justification and evidence, rather than as being a property of the world. 

Probability can in some ways function like a modal operator, but it is different from necessity because it admits of degrees. Whereas a proposition might be considered necessary or possible or neither, probability uses a numerical range to allow comparisons. The dual of probable is simply improbable: to the extent that A renders B probable, A renders not-B improbable, and if A is independent of B, then A is independent of not-B. So your use of "justifies" translates to renders probable, and "consistent with" to independent of. 

As to how we cash out exactly what "renders probable" and "independent of" mean, one of the most fruitful approaches is to employ the concepts of relative entropy and mutual information. These allow us to quantify the shared information between probability distributions, which in turn allows us to interpret "A makes B probable" as if I have the information A then I have this much information about B. This does not by any means solve all the problems of statistical inference, but it allows us to explain what epistemic probability is without recourse to imprecise concepts such as "rational belief". 
A few thoughts, probably too quickly. It seems to me that an ought could be understood as the "sense" of the moral sentiment, but in particular it is a moral judgment. The sentiment then correlates with a feeling of shame, responsibility, piety -- perhaps at bottom simply the desire to obey. Moral passions are negative in this sense, relying on guilt and deprivation as implicit motivators. Maybe more simply: to my mind the sense of an ought is the constraint of free action, the separation of bodies and forces from what they can do. In this sense the ought subjugates, interposes, nullifies.

But perhaps the opposite is true as well, at least once we detach the ought from axiomatic moral judgment; maybe this could be explored in terms of Hume's empiricism and the inseparable gulf between an is and an ought. This can also perhaps be read alongside Nietzsche's doubts about ways of life that denounce existence in favor of something else. In other words: there is also an ethical relation of obligation that is independent of moral axioms and the logic of judgment and punishment; a positive order of joy and levity, characterized by an empirical investigation into ways of living, thinking and feeling -- this is ethics in the classical sense as an art of living. 

It is the abandonment of moral judgments about reality ("existence is blameworthy") that seems key to me here; an experimental ethics yields a very different spirit of analysis than a transcendental morality that knows everything in advance. Very simply it is less gloomy, less bored -- it lacks the gravity and sad passions that motivate moral responsibilities (bitterness, grief, melancholy, resentment, vengeance and so on.) And so just to complicate this schema a bit, perhaps an absolutely immanent ethics makes use of a certain "transcendent" moment in its own way. But there is a different relation; it is no longer a starting point (Spinoza makes this very clear -- it is important not to start out with the idea of God, but rather to reach it as swiftly as possible...)
Validity is a strong requirement; to be valid is to roughly ask for guarantee of having no possibble refutation; any refutation would neccessarily be a logical contradiction.

Claiming validity for an invalid argument is a fallacy. But the very invalid argument for which the claim is made is not always a fallacy. (But it is true that some would go as far as to say that all invalid arguments are fallacies. However that opinion is in no way the prevalent one).

Having said that, there are many nontrivial, sound and valid arguments. Unfortunately most of them that I'm aware of have rather long proofs. 

Arguments coming from some lightweight math system might be a good example. "Lightweight" because often people don't agree which math axioms are sound (which doesn't really matter for mathematicians, they are exploring systems that needn't have to do with "what's true" or even what's relevant to the real world). So an example should be from a lightweight system, one that doesn't claim much. One such is Heyting arithmetics. This system is about things like "1 + 1 = 2", but with a smaller set of inference rules than what the usual Peano arithemtics offers. For example, you can't even prove "F or not-F" for any given closed formula F. 

I don't have experience with Heyting arithmetic, but few things you can prove with it:


For all nonnegative integers x and y, x + y = y + x
Every nonnegative integer is either zero, or is some other nonnegative number incremented by one


(Proofs would be very uninteresting and would contain long weird formulas, while the inference rules would be modus ponens, generalization and instantiation)
It is easy to come up with a set of premises that are all true, or logically true, but have the conclusion drawn from them be invalid. The most obvious way would be by not having a full enough set of premises. It would not be fair to say...

All humans are primates.
All primates are mammals.
Therefore all mammals are orange.

The conclusion is not  explicitly derived from the premises, but can still be presented in this way.
The definition of an argument being (logically) valid is :


  whenever the premises are true, also the conclusion must be true


or, alternatively, (as in you post) :


  it is not possible for the premises to be true and the conclusion false.


If we write the last definition in a logically more perspicuous form, it says :


  
    if (all premises are true), then (the conclusion is false).
  


This is : "if P, then Q"; the negation of this formula is : "P and not Q, which is :


  (all premises are true) and (the conclusion is false).


This means that the condition that "all the premises are true and the conclusion is false" is the negation of the condition defining valid.

In conclusion : 


  
    if all premises are true and the conclusion is false, the argument is not valid.
  

Possibility and necessity in St. Thomas's sense cannot be understood without Aristotle's doctrine of matter and form (hylemorphism). Possibility (or necessity) in the modern philosophical sense (the Humean sense) is more about whether we can conceive another world in which something can be (or must be).

Regarding how
"All the objects collectively have a tendency to go out of existence simultaneously at some point."
follows from
"Every object has a tendency to stop existing at some point or the other.":

Perhaps rephrasing it as the following would help:
Every object can potentially stop existing at any time (including at, say, the particular time t₁).
All objects collectively can potentially stop existing at the same time (at t₁ in this case).
With Natural Deduction :

1) ¬A → ¬B --- premise

2) A → C --- premise

3) B ∨ D --- premise

4) D → E --- premise 

5) ¬A --- assumed [a]

6) B --- assumed [b]

7) ¬B --- from 1) and 5) by →-elimination (modus ponens)

8) ⊥ --- from 6) and 7)

9) A --- from 5) and 8) by ¬-introduction and Double Negation, discharging [a]

10) C --- from 2) and 9) by →-elimination (modus ponens)

11) E ∨ C from 6) by ∨-introduction

12) D --- assumed [c]

13) E --- from 4) and 12) by →-elimination 

14) E ∨ C from 13) by ∨-introduction

We have derived E ∨ C both under assumption B and under assumption D; thus, we can apply ∨-elimination, with premise 3) B ∨ D :


  15) E ∨ C from 3), 5)-11) and 12)-14) by ∨-elimination, discharging assumptions [b] and [c].

Mauro has already provided one correct derivation, but since you asked for help and advice, here's an expansion on the technique:

When doing a derivation like this, you want to look at the overall structure of the final result.  If proving something of the form A -> B, assume A and derive B.  If proving something of the form A & B, you usually want to prove A first, then B, then put them together.  If all else fails, assume the opposite of what you want to prove, and derive a contradiction.

In this case, the larger form of this is [complex expression] -> A, so you'll first want to assume the complex expression (Mauro's step 1).  From here you'll want to derive A, so assume the opposite and look for contradictions (Mauro's step 2).  The next steps require a little creativity --basically you want to look for anything you can create that contradicts something you already have. You have NOT A so you can get NOT ( A -> B), which is an obvious statement to try to contradict.  This means that you want to create A -> B, which is a problem like the one you started with, except easier and simpler.

In general, that's the whole technique of natural deduction --try to break a longer, harder problem into a series of shorter, simpler problems. 

There are purely mechanical ways to get the same results, but in my opinion, natural deduction leads to greater understanding.
English, like all natural languages, is a fuzzy construct with inexact definitions.  One would need a more rigorous language (such as mathematics) to be particularly comfortable with it being a "proper paradox."


  Describe - give an account in words of (someone or something), including all the relevant characteristics, qualities, or events.


I would argue, by this definition, "The indescribable is that which has no description" fails to be a description, because there are clearly characteristics or qualities of an indescribable thing for which the speaker feels they fail to fully capture.

Your second description could be thought of as valid.  In purely mathematical terms, you could define a predicate Describable(x) which returns true if "x" is describable, and then declare the set known as "The Indescribable" to be the set {x | x ϵ , ¬Describable(x)}.  You could then go to show that it is not an empty set by one method or another.  However, this would be a non-constructive proof, which limits the situations it can be applied.  The English equivalent of this is exactly as you say, a "label."  The label "indescribable" is sufficiently described as to not be indescribable, but the contents to which that label is applied are not sufficiently described.
I don't think there is an hidden modal logic in ordinary logic. 
Modal logic requires the introduction of a new operator (and so it is based on, but distinct from ordinary logic), just as probabilities require the introduction of a measure associated with propositions through new axioms.
At most modal logic can be trivialized into ordinary logic by assuming $ \Box p \eq p $.
Similarly, probabilities can be trivialized into ordinary logic by assuming that they can take only two values (0 or 1).

Probability is a modal concept but I don't know if it can be represented with modal logic, because you'd need a measure on possible worlds, whereas modal logic only has an accessibility relation (something is possible or not, not "more or less possible").

Finally the modal aspect of probabilities depends on the interpretation. For example frequentist interpretations don't need possible worlds and modal concepts: probabilities are just a ratio between a number of events of a type and a class of reference.

You can think of probability calculus and interior algebra as a mathematical machinery. The modal interpretation comes afterwards.

Note that modal logic can be used in proof theory (which is related to logic), but I don't think that's what you were after.
How are generalizations created, justified and accepted? 

Often, it's what benefits mathematical practice.  For instance, unifying fields of study and discovering interesting results are considered beneficial, and this desire can drive a lot of the attempt to find generalizations, and can be used as a criteria to justify and accept these generalizations.

Could we have chosen different generalizations (I'm suggesting that we could). 

Yes.  

Is every generalization a creation of another axiom?

No.  Some generalizations simply discover new results, others are isomorphisms among pre-existing fields, still others could be different formalisms.

Is there the only, ultimate, correct and valid generalization?

If you're a Platonist, perhaps, but personally I think not.  Even finding the one generalization that unifies all others may not qualify as some generalizations facilitate certain practices at the expense of others. Since different mathematics have different goals, it stands to reason that different generalizations will be adopted as the needs dictate. 
1) A ∨ (B ∧ C) --- premise

2) (¬ B ∨ ¬ C) ∨ D --- premise

3) A --- assumed [a] from 1) for ∨-elim

4) A ∨ D --- from 3) by ∨-intro

5) (B ∧ C) --- assumed [b] [from 1) for ∨-elim

6) D --- assumed [c] from 2) for ∨-elim

7) A ∨ D --- from 6) by ∨-intro

8) (¬ B ∨ ¬ C) --- assumed [d] from 2) for ∨-elim

9) B --- from 5) by ∧-elim

10) C --- from 5) by ∧-elim

Now, with a third ∨-elim we derive ⊥ (the contradiction) from both 8), 9) and 8), 10); thus :

11) ⊥ --- from 8) by ∨-elim

12) A ∨ D --- from 11) by ⊥-elim

Having derived A ∨ D from both 6) D and 8) (¬ B ∨ ¬ C), we have :

13)  A ∨ D --- from 2) by ∨-elim discharging assumptions [c] and [d]

Having derived A ∨ D from both 3) A and 5) (B ∧ C), we have :


  14)  A ∨ D --- from 1) by ∨-elim discharging assumptions [a] and [b].


Conclusion :


  
    A ∨ (B ∧ C), (¬ B ∨ ¬ C) ∨ D ⊢ A ∨ D --- from 1), 2) and 14).
  

Not knowing what GS is, as Conrad points out, I cannot yet make a definitive answer, but it is possible that Feyerabend is drawing a distinction between principles-and-rules which can go wrong and principles-and-rules which have been rewritten in a form which allows doubt.

"What goes up must go down" fails remarkably if someone holds the object at its apex

"What goes up must go down unless someone opposes it" fails remarkably once you exceed escape velocity.

"'What goes up must go down' has a remarkable track record of being useful" can no longer "go wrong" because the fuzzy words "remarkable" and "useful" temper it, without losing sight of the value of the original rule.

It appears Feyerabend is arguing that we should ignore rules given to us until they are in such a form, because rules which are not in that from have a tendency to squelch contrary ideas (which may be true, but you'd never know it if you squelched them)
I think you have in mind probabilistic inferences, where P1 is not a deductive certainty, but an observational one. In such cases, the conclusion C (O is a B) is stated with some probability/confidence, but it's not certain either.

More info on this can be found at


http://plato.stanford.edu/entries/logic-probability/ http://plato.stanford.edu/entries/logic-probability/
http://en.wikipedia.org/wiki/Probabilistic_logic http://en.wikipedia.org/wiki/Probabilistic_logic


and the references therein.
See John Burgess, https://books.google.it/books?id=k32w3_wjBoYC&pg=PA84 Philosophical logic (2009), page 84 :


  let A hold at both u and v, C fail at u and hold at v, and B hold at u and fail at v. 
  
  Then the least remote A-state u is an B-state, but the least remote (A & C)-state v is not an B-state. 


Thus, in u A > B holds but (A & C) > B does not, and so :


  the inference from A > B to (A & C) > B is not valid.

Note that if you condense each question-answer pair into one completed proposition, you get self-referential statements such as P = "P is true". Aside from the trickiness associated with such statements, you're not so much arguing about the correctness of P, but the internal consistency of the correctness or incorrectness of P, which in each case  is an example of circular reasoning.
Kant does mention Aristotle's categories. However, Kant's interest in the categories was internal to his own system, and not an interest in reconstructing Aristotle. Kant's list of categories is quite different from Aristotle's.

Aristotle's categories are general-logical and metaphysical terms. He defines the categories as "what is signified by expressions which are in no way composite" (Categories, §4). His list consists of ten categories: substance, quantity, quality, relation, place, time, position, state, action, affection.

Kant's categories are logic-of-epistemology terms. He defines them as "conditions under which alone the manifold content of the sensuous intuitions can be united in one consciousness" (Critique of Pure Reason, Transcendental Logic, §16). His list consists of twelve categories, divided into four sets of three: (1) of quantity: unity, plurality, totality; (2) of quality: reality, negation, limitation; (3) of relation: substance- and- accident, cause- and- effect, reciprocity; (4) of modality: possibility, existence, necessity.
I take it that when you say "equivalent" you do not mean simply that the original axioms and their substitutes ought to each imply each-other (since that is true of any two axioms at all), but that they can effectively serve the same role in the system. That is, given the substituted axiom and the rest of the original axioms, you can get back to the original axiom for which the substitution was made. Then it seems to me that you are asking whether you can prove (1'') → (1) and (4'') → (4) given only the other original axioms and your two additional rules. Well, I can try...

This first proof strikes me as cheating just a little bit, since it does not convince me that there is really any sort of logical relationship between 1'' and 1, but I'll show you what I came up with anyway and let you decide:


((A → A) → B) → B; assume (premise)
| A; assume (open conditional proof)
| A; by 2
A → A; close conditional proof
(((A → A) → B) → B) → (A → A); conclusion


So we have that (1'') → (1). I'll see whether I can come up with something more satisfying for the second dilemma:


A → ((A → A) → A); assume (premise)
| A → (B → C); assume (open conditional proof)
| | B; assume (open conditional proof)
| | | A; assume (open conditional proof)
| | | B → C; by 2, 4, modus ponens
| | | C; by 3, 5, modus ponens
| | A → C; close conditional proof
| B → (A → C); close conditional proof
(A → (B → C)) → (B → (A → C)); close conditional proof
[A → ((A → A) → A)] → [(A → (B → C)) → (B → (A → C))]; conclusion


Then (4'') → (4). So, nope - it seems that I can't. Well, I don't feel that I have helped you (and, now that I notice it, I am not even sure that the R allows for the conditional proof structure I've used), but on the off chance that you might appreciate my reply to some small degree, I think I will post it anyway.

Best of luck with any future answers!
Real life examples of following the moral law is a hard point for Kant. Specifically, this is because for an action to be moral, it is not sufficient that you do the right thing -- you must also do it from the right maxim (basis for action). Consequently, it is never going to be apparent in the phenomenological world that someone acted from the right sort of basis in doing the right thing.

The contrary cases (actions that clearly violate the moral law) is not equally difficult. There are some actions that Kant considers utterly incompatible with the moral law: murder, lying, cheating. Thus, any time there is a murder or a lie, it is clear that these cannot be motivated by the moral law (as Kant understands the term).

For more on this topic, I would recommend Kant's Metaphysics of Morals. You can find Kant delineating this in the preface and in the Doctrine of Right a.k.a. Metaphysical Principles of Justice. (= German, Rechtslehre). The Doctrine of Right looks at what we should make law and what we can obligate others to do. The second part, the Doctrine of Virtue (= German , Tugendlehre) looks at what it takes for our actions to be moral. The former is a good guide for where Kantians think you can find examples of not following the moral law. But examples of following the moral law are harder precisely because their accomplishment includes but is not limited to doing the right thing.
Some doesn't mean all.

It is usually taken to mean at least one.

So if I have some cards in my hand - I have at least one; not all and nor none.
A. P. French wrote in 1971 in his book "Newtonian Mechanics", page 317 and figure 9-4, that if you consider a gun firing particles analogous to photons at a barrier some distance away, that since the photon-like objects cannot be seen, there is the appearance that Newton's third law does not apply until it is absorbed by the barrier.

Scientifically we say that photons have momentum, and we could measure it anywhere along the path from the gun to the barrier, however the issue is that a photon does not take a quantifiable state until such an absorption occurs.  That is to say, it defers to the philosophical question of "Where are photons between emission and absorption?" (Which does not have a precise philosophical answer in the context of the double-slit experiment).
No.  Statistics, a form of mathematics, is a key tool for sociologists.  Standard deviations, mean, averages, remember the "Bell Curve" controversy?
No, you don't need to do a truth table, and the argument is valid.

Again, the definition of validity is that an argument is valid if it is not possible for the premise to be false if all of the conclusions are true.

In the case you are describing, if the premises are false, then the conclusion is false, and if the premises are true, then the conclusion is true. Thus, no need for a truth table since no case could exist where there are true premises and a false conclusion.
No. If the conclusion is a tautology, that means it is always true, and it is impossible for it to be false. An argument is truth-functionally valid if it is impossible for the premises to be true and the conclusion to be false. If it's impossible for the conclusion to be false, then the argument is automatically valid for that reason.
Forget for a moment about "inferences" and consider only formulae.

Tautologies (or valid formulae) are formulae that are true in all interpretations, like A → A or ∀x(x=x).

If we consider one of Peano's axiom for arithmetic, like ∀x(x+0=x), it is not a "logical truth" (i.e. a valid formula) : it is true only in the "arithmetical" interpretation.

But the "logical laws", like ∀x(x=x), are still true in the "arithmetical" interpretation (they are ture in all interpreations !)

Of course, in the arithmetical interpretation also the theorem deriving from the arithmetical axioms will be true (they are logical consequences of the axioms).

Conclusion : adding axioms will reduce the number of possible interpretations, and vice  versa.



The same for modal logics; we "specify" a normal modal systems adding some axiom to the "basic system K.

This amounts to adding "restrictive" requirements to the class of interpretations, and thus "reducing" the number of interpretations ...
The example which I know of is http://plato.stanford.edu/entries/logic-relevance relevance logic, in which propositions such as A⇒B mean that B can be inferred more-or-less semantically as a consequence of A; not that either B is true or ~A is true.

For instance: consider a counterfactual exclamation of the form "If the US Dollar was valued at 2 British Pounds yesterday, then I'm the King of France".


This statement is no mere facetious dismissal, but is in fact a valid proposition in classical logic: because the US dollar was not valued at 2 British Pounds yesterday, I can claim that any sort of ridiculous thing — including my being the monarch of the Fifth French Republic — would be true if the incorrect premise did hold.
Such a statement would not be a valid proposition under relevance logic, unless we were acting in a logical system with axioms which would clarify how the state of affairs of such a high valuation of the US Dollar would have any causal connection to the ruler and form of government of France, and my relation to that situation.


To elaborate on Atamiri's comment above, Linear Logic is one particular form of relevance logic, in which propositions (from which one may derive consequences) are used largely as non-renewable resources which are consumed, and implication as a procedure to transform one sort of (propositional) resource into another. Thus an implication A⇒B stands in for a sort of procedure, and can be applied to transform an instance of a proposition A into an instance of proposition B; but from this it does not follow that you have either a ~A or a B to begin with.
Follows from their definitions:

p XOR q = (p AND not q) OR (not p AND q)

p => not q = (p AND not q) OR (not p)


That is, p => not q is vacuously true when p is not true, independent of q's value.
There are definitely places where it can be done.  The http://en.wikipedia.org/wiki/Riemann_hypothesis Reimann Hypothesis comes to mind:


  The non-trivial zeros of the Riemann zeta function all have real part
  1/2.


This is certainly a claim about existence, and it may be falsifiable in the future.  By a loose definition of "ontology," this would qualify.

However, usually "ontology" is not concerned with such things.  While such a thing as a zero of the Riemann zeta function could certainly be said to exist, it also exists wholly exists within the realm of mathematics, and mathematics has far more powerful tools to analyzing the existence or non-existence of anything in its domain.

It does seem to me that ontology captures "being" questions which are not accessible through other ways of thought.  In the realm of "pure thought," like mathematics, there are usually "better" ways to deal with an issue.  It's only when dealing with "the real word" that the perception issues of the division between body and mind become difficult enough that ontology really starts to shine.  And, unfortunately, nobody has any reason to believe that problem (which has stood with us for millennia) will be solved any time soon.

There is also the issue of linguistic relativity.  If there does not exist a discipline which can solve a particular problem, and then the discipline is discovered, can we say for certainty that the question, as originally posed, meant the same thing to one's predecessors as it now means to you?  This may be sufficient ambiguity to create an Ontological claim which becomes falsifiable over time.

One final solution I'd recommend is simply using Ontological terms in situations where Epistemology (and/or science) is sufficient.  Consider Schrodinger's Cat.  You can make an ontological claim, which becomes falsifiable when you open the door.  Or, for a more concrete example, consider a box that has a radioactive atom suspended in it.  Put detectors all around it. You can claim "The radioactive atom exists," which is an ontological claim.  If you cannot open the box (for some reason) it remains unfalsifiable for now.  However, when the atom "chooses" to decay, and that decay is observed, now the ontological statement "The radioactive atom exists" is falsifiable (or at least an interesting enough discussion to start with "falsifiable" and debate the proper adjective).
Two usually discussed challenges are by Nozick and Dretske. 

Nozick's analysis of knowledge includes the following condition for S to know that p: 

(sensitivity) Were p false, S would not believe p.

Truth conditions for subjunctive conditionals are given in the usual way: Sensitivity requires that in the close worlds to the actual world in which not-p holds, S does not believe p.

Sensitivity is motivated for example, because it explains failure to know in Gettier-type cases and also helps us to deal with the notorious skeptical paradox. (See the links.) Sensitivity  enables us to give counter examples to closure.

Take the following propositions:

p=Lucky has hands.
q=Lucky is not a brain in a vat.

Now assume that Lucky (who has hands) is actually in a world where he is not in a brain-in-a-vat scenario. 
Now, sensitivity is satisfied for p (and in fact, so are all the other Nozick's conditions for knowledge), and so Lucky knows that he has hands. (Sensitivity is satisfied, because in all close worlds where he does not have hands, e.g. when he loses them in an accident or something like that, he does not believe he has hands.) 

However Lucky does not know that he is not a brain in a vat, because for that proposition sensitivity is not satisfied: 
Take a close world where q is false, so in that world he is a brain in a vat. Lucky would still, in that world, believe that he is not a brain in a vat. So "were q false, Lucky would not believe q" is not satisfied.

(One could also claim that both of these results are even intuitive.)

So:
Lucky knows that p. Lucky knows (assuming he has thought about the matter) that p --> q.
But Lucky does not know, that q.

There are also more mundane examples of p and q where closure fails, because sensitivity is not closed under entailment.
Nozick's analysis of knowledge is obviously in some way externalistic. 
Nozick's analysis of knowledge has been criticized by showing that it leads to some other counterintuitive claims. It's up to the critic to provide a better more plausible analysis, that can also handle Gettier cases and the skeptical paradox.

Another issue related to closure is transmission. In failure of transmission the justification, for believing that p, fails to "transmit" to some entailed consequence q. Some failures of transmission are uncontroversial.

For more on Nozick, Dretske and transmission see: http://www.iep.utm.edu/epis-clo/ http://www.iep.utm.edu/epis-clo/
http://www.iep.utm.edu/transmis/ http://www.iep.utm.edu/transmis/

There is also a good chapter titled "Skepticism and Closure" in A Companion to Epistemology (Dancy, Sosa, Steup). 
The meaning of a sentence seems to me to be a relation between the sentence and something which is beyond the sentence. A senseless sentence is just a sentence, a mere sentence . A meaningful sentence is somehow more than a mere sentence. It is so by being related, in a way that a senseless sentence is not. A meaningful sentence is related to "the external world". The "world" is "external" to the related sentence, it is beyond the sentence.

The above raw definition covers both semantic meaning, where one uses terms like 'refer' or 'denote' to express the relation of words to the world; and pragmatic meaning, where the word-world relation is thought to be practical ("meaning is use").
The uses of 'theory' in the theory of object theories, are in the sense of the 'Theory of a Complex Variable'.  They refer to a characterization of a set of theorems that make up a compelling and cohesive whole, not to any hypothetical model.  Good examples of object theories are the kinds of structures described in finite algebra.

Take Group Theory.  Each concrete group, for instance, clock arithmetic, is described in a declarative manner, as the points on the clock, or as a formal ratio of the integers modulo a given integer, or as the possible positions of a single cycle.  Each of these is a different object theory.

Group Theory itself is the metatheory which identifies why all of these separate concrete representations for the same cyclic group, are in effect equivalent, and how similar such structures fit together.

The real (or informal) theory behind group theory is that there are intuitively useful aspects to the notion of a single arithmetic operator in isolation, across models, and many things act 'enough like arithmetic' to be thought of as alternative perspectives on how number theory itself applies to different situations.
You misunderstood when you thought you can be right or wrong, the unity can't have that, so it does not matter the kinds of unity, they are just a way to explain it, you have to understand that every logic created is not the unity, it is a dualistic ilusion. 

So if it is rational, words saying something inside, it is not unity, it is "wrong", and the truth is beyong inner words to describe it, because words can only bring keys to your own inner doors to reach that metaphysical unity, not making the realization of it, only by practicing the "critica total da razão" you cant reach it. 

The thing is that Kant is studied by people (not saying it about you, but maybe your teachers) so that they can say: "look at how smart I am, I read Kant", but Kant is keys to self understanding, for later on understanding the world with the eyes of unity. And people just looking for intelectual fame just "Kan't" understand that. Peace!
There must be applicable quotes from (the later) Wittgenstein. 

E.g. the http://www.goodreads.com/quotes/47649-hegel-seems-to-me-to-be-always-wanting-to-say following one seems to argue that Hegel would be interested in categories of things (and perhaps denote them by symbols like X) whereas Wittgenstein would doubt such generalization (and perhaps describe family resemblances using ordinary sentences and words instead). 


  Hegel seems to me to be always wanting to say that things which look
  different are really the same. Whereas my interest is in showing that
  things which look the same are really different. I was thinking of
  using as a motto for my book a quotation from King Lear: 'I’ll teach
  you differences."


Here is a similar argument from Nietzsche's http://en.wikipedia.org/wiki/On_Truth_and_Lies_in_a_Nonmoral_Sense On Truth and Lies in a Nonmoral Sense:


  The very concept arises from the equation of unequal things. Just as it
  is certain that one leaf is never totally the same as another, so it
  is certain that the concept ”leaf” is formed by arbitrarily discarding
  these individual differences and by forgetting the distinguishing
  aspects. This awakens the idea that, in addition to the leaves, there
  exists in nature the ”leaf”: the original model according to which all
  the leaves were perhaps woven, sketched, measured, colored, curled,
  and painted–but by incompetent hands, so that no specimen has turned
  out to be a correct, trustworthy, and faithful likeness of the
  original model.


http://en.wikipedia.org/wiki/Ludwig_von_Mises Ludwig von Mises' description of economic behavior in Human Action also comes to mind: if memory serves he did not use a single formula (and very few symbols such as X) in a long book whose subject area (i.e. economics) would seem to offer many opportunities for symbolic notations. I am also curious whether he was influenced by a (philosophical) school in making this choice.
The Kalam argument, which is an Islamic version of the cosmological argument maintains that the universe cannot have existed for an infinite period of time. This is because if there was an infinite period of time before us we never would've reached the present day. This is a view shared (or may have even been his own original work) with William Craig too.
First of all this would be a paradox as; if everything B knows is opened and undefined then the argument can not begin. Whether, B accepts an argument or not does not matter.

But, the paradox is that B seems to know what "meaning" is as he questions the meaning of every proposition. So, it seems B is not entirely purely skeptical; this provides us a beginning.
1) Your personal suggestion is https://en.wikipedia.org/wiki/Logical_possibility seconded by Wikipedia. 

2) It is possible that a concept does not "exist" in any way (even in "possible worlds"), if there is a contradiction in its definition, e.g. https://en.wikipedia.org/?title=Barber_paradox#Paradox Russell's barber who shaves all those and only those who do not shave themselves. Trying to answer if he shaves himself leads to a contradiction. However, if a concept is free of contradiction then per your, and Wikipedia's, definition it does "exist" 'necessarily'. This use of 'necessarily' is colloquial however, in the technical sense of modal logic "possibility exists necessarily" is not a sentence allowed by the syntax, so your question can not be asked.

3) You are trying to apply necessity and contingency to "existence" or "possibility" of objects, which is not a first order predicate, so in the standard semantic of possible worlds such application can not be interpreted. What you need is something like second order modal logic with meta-possible worlds filled with collections of possible worlds, so that you can interpret modal operators applied to second order predicates. I have seen higher order modal logics, see e.g. http://semanticsarchive.net/Archive/WQ0ODMxM/homl.pdf Muskens, but not with meta-extensional interpretation like that.
Not on this at least. Wittgenstein is alluding to Frege on logical syntax. From Tractatus:"Frege says that any legitimately constructed proposition must have a sense. I say that any proposition is legitimately constructed". Laws of syntax are similar in form to ethical laws: thou shalt not (form such and such sentences). Wittgenstein's response in both cases is "and what if I do?" Frege attempted to justify syntactic rules ("laws of logic") by appealing to other laws of logic, so did Russell with his type theory to avoid contradictions of self-reference. Wittgenstein rejects this idea as "clearly" wrong. 

https://books.google.com/books?id=fR15Jj7oza0C&pg=PA67&lpg=PA67&dq=Clearly+the+laws+of+logic+cannot+in+their+turn+be+subject+to+laws+of+logic&source=bl&ots=vUEIdVj-kz&sig=XjK8-xL3po0Ym7cXZth1jPAVq7Q&hl=en&sa=X&ei=mXmMVZjhGseGsAXg46bADw&ved=0CB4Q6AEwAA#v=onepage&q=Clearly%20the%20laws%20of%20logic%20cannot%20in%20their%20turn%20be%20subject%20to%20laws%20of%20logic&f=false Bearn explains:"If the combination of signs is nonsense then we don't need a law to tell us that we should not combine the signs in this way. What we need is a logical syntax that makes logical structure, which is already there, clear. Logic must take care of itself... According to Wittgenstein, the so-called laws of logic are built into unspeakable structure of logical space... They are manifest in the fact that some combinations of signs and not others make sense. Russell misconstrued the task of logic as the installation of rules obedience to which would keep our propositions within the realm of sense. The theory of types was of precisely this nature. Moralists make the same error. They attempt to construct a set of moral rules obedience to which will give our lives meaning." In other words, Wittgenstein favors unified logic that fuses language, meta-language, meta-meta-language, etc., and has a single set of "logical laws" that function synthetically, not formally, "one is enough". As Russell pointed out in another context, such a view https://philosophy.stackexchange.com/questions/24122/how-does-russells-argument-refute-that-of-wittgensteins/24143#24143 would make mathematical logic "impossible".

Here is Friedman's characterization of the Tractatus from http://mcps.umn.edu/philosophy/11_3Friedman.pdf Logical Truth and Analyticity in Carnap's "Logical Syntax of Language" (p.85) more generally:"For Wittgenstein, there can be only one language the single interconnected system of propositions within which everything that can be said must ultimately find a place; and there is no way to get "outside" this system so as to state or describe its logical structure: there can be no syntactic metalanguage. Hence logic and all "formal concepts" must remain ineffable in the Tractatus... Of course, the Tractatus is itself quite clear on the restricted scope of its conception of logic and mathematics in comparison with Frege's (and Russell's) conception. Wittgenstein's response to this difficulty is also all too clear: so much the worse for classical mathematics and set theory".

As for Wittgenstein's relation to Gödel's incompleteness, his http://www.quora.com/Did-Wittgenstein-misunderstand-G%C3%B6dels-first-incompleteness-theorem "notorious paragraph" shows, depending on one's point of view, that he either misunderstood it, or concluded that it has nothing philosophically valuable to say. Either way he would not have been anticipating it. Essentially, he rejects a premise of Gödel's proofs, that "truth" can be interpreted as distinct from "provability", which renders completeness issues moot:"Just as we ask: “‘provable’ in what system?”, so we must also ask: “‘true’ in what system?” ‘True in Russell’s system’ means, as was said: proved in Russell’s system; and ‘false in Russell’s system’ means: the opposite has been proved in Russell’s system... If you assume that the proposition is provable in Russell’s system, that means it is true in the Russell sense, and the interpretation “P is not provable” again has to be given up.".
No, both events cannot occur at the same time.

According to the Special Theory of Relativity time and space have no separate and independent existence. Instead, space and time are coordinates in 4-dimensional spacetime. Events are the points of spacetime. 

The structure independent from any choice of coordinates is the light cone of each event. It is a 3-dimensional double cone. The light cone defines the domains of causal dependence: 

Event 1 is located in the vertex of its light cone. Each event within the forward cone can be affected by event 1, each event within the backward cone could have effected event 1. If event 1 can affect event 2 as you suppose, then event 2 is located within the forward cone of event 1. 

Now you can choose the coordinates space and time, taking the vertex as origin of the coordinate system. But the crucial point is: There is no choice of coordinates, such that an event in the forward cone has the same value of its time coordinate as the vertex, which has time = 0. And the second remarkable fact: In case event 2 is located outside the light cone of event 1 - i.e. event 1 cannot affect event 2 - then a choice of space and time is possible such that both events are simultaneous.

The above considerations derive from the Special Theory of Relativity and refer to Minkowski space. Special relativity holds in the absence of massive objects. In the General Theory of Relativity you can approximate spacetime locally by Minkowski space. But the global structure of spacetime depends on the mass distribution.   

Added on request: The Special Theory of Relativity got its name from Einstein's discovery, that many physical quantities have only a relative meaning. The two most prominent are space and time: There is no absolute time difference between two events, there is no global meaning of two events being simultaneous, there is no absolute spatial distance of two events. Many quantities depend on the coordinate system which the observer chooses. On the other hand, the speed of light is always the same, independent from any coordinate system it refers to. As a consequence, it has an absolute meaning whether one event can affect another or not. Hence a unique structure of causal dependency exists.     
Actually, that attitude is constructivism, which, at root, is a variety of formalism.  Mathematics is mathematics only when it takes the form of a construction.

Surely, the easiest way to reason naively without getting tricked by the Law of the Excluded Middle into overestimating one's certainty is to stay grounded in finite constructions, but it is not the only way.  And it is, to some degree a delusion to believe everything can be shown, or that those things that cannot be show can always be captured in axioms.  Diagonalization survives in intuitionism, and Goedel's incompleteness theorem is transfinitely constructive by nature.  So if it weren't obvious, you could prove intuitionistically that this basic notion of constructivism is as weak as all other formalist positions.

Unlike in formalist takes (including constructivism), there need be no attempt at the "arithmetization of analysis".  We can accept Euclidean space from intuition and simply clarify it over time, taking the abstracted version of the geometry of our sensory apparatus as basic, rather than trying to construct it from set theory, and in the process avoiding artificial meanings for all the contents of geometry.  This is quite necessary, because a lot of the power of artificial constructions we have become accustomed to (e.g. that all Dedekind cuts are well-defined) implicitly relies upon strict negation.

But deeper ontological considerations aside, the two notions do have in common that we cannot presume the map of meaningful statements to just two truth values is total.  They react to it in different ways.

Heyting provided an underlayment for intuitionism that is not entirely constructivist, but has a strong definition of proof.  From that point of view, statements simply have another dimension to their truth value, they can be proven or unproved, but when unproven, they can also be disproven or un-disproven.  Everything is determined by this forking of the state, and you can pretty readily get used to this tristate logic.

You can also model intuitionistic deduction as the strongest modal logic where box means 'can be proven' and diamond means 'might be proven' and all axioms and preconditions can only be given as box-statements.  All constructivisms are also to some degree captured by different modal logics like this, but they have a further refinement of what is meant by 'proof' beyond what the modal operators would necessarily force down on them.

This kind of cleanliness is not possible in a more absolute constructivism, where the gradations of truth become either infinitely fine, or barbarically coarse, depending upon your point of view.

Constructive statements can be proven relative to one another, (the same way ZF statements can be proven relative to CH) and if all axioms are carefully traced, we can use the connected facts provisionally when we take their axiomatic bases to be likely in a given circumstance.  (So we can do physics without restricting our use of intervals in bizarre ways.)  But ultimately, statements not grounded absolutely in finite procedures limited by definite rules are simply meaningless mathematically.
If you agree with the principle :


  □p→p,


replacing p with ¬p gives : 


  □¬p→¬p


that is :


  
    ¬◊p→¬p.
  


We can red it as :


  if p is impossible, then p is false.

Acceleration is a proportion.  Proportion is not one of the Categories.  There is a reason.

We obviously only find things proportional when we measure them.  And measurement is subject to the shape of space, whether you take that to mean the space that is an aspect of humanity in Kant, or you take that to mean space as currently understood by General Relativity.

So as given, the statement is not expressed in a form we can reasonably make any a priori deduction about, for pretty much the reason we have ultimately given it up (since gravity in general relativity is an aspect of space, and not a force per se).

Equal, Opposite and Like in Kind are basic or derived Categories.  So in some sense the Third Law lies at a far more basic level of logic.
It was not an ad-hoc hypothesis. Instead, searching and detecting a further planet - Neptune - was a brilliant confirmation of Newton's theory of gravitation.

In the beginning of 20th century most of the precession of mercury could be explained by Newton's theory of gravitation. But all attempts failed to explain also the rest. I do not know whether ad-hoc explanations have been suggested. Nevertheless, when finishing the General Theory of Relativity Einstein derives the full precession from a preliminary version of his theory. That's another brilliant example that a theory explains many facts, which have not been used as input to develop the theory.
On the Introduction of Noumena

Kant introduces noumena in order to avoid the dialectical illusions, embodied by antinomies, in which reason would necessarily find itself when trying to cross the boundaries of experience, and into Metaphysics, trying to reach for the unconditioned.

Indeed, in the Preface to the Second Edition of the http://www.gutenberg.org/files/4280/4280-h/4280-h.htm Critique of Pure Reason (all following are translations by J. M. D. Meiklejohn), he says:


  Now, if it appears that when, on the one hand, we assume that our
  cognition conforms to its objects as things in themselves, the
  unconditioned cannot be thought without contradiction, and that when,
  on the other hand, we assume that our representation of things as they
  are given to us, does not conform to these things as they are in
  themselves, but that these objects, as phenomena, conform to our mode
  of representation, the contradiction disappears [...]


What he means by the unconditioned is the first term of any series of conditions e. g. the beginning of time. According to the arguments presented in the first antinomy, the beginning of time cannot be thought without contradiction, for:


If time had a beginning, then it would have been preceded by nothing, and nothing could arise from nothing;
If time didn't have a beginning, then an infinite amount of time would have actually occurred, which is impossible;


In the case that time is conceived as having a begin ning, it would be "too small" for reason, which always aims for the unconditioned. On the other hand, conceived as having no beginning, it is "too big" for the understanding, and cannot be conceived in accordance with experience. By distinguishing all things between noumena and phenomena, Kant aims to distinguinsh between what is purely thought in a thing and what is imagined in a thing.

Indeed, in Chapter III of the Analytic of Principles, he says:


  [...] this conception [noumenon] is necessary to restrain sensuous
  intuition within the bounds of phenomena, and thus to limit the
  objective validity of sensuous cognition; for things in themselves,
  which lie beyond its province, are called noumena for the very purpose
  of indicating that this cognition does not extend its application to
  all that the understanding thinks.


The unconditioned, on the other hand, is in itself a necessary concept of reason, which is the faculty by which the mind ascends the series of conditions, as if going up though the terms of syllogism. Reason is what, given some number of hypotheses, leads us to find an adequate thesis which would join these facts in a common knowledge. Because the unconditioned itself is necessary, it must be conceived in some way. But it can only be conceived in a negative way, as how things are as we can't know them: completely void of content.



Refutation of Idealism

Kant doesn't directly refer to "solipsism", but he presents an argument directed against Descartes' and Berkeley's doubts of the existence an external world under the name "Refutation of Idealism". Though he makes no reference to noumena nor to things in themselves, his argument lies in the critical assumption that time and the self are not in themselves perceptible, but are transcendental objects.

The argument is as follows:


  THEOREM
  
  The simple but empirically determined consciousness of my own
  existence proves the existence of external objects in space.
  
  PROOF
  
  I am conscious of my own existence as determined in time. All
  determination in regard to time presupposes the existence of something
  permanent in perception. But this permanent something cannot be
  something in me, for the very reason that my existence in time is
  itself determined by this permanent something. It follows that the
  perception of this permanent existence is possible only through a
  thing without me and not through the mere representation of a thing
  without me. Consequently, the determination of my existence in time is
  possible only through the existence of real things external to me.
  Now, consciousness in time is necessarily connected with the
  consciousness of the possibility of this determination in time. Hence
  it follows that consciousness in time is necessarily connected also
  with the existence of things without me, inasmuch as the existence of
  these things is the condition of determination in time. That is to
  say, the consciousness of my own existence is at the same time an
  immediate consciousness of the existence of other things without me.

There is a difference between semantic consequence expressed by truth tables, and https://en.wikipedia.org/wiki/Logical_consequence#Syntactic_consequence syntactic consequence in a deductive system, some authors use ⊨ for the former and ⊢ for the latter, and the corresponding difference in equivalence. The latter can be used to capture what you are describing somewhat. In https://en.wikipedia.org/wiki/Analytic%E2%80%93synthetic_distinction#Conceptual_containment Kant's theory of conceptual containment equivalence "not only because both statements are true, but also in that they are saying the same thing in terms of their content" is called analytic equivalence, and it can be formalized as follows.

Note that in your examples truth of the statements depends on using the laws of arithmetic. One can select a subset of them relative to which 3>2 and 3-2 > 0 are still provably equivalent, but 4+6=10 is unprovable. More generally, you can designate some axioms you accept as "analytic" (say laws of pure logic only), and others as "synthetic" (say arithmetical laws). Equivalence is declared analytic if only analytic axioms are used to derive it. However, with purely logical analyticity 3 > 2 and  3 - 2 > 0 are not analytically equivalent. And they shouldn't be if you think about it. The latter involves 0 and subtraction, whereas the former does not, so it does add additional arithmetical content and properties. But you can get what you want by moving some of arithmetic into the analytic column. For example, Frege thought that all of arithmetic is analytic, and Kant thought that none of it is, see https://philosophy.stackexchange.com/questions/24373/locke-and-analytic-knowledge/24390#24390 Was Locke right that analytic knowledge is vacuous?. 

The root of the problem is the definition of material conditional (and equivalence) in terms of truth tables. "The material conditional does not always function in accordance with everyday if-then reasoning... One problem is that the material conditional allows implications to be true even when the antecedent is irrelevant to the consequent." Conditional that takes "relevance" into account is called https://en.wikipedia.org/wiki/Indicative_conditional indicative conditional, and a theory of that can not be as simple or formal as that of material conditional exactly because it has to take "meanings" into account, and those are a handful. 
I don't think that Kant's existence-argument poses a special problem for bundle theories.

The thrust of Kant's argument is that the following two questions are separate and independent: the first, whether x exists, the second, what is x like, i.e. what properties x possesses. Existence is, according to Kant, neither a property, nor an entity of any kind.

Bundle theories pertain to the second question, what x (in this case, a particular entity) is like. Bundle theories hold that a particular is a bundle of properties, without an additional substance or substratum.

The question, what is a particular like is, according to Kant's argument, independent of the question whether the particular exists or not. Kant's famous example was: a hundred thalers possesses the same properties, whether it is imaginary or real. And it is immaterial to the point of the example whether the hundred thalers is a substance, a property bundle or whatever.

So if Kant was right, there is no use for an existence-property, whether we believe in bundles or in substances. There are other theories which do support existence-properties, but these theories are unrelated to the question of bundles.
Logic like mathematics is based on an axiomatic system, it is a formal system. Therefore, it is producing immanently tautologous propositions out of itself by its axioms. I am not sure if there has been a Kolmogoroff of all systems of logic exemplifying the axioms, though.
missing the point is proving something different than what was sought.

false cause is asserting a predecessor statement that is not required to arrive at the desired conclusion. 
Another way to understand it is that intuisionism is naturally interpreted as the logic of proof rather than the logic of truth. To the intuitionist, A means "I can prove A". ¬A means "I can prove there is no proof of A". A ∨ B means "I can prove A or I can prove B". A ∧ B means "I can prove A and I can prove B". This is why A ∨ ¬A (LEM) is not a theorem of intuitionistic logic, because in general there might be no proof of either. Thus ¬(¬A) means "I can prove that there is no proof that there is no proof of A" but clearly this is a weaker claim than I can prove A, so this is why intuitionistic logic lacks the rule of double negation elimination. 
You are correct, structurally speaking this is the fallacy of appeal to authority.  The person isn't providing any actual argument against your position, he is merely claiming to be in a position of authority to judge between the two positions.

As with all informal fallacies, this argument gains its force from superficial similarity to a good argument.  For instance, if I tell my child "I was once your age, and I ate a whole bag of candy, and I was sick all night, so you should not eat that whole bag," that is a good argument that is very similar to the bad one, except in this case I've actually provided support for the relevance of my experience.
I'm a bit lost in some of your argument, but will unwarily speak my piece.

First, as Conifold has indicated, you see to be anticipating some of Kant's arguments by alluding to "necessary inferences."  But I don't think it is quite accurate to say that Hume has a "problem with induction." Induction works and is, by definition, "mostly" correct. Hume was only pointing out that certain "necessary truths" were far more inductive than people had suspected. The warrant of Newton mechanics was by no means absolute, as proved to be the case. It's just that the "laws" of science, however reliable, do not get us beyond induction.

It has been pointed out that Hume seems inconsistent in arguing against necessary "causality" in observed phenomena, while apparently accepting some Lockean object "I-know-not-what" that "causes" our experience of that object. I believe this is what you are saying, partly. 

But here again, Hume is only pointing to apparent regularities. He cannot be arguing that the "I-know-not-what" necessarily "causes" our experiences in some incorrigible correlation. How would we know? As with induction in natural sciences, such a necessary "correlation" behind our experiences cannot be observed or proved. Hume does not think all of this is a big problem. It is the overextension of knowledge on the basis of unwarranted certainties that are a problem, as the financial industry would do well the remember.

I hasten to add that I am not a scholar of Hume or anything else, and I may have misunderstood your question. I hope others will intervene if so. In sum, there is no big problem of induction, as long as you know that is what you are doing. Science has come to agree with Hume (or with Kant--they are in some ways more similar than not), that "causality" is our mathematical determination of probabilities to be formulated according to experience on a case-by-case basis. Not a universal glue. Only a "necessary" prop or condition of being what we are.          
Yes, literally that is circular reasoning.

However, it may be the case that one of the following is true:


A is probable because of B and C. D is probable because of E, F and A.
A is probable because of B, C and D. D is probable because of E and F.


And obviously, these don't use circular reasoning.
Almost all axioms in mathematics have been validated as intuitive concepts before efforts are undertaken to define them logically.  For instance, the Greeks axiomatically believed there were no irrational numbers partly because the lack thereof allowed them to believe numbers and geometry were inseparable, and that was intuitive at the time.  They held their intuitive belief until someone formally showed that their approach lead to paradox ( Hippasus, presumably).  The axioms for addition and multiplication were based on intuitive needs for day to day life, and only axiomatized later when we wished to use them to solve difficult problems such as those involving infinity.  Even the modern axioms of set theory, which are used in modern mathematics to "prove" nearly everything, were initially founded in the intuitive idea of "a collection of things," and only had to be rigorously defined as a result of that intuitive idea allowing paradoxes into the mathematical constructions in ways that were deemed unacceptable.

In modern day schools, we even teach the intuitive approach in many cases.  Consider calculus.  Derivatives are typically taught intuitively, rather than relying on their formal definition using epsilon-delta proofs.  And the theory behind Common Core, the method of teaching mathematics which is currently taking over America, the goal is to develop a more intuitive understanding of the math first before teaching the abstract versions of that math later.
The answer is: No, endless division does not lead to nothing. It leads to "not nothing." This is one answer, anyway, to this a wonderful old question, going back at least to Zeno and "resolved," for practical purposes at least, by the "limit" in Calculus.

The uneasy relations between "zero" and "infinity" led the Greeks to abhor and avoid both. They are unobservables that simply bring in endless problems of incoherence and the ever-lurking "infinite regress," the quicksand of dialectic. By contrast, the atom of Democritus was, unlike ours, definitively "something" bordered by definitive "void."

What you get at the "very bottom" of infinite divisibility became both a mathematical and theological debate when Newton and Leibniz formalized the "infinitesimal" in calculus. Was the least possible "something" the same as "nothing"? All of physics depends on the answer being... no. It is a "not nothing," which Berkeley memorably characterized as "the ghosts of departed quantities."

In physics, Wheeler addressed the problem by coining the phrase "it from bit." This means that "information" is the fundamental unit, and is minimally defined as a "bit" or a "something/nothing" an "either/or" a "(0/1)" And it is precisely because of this irreducible instability or "possibility" that we have "something" and motion, rather than just "nothing."

This is, to the horror of hard-headed physicists, not unlike what Hegel suggests at the beginning of his Logic by deriving "becoming" from the irreducible overlap and interplay of the meanings of "being" and "nothing." To say something "is" is to say "nothing" about it. Thus, the idea of "being" itself gives rise to the idea of "nothing." Which now itself has a "being." And because these bare ideas collapse into one another they give rise to "becoming"... or transformation, motion, change.   

So, the problem is not that such knowledge is "impenetrable," but that it is all too penetrable. One arrives by analytic divisibility not at the "atom" or at "nothing," but at an irreducible instability. The answer is that infinite divisibility leads to something that is "not nothing" and is also the generative power of "nothingness" or "negation." Which Sartre, incidentally, equates with us. For after all, there is always "something else" which is doing this endless dividing.      
Given that your conclusion is contradictory, it cannot be true in classical logic, but that doesn't necessarily mean that any argument that has it as its conlusion is invalid. It is possible for an argument to have inconsistent premises. So, for example, the following argument is valid:


The sky is blue.
It is not the case that the sky is blue.
Therefore, the sky is blue and it is not the case that the sky is blue. 


This is valid syntactically because 3 follows from 1 and 2 by rule of conjunction, and it is valid semantically because every model of the premises is also a model of the conclusion (because there are none of either). 

The important thing to remember about validity in logic is that it is not an evaluative term meaning that an argument is good or strong or convincing. When assessing an argument it is important to ask whether it is sound (the premises are true) and whether it is cogent (the premises support the conclusion). 
Hint

For the first part :

1) P → Q --- premise

2) ¬Q --- assumed [a]

3) P --- assumed [b]

4) Q --- from 1) and 3) by →-elimination

5) ⊥ from 2) and 4)

6) ¬P --- from 3) and 5) by ¬-introduction, discharging [b]


  7) ¬Q → ¬P --- from 2) and 6) by →-introduction, discharging [a].


The other part of the proof is similar.
First of all, I'm going to assume that you're working within classical propositional logic. Therefore, we can symbolize the expression 'Grass is green and it is not the case that grass is green' as:

(G & ~G).

Now, note that the formula (G & ~G) is unsatisfiable (i.e. it cannot be made true). To convince yourself of this, just draw the truth table for (G & ~G).

So, how does this relate to the answer to your question? Well, the standard definition of a valid argument is an argument in which there is no way to make all the premises true while making the conclusion false.

It seems that the conclusion of this argument is always going to be false. So, in order to check whether the argument is valid, we have to check whether or not there is a way to make the premises all true. If there is a way to make all the premises true, then the argument will be invalid because then the premises will be all true while the conclusion is false. If there is not such a way to do this, then the argument is valid.

Overall, it looks like you would need more information to decide whether or not the argument is valid. In particular, we would need to know whether or not we can satisfy all the premises of the argument at once. No, we cannot tell whether or not the argument is valid without knowing the premises.
An argument is formally valid just in case it can never have true premises and a false conclusion.  Validity certifies truth preservation.  There are two trivial cases of validity --when the premises are necessarily false or when the conclusion is necessarily true.

In this case, the conclusion cannot be true, but in the case the premises are necessarily false, the argument will be still be valid, because truth is "preserved" (there is no truth in the premises, so none is certified for the conclusion).
There's three basic answers you're going to run into in philosophy.

First, there's a Platonist answer where what we encounter in the world are echoes of ideas -- not words. And thus, things correspond to more real ideas. These are the Forms in Plato's philosophy and they are eternal and unchangeable. I wouldn't say this view is too popular.

Second, there's an Aristotelian answer where things have essences or souls (in the case of living things) that guide what they are. We in turn perceive these through our senses, and ideally our linguistic terms refer to these essences. Again, the main philosophical element in these views will not be words. Instead, words will be terms for these things we capture (or words for things we mis-capture)

Third, there are views that take words to belong to us and then to somehow relate to things in the world. Goodies' answer is largely about one version of this. What these views share is a belief that terms are in our head. One of the older versions of this is "nominalism" which was the medieval view that the primary thing we are working with is names rather than essences or forms.

I would say most contemporary philosophers have some version of the third view. As far as I'm aware, there are roughly speaking three versions of the third view.

One version follows the logical positivists to some extent in analyzing language into pieces. Some followers of this view will talk a lot about "language games" (following Wittgenstein) Others may speak about "rigid designators" and how our words pick out particular things across all possible worlds (following Kripke). A third view looks at "speech acts" and the performative role of language in our cultures.

A second version follows Kant in believing that ideas are in our head but have a normal arrangement as a function of the sort of rational beings we are. (I take it that this would capture the views of language of Marcia Baron, Christine Korsgaard, O'Nora O'Neill)

A third version follows Hegel and thinks that ideas are shaped by our thought but that the arrangement is dynamically changing. (Contemporary followers of this sort of account may also pay homage to the second variation. I'd put Seyla Benhabib and Jurgen Habermas here)

I take most of the followers of these versions to believe our words do have some relation to reality but this may not be true for each individual in each camp. For instance, Rorty seems to think our words have a merely arbitrary relation to reality and he's a descendent of the first variation on this third one.

Some "post-modern" interpretations -- so called -- in part because they reject this is that our words have no relation to anything. How true this is of any person any labels "post-modern" is unclear. But at certain moments Derrida seems to be doing this.
I feel like this question is predicated on a misunderstanding of mathematical platonism. Mathematical Platonism is not, strictly speaking, part of Platonism. The name comes out of a kind of analogy to Platonism. Mathematical Platonism is the view that The language of mathematics refers to and quantifies over abstract mathematical objects, and that these objects are independent of any rational agent.

The mathematical ideal of twoness is not two pebbles or two bottles. It's (usually) the set {{}, {{}}}. You can use another definition, but this one is the most common construction of the cardinals. Likewise, the other natural numbers are defined (recursively) as follows:

0 = {}
1 = {{}}
2 = {{}, {{}}}
3 = {{}, {{}}, {{}, {{}}}}
n = {0, 1, ... , n - 1}

There's no sense in which a circle is more abstract than the number 2. They are all abstract object. Asking if they are objects of the same kind is complicated because in order to be meaningful, that must be done within a mathematical context, and mathematics is very careful about assigning structure to objects. For example, strictly speaking, the group of integers and the ring of integers are different objects. Some mathematical objects have arithmetic properties, some geometric, some both, and some neither.
This question needs quite a bit of unpacking. For a start, one must distinguish between truth and evidence. To a realist, something may be true without there being any evidence for it. I have no evidence as to whether it rained on Lands End in England at noon on May 1st of the year 1 CE, and I strongly suspect that no such evidence exists or will ever come to light. But I believe that proposition is either true or its negation is true. I have no evidence as to whether matter existed prior to the big bang, but I'm willing to believe that either it did, or it didn't. 

This means that the primitive idea that only verifiable sentences are meaningful is simply false. Furthermore, sentences, and even whole theories, typically are not verifiable at all, but only falsifiable. Even if we accept a kind of inductive reasoning, such as that of bayesianism, we cannot speak of verifying hypotheses or theories in any absolute sense, but only in comparison to rival hypotheses. If I have two rival hypotheses A and B, and A gives a better account of the data than B, then I'm willing to say that A is confirmed relative to B, but I cannot rule out that an even better hypothesis C might come along in future and trump both of them. 

I suspect that despite your question being framed in terms of truth, what matters more to you is whether one has evidence for a belief. This question might be phrased, should one believe anything without some evidence? Even this is a highly contentious claim. To begin with, you have to address Agrippa's Trilemma. The only thing that can provide evidence for a belief is apparently another belief, so where does evidence come from? Any evidential claim must either (1) be circular, (2) involve a regress, or (3) involve an appeal to a belief that is infallibly and indubitably true. None of these options is very appealing. 

Even if you think you can surmount this hurdle, one must address conventionalist objections that what counts as evidence depends on all kinds of methodological and linguistic assumptions. So you and your opponent may be working with different criteria of what counts as evidence with respect to some domain. What counts as evidence of the existence of God, for example? (Wittgenstein, and the later Carnap, regarded theological language as a distinct language game with its own rules that allowed assertions to be made that didn't admit of falsification in the usual way.) 

And even if, in straightforward scientific cases, all this can be resolved satisfactorily, does it generalise to other kinds of judgement, e.g. moral or aesthetic judgements? Is it true that one ought to do unto others as you would have them to do to you? Is it true that Adolf Hitler was a bad man and Albert Schweitzer was a good man? Is it true that wisdom is a virtue and greed is a vice? Is it true that Bach's music is beautiful and my tuneless whistling is not? One might reasonably have firm beliefs about any of these without being able to explain how it is supported by logic or empirical data. 
NO, but there are https://en.wikipedia.org/wiki/Decidability_(logic)#Decidability_of_a_logical_system decision procedures for modal logics.

See : James Garson https://books.google.it/books?id=rFpbAgAAQBAJ&pg=PA187 Modal Logic for Philosophers (2nd ed 2013), page 187,

or : Brian Chellas, https://books.google.it/books?id=YupiXWV5j6cC&pg=PA62 Modal Logic : An Introduction (1980), page 62.
The tricky part of an argument like this is the proof of Utility(X).  This argument works very well in situations where the utility is obvious (I either do God's will by helping this orphan child, or I make myself feel better by helping this orphan child), but it gets more difficult to assign meaningful utility values in other situations.  We humans are not very good at estimating these sorts of values (is 54627 * 183059 - 2,984,120 * 1 greater or less than 0?)  This makes the wager tricky.

However, even if we are given our Utility function, its still tricky, because we have to use that function after it has been proven in order to determine if any action is good or bad.  Thus our Utility function must include the utility costs of applying the formula.  Otherwise we may spend forever in a chaotic loop around U(X) = 0, unsure how to proceed.  The Utility function must be self-referential to avoid this issue.

The fact that you are going to use probabilities on these utility functions implies you need the basics of arithmetic.  This doesn't sound like anything too important, but it's not a trivial detail, as we soon find out.

Now the final question before things get crazy is: do you have to be able to prove the validity of your results?  If you are willing to take this on faith, as most if not all religious beliefs are taken, you can rest assured that this sort of logic can be made to work.  However, for the faithless, who must rely on hard mathematics and provability, the concept gets squirely fast.

Why?  Well, if you put all of these pieces together, you have the basis of https://en.wikipedia.org/wiki/Tarski's_undefinability_theorem Tarski's undefinability theorem, and whenever his name gets involved, you know you're in the really messy bits of logic and mathematics.  His theorem basically states that no self-referential system which can prove all truths of mathematics (read: contains the Peano axioms of arithmetic) and contains negation can define its own semantics.  In particular, it cannot define what "true" means within the system.  His proof involves the diagonal lemma, which makes all sorts of things frustrating, but it ends up using the system's descriptions of arithmetic to form inconsistent results.  The way this occurs is specific to the system, but Tarski proved that it must occur in every system satisfying the ruleset he was exploring (and its surprisingly easy to accidentally support that ruleset).

This means that you get stuck with the interpretation of the Utility function as being handled outside of the language of said functions (a metalanguage feature).  This creates all sorts of interesting loopholes that are impossible to plug within the Utility function.

There are ways around this.  I've enumerated a few in my own exploration, but the key is that you have to describe the Utility functions in a language which sidesteps Tarski's work.  If you accidentally step into the languages he described in his undefinability theorem, that step seals the fate of not only the resulting Utility function, but the entire class of Utility functions created in that form.


You can design the system to refuse to admit the diagonalization lemma.  Dan Willard has created a class of curious self-verifying theories which are intentionally too weak to support diagonalization (in particular, multiplication is not total in his theories).  Wisps of this sort of thinking show up because Dan's systems start with an infinity (everything), and subtract and divide from there, rather than starting from 0 and adding/multiplying, and I've seen several belief systems which focus on being "part of everything."
You don't have to prove your system.  This can be undervalued, but remember that nobody has been able to demonstrate a proof of their religion which is satisfactory for all other people.  Faith is kind of a big deal for religions.
You can admit uncertainty.  If your system is not complete, meaning it does not admit a proof for everything, you can avoid the provability issues Tarski addressed.  The phrase "if it is God's will" comes to mind.
You can omit negation from logic.  Negation is a key part of Tarski's proof.  If you never negate anything, his work cannot disprove yours.
You can weaken the wager slightly.  If you can demonstrate that U(X) >= 0, whether or not there is a God (i.e. it's not bad, no matter what), then it is trivial to show that U(X) cannot be less than 0.  "Do what you will, may it harm none"


I'm certain there are others, but those are the ones I have found interesting.

  https://en.wikipedia.org/wiki/Fallacy A fallacy is the use of invalid or otherwise faulty reasoning for the construction of an argument.


However, in this case, there is no argument. Alice says something which we know is not true. Then if Alice had used an argument to argue "Bob is smart", that argument would probably have been fallacious. But the simple act of saying something untrue is not fallacious, because there is no reasoning involved.
Subject and predicate are linguistic categories;  thus, from a logico-linguistic point of view, we have to consider also so-called http://plato.stanford.edu/entries/logical-constants/#SynTer Syncategorematic terms, such as the connectives : “and”, “or”, “if … then”, etc.

If we consider instead  http://plato.stanford.edu/entries/categories/ Categories as a way to classify the kinds of entities in the world, we have to speak of http://plato.stanford.edu/entries/object/ Objects and http://plato.stanford.edu/entries/properties/ Properties; in this case we have to face with severasl issues :


what are http://plato.stanford.edu/entries/relations-medieval/ Relations ?
what are http://plato.stanford.edu/entries/abstract-objects/ Abstract Objects ?


In a sentence like :


  "Courage is a virtue"


we are predicating virtuosity of the subject courage; is thus courage an "existsing thing" ?
Yes, according to cosmology at a large scale the universe is homogeneous and isotropic, i.e. it looks alike in all space directions. As a consequence, no distinguished point in space exists, i.e. no cosmic center.

The statement that the universe started from one point, does not mean that a certain point in space was distinguished as the birth place of our universe. The start of our universe was also the birth of spacetime. One cannot locate this event in a space which existed before.  

The above mentioned result about the homogenity and isotropy of our universe does not derive from the Theory of Relativity. Instead, it is a result of observation, mainly of the cosmic microwave background radiation (CMB).

But the most simple solutions of Eintein's field equations from the General Theory of Relativity are homogeneous and isotropic. Hence the observation is in accordance with theory.

Secondly, a deep implication of the Special Theory of Relativity is the relativity of space and time. Both concepts do not have an absolute meaning, only the combined notion spacetime has an absolute meaning. Absolute means that spacetime is independent from the observer and his coordinate frame. While the decomposition of spacetime into space and time depends on the selected coordinate frame - up to a certain degree. As a consequence simultaneouness depends on the observer. Two observers moving with a certain velocity relatively to each other observe different events as simultaneous.

According to my opinion, the latter set of philosophical implications has not yet been fully received by philosophers. See also a previous discussion in this blog https://philosophy.stackexchange.com/questions/26689/time-and-space-a-subject-of-metaphysics Time and space – a subject of metaphysics?
1) An invalid argument may have a true conclusion: 

Consider the invalid argument: "Because Socrates is mortal, all humans are mortal." Nevertheless the conclusion is true: "All humans are mortal."

2) An invalid argument may have a false conclusion:

Because Socrates is Greek, all humans are Greek. 

3) A valid argument may have a false conclusion:

Because all humans live in Africa, also Socrates lives in Africa. (this kind of valid argument is named "ex falso quodlibet")

4) A valid argument may have a true conclusion:

Because all humans are mortal, also Socrates is mortal.
Excellent question. The semantic function of 'meaning' .. assigning value or implication to a stated proposition .. can be the same in both abstract and concrete scenarios. But with abstract concepts (like 'life') it is far more difficult to establish clarity because there are so many valid possibilities. All language is essentially a mapping, words mapped to underlying definitions by convention. Useful results of that mapping grow more difficult as subject matter is further abstracted.

That's why metaphor is often a powerful tool to make abstract notions more understandable. By its own definition (Lakoff, 1980) it maps the abstract to the concrete.

Can't help but turn to Wittgenstein's classic quote: "The limits of my language mean the limits of my world" Tractatus, 5.6 (1921). This provides a famous example of the use of meaning in philosophy, while shedding light on both langage and it's role in mapping value among abstract concepts. It's hard to talk about meaning without using it in a sentence. Even for Wittgenstein.

Also need to give Kant a nod on this one. His nominal definiton of 'truth' in Critique of Pure Reason "agreement of knowledge with its object" CPR(A) Part 2, Transcendental Logic III, is equally telling. Here Kant again demonstrates the importance of semantic mapping, in pursuit of understanding. In his framing, as I understand it, the predicate of an analytic proposition provides immediate elucidation of a subject, while a synthetic prosposition expands that meaning by relating the subject to other subjects. In either case, we are still mapping A to B, in order to establish meaning of the subject, or more specifically, to clarify knowledge. There is certainly more to the story from Kant, but since I am still deep reading CPR, I best stop here. I am just at the edge of my understanding of his Categories.

In modern terms, I'd argue it's about creating context, a critical obligation for those who traffic in the understanding of abstract notions. Philosophers, for example. Hope this helps. 
Principles, or arche is what Aristotle in line with his predecessors take as the principles by which Nature is understood (this is to be contrasted with his own concept, which is nature or entelechy).

Some are prior to others; so there are the simplest such, in that they cannot be reduced to any more basic; he says that there is more than one and less than three. 
The best way to do the proof depends greatly on what rules of inference you are allowed to use and whether you can do proof by truth tables. For general information, see https://philosophy.stackexchange.com/questions/30139/how-do-i-check-if-two-logical-expressions-are-equivalent/30141#30141 How do I check if two logical expressions are equivalent?

With Material Implication

The simplest proof is you are allowed to do https://en.wikipedia.org/wiki/Material_implication_(rule_of_inference) material implication:

 1. | P  -> Q                 A
 2. | ~P v Q                  Material Implication 1
 3. (P -> Q) -> (~P v Q)      CP 1,2
 4. | ~P v Q                  A
 5. | P -> Q                  Material Implication 4
 6. (~P v Q) -> (P -> Q)      CP 4,5
 7. (P -> Q) <--> (~P v Q)    Biconditional Introduction 3,6


Without Material Implication with DeMorgan

 1. | P  -> Q                 A
 2. | | ~ (~P v Q)            A
 3. | | ~~P & ~Q              Dem 3
 4. | | ~~P                   &E 3
 5. | | P                     DN 4
 6. | | Q                     MP 1,5
 7. | | ~Q                    &E 3
 8. | (~ P v Q)               Contradiction Elim. 2-7
 9. (P -> Q)  -> (~P v Q)    CP 1-8


And then repeat similarly for the opposite side...

Truth Table Proof

P   |  Q  |  P -> Q  | ~P v Q
T   |  T  |    T     |    T
T   |  F  |    F     |    F
F   |  T  |    T     |    T
F   |  F  |    T     |    T


See also https://math.stackexchange.com/questions/38713/help-to-understand-material-implication https://math.stackexchange.com/questions/38713/help-to-understand-material-implication
You presume that existence is based on a mathematical proof, which is not a well agreed upon assumption.  For instance, virtually every religion disagrees with your assumption.  Thus it will be difficult to convey any results of that assumption without stating it.  Most people believe there are things that exist for which they have no proof, suggesting you may be redefining existence is a way dissimilar to how others use it.

That being said, you may want to check out Dan Willard's self verifying theories.  They are mathematical theories which prove their own consistency.  They would be where I would look first.  They escape Godel's incompleteness theorem, which would be a strong first step.

ponders: how does the universe exist by this definition?
It would be correct to say: "An infinite set has proper subsets which are infinite again."

Otherwise trivial counter examples exist as Mauro points out. 

The concept of infinity as detected by Cantor is much more subtle than a simple argumentation with symmetry captures. 

I agree with Mauro that Cantor detected an infinite number of different kinds of infinity. It is a fascinating domain from mathematical set theory. See my comments to @Iowa for some example definitions and results.
The "foundations" of http://plato.stanford.edu/entries/frege/#FreLan Frege's analysis of language are in his articles :


https://en.wikipedia.org/wiki/Function_and_Concept Funktion und Begriff (1891)
https://en.wikipedia.org/wiki/Sense_and_reference Über Sinn und Bedeutung (1892)
https://en.wikipedia.org/wiki/Concept_and_object Über Begriff und Gegenstand (1892).


Relevant for your question is the first one; see :


https://books.google.it/books?id=MVwgAQAAIAAJ& Translations from the Philosophical Writings of Gottlob Frege (P.Geach & M.Black editors, 1952), page 21-on,


or :


https://books.google.it/books?id=zVR0QgAACAAJ Collected Papers on Mathematics Logic and Philosophy (B.McGuinness editor, 1984), page 137-on.


Following a detailed analysis of mathematical formulae :


  The two parts into which a mathematical expression is thus split up, 
  the sign of the argument and the expression of the function, are dissimilar; for the argument is a number, a whole complete in itself, as the function is not. [page 141]


Frege generalize this analysis to natural language statements [page 146] :


  Statements in general, just like equations or inequalities or expressions in Analysis, can be imagined to be split up into two parts; one complete in itself, and the other in need of supplementation, or 'unsaturated.' Thus, e.g., we split up the sentence 
  
  
    'Caesar conquered Gaul' 
  
  
  into 'Caesar' and 'conquered Gaul'. The second part is 'unsaturated' - it contains an empty place; only when this place is filled up with a proper name, or with an expression that replaces a proper name, does a complete sense appear. Here too I give the name 'function' to what is meant by this 'unsaturated' part. In this case the argument is Caesar. 


Previously [page 146], Frege writes :


  We thus see how closely that which is called a concept in logic is connected with what we call a function. Indeed, we may say at once: a concept is a function whose value is always a truth-value. 


Thus, the Fregean analysis of the statement :


  "the sky is blue"


must decompose it into two parts : the saturated part, i.e. the name of an object : "the sky", and the unsaturated one, i.e. the expression for a concept : "___ is blue".

In the "functional" syntax of https://en.wikipedia.org/wiki/Begriffsschrift Begriffsschrift (i.e. concept notation) :


  
    blue(sky).
  

Because I is particular affirmative and Rule 4 states :


  A Negative Premise Requires a Negative Conclusion,


i.e. if one of the premises is negative, also the conclusion is.
http://plato.stanford.edu/entries/peirce-semiotics/ Peirce' Theory of Signs is complex and - unfortunately - there are no complete treatises dedicated to semiotics by Peirce himself :


  Across the course of his intellectual life, Peirce continually returned to and developed his ideas about signs and semiotic and there are three broadly delineable accounts: a concise Early Account from the 1860s; a complete and relatively neat Interim Account developed through the 1880s and 1890s and presented in 1903; and his speculative, rambling, and incomplete Final Account developed between 1906 and 1910. 


For an early exposition, see : http://www.peirce.org/writings/p32.html On a New List of Categories, Proceedings of the American Academy of Arts and Sciences, 7 (1868). [See also https://en.wikipedia.org/wiki/Categories_(Peirce) Peirce's Categories.]

For a full-lenght book on Peirce's semiotics, see :


T.L. Short, https://books.google.it/books?id=NyGsVelOwKYC&pg=PR5 Peirce's Theory of Signs (2007).




For relevant "semeiotics" quotes, see http://www.commens.org/dictionary/term/sign here. [See also https://en.wikipedia.org/wiki/Sign_(semiotics)#Triadic_signs Peirce's Triadic signs.] 

It seems to me that the diagram is not present, also if we have clear "descriptions" of it :


  The easiest of those which are of philosophical interest is the idea of a sign, or representation. A sign stands for something to the idea which it produces, or modifies. Or, it is a vehicle conveying into the mind something from without. That for which it stands is called its Object; that which it conveys, its Meaning; and the idea to which it gives rise, its Interpretant. [from a 1893-5 Ms., Chapter II: The Categories.]
  
  A sign, or representamen, is something which stands to somebody for something in some respect or capacity. It addresses somebody, that is, creates in the mind of that person an equivalent sign, or perhaps a more developed sign. That sign which it creates I call the interpretant of the first sign. The sign stands for something, its object. [from a c.1897 Ms., On Signs.]
  
  [...] a sign is a thing related to an object and determining in the interpreter an interpreting sign of the same object. It involves the relation between sign, interpreting sign, and object. [from the 1903 Ms.prepared for the Lowell Lectures of 1903.]




Possible source of the diagram (if my conjecture about its non-Peircian origin is sound) :


https://en.wikipedia.org/wiki/Charles_Kay_Ogden Charles Key Ogden & https://en.wikipedia.org/wiki/I._A._Richards Ivor Armstrong Richards, https://books.google.it/books?id=EPeVQgAACAAJ The Meaning of Meaning : A Study of the Influence of Language upon Thought and of the Science of Symbolism (1923), page 11, with no explicit reference to Peirce.


But Peirce's ideas are summarized into Appendix D [page 279-90] :


  by far the most eleaborate and determined attempt to give an account of signs and their meaning is that of the American logician C.S.Peirce [...].


The diagram of Ogden & Richards is reproduced into :


https://en.wikipedia.org/wiki/Umberto_Eco Umberto Eco, https://books.google.it/books?id=BoXO4ItsuaMC&pg=PA59 A Theory of Semiotics (original ed.1975), page 59 :



  The diagram in question is the well-known triangle, diffused in its most
  common form by Ogden and Richards (1923) [...]. The triangle apparently translates Peirce’s : [here you can find reproduced your diagram].

Your question reminds me of an old joke: if the tail of a tiger were defined to be a leg, how many legs would a tiger have?  Answer: four, because defining a tail to be a leg doesn't make it one. 

The point is that definitions are not true or false, just appropriate or inappropriate. Even a nominal definition is not arbitrary. The purpose of a good definition is to make a distinction where there is a difference and avoid making a distinction where there is not. Good definitions carve reality at the joints. So, defining a tiger to be an eight-legged invertebrate is entirely unhelpful, as is defining a tail to be leg, but not actually false. We might call such a definition idiosyncratic or bizarre, but if someone wishes to use such a definition for their own purposes, then that is their affair. 

Of course, if one is talking about documenting the definitions of words as they are employed in common use, i.e. the sort of thing a lexicographer does when compiling a dictionary, then definitions can certainly be incorrect. In the normal sense in which "tiger" is used by English speakers, it definitely does not mean an eight-legged invertebrate. 
Taking your example: It is an aspect of the dam/gate that it is actually holding the water back. The dam is damming the water, preventing it from flowing. That is essential part of its being. Impeding itself already has the effect of impedance, it is negating something and by this has a positive effect.

Only because of that, as it is negating something, its removal can have this effect - or rather an effect different from the one that is caused by its being - at all: by ending of the effect of its being.

Thinking of the lack of a dam as causing something without ever having had any effect, i.e. without ever having been there and actually damming the river, seems to exaggerate the speech of cause and effect.
There are multiple options depending on how the anecdote is presented within the context of the argument.

To take an example I recently came across, many autistic people like to be referred to like that instead of as "person with autism."
If I point out a survey of 1k autistic adults showed a strong preference, and you reply "well, my sister has autism and she would hate to be called that" then we can either take this to be a false generalization or moving the goal posts depending on how the words are meant. There's a related case where it's a straw man.

If the intention is "my sister has preference x and therefore it's correct for everyone" then it's a false generalization, because you're portraying your sister's preference as generalizable without justification.

If it's changing the topic of the argument from "what do autistic people prefer" to "what does this particular person prefer" it's moving the goal posts.

In both these cases it's also a non sequitur. Any argument where the conclusion doesn't follow from the premises is a non sequitur

The strawman occurs if the point is that your survey doesn't apply to your sister, and therefore the data is wrong, because you're misrepresenting my position as "all autistic people..."

  Do impossible properties include all properties?


Why so ? I assume that you are referring to https://en.wikipedia.org/wiki/G%C3%B6del%27s_ontological_proof Wiki's exposition of Gödel's proof:


  Theorem 1: If a property is positive, then it is consistent, i.e., possibly exemplified.


Proof

1) P(φ) --- premise

2) ¬P(ψ) --- form 1) : if φ is positive, by Ax.2, ¬φ is negative, and thus there is a negative property ψ

3) ¬∀x(φ(x) → ψ(x)) --- by Ax.1: a property entailed by a positive one is also positive

4) ¬□∀x(φ(x) → ψ(x)) --- from 3)

5) ⋄∃x(φ(x) ∧ ¬ψ(x)) --- from 4) by properties of modalities, quantifiers and propositional logic


  6) ⋄∃xφ(x) --- from 5).


Thus, from 1) and 6), by Deduction Theorem:


  
    P(φ) → ⋄∃xφ(x)
  


i.e. If a property is positive, then it is possibly exemplified.
Kant speaks in B59f of Critique of Pure Reason (CPR) about the thing-in-itself.

The passage is an appendix to the part of CPR on Transcendental Aesthetic. The latter deals with time and space as the two forms of human intuition. Transcendental Aesthetic does not deal with synthetic a priori knowledge. The latter is  constructed by the human mind by the help of the categories, see the part on Transcendental Analytics.

Things-in-themselves are noumena (B310) – the equation holds at least in first approximation.

As a consequence, the transition is introduced as a transition from intuition, the human facility to process the input of the senses, to noumena. 

In B60 Kant clearly states: “The forms of space and time alone we can know a priori, that is, prior to all actual perception, and such knowledge is therefore called pure intuition. […] Even if we could impart the highest degree of distinctness to our intuition, we should not thereby come one step nearer to the constitution of object in themselves.”

How does Kant arrive at the existence of things-in-themselves? I think, that’s not difficult to understand: 

It is necessary to hypothesize a source of our sense perceptions. These hypothesized objects are the things-in-themselves. But all our experience results from our processing the input of the senses by the capabilities of intuition and mind. That’s the boundary. We cannot know how the world looks  before the boundary. But we can hypothesize that there are objects before the boundary.
The difference is that children at Halloween and in school plays can also get dogginess, and that simply "being brown" brings along no additional information. The whole nature of "natural kinds" is that 1) they cannot be obtained artificially and 2) they bring in a whole family of related traits also.
Following Kant's own system:

As the schematisms of the categories and for corporal (i.e. physical) objects are the same, the concepts (whatever their language-specific label may be) are the same for any human being. The schematism or necessary construction of concepts has the task to ensure that this argument cannot be held against Kant. 

That is the whole point of the Metaphisical Foundations of Natural Science and why this book is so important for completing Kant's transcendental theoretical philosophy of the CPR, which lacks a schematism of space and therefore all external being (world, things).

Again, this is described by Förster in The 25 Years of Philosophy.
NO: A premise is a logical sentence.  It can be atomic, (e.g. A) or complex (e.g. ¬A → B).

But no sentence can be at the same time a tautology, i.e. true for every interpretation, and contradictory, i.e. false for every interpretation.
Logic and rhetoric are not linked in this way.  There is no point at which the quality of a human being's ability to interact has anything to do with his logic.  So there is no point where an ad hominem fallacy stops being bad logic.

That does not mean it is bad behavior, especially not bad psychology.  Shame is an appropriate rhetorical and psychological tool.  You don't need input from philosophers, you need input from the Super-Nanny.  'The Gentle Art of Verbal Self-Defense' suggests that the appropriate way of dealing with arguments that are not arguments is to pretend you only understand logical statements until the other party becomes bored with you, and then start the argument over without them.
Firstly, in a strictly formal setting, we note that Gödel's first incompleteness theorem tells us that truth is not reducible to proof, so there are many truths which are not derivable.

In a formal setting, we chose our axioms because we believe them to be self-evident truths - i.e., true for no (logical) reason.  But why do we assume that only self-evident truths are not derivable.  

More generally, beyond the formality of mathematics, if one accepts that nature includes random processes, then such processes may provide examples of brute facts which are true simply because they are true and for no other (logical) reason.  For example, if one accepts that the human evolutionary process is driven by random mutations of our genetic material, then we are who and what we are for no logical reason.  It is true that humans exist on planet Earth, but it is not a logical necessity and it could have been otherwise.

Regarding true statements that contradict logic, one might argue that quantum superpositioning may provide examples.  Superpositioning is a phenomenon that is supported by experimental evidence, but it certainly appears to be illogical to assert that a particle can be simultaneously in two different states or two different locations.  Having said that, it may follow logically from the formalism of quantum theory that superpositioning is a logical necessity.  I'm not a physicist, so I'm not entirely sure.
Sure, you do it this way: 


If phi is an atomic sentence, phi is a wff.
If phi is a wff, then so is "not phi".
If phi and psi are wff, then so are "phi and psi", "phi or psi" "if phi, then psi".
nothing else is a wff.


You need some extra clauses if your logic includes quantifiers or modal operators, but the basic idea is the same. The definition of a WFF isn't circular--it relies upon the primitive notion of an atomic formula, but that isn't circular.
According to the Trancendental Aesthetics of Kant's Critique of Pure Reason space and time are the two kinds of human intuition. I.e. the two basic dimensions to classify the input from our senses, step one of mental information processing. 

The second step is the information processing by means of the categories. The output of both steps is named experience. 

Quite different is the contemporary concept of space and time according to current physics. Here the best elaborated theory is the Theory of General Relativit. It combines space and time as two components of 4-dimensional spacetime. Both component have only a relative meaning, relative to a given observer and his system of coordinates. 

But more: Spacetime is a physical quantity. It is affected by other physical quantities like masses. The first confirmation of the resulting curvature of spacetime was the observed bending of light when passing near big masses like the sun. An actual confirmation would be the observation of gravitational waves (LIGO experiment). They are oscillation of spacetime due to the radiation of gravitational energy by the merge of two black holes.

According to the General Theory of Relativity spacetime is an objective physical quantity "existing in itself", i.e. independent from the observer. This view is different from Kant's concept of space and time.

During the history of physics before Einstein many different concepts of space  or time were proposed, e.g. by Newton, Leibniz, Mach.

Added. If the question is, whether Kant considers space a noumenon, my answer is "no". Space is an intuition, i.e. one of our means to classify the input from senses. On the opposite, noumena are hypothetical objects in the outer world, which create the input for our senses.
Martin-Löf has a short article on this.

Kant and Contemporary Epistemology, Volume 54 of the series The University of Western Ontario Series in Philosophy of Science pp 87-99.
Analytic and Synthetic Judgements in Type Theory, Per Martin-Löf

He claims 


  So the usual form of judgement, A is true, is indeed  a special case of the existential form of judgement.


He relies thus on the intuitionist explanation of truth, [...], defined as existence of a proof, or construction, of the proposition.

And there, he says that the synthetic forms of judgement are the existential forms of judgement.

Since I cannot copy-paste the text, I give a you a picture of the relevant pages.

https://i.stack.imgur.com/uRd2F.jpg 

Another article without the machinery of the types is
Truth and Knowability on the Principles C and K of Michael Dummett 1998
He explicitly distinguishes between judgements, propositions, truths and proofs.

all these articles are available on

https://github.com/michaelt/michaelt.github.io/tree/master/martin-lof https://github.com/michaelt/michaelt.github.io/tree/master/martin-lof
The key word in your first quote is "certain". There is a widely held position, called fallibilism, under which no proposition, hypothesis, theory, or whatever, can ever be called truly certain, because it is always possible for us to be mistaken, or to be in a position where we would revise our claim that it is true if we came to possess new evidence. Several pragmatist philosophers, including Peirce and Dewey maintained this position, and also, in a slightly different form, did Popper and Quine. 

This is not a skeptical position: it is not saying we don't know anything, or have no reasons to believe what we believe, only that certainty is not achievable. One way to think about fallibilism is that if we supposed the opposite, i.e. that some proposition was infallibly and indubitably true, it would imply that no amount of evidence could possibly persuade us to change our minds about it. 

Historically, some rationalist philosophers have attempted to establish some propositions as indubitably true, e.g. Descartes with the Cogito, and some empiricists have attempted to build a kind of foundationalism from experiences that are supposedly primitive and incorrigible, e.g. early Russell and Ayer with the sense-data theory. These attempts have not achieved any wide acceptance, though both still have their defenders, and the question is still debated today. 
I think you are talking about the "tone argument" fallacy: http://rationalwiki.org/wiki/Tone_argument Tone_argument
An axiom of http://plato.stanford.edu/entries/logic-modal/ Modal Logic (at least: of some ML) is:


  (M) □A → A : "whatever is necessary is the case".


Thus, with ~A in place of A and using contraposition:


  ~~A → ~□~A.


With double negation and the definition of the operator ◊ (‘It is possible that’) in terms of □ (‘It is necessary that’): ◊A := ~□~A, we conclude with:


  A → ◊A.


Now we can apply transitivity to (M) and the last formula to get:


  
    (1) □A → ◊A.
  

Ontology and Ideology
Author(s): W. V. Quine
Source: Philosophical Studies: An International Journal for Philosophy in the Analytic Tradition
Location: Vol. 2, No. 1 (Jan., 1951), pp. 11-15
Stable URL: http://www.jstor.org/stable/431810


http://www.jstor.org/stable/431810 http://www.jstor.org/stable/431810

This being Quine we have the typical distortion and abuse of normal language. For that reason, as ultimately pointless though it is, we must first try to understand what Quine means when he uses the phrases 'ontological commitments' and 'ideological commitments'.

First of all Quine is speaking about the ontology of a theory and ideology of a theory. Or as he says:


  ontology and ideology in their relativized aspect


We get a definition of ideology of a theory:


  I have described the ideology of a theory vaguely as asking what ideas are expressible in the language of the theory.


He doesn't come out and say it straight but we can guess at his definition for ontology of a theory, presumably something along the lines, the ontology of a theory asks what can be said to exist as expressed in the language of the theory.

With those definitions (which already draw a distinction for us) let us turn to the explanations so we can further make clear the distinction, at least from Quine's perspective.


  -- the ontology of a theory, the ideology of a theory-belong to what is commonly called semantics. But, as I have urged elsewhere, a fundamental cleavage needs to be observed between two parts of so-called semantics: the theory of reference and the theory of meaning. The theory of reference treats of naming, denotation, extension, coextensiveness, values of variables, truth; the theory of meaning treats of synonymy, analyticity, syntheticity, entailment, intension. Now the question of the ontology of a theory is a question purely of the theory of reference. The question of the ideology of a theory, on the other hand, obviously tends to fall within the theory of meaning;


So there you have it. Though now you do don't you wish you didn't? If we take meaning to be synonymous with sense then we have the old sense/reference dichotomy. To answer your question then, for Quine the difference between an ontological commitment and an ideological commitment is a distinction of reference versus sense.
Wittgenstein's Tractatus is said to be Kantian because it shares an abstract theoretical structure which we identify as typically Kantian. Kant's metaphysics is based on broadly epistemological premises (the nature of experience, of concepts, of judgments, etc) rather than on properly metaphysical premises. Analogously, The Tractatus' metaphysics is based on linguistic premises (the nature of linguistic meaning) rather than on properly metaphysical premises. 

Kant:


  Now, if it appears that when, on the one hand, we assume that our cognition conforms to its objects as things in themselves, the unconditioned cannot be thought without contradiction, and that when, on the other hand, we assume that our representation of things as they are given to us, does not conform to these things as they are in themselves, but that these objects, as phenomena, conform to our mode of representation, the contradiction disappears. (Critique of Pure Reason Preface to 2nd edition)


Wittgenstein:


  5.6 The limits of my language mean the limits of my world.
  5.61 Logic pervades the world: the limits of the world are also its limits. So we cannot say in logic, ‘The world has this in it, and this, but not that.’ For that would appear to presuppose that we were excluding certain possibilities, and this cannot be the case, since it would require that logic should go beyond the limits of the world; for only in that way could it view those limits from the other side as well. We cannot think what we cannot think; so what we cannot think we cannot say either. (Tractatus)


Further, Kant argues about a barrier beyond which nothing can be known. Analogously, Wittgenstein argues about a barrier beyond which nothing can be said.

Kant:


  We may look upon it as established that the unconditioned does not lie in things as we know them, or as they are given to us, but in things as they are in themselves, beyond the range of our cognition. (Critique of Pure Reason Preface to 2nd edition)


Wittgenstein:


  5.62 This remark provides the key to the problem, how much truth there is in solipsism. For what the solipsist means is quite correct; only it cannot be said, but makes itself manifest. The world is my world: this is manifest in the fact that the limits of language (of that language which alone I understand) mean the limits of my world. (Tractatus)


P.S. Transcendental logic does not seem to be related here. Perhaps you are mixing transcendental (a property of inquiries) with transcendent (a property of things)?
Not an answer but some notes on language: "one cannot give more than one has," is just a vague common sense saying. It looks much like an impredicative definition - no matter how much one gives, it is never more than what one has, indeed. If you 'have' only the even integers you could give all integers just by halving them; or with only  0 and 1 you can give all numbers...

(and, also the diagonal of square is an irrational - 'infinite' - number when the side is an integer but you could chose it to be the obverse: an integer diagonal & irrational side)

'Contain' is a spatial metaphor (or catachresis), sub speciae eternitatis all consequences are 'contained' in principles or axioms; but for a temporal being an eternity is needed to derive them. And 'after an eternity' is a polite way of saying never...

And, btw  are you sure that you are not a virtual killer?  'In potentia' is one of these expression that gave a bad name to scholastics. 
If such a universal logical language exists, it would be subject to some very peculiar limitations which were developed by Alfred Tarski.  His https://en.wikipedia.org/wiki/Tarski's_undefinability_theorem undefinability theorem puts some very interesting limitations on such a language.  In particular he considered a language which:


Is a formal language (its particularly hard to provide a concrete semantic model for non-formal languages)
Was self referential (necessarily for arguing that a language is truly universal.)
Contains a negation operator (we like to think negation exits in logic, so its a reasonable requirement)
Is powerful enough to prove all the truths in arithmetic (otherwise discussing matters of mathematics could be tricky)


He demonstrated that any such language cannot define its own semantics.  He argued that, to define its own semantics, such a language would need to define a True(n) predicate which returns true if and only if n was some form of a sentence in that language.  His particular proof involved encoding a sentence using Godel numbers, and making True(n) be a predicate that accepts a number as an argument.  He then used the diagonalization lemma to demonstrate that there must exist a sentence which is true but True(n) is false.

There are a few subtle limits.  Dan Willard, for example, explored mathematical systems where multiplication was not a total function, which was just enough of a tweak to prevent the diagonalization lemma from being proven in the language.  However, those systems are not as mainstream as the ones you and I learned in school.

Now, amusingly enough, this suggests that the semantics of English may not be fully sufficient for us to agree upon what "logic" and "mathematics" are, semantically, so I cannot claim this proves that such a universal language cannot exist.  However, hopefully those limitations provided by Tarski will help you explore your own answer to the question.
In a world without S, "All S is P" is true. You must keep in mind how it is formulated in first-order logic: 


  ∀x(Sx→Px)


Since there are no S things, the implication is always (vacuously) true.

So there's no case in which "All S is P" and "Some S is not P" are both false.
Generally, there are 2 main ways to demonstrate that a given formula is a tautology in propositional logic:


Using https://en.wikipedia.org/wiki/Truth_table truth tables (a given formula is a tautology if all the rows in the truth table come out as True), which is usually easier.
Using natural deduction with no premises, which is usually harder. If you get a conclusion using no premises then it is a tautology, since propositional logic (with respect to natural deduction) is https://en.wikipedia.org/wiki/Soundness sound.

One possible answer is this: http://plato.stanford.edu/entries/hilbert-program/ Hilbert's program was to prove the consistency of math (including e.g. set theory) using a very modest set of assumptions. Gödel's second incompleteness theorem shows this to be impossible. In fact, it shows that the method should be reversed -- we must use stronger theories to prove consistency, rather than weaker ones.

Peter Smith has a clear written passage about this in his great Introduction to Gödel's Theorems:


  The real impact of the Second Theorem isn't in the limitations it places on a theory's proving its own consistency. The key point is this. If a nice arithmetical theory T can't even prove itself to be consistent, it certainly can't prove that a richer theory T+ is consistent. Hence we can't use 'safe' reasoning to prove that other more 'risky' mathematical theories are in good shape. For example, we can't use unproblematic mathematical reasoning to convince ourselves of the consistency of set theory (with its postulation of a universe of wildly infinite sets).
  
  And that is a very interesting result, for it seems to sabotage what is called Hilbert's Programme, which is precisely the project of defending the wilder reaches of infinitistic mathematics by giving consistency proofs which use only 'safe' methods.


Does this actually trouble mathematicians? Probably not in practice.

As for the first incompleteness theorem, again I don't think it troubles mathematicians. Even if confronted with an unprovable sentence, they can try and prove it (or its negation) in a stronger theory. See, for example, the case of https://en.wikipedia.org/wiki/Goodstein%27s_theorem Goodstein's Theorem.
Not at all. As with every scientific theory, relativity didn't come up from nowhere. Relativity is a response to some theoretical, predictive and explanatory failures of classical mechanics, like explaining some gravitational phenomena and the behavior of light. It even inherits some notions of classical mechanics (e.g. Newton's mechanics also included relative time and space, and Relativity ditches absolute time and space) and builds on them to come up with a better theory. 
As for the term for this in philosophy of science, I don't quite understand what you mean. Are you asking about progress in science?
In physics time is measured by clocks. 

In the more refined version of the Special Theory of Relativity time is one component of the four-dimensional spacetime. One has a certain choice how to decompose spacetime into time and space. Hence the measured time depends on the choosen system of coordinates. There is no global time, each observer measures his own local time. But there are rules how to convert the different results.

But as you write, a term like "duration of time" is not employed in physics.

It is discussed in the philosophy of physics whether time flows or whether we move through a preexistent 4-dimensional spacetime. See "Chapter 5. The Frozen River. Does Time Flow?" from Greene, Brian: The Fabric of the Cosmos (2004) 

My question: Who does speak about a duration of time?  
Assume A U {¬B} ⊢ ⊥

Now we need to show that A ⊢ B:

Assume ¬B, get a contradiction from premise A and from A U {¬B} ⊢ ⊥, and then conclude B. (You should fill here the natural deduction steps.) That's it.
To prove the equivalence P = Q we must prove P > Q and Q > P. The first is not very hard, the second is a bit tricky.

Here it is:

1.  | P           assumption
2.  | P > ~R      P1
3.  | ~R          1,2
4.  | | ~Q        assumption
5.  | | ~Q > R    P2
6.  | | R         4,5
7.  | | R * ~R    3,6
8.  | ~~Q         4-7
9.  | Q           8
10. P > Q         1-9
11. | Q           assumption
12. | | ~Q        assumption
13. | | | ~P      assumption
14. | | | Q * ~Q  11,12
15. | | ~~P       13-14
16. | | P         15
17. | ~Q > P      12-16
18. | | P         assumption
19. | | P         18
20. | P > P       18-19
21. | P v ~Q      P3
22. | P           17,20,21
23. Q > P         11-22
24  P = Q         10,23

There's no problem in saying that


  '7 is a prime number' is true if 7 can only be divided by 1 or itself


since it is of course consistent with


  '7 is a prime number' iff 7 is a prime number.




It is important to note that the so-called schema T:


  X is true if and only if p


is not Tarski's definition of truth but rather it is a condition that, on his conception, any definition of truth should satisfy (that is, a correct truth definition should entail all instances of schema T). And so it does not mean that p is the only way, so to speak, in which X is true.
Probability theory can be understood as an extension of the propositional calculus, and even of Aristotelean logic, but not of predicate logic in general. To clarify, propositional calculus is basically the truth-functional calculus of 'and' 'or' 'not' and material implication. Aristotelean logic permits quantification in one variable, so that one can capture statements such as "all A's are B's". Predicate logic is a great deal more expressive and permits quantification in any number of variables in a single sentence: for example it allows one to capture the difference between "there is some girl whom every boy loves" and "every boy loves some girl" and prove that the latter entails the former and not vice versa - not something one could do with Aristotelean logic. 

If one uses an epistemic interpretation of probability, then one may speak of the probability of a proposition being true. Probability calculus provides a way to deal with ands, ors and nots that is compatible with the propositional calculus, but it does not allow one to peek inside an atomic proposition and say that individual components of it are more or less probable. If a proposition contains multiple quantifiers, there is no completely general way of accounting for how the probability of the expressions within it relate to the probability of the proposition as a whole. Some progress has been made into what is called probability logic and it remains an active area of research. Jon Williamson has written about it in the paper "Probability Logic" in "Handbook of the Logic of Argument and Inference" (Elsevier, 2002) and in his book "In Defence of Objective Bayesianism" (Oxford, 2010). 
Circular reasoning is generally used to refer to an argument (or part of one) where the conclusion is essentially one of the premises. In short, you could think of it as something like:

A ⊃ B, B ⊃ A, ... , ∴ A.

Naturally this of often more subtle that the above line makes it look but the idea is the same - you're using the conclusion in a premise to prove the conclusion. 

A tautology is any argument where for any combination of truth values (true/false) assigned to the predicates within it, the logical flow of the argument is such that the conclusion will always turn out true.

Part of the confusion between the two is that the term "tautology" is often used in everyday language to mean a statement of the kind A ⊃ A. The reasoning for this, as far as I can tell, is to do with the fact that the statement A ⊃ A cannot be false by the meaning of material implication (the problem is that a statement that is always true is somewhat different from an argument that always has a true conclusion). In this case, the 'tautology' is obviously circular, it's just not a Tautology in the way logicians use the term.
Because time can be measured by clocks, time is a property of our environment and exists independently from the human mind. It is neither a mere idea nor solely a form of human intuition.

That answer from the viewpoint of physics contradicts the characterization due to the philosophical viewpoint of Kant, as correctly stated by Philip.

One of the deepest insights into the nature of time is due to the Special Theory of Relativity: Time has always to be considered in connection with space as part of a four-dimensional physical object called spacetime. 
There is a certain arbitrariness how we decompose spacetime into space, measured by yardsticks, and time, measured by clocks. As a consequence, events which happen simultaneously for one observer, may happen at different times for a second observer. 

To understand these features of time requires some familiarity with physics - but I think you did not ask for an explanation which goes too far into this subject :-)
You can see:


Irving Anellis, https://escarpmentpress.org/russelljournal/article/viewFile/2056/2081 The Genesis of the Truth-Table Device (2004)


as well as:


Irving Anellis, https://arxiv.org/abs/1108.2429 Peirce's Truth-functional Analysis and the Origin of the Truth Table (2012).


Before Bertrand Russell (Harvard logic course: 1914) and Ludwig Wittgenstein (Russell and Wittgenstein's manuscript dated 1912; see also: https://books.google.it/books?id=p86r0bvRxQAC&pg=PA38 Tractatus (1921), 4.31 and 4.442 for material implication), http://plato.stanford.edu/entries/peirce-logic/ Charles Sanders Peirce and his followers must be credited.

See https://en.wikipedia.org/wiki/Christine_Ladd-Franklin Christine Ladd-Franklin, “On the Algebra of Logic”, in Charles Sanders Peirce (editor), https://books.google.it/books?id=GRlCAAAAQBAJ&pg=PA62 Studies in Logic (1883), page 17-on; see pages 61-62.



The "verbal" description of conditional is in Frege's https://en.wikipedia.org/wiki/Begriffsschrift Begriffsschrift.

But it was already verbally stated in http://plato.stanford.edu/entries/logic-ancient/#SynSemComPro Ancient Stoic Logic with the so-called philonian conditional; see https://en.wikipedia.org/wiki/Sextus_Empiricus Sextus Empiricus, https://books.google.it/books?id=tldNUcD9Qn0C&pg=pa112 Πρὸς μαθηματικούς (Pros mathematikous) Book VIII, 113:


  Philo, for example, said that the conditional is true when it does not begin with a true proposition and finish with a false one, so that a conditional, according to him, is true in three ways and false in one way. For when it begins with a true one and finishes with a true one, it is true, as in “If it is day, it is light.” And when it begins with a false one and finishes with a false one, it is again true – for example, “If the earth flies, the earth has wings.”


  By predicate, I think he means a "property" of the entity, for example, the predicate of being tall. This is the meaning that I'm aware of and which is the meaning we use in mathematical logic.


Exactly; in mathematical logic "existence" is a quantifier acting on a predicate; we read:


  ∃xPx


as: "there is an object having property P".

The existence of such an object is a fact that we have to ascertain through an empirical verification or a proof.

Existence is not part of the "concept" P itself.



But there are other possibilities; see http://plato.stanford.edu/entries/meinong/ Alexius Meinong and http://plato.stanford.edu/entries/nonexistent-objects/ Nonexistent Objects, as well as logics with an http://plato.stanford.edu/entries/existence/#AntMeiFirOrdVie existence predicate in addition to the existential quantifier.
Yes. It is a contradiction. Take as an example "Nothing is true". Stating the latter would mean that even that affirmation is false, and therefore its exact opposite would be true (we know from Aristotle's Metaphysics that there is no grey area in regards to the terms of a contradiction; so if one is false, the other one must be true).

Following this line of thought, the affirmation "Nothing is absolute" would imply that itself is relative, and therefore a subject of relative truth. 

Moreover, stating that everything else is relative, but this affirmation is absolute ("Nothing is absolute, excepting this affirmation.") is an obvious absurdity.

For a very detailed analysis on why "Nothing is true" (and analogously "Nothing is absolute") lacks any logic, I can refer you to Aristotle's Metaphysics, Book IV, Chapter 5, reference point 1009b. 
Your primary conviction appears to be this: without some way to escape the arbitrariness of [allegedly] axiomatized moral systems, we have no way to ground the superiority of one moral system over another.

Perhaps you've labeled this question with the tag "moral skepticism" because you feel that such a position might exemplify a skeptical outlook. However, I believe that if you adopt a more thorough skepticism, you will find the need to abandon moral claims (and those fictitious 'moral axioms') altogether. One incarnation of this approach is emotivism, a variety of moral fictionalism.

In other words, there is no need to establish the superiority of one set of axioms over another (in the realm of ethics, at least) because all claims about good and bad behavior can be reduced without remainder to claims about emotion and preference.

Of course, if your main goal is to retain ethics as a legitimate field of study...or to retain God as the supreme rule-giver, then there certainly be some leftover after the reduction I've suggested above. However, doing this would amount to the unscientific attempt to bolster some previously-selected perspective. Additionally, we have no reason to accept ethics as legitimate and no reason to accept the reality of a Creator.

Consider for a moment that in this world there are no moral restrictions...and, of course, no hideous axioms from which philosophers and theologians are forced to infer hideous rules. There is fear, disgust, pain, happiness, and a bunch of emotions. There are also laws that differ from place to place and pieces of advice passed down from parent to child...from friend to friend. Some people approve of x, but not of y...and some of these people prefer to parse these feelings in terms of laws given from God or in terms of laws that were uncovered by way of mathematical deduction. The main goal of this sophistical trend appears to be this: if you can trick mankind into thinking that he must adhere to system A or system B, you can reduce those behaviors you consider offensive and encourage those behaviors you consider beneficial.

To answer in a different way. No, morality is not based on axioms but this does not imply that one system must be superior to another. The third option is that ethics itself is a towering pile of nonsense. A person, like myself, who subscribes to this line of thinking does not refrain from murder because it interferes with some immaterial law or because it implies some contradiction or because it originates from some superior ethical system; he decides not to murder because he finds the act to be a disgusting one that causes great harm.
The way you have chosen to express the rules implies you are assuming a non-monotonic form of reasoning. Rule #1 as stated has no exceptions, while rule #2 expresses an exception to rule #1. In a monotonic system of logic (which includes classical logic) this would lead to a contradiction: if Bob hits Charlie, rule #1 says Charlie may not hit Bob back, but rule #2 says he may. In non-monotonic systems, rules may allow the inference of propositions that hold by default but may be defeated or overriden by the addition of other propositions. In such cases you would need some meta-rules that tell you how to apply the rules. For example, the rules might have some explicit priority value that tells you when one overrides another, or there might be a general consideration that more specific rules override general ones. In your example, rule #1 might then be assumed to hold by default but be defeasible where rule #2 applies, because rule #2 is more specific. You don't need to infer that the rule applies, you only need to check that no defeating conditions are present. 

If you wished to avoid using non-monotonic reasoning, an alternative approach would be to attempt to express the obligation in a single rule, e.g. "no man shall hit another man who has himself never hit others". You can then infer that if Charlie is a man who has never hit others, then Charlie should not be hit. 

The kind of reasoning we are using here is called deontic logic - the logic of obligation. Obligation can be treated as a propositional modality and attempts have been made to define formal logics for it, though it has proved highly problematic. The Stanford Encyclopedia has an http://plato.stanford.edu/entries/logic-deontic/ article on deontic logic. 
To say that A if and only if B is to say that A and B are logically equivalent, i.e. they always have the same truth value. That's because whenever A is true, B is also true (A only if B), and whenever B is true, A is also true (A if B). So A and B are true in exactly the same cases, and false in exactly the same cases.

Here's a simple example. Say the following is true:


  It's snowing if and only if it's winter.


It follows that the days when it's snowing and the days when it's winter are exactly the same days. Because for each day, if it's snowing then it also has to be winter (it's snowing only if it's winter), and if it's winter then it's also snowing (it's snowing if it's winter).

  Kant's epistemology: There are facts out there, but we can never access them directly, we can only perceive them the way they are presented to us by our own minds.


No, this specific piece has no special relation to Kant. It has been accepted by (almost) every western philosopher in the last 500 years, together with the rejection of the Aristotelian/Scholastic direct-perception epistemology.


  Quine: There are facts out there, but we can never access them directly, we can only represent them using our language structure.


Roughly, although the words "facts" and "represent" have no place in Quine's vocabulary (see more on this below).


  Is Quine really saying the same thing as Kant, except using the concept of language as oppose to that of mind dependent sense perception?


More or less, but see my comments above. The above formulations have no specific relation to Kant or to Quine. You might as well have said that all the western philosophers in the last 500 years are saying the same.


  If that is indeed the case, why is Kant considered and idealist while Quine is considered an empiricist?


First, idealism and empiricism are not mutually exclusive. On the contrary, almost all the empiricists, from Berkeley to Ayer, were idealists. Quine himself was non-commital about idealism (see his "on what there is"). What "saved" Quine from being an idealist like his empiricist predecessors, is that he usually rejected "sense data", the supposed mental intermediary between mind and world. Instead, Quine held that our connection to the world is merely causal. That is, without intermediary, but also without representation.


  Why does Kant's epistemology require the analytic/synthetic distinction while Quine can do away with it?


Kant's epistemology requires the analytic/synthetic distinction, because it puts crucial emphasis on synthetic-a-priori statements. Quine does away with the a-priori entirely, and therefore does also not require the analytic/synthetic distinction.
A common structure of sentences is that of subject-predicate. The subject is what the sentence is about, and the predicate is what is said about it. Usually the subject signifies an object and the predicate signifies a property.

A few examples should make this clear:


  Aristotle is wise (subject = Aristotle, predicate = is wise)
  
  The sky is blue (subject = The sky, predicate = is blue)
  
  Russell likes math (subject = Russell, predicate = likes math)


Predicates can be thought of as open sentences, i.e. sentences with variables. For example,


  x is wise


is a predicate1.

To predicate a property of a subject is just to substitute a subject for the variable in the open sentence that represents  that property. For example, to predicate the property of being blue is just to fill in the blank in '___ is blue'. In this case the property of being blue is predicated of something.



1 'is a predicate' is also a predicate.
I believe there are a few problems with the two statements you consider to be analytic: 

1) Cats have four legs -- Cats with serious injuries and cats born with deformities are still cats. Even if we pretend that injuries and deformities are somehow irrelevant, or if we imagine that the only cats that exist are fully-formed and healthy, there is still no concept of "4" in "cat". Still, if we insist that a thoroughly robust concept of cat must contain the notion of 'having four legs', we'd be relying on a very quirky understanding of analyticity; one that allows analytic statements to be falsifiable (for example, if a previously-pristine feline undergoes an amputation).

We might as well claim that such an amputee STILL HAS FOUR LEGS (but perhaps in a weaker sense) in order to inject all the desired analyticity into unsuspecting felines, but why in the world would we want to do that?

2) What can be proved is true -- This seems very wrong, primarily because we can prove something false by establishing that a contradiction was obtained. "Proof" might commonly refer to formal or empirical demonstrations aimed to fortify alleged veridicality, but we can also construct proofs that successfully establish that something is not the case. Sure, one might suggest that a valid proof of contradiction (and therefore falsity) results in a judgment like: "it's TRUE that x is false" but this doesn't really get us anywhere closer to the original goal, for reasons articulated by Kant a couple years ago.
There are two kinds of infinite expressions (formulas, strings, words): 1) Those which can be described by finite expressions, and 2) those which have an infinite complexity and cannot be reduced to finite expressions.

Examples of the first in the domain of real numbers are 0.111... or SUM(1/n!). These expression are precisely defining numbers as their limits, and analogous logical expressions can be used too.

Examples of the second kind are most of the real numbers because there are uncountably many but only countably many finite expressions. These surplus numbers are undefinable and therefore cannot have a numerical value that could be communicated in mathematical discourse. Same is true for infinite sequences of logical atoms or other infinite expressions. They cannot have a truth value because every truth value obtained up to a certain step could be negated in the next step. Without an "End Of File" there is no value discernable.
You can trace Western skepticism about our perception at least all the way back to http://plato.stanford.edu/entries/parmenides/ Parmenides of Elea, who posited that motion is impossible and that therefore our senses are illusory. There's a whole network of metaphysical reasoning invoked to argue for and support this claim, and it's been quite influential in the grand scheme of history of philosophy.

Parmenides then fostered the thought of https://en.wikipedia.org/wiki/Zeno_of_Elea Zeno of Elea, who is infamous for his http://plato.stanford.edu/entries/paradox-zeno/ paradoxes, and things have somewhat snowballed from there. 
A sentence is a statement or an assertion telling you the facts of the matter; its not asking you to evaluate whether it is true or not.


  Socrates is Greek


A sentence is a proposition when it is asking you to evaluate whether it is true or not; in English this is shown by a sentence in the form of a question


  Is Socrates Greek?


In formal logic question marks aren't used; one has to understand this from the context; I could write this out as


  IsGreek(Socrates)


This makes it obvious what the connection is to the above sentence; there are two parts to this sentence and the first part 'IsGreek' is called the predicate, and the second part 'Socrates' is called the subject.

This is a style of refering to propositions that one sees in verbose programming languages; but equally I could write it out more enigmatically as:


  G(Socrates)


which by merely looking at it without any explanation tells you very little...could you guess that G(x) stands for "Is x Greek?"
Well, it is, as you say, concise. And accurate, I believe. Perhaps uselessly so.

From Protagoras' "man is the measure of all things" to "naming" in Genesis to Locke's modern nominalism and, finally, to the nominalism of Ayer and the "now-called-naive" positivists, the idea that our shared knowledge is inevitably linguistic and conventional has many variations, complexities, and nuances.

I am not in the academic philosophy business, so I answer with extremely limited knowledge and credentials, but as far as I know, there is no satisfactory definition of "nominalism." It is what we marxists wisely call a "tendency."

Of course, Kant was the great figure in our modern translation of "what is" (ontology) into "what we call it" (epistemology). I am now passing out in my chair from excessive work and drink,so must conclude that the truly definitive answer to your question is...zzzzzzz.    
"Trivial" in mathematics is a subjective term. Something is "trivial" if my audience, in my opinion, comes faster to the conclusion that they can definitely provide a proof, then the time it takes me to demonstrate such a proof. 

Such a proof can actually be quite lengthy, as long as all the steps involved are simple or purely mechanical. For example, one proof that some 16 digit number is a prime would be trivial (because it is completely mechanical) but would still be very long. 

The same proof can be trivial if the audience consists of experienced mathematicians and be not trivial if the audience consists of beginners. 

  My question is, what meaning should we ascribe to theories of infinite objects, such as set theory? For example, the axiom of choice is independent of ZF set theory, yet it seemingly very different the statement from the PA is consistent. Its not clear what it would mean for the axiom of choice to be true or false though. In fact, its not even clear what it would mean for the axiom of infinity to be true or false.


There's really no way to answer your question unless you can elaborate on what you mean by ascribing meaning to sets. Sets just aren't the sorts of objects that have meanings, as standardly conceived. Linguistic objects, like words and sentences, have meanings. For example, taking a broadly Fregean view the word "Antartica" has a meaning that consists of a sense and a reference. The reference is the continent of Antartica and the sense is the mode of presentation of the reference (for example, "Antartica" and "The least inhabited continent" have the same reference but different senses). Now what meaning does {1,2,3} have? It certainly doesn't refer to anything; it's just a collection of objects. It doesn't seem to have a sense either. What, then, do you mean by a set having a meaning?

Secondly, do you really think it's unclear what it would mean for the axiom of choice to be true or false? What about the statement of that axiom confuses you? What do you think mathematicians are doing when they routinely use the axiom of choice? What are they doing when (less routinely) they prove theorems containing sentences like "Suppose the axiom of choice does not hold"?
Modus ponens can be stated as: (A → B and A) ⊢ B. If you look in the truth table, every line that has t for both A → B and A (only the last) has t for B as well. Hence, there is no situation where A → B and A are true, but B is not.



As for implication introduction: proving B under hypothesis A means that there is no situation where A is true, but B is not. Hence, the third line of the truth table is irrelevant. Then the only lines that remain are where A → B has value t. Hence, it is always true given that B can be proven using A.
Long comment

"Grasping a Thought" (or a sense) is a sort of "primitive" in Frege's philosophy; it is a basic assumption that is not analyzed further. 

Frege says that thoughts are real ("wirklich") because they act on the mind, like a physical object acts on the eye. The act of "seeing" performed by the eye is the (visual) perception of the object; in a similar way, we may say that the act of "grasping" performed by the mind is the (mental) understanding of the thought (or sense). 

It can be frustrating ... bu Frege has no "theory of mind", nor a thpoery of language aquisition.

A corresponding primitive in his thought is reference ("Bedeutung") and in the same way Frege does not explain how the relation of denotation (i.e. the relation between a name and an object) works.

Only some philosophers may think that we lear it "by ostension"...



Regarding "the picture theory of meaning that the later Frege rejected", we have to consider the fundamental https://en.wikipedia.org/wiki/Context_principle Context principle that Frege formulated already  in 1884 and that is present also in Wittgenstein's TLP. 

There is a discussion of TLP in the Frege-Wittgenstein correspondence [1918-1919; see: Burton Dreben and Juliet Floyd, Frege-Wittgenstein Correspondence, into : Enzo De Pellegrin (editor), https://books.google.it/books?id=Hul2-E05xfcC&pg=PA57 Interactive Wittgenstein : Essays in Memory of Georg Henrik von Wright (2011), with a reference by Frege in a footnote to Der Gedanke], but it seems to me that there is no discussion of this topic.
The issue is complex and any "significant" answer is hardly reducible to the Yes/No pattern.

In modern mathematics, 2+2=4 is a theorem of arithmetic provable from https://en.wikipedia.org/wiki/Peano_axioms Peano axioms.

In a nutshell, assuming the definition of 1 as "the successor of 0" and of 2 as "the successor of 1" and ... and of 4 as "the successor of 3" (and thus "the successor of the successor the successor of the successor of 0"), the axioms formulated with the primitive notions of:


  0, S (successor), + (sum) and (×) multiplication,


allows us to conclude that:


  "the sum of the successor of the successor of 0 with the successor of the successor of 0 is equal to the successor of the successor the successor of the successor of 0".


Having established this fact, we may assert that it is a logical consequence of Peano axioms, or that it follow (necessarily) from them, or that it holds in every model of the axioms.

If we want, we may paraphrase it as follows: it is true in every possible world in which Peano axioms hold.

Now the question is:


  
    are Peano axioms necessary truths ?
  


i.e. do they hold in every possible world ?

I would say: no.

There are models of https://en.wikipedia.org/wiki/Robinson_arithmetic Robinson arithmetic [that basically differ from Peano's one for the lack of induction] where the usual associative, commutative, or distributive laws for addition and multiplication do not hold.

Neither the law Sx≠x can be proved with only Robinson's axioms [see this https://math.stackexchange.com/questions/1066087/model-of-robinson-arithmetic-but-not-peano-arithmetic post for details].

Thus, it seems that the "usual" arithmetical axioms are not necessary truth (according to the above naive view : necessary = true in every possible world) and thus neither the "usual" arithmetical facts are.
The explanation seems to me a little bit blurred...

The source is "traditional" logic.

See http://plato.stanford.edu/entries/port-royal-logic/ Port Royal Logic:


  [for] Port-Royal [...] the significance of general ideas has two aspects: the comprehension [la comprehension] and the extension [l'étendue]. The comprehension consists in the set of attributes essential to the idea. For example, the comprehension of the idea ‘triangle’ includes the attributes extension, shape, three lines, and three angles. The extension of the idea consists in the inferiors or subjects to which the term applies, which for Port-Royal includes “all the different species of triangles”.


See: https://en.wikipedia.org/wiki/Antoine_Arnauld Antoine Arnauld, https://en.wikipedia.org/wiki/Pierre_Nicole Pierre Nicole, https://books.google.de/books?id=fIIPAAAAQAAJ&pg=PA69 La logique ou l'art de penser (3eme ed, 1668), page 69.

We can consider the "trivial" example :


  
    human=animal+rational.
  


In this case, we have that the concept "humanity" has two attributes: "animality" and "rationality". They are its comprehension (later: intension).

The "subordinate" concepts of European, American, etc. (“all the different species of humans”) are the extension.



Following the Scottish philosopher https://en.wikipedia.org/wiki/Sir_William_Hamilton,_9th_Baronet William Hamilton, in his https://books.google.it/books?id=rUEdAQAAMAAJ&pg=PA59 Logic, page 59, the distiction has been reformulated as that between intension and extension:


  As a concept, or notion, is a thought in which an indefinite plurality of characters is bound up into a unity of consciousness, and applicable to an indefinite plurality of objects, a concept is, therefore, necessarily a quantity,and a quantity varying in amount according to the greater or
  smaller number of characters of which it is the complement, and the greater or smaller number of things of which it may be said. This quantity is thus of two kinds ; as it is either an Intensive or an Extensive. The Internal or Intensive Quantity of a concept is determined by the greater or smaller number of
  constituent characters contained in it. The External or Extensive Quantity of a concept is determined by the greater or smaller number of classified concepts or realities contained under it.
  
  The Internal Quantity of a notion, its Intension or Comprehension, is made up of those different attributes oi which the concept is the conceived sum; that is, the various characters connected by the concept itself into a single whole in thought. The External Quantity of a notion or its extension is, on the other hand, made up of the number of objects which are thought mediately through a
  concept. For example, the attributes rational, sensible, moral, etc., go to constitute the intension or internal quantity of the concept man; whereas the attributes European, American,philosopher, tailor, etc., go to make up a concept of this or that individual man. [...] Both quantities are said to contain ; but the quantity of extension is said to contain under it ; the quantity of comprehension is said to contain in it.
  
  By the intension, comprehension, or depth of a notion, we think the most qualities of the fewest objects ; whereas by the extension or breadth of a concept, we think the fewest qualities of the most objects. [...]
  
  Again you will observe the two following distinctions : the first — the exposition of the Comprehension of a notion is called its Definition (a simple notion can not, therefore, be defined) ; the second — the exposition of the Extension of a notion is called its Division (an individual notion can not be divided).




The concept of extension evolved in modern logic into the https://en.wikipedia.org/wiki/Axiom_schema_of_specification#Unrestricted_comprehension Axiom of comprehension; see Bertrand Russell, https://books.google.it/books?id=SNWLAgAAQBAJ&pg=PA103 The Principles of Mathematics (1903), §102:


  The reason that a contradiction [i.e. http://plato.stanford.edu/entries/russell-paradox/ Russell's Paradox] emerges here is
  that we have taken it as an axiom that any propositional function containing
  only one variable is equivalent to asserting membership of a class defined by
  the propositional function.


The axiom licenses the existence of a collection (or set) for every formula expressing the properties or attributes of some concept (or idea). 

The properties described by the formula ate the comprehension of the concept, while the collection of the objects satisfying the formula is its extension.

  How to symbolize the argument ?


You are on the right track. With the sentence letters you have introduced to symbolize the different statements, you have to write down the premises of the arguments (you have quite done it) and you have to verify the possible conclusion.

The conclusion must be one of : B and ¬ B : "Petra went to Berlin or not (and if not, she went to Cologne, by first premise)".

Thus :


  "if she didn't (Petra didn't went to Berlin) then she went to Cologne"


must be :


  
    ¬ B → C
  


and


  "she didn't go to both Cologne and Dusseldorf"


is 


  
    ¬ (C ∧ D)
  


and


  "she went to at least one of Dusseldorf or Essen"


is


  
    D ∨ E
  


and


  "if she went to Essen she didn't go to Cologne"


is 


  
    E → ¬C.
  

The basic problem here is that https://philosophy.stackexchange.com/q/35897/9166 existence is not a property.  If we follow the concept of the property of existing down, we get lots of nonsense.  (Including one particularly annoying "https://en.wikipedia.org/wiki/Ontological_argument proof of God" that has been shot in the heart, the head and the liver, and just won't go lay down and die.)

In particular, we cannot logically say things like 


  ¬Exists(x) & Predicate(Q) → ¬Q(x)


which you seem to assume.  (So, proposing this is your particular mistake.)

After all, if Exists is a predicate, so is ¬Exists.  Then by this logic,


  ¬Exists(x) & Property(¬Exists) → ¬(¬Exists)(x)


Nonexistent things just can't have the property of not existing.

And if Exists is not a predicate we don't know what the left side means, because we don't know how non-predicates get along with things like & and ¬. 

Following on that, the proper way to reference http://plato.stanford.edu/entries/nonexistent-objects/ non-existent objects has a long history including two mainstream approaches: modal realism, including Meinongianism, and disontologizing their descriptions, for instance by forcing them to always be stated via quantification.

My favored approach is a compromise between the two: insisting fictional objects can be meaningful only in terms of implications they would fulfill were they to exist.  (This conjures up the whole machinery of modality, since "would" is a modal verb.  But it does not assert modal existence, just that modal grammar has implications about real things.)

It is true that nonexistent animals cannot have one horn?

For Meinong or modal realists, no, the nonexistent things exist in another sense, (in a 'modality' like might, should or can), and still have the attributed properties of their definitions, so you cannot derive your contradiction.

You already have two versions of the forced quantification approach.

For the implication approach 


  Unicorn(x) → |{y: y ∈ horns(x)}| = 1


remains true if {x : Unicorn(x)} is an empty set, because a false premise implies anything.  Every real unicorn has one horn, it also has none, and seventy-two.  Again, you can't get your contradiction.
See http://plato.stanford.edu/entries/induction-problem/ The Problem of Induction.

A paradigmatic example of induction is :


  up to now the sun raised every morning; therefore, the sun raises every morning.


This inference is not justified by deductive logic.

Thus, when the author says that :


  In general, inductive arguments are such that the content of the conclusion is in some way intended to “go beyond” the content of the premises


it seems that he is alluding to the fact that the premises alone cannot justify (logically) the conclusion, but we have to "add" to the argument some extra-premise, like an implicit assumption about +natural laws* (i.e. the "uniformity" of natural phenomena).
Usually when there's a disjunction you're going to have to use the disjunction elimination rule. So when you see it you have to think how that rule can get you to the conclusion.

In this case, we would need the following:

1. Q v R
2. Q → R
3. R → R


Using disjunction elimination, these would get you to R. You already have 1, and 3 is trivial. So we need to figure out how to get 2. This can be done in the following tricky way:

1. ~Q
2. | Q
3. | | ~R
4. | | ~Q
5. | | Q
6. | | Q & ~Q
7. | ~~R       
8. | R
9. Q → R


Now you have all 3 statements needed for the disjunction elimination rule which will get you to R.
The word "math" is here ambiguous. As a matter of fact (pun intended) Hume distinguished between (1) arithmetic and algebra, which are, according to him, based on relations of ideas, (2) geometry, which is based on matters of fact, but is relatively certain and reliable, and (3) other matters of fact.

Concerning arithmetic and algebra:


  It appears, therefore, that of these seven philosophical relations, there remain only four, which depending solely upon ideas, can be the objects of knowledge and certainty. These four are RESEMBLANCE, CONTRARIETY, DEGREES IN QUALITY, and PROPORTIONS IN QUANTITY OR NUMBER...
  
  There remain, therefore, algebra and arithmetic as the only sciences, in which we can carry on a chain of reasoning to any degree of intricacy, and yet preserve a perfect exactness and certainty.


Concerning geometry, Hume reiterates the old doubts about the axiom of parallels, doubts that were to be fully justified about a hundred years after Hume, with the development of non-Euclidean geometries.


  I have already observed, that geometry, or the art, by which we fix the proportions of figures; though it much excels both in universality and exactness, the loose judgments of the senses and imagination; yet never attains a perfect precision and exactness. It's first principles are still drawn from the general appearance of the objects; and that appearance can never afford us any security, when we examine, the prodigious minuteness of which nature is susceptible. Our ideas seem to give a perfect assurance, that no two right lines can have a common segment; but if we consider these ideas, we shall find, that they always suppose a sensible inclination of the two lines, and that where the angle they form is extremely small, we have no standard of a right line so precise as to assure us of the truth of this proposition.


But geometry is still, according to Hume, accurate and reliable relative to knowledge of other matters of fact, because it depends "on the easiest and least deceitful appearances".


  Though geometry falls short of that perfect precision and certainty, which are peculiar to arithmetic and algebra, yet it excels the imperfect judgments of our senses and imagination ... since these fundamental principles [of geometry] depend on the easiest and least deceitful appearances, they bestow on their consequences a degree of exactness, of which these consequences are singly incapable.


(The quotes are from A Treatise of Human Nature "Of Knowledge")
The "standard" view, shared by most commentators, has been synthesized by Kurt Gödel :


  It is to be regretted that this first comprehensive and thoroughgoing presentation of a mathematical logic and the derivation of mathematics from it [is] so greatly lacking in formal precision in the foundations (contained in *1 - *21 of Principia) that it presents in this respect a considerable step backwards as compared with Frege. What is missing, above all, is a precise statement of the syntax of the formalism [emphasis added]. (Kurt Gödel, "Russell’s Mathematical Logic", Reprinted from The Philosophy of Bertrand Russell, Paul A. Schilpp (editor), 1944).


For a similar view, see :


Alasdair Urquhart, https://books.google.it/books?id=6bIZBAAAQBAJ&pg=PA286 "The Theory of Types", The Cambridge Companion to Bertrand Russell (Nicholas Griffin, ed.), 2003.


Contra the "standard" view, see :


Stephen Boyce, https://arxiv.org/abs/1003.4483 The metatheory of first-order logic : a contribution to a defence of Principia Mathematica (2010).

You are right : the correct way is to use proof by cases:

1) Q --- assumed for the proof by cases [a-1]


  2) P → Q --- from 1) by →-intro


3) ¬P --- assumed for the proof by cases [a-2]

4) P --- assumed [b]

5) contradicition !

6) Q --- from 5)


  7) P → Q --- from 6) by →-intro, discharging [b]


and it is done.
I feel that this is an issue best addressed by analysis of the question. The phrase 'intuitive intellect' may be an oxymoron. When people speak of intuitive knowledge they usually mean 'knowledge by identity' and not the results of intellectual activity. Thus they speak of 'non-intuitive immediate knowledge'. 

Where knowledge is produced by intellectual activity then it may be incorrect activity (there's many a slip 'tween cup and lip) and so it can only ever be provisional or contingent. Aristotle concludes that true knowledge entails an identity of knower and known precisely because intellectual activity is never fully trustworthy or in complete contact with the reality being intellectualised. 

The idea of knowledge that is 'not broken into pieces' is discussed by the physicist Paul Davies in his popular book 'Mind of God'. He concludes on grounds of logic that if it is possible then it must be by way of the practices of the mystics, who seek to go beyond the intellect to realise the unity of consciousness. 

Where the intellect is informed by a realisation of unity this would be a profound state associated with with the phenomenon of 'nonduality' and also with what in esoteric Christianity is called 'Christ-consciousness'. Only at this level would knowledge cease to be fragmented by our language and concepts, and it could be attained only by becoming what we are, in other words by means other than the intellect. 

For progress we would need to define the phrase 'intuitive intellect' very carefully, and I suspect that there's no way to do it that makes much sense.       
Rereading the question, I would say that if you can show that a position is self-refuting, then that's pretty damning for that position. The key, however, lies in the "if you can show" bit of that claim.

I'll limit myself to just one example. There's a common refutation of hedonism that goes something like this:


The hedonist says we just need to value enjoyment and not waste our time thinking / having values. 
aha! the hedonist is thinking / has a value.
Ergo, the hedonist has a self-referentially incoherent view.


Despite the self-reference problem, this actually isn't a refutation of hedonism per se. The difficulties are whether or not we accept step 2 with respect to the hedonist and whether we can really attribute 1 to the hedonist. Put another way, this is a refutation of "reflective hedonism" -- in other words, you can't coherently reflect on how you should live your life and be a hedonist of the rawest sort. But does a hedonist have to even hold to 1 in accessible way?

The same sort of outcome will play out in most self-referential problems in philosophy. For instance, someone might say believing in God and believing in causality is self-referential incoherent. But most forms of theism see God as precisely that which lies outside of the system of causation and its rules.

Maybe to reword all of that and some up, self-referential is a real problem, and if you write a paper showing that some commonly held philosophical view is self-referentially incoherent, it can probably get published and would be considered "scholarly." But odds on the people who hold this view would argue against the accuracy of the self-referential incoherence by suggesting you misunderstood their view or are misrepresenting a term in it as being on a different level than where they use it.
It is not.

See any mathematical logic textbook; e.g. Herbert Enderton, https://books.google.it/books?id=dVncCl_EtUkC&pg=PA83 A Mathematical Introduction to Logic (2nd ed - 2001), page 83.

To say that:


  M⊨t=s[v] iff v(t)=v(s)


where t and s are terms, i.e."names" and v(t) and v(s) are the elements of the domain of the interpretation M (i.e. objects) that are the reference (assigned by the function v) of the said terms is nothing other than saying:


  the sentence t=s holds (it is true) in M iff t is equal to s.


See also https://plato.stanford.edu/entries/tarski-truth/#195DefOff Tarski's Truth Definition and :


Alfred Tarski & Robert Vaught, https://eudml.org/doc/88848 Arithmetical extensions of relational systems (1956), page 84-on.

Hegel proffers weltanschauung to be either agreed or disagreed with. Note that while this is distinct from advancing knowledge claims, it is the logical equivalent of poetry and no more philosophy than opinion or sentiment. So, sure - it's possible to clarify. What then does the poet mean when one interpretation is as valid as the other? From "http://home.igc.org/~venceremos/whatheck.htm Dialectics for Kids" 


  Many changes are cyclical--first one side dominates, then the other--as in day/night, breathing in/breathing out, one opposite then another. Dialectics argues that these cycles do not come back exactly to where they started; they don't make a perfect circle. Instead, change is evolutionary, moving in a spiral.


...these are the http://home.igc.org/~venceremos/spiralaz.htm ABCs of spirals:


  A - An Acorn falls in the woods. It sprouts into a tree that eventually makes new, and different 
  Acorns.
  
  B - You're walking along and you happen to trip. Next time you're more careful to keep your
  Balance. 
  
  C - It's the big game. You're team starts to lose and you're feeling bad. But then you feel great when your team makes a 
  Comeback.
  
  D - A baby doesn't know how to use a bathroom so we use Diapers to keep from having a mess. When a toddler learns to use a bathroom we're glad that there is no more need for the Diapers


...and so on.

Here as well is further example by poetic use of analogy, from http://home.igc.org/~venceremos/Popcorn.htm Popcorn, Haircuts and other Changes:


  Circles and Spirals
  
  Some changes go in circles, returning to where they started.
  
  Like a Ferris wheel that goes up and comes back down to the same 
      spot.
  Or the day which turns into night and then into day
  Or the seasons--Winter--Spring--Summer--Fall--and Winter again
  Or our breathing--in and out and in
  
  Some changes go in spirals--they look like they come back to where they started, but something is different.
  
  Like a winding staircase that moves in a circle, but comes around to
      a higher point.
  Or going to school and coming back home, but learning more about 
      the world every day.
  Or losing something; then finding it, and then putting it in a special 
      place so you won't lose it again.
  Or like children growing up to be parents with children of their own.


(See the linked page for the poetic formatting)
Is that actually what he said?

From William James' "http://intersci.ss.uci.edu/wiki/eBooks/BOOKS/James/The%20Meaning%20of%20Truth%20James.pdf The Meaning of Truth: A Sequel to 'Pragmatism'" (1909 - page 104, boldface and italics my own):


  This subjectivist interpretation of our position seems to follow from
  my having happened to write (without supposing it necessary to explain
  that I was treating of cognition solely on its subjective side) that
  in the long run the true is the expedient in the way of our thinking,
  much as the good is the expedient in the way of our behavior! Having
  previously written that truth means 'agreement with reality,' and
  insisted that the chief part of the expediency of any one opinion is
  its agreement with the rest of acknowledged truth, I apprehended no
  exclusively subjectivistic reading of my meaning. My mind was so
  filled with the notion of objective reference that I never dreamed
  that my hearers would let go of it; and the very last accusation I
  expected was that in speaking of ideas and their satisfactions, I was
  denying realities outside. My only wonder now is that critics should
  have found so silly a personage as I must have seemed in their eyes,
  worthy of explicit refutation.


...and the original from "https://ia902606.us.archive.org/19/items/pragmatismanewn00jamegoog/pragmatismanewn00jamegoog.pdf Pragmatism" (1907 - see page 222)


  'The true,' to put it very briefly, is only the expedient in the way
  of our thinking, just as 'the right' is only the expedient in the way
  of our behaving.


See https://en.wikipedia.org/wiki/Pragmatic_theory_of_truth#James here for an adequate interpretation:


  By this, James meant that truth is a quality the value of which is
  confirmed by its effectiveness when applying concepts to actual
  practice (thus, "pragmatic"). James's pragmatic theory is a synthesis
  of correspondence theory of truth and coherence theory of truth, with
  an added dimension. Truth is verifiable to the extent that thoughts
  and statements correspond with actual things, as well as "hangs
  together," or coheres, fits as pieces of a puzzle might fit together,
  and these are in turn verified by the observed results of the
  application of an idea to actual practice. James said that "all true
  processes must lead to the face of directly verifying sensible
  experiences somewhere." He also extended his pragmatic theory well
  beyond the scope of scientific verifiability, and even into the realm
  of the mystical: "On pragmatic principles, if the hypothesis of God
  works satisfactorily in the widest sense of the word, then it is
  'true.'"


...and his writing is coherent considering a standard reading of the "http://www.etymonline.com/index.php?term=expedient expedient" as the "quality of being convenient and practical (despite possibly being improper or immoral)"

Considering this definition of expedient, note that James uses right and good interchangeably. I think he means them in more of a sense of "acceptable" or "justifiable" than "correct" in any absolute or pure sense.
Here, the logical meaning is not "produced" by the adverb. The phrase "only if" is to be regarded as a single lexical item with an accepted meaning, the one you mention in your question.
This question is quite vague (esoteric?), so the answer will not be very precise, too. First, I agree that "oneness" and "twoness" are fundamental because the former gives us the basic unit, the latter binary opposition and connection.

As you noticed, a relation r: X → Y is about "twoness". After all, it is formally defined as a subset of the Cartesian product X × Y = {(x, y): x ∈ X, y ∈ Y}.

A lot more complicated structures can be built from that, i.e. an arithmetic operation "⊙" on a set X (which may make it a group if certain requirements are met) could be defined by a function

g: M → X with M = X × X

and then setting

x₁ ⊙ x₂ := g(x₁, x₂).

At no point we need to bother with relating more than two sets or two elements with each other.

Now, seriously, we may have to sober up a bit... Perhaps this flight of thoughts got to our heads far too much?

If 1 and 2 are the basic concepts of mind, what difference is there between a child, that can count to two, and a great mathematician? That the great mathematician can juggle much better with 1s and 2s?

That seems preposterous. 

And if we look at the natural numbers there is one important axiom, the axiom of mathematical induction:


  If P is a proposition about natural numbers such that:
  
  
  P is true for 0
  for every natural number n, P being true for n implies that P is true for n + 1,
  
  
  then P is true for every natural number.


we must remember that there are much simpler "toy" subsets of the usual axioms. They basically just define counting and calculating but can’t express general facts about numbers at all.

So how would we get to an idea like mathematical induction? Do we really grasp the infinite structure of the natural numbers itself from the concepts of 1 and 2?

No, it obviously doesn't work that way. 1 and 2 do not beget infinity.

So intuitively and as this simple example shows, there really is more to math, something we fundamentally cannot get from the concepts of 1 and 2.
It can be proven with substitution, also known as identity elimination (=E): 


{1}      1.  a=b & b=c                      Assum.
{1}      2.  a=b                            1 &E
{1}      3.  b=c                            1 &E
{1}      4.  a=c                            2,3 =E
-        5.  (a=b & b=c) → a=c              1,4 CP
-        6.  ∀z[(a=b & b=z) → a=z]          5 UI
-        7.  ∀y,z[(a=y & y=z) → a=z]        6 UI
-        8.  ∀x,y,z[(x=y & y=z) → x=z]      7 UI


  The fact that X does not come into being by being defined to exist implies that there is a force or being external to logic preventing it from existing.


No, this does not seem to be implied. Consider the following analogy.

I try to open my flat door, not by using a key, but by chanting "Open Sesame".  Surprisingly, the door does not open! Does this imply that there is a force,  or an external being, preventing the door from opening? Well, hardly. Rather, it implies that chanting is powerless, and therefore no force or external being is needed to prevent its effect.

Similarly, the fact that X does not come into being by being defined to exist, does not imply that there is a force or being external to logic preventing it from existing.

What it does imply is that defining something to exist is powerless to actually cause it to exist, to come into being. And therefore no force or external being is required, to prevent it from happening.
The statement "If the world is everything always, then the world is always" can be written in https://plato.stanford.edu/entries/logic-temporal/ temporal logic as:


  Let i = "The world is everything" 
  
  Let e = "The world exists" 
  
  Let H be the temporal operator "It has always been the case that"
  
  Let G be the temporal operator "It will always be the case that"
  
  (Hi ^ Gi ) -> (He ^ Ge)


If what you mean by "is the case" is giving i and e True truth values, then yes, the implication has a value of True. 
For any rational physicalist, evolution by natural selection is the the most likely method by which our brains evolved. Evolution is a process whereby biological forms and behaviours which less effectively exploit niches in competition with other more efficient forms and behaviours produce fewer and fewer copies until eventually none are left. It therefore follows that any behaviour encoded in our DNA would constitute "knowledge" of a successful method for survival in the niche in which the behaviour evolved, so technically this would qualify as a priori knowledge.

There are also schools of thought such as Ian Stewart's concept of morphological form, which use the limits of the physical world on biology to constitute a kind of "knowledge" of what can and cannot exist. Because our brains evolved within limitations such as linear time, three-dimensional space and gravity, such truths are hard-wired into our brains. Of course, in this case they are pragmatic "truths", not actual truths as science has proven all of them to be other than that which we intuitively believe them to be. If, however, you accept definitions of knowledge such a those of Pierce, then this kind of hard-wiring would also constitute a priori knowledge for a physicalist.

The important question really for physicalists, is not whether such knowledge exists, but how to identify it as such, and how useful or applicable it is outside of the specific niche in which it evolved.
This depends on your use of constants. There is a https://plato.stanford.edu/entries/logic-modal/#QuaModLog big debate about this very question and I am going to present to you the most popular alternatives:


Constant domain modal propositional logic


This system assumes that every constant you can speak about exists in every possible world. Analytical statements might be tautologically true.


Variable domain modal propositional logic (VMPL)


This system takes some assuption from free logic and uses them for MPL. In free logic any allquantified statement can either be true about existing constants or it is vacuously ture about non existing objects.

For this distinction free logic and VMPL fix one predicate to be Existance predicate. The valuation function of this predicate maps to the n-tupel of existing constans in the domain of a world. This means that analytical statements can be vacuously or tautologically true.

Some people argue for a negative constraint on VMPl. They want statements for nonexisting constants to be false. For them your analystical statement is vacuously false.
From Ax.4, by Contraposition and using the abbreviation of ¬∀¬ with ∃ we can prove :


  ϕ(t / x)→∃xϕ.


Now :

1) ⊢ ∀xRax → Rab --- Ax.4

2) ∀xRax --- premise [a]

3) Rab --- from 1) and 2) by Modus Ponens

4) ⊢ Rab → ∃yRay --- from Ax.4

5) ∃yRay --- from 3) and 4) by Modus Ponens

6) ∀x∃yRxy --- from 5) by Universal Generalization


  7) ∀xRax → ∀x∃yRxy --- from 2) and 6) by Deduction Theorem.

If you rephrased the proposition to "vanilla ice-cream tastes nice to me"  it would leave you less vulnerable to skepticism.  

When we use the words "vanilla ice cream is nice" most people are expressing the sentiment "I think that vanilla ice-cream is nice", though the words themselves don't make that clear. 

Tastes nice" really denotes a https://en.wikipedia.org/wiki/Binary_relation binary relationship, a thing cannot taste nice independently, it requires a thing to do the tasting.  Therefore that statement can only ever be true or false in relation to a perceiver of the taste.

Whether we can have objective knowledge of matters of taste is a contested subject in the field of aesthetics.  David Hume suggests that it is, and puts forward a theory in https://bradleymurray.ca/hume-of-the-standard-taste-pdf/ Of the Standard of Taste that addresses the inconsistency in us stating that taste is entirely subjective yet still knowing deep down that Justin Bieber is an inferior musician to Bob Dylan. (Or any other musician in the history of civilisation). 
Regarding the transcendental determination of the idea of 'everything' - e.g. 'the universe' - all predicates are possible to it: i.e. "all things are possible".  Once existence comes into play specific possibilities are crystalised and mutually exclusive ones disappear - they are no longer possibilities.

This transcendental concept is laid out in Immanuel Kant's Critique of Pure Reason:-

http://www.philosophy-index.com/kant/critique_pure_reason/i_ss_ii_iii_ii.php Ch. III. Section ii. Of the Transcendental Ideal (Prototypon Trancendentale).


  If a transcendental substratum lies at the foundation of the complete
  determination of things - a substratum which is to form the fund from
  which all possible predicates of things are to be supplied, this
  substratum cannot be anything else than the idea of a sum-total of
  reality (omnitudo realitatis). In this view, negations are nothing but
  limitations - a term which could not, with propriety, be applied to
  them, if the unlimited (the all) did not form the true basis of our
  conception.
  
  This conception of a sum-total of reality is the conception of a thing
  in itself, regarded as completely determined; and the conception of an
  ens realissimum is the conception of an individual being, inasmuch as
  it is determined by that predicate of all possible contradictory
  predicates, which indicates and belongs to being. It is, therefore, a
  transcendental ideal which forms the basis of the complete
  determination of everything that exists, and is the highest material
  condition of its possibility - a condition on which must rest the
  cogitation of all objects with respect to their content. Nay, more,
  this ideal is the only proper ideal of which the human mind is
  capable; because in this case alone a general conception of a thing is
  completely determined by and through itself, and cognized as the
  representation of an individuum.


That is to say, a thing - in our case 'everything', or 'the universe' - notionally has every possibility available to it.  Once actualised, the possibilities become limited.  If the universe had the possibility of not existing and the eventuality was that it didn't, then the situation would be that "all things are not possible".  The possibilities of being possible and not being possible wouldn't exist simultaneously.
Your example is related to the so-called https://plato.stanford.edu/entries/epistemic-paradoxes/#KnowPar "knowability paradox" concerning sentences of the form "p but p is not known", pointed out by Church in 1945. The 
Church sentences are not contradictory, but simple argument shows that they can not be known for any p. Indeed, if the Church sentence is known then p is known, but it is also known that p is unknown, which is incoherent. In other words, if there are unknown truths (ones that never become known, if we think temporally), then there are unknowable truths. Church may have been inspired by https://plato.stanford.edu/entries/epistemic-paradoxes/#MooPro Moore's paradox, proposed in 1942, featuring sentences like Moore's "I went to the pictures last Tuesday, but I don't believe that I did". Moore's point was different though, that such sentences create a contradiction whenever uttered, because (honest) utterance requires belief.

If one adopts an epistemology under which there are no unknowable truths (like intuitionism), then one must accept that nothing not known can be true (or one can adopt some non-traditional epistemic logic). To put it positively, "if p is true then p is known", the inference you are using is that with p="X is impossible". This is unpopular, but not as crazy as it sounds. The belief in unknowable, or verification-transcendent, truths is a hallmark of realism. https://en.wikipedia.org/wiki/Anti-realism Anti-realists (about a particular domain) impose stringent proof-theoretic requirements on knowledge, so that nothing not supplied with a proof is considered true, and anything supplied with a proof will of course be known to be true. On this model of truth if X is impossible then you already know it, and if you do not, then no truth value attaches to the claim. This is why intuitionists and anti-realists reject the law of excluded middle, we can not know that p or not p without knowing which, otherwise we admit unknowable truths. If one adopts such a notion of truth then epistemically possible does imply possible (and so the opposite is false, and hence unprovable), but then there seems to be little point to distinguishing between epistemically possible and just possible. 

Wittgenstein held something like such a position concerning mathematics in his intermediate period, according to him "a mathematical proposition is an allusion to a proof". On Shanker's reading, to Wittgenstein unproven conjectures have no truth values because they have no meaning. Here is a commentary from https://www.academia.edu/5906282/A_Chalet_on_Mount_Everest_Interpretations_of_Wittgensteins_Remarks_on_G%C3%B6del Matthíasson's thesis:


  "Wittgenstein abandoned the view that language had one underlying logic or calculus. He now believed, says Shanker, that it consisted in “a complex network of interlocking calculi: autonomous ‘propositional systems’ each of which constitutes a distinct ‘logical’ space”. 
  
  [...] The relation of a proof to its proposition is internal and creates the meaning of the mathematical proposition, i.e. the role of proof is not to merely convince its reader of the truth of the proved proposition (which would be an external relation on this picture) but is necessary to establish the very meaning of the proposition being proved — a proof is thus an essential part of the proposition it proves.
  
  [...] This of course immediately raises the following problem: If the meaning of a mathematical proposition is dependent on its proof, a mathematical conjecture changes its meaning when it has been proven. It then follows that a mathematical conjecture can never be proven (since the proposition proven is not the same as the one conjectured). For Shanker, conjectures are strictly speaking meaningless but provide a ‘stimulus’ for the mathematician to come up with a proof, and thereby a new calculus."

This depends on the sense in which you mean the word 'useful' -- mathematicians try to make a study of the logical objects which are 'useful' or interesting in some sense, and pretty much any set-theoretic encoding (or type-theoretic encoding) of this information will require assumptions, which are identical to axioms from the standpoint of philosophy of set theory.

Perhaps this is not true by necessity, though. There is currently a development underway that is referred to as the 'reverse mathematics of second-order set theory', which seeks to understand what assumptions are necessary to prove certain well-known theorems in mathematics. This is a reversal of classical mathematics, in the sense that we are moving from a place of 


  'we believe this theorem is true in most reasonable contexts -- what is the bare minimum that we need to assume to prove it so', 


as opposed to 


  'here is what we are assuming is true, what are the consequences'.


A development with minimal axiomatic use for maximum proof capability would probably fit somewhere into this hierarchy of assumption strength. I don't know how one would proceed with no assumptions, however -- even your suggestion of Occam's razor is ultimately still just an assumption.
I think there is a point not enough clear in your question...


  The most familiar https://plato.stanford.edu/entries/logic-modal/#ModLog logics in the modal family are constructed from a weak logic called K (after Saul Kripke). [...] A variety of different systems may be developed for such logics using K as a foundation. [...]
  
  K results from adding the following to the principles of propositional logic [...] [emphasis added].


The last statement means that the axioms and rules (and meta-logical concepts) of propositional logic still apply, like e.g. modus ponens.

In particular, this means that in system K we can use the concept of propositional tautology as well as the "standard" completeness of the propositional part with regard to the truth functional semantics.

All this long premise to say that a formula ¬α ∨ α of K is still a tautology, and thus provable, for α whatever.

Thus, substituting with Lp for α we get ¬Lp ∨ Lp.

The reason is simple: if Lp is true, then clearly ¬Lp is false, and thus, by truth table for ∨, ¬Lp ∨ Lp is true, and the same with Lp false.

For a detailed proof, see e.g. Edward Zalta, https://mally.stanford.edu/notes.pdf Basic Concepts in Modal Logic (1995): Ch.3.2 Tautologies are Valid.
Yes, a condition can be sufficient but not necessary. For example, being in Paris is sufficient for being in France, but it is not necessary. More: running 20km every day is sufficient for being in good shape, but not necessary. Being 80 is sufficient for being old, but not necessary.

More generally, in any case where X is a strict subset of Y, being a member of X is sufficient, but not necessary, for being a member of Y.
The answer is a bit complicated.

First, let's start with the symbolic formulation you wrote:

((x = y) ∧ (x = z)) → y = z

I assume that this is intended to be a formula in standard (first-order) logic. In words, this formula says: "If x is identical to y, and x is identical to z, then y and z are identical to each other." This is true. If y and z are identical to the same thing, they are identical to each other. 

Identity is actually a fairly subtle concept, so we need to be careful in interpreting this formula. First of all, this formula neither asserts nor presupposes the existence of more than one object. In other words, the formula would remain true even in a world that had exactly one object. This is important because it's easy to get the sense that we are talking about three different things here: x, y, and z. But if the formula is true, we're actually only talking about one thing: the thing to which x, y and z are all identical.

A more helpful way of thinking about the formula 'x=y' is as follows. In order to tell whether this formula is true, we first ask: what does the symbol 'x' refer to? And what does the symbol 'y' refer to? If we find that these two symbols refer to the same individual object, then 'x=y' is true. However, there is a subtlety here also. What I just said is a description of what we do to figure out whether the formula is true. It is not what the formula itself says. The formula 'x=y' is not even talking about symbols or reference. The symbol 'x' is used in the formula, but it is not mentioned.

You asked: is this the correct symbolic formulation for the idea that one symbol can't refer to two different things? (I made one change to your question; I'll get back to that.) Well, it is true that one symbol cannot refer to two different things (in first order logic) 1. That is a fact about the semantics of first-order logic. And ultimately, it is this fact about the semantics of first-order logic that makes your symbolic formulation true. Because 'x' cannot refer to two different things, if it is coreferential with 'y' and 'z' then those terms are coreferential themselves. But your symbolic formulation does not say this.

I changed your question to be about reference rather than meaning. That's because, in first-order logic, formulas and symbols are not explicitly associated with meanings. A possible formulation for your original idea (in first order logic), then, is: 

S(x) & M(x,a) & M(x,b) → a=b 

Where 'S(x)' means 'x is a symbol," and 'M(x,a)' means 'x has the meaning a.' This formulation regards both symbols and meanings as first-order entities. 



1 More precisely: one symbol cannot refer to more than one thing, given an assignment function.
Proving whether 1) we ourselves exist and proving whether 2) an independent world exists are two distinct but related questions.

The most influential answer to the first question is Descartes': https://en.wikipedia.org/wiki/Cogito_ergo_sum cogito ergo sum, or "I think before I am". Various critics, including Nietzsche and Kierkegaard among others, have pointed out that the meaning of "I" here is critical and not clearly addressed. But the experience of doubt necessarily implies the existence of some subjective entity, the doubter.

As for the second question, the existence of a world independent from the thinker essentially amounts to https://en.wikipedia.org/wiki/Materialism materialism, as opposed to https://en.wikipedia.org/wiki/Idealism idealism. The fundamental problem with strong forms of idealism is that the many things (our apparent ability to communicate with other people, the regularity of observed physical phenomenon, etc.) are difficult to explain, short of some far-fetched "https://en.wikipedia.org/wiki/Brain_in_a_vat brain in a vat" scenario. 
Recall that to Kant since Aristotle "logic  has  not  been  able  to advance  a  single  step,  and  is  thus  to  all  appearance  a  closed  and  completed body  of  doctrine" (http://www.stephenhicks.org/wp-content/uploads/2013/03/kanti-critique-pure-reason-2nd-preface.pdf Critique of Pure Reason): no propositional variables, no connectives, no multi-place predicates, and no quantifiers. So Kant's notion of analytic is so impoverished that he would not lose much by simply accepting that everything, including logic, is synthetic. After all, he already declared that mathematics is synthetic. But it is also a priori, which means that  synthetic is compatible with a priori. Moreover, some pragmatic shadow of analyticity is preserved even by http://www.jstor.org/stable/40231747?seq=1#page_scan_tab_contents late Quine of Two Dogmas in Retrospect:


  "Analyticity undeniably has a place at a common-sense level, and this has made readers regard my reservations as unreasonable. My threadbare bachelor example is one of many undebatable cases... In Roots of Reference I proposed a rough theoretical definition of analyticity to fit these familiar sorts of cases. A sentence is analytic for a native speaker, I suggested, if he learned the truth of the sentence by learning the use of one or more of its words. This obviously works for 'No bachelor is married' and the like, and it also works for the basic laws of logic."


A much more serious revision of Kant is not Quine's dissolution of the analytic/synthetic distinction but rather his epistemological holism, including the denial that anything is a priori. This he also moderated in late years, although not on principle: 


  "Looking back on it, one thing I regret is my needlessly strong statement of holism... "no statement is immune to revision". This is true enough in a legalistic sort of way, but it diverts attention from what is more to the point: the varying degrees of proximity to observation..." 


But on this score we have a response from a modern neo-Kantian, Michael Friedman, and his theory of relativized a priori (anticipated already by logical positivists, like Reichenbach), see https://philosophy.stackexchange.com/questions/37726/what-are-the-more-complex-interesting-examples-of-synthetic-a-priori-statements/37733#37733 What are the more complex/interesting examples of synthetic a priori statements? and https://philosophy.stackexchange.com/questions/26608/are-there-necessary-truths-in-physical-theories-more-or-less-strictly-speaking/26612#26612 Are there necessary truths in physical theories, more or less strictly speaking?

It is a natural conciliation of Quine with Kant which admits that yes, everything is empirically revisable, but no, theoretical knowledge is more structured than an undifferentiated "web of belief" with its parts differing only by being more or less "entrenched". Certain "philosophical meta-principles" (like locality, causality, etc.), and "coordination principles" (connecting theories to observations) must be assumed in advance to even enable empirical measurements and their interpretation. They can not therefore be tested empirically in any straightforward sense, nor do they come from any kind of empirical induction, they are a priori and rational in origin. But they are not absolute, as Kant thought, for they can be adopted or abandoned based on the overall success of a paradigm (this is Friedman's infusion of Kuhn into Quine), judged in a loosely empirical manner, like classical determinism or Euclidean geometry were. Here is the gist of Friedman's argument against Quine's holism in http://press.uchicago.edu/ucp/books/book/distributed/D/bo3634648.html Dynamics of Reason:


  "Quine's epistemological holism pictures our total system of science as a
  vast web or conjunction of beliefs which face the "tribunal of experience" as a
  corporate body... But can this beguiling form of epistemological holism really do justice to the revolutionary developments within both mathematics and natural science that have led up to it? Let us first consider the Newtonian revolution that produced the beginnings of mathematical physics as we know it - the very
  revolution, as we have seen, Kant's original conception of synthetic a priori
  knowledge was intended to address. 
  
  "The combination of calculus plus the laws of motion is not happily viewed... as a conjunction of elements symmetrically contributing to a single total result. For one element of a conjunction can always be dropped while the second remains with its meaning and truth-value intact... the mathematics of the calculus does not function simply as one more element in a larger conjunction, but rather as a necessary presupposition without which the rest of the putative conjunction has no meaning or truth-value at all. The mathematical part of Newton's theory therefore supplies elements of the language or conceptual framework, we might say, within which the rest of the theory is then formulated. 


Incidentally, this allows Friedman to resurrect the analytic/synthetic distinction as well, but not in the Carnapian form attacked by Quine ("I have no desire to defend Carnap's particular way of articulating this distinction here"). Rather than distinguishing different sentences within a theory as analytic or synthetic, he proposes a meta-distinction between theories and their presuppositions.
I don't know what rules you're allowed to use, but here's an outline that should help.

10.3 is logically valid:


  
  ∀xCube(x) → ∃ySmall(y)
  ¬∃ySmall(y)
  ¬∀xCube(x) (From 1,2: p → q and ¬q entail ¬p)
  ∃x¬Cube(x) (From 3: ¬∀xFx = ∃x¬Fx)
  


10.6 is also logically valid:


  
  ∃x(Cube(x) Λ Large(x)) → (Cube(c) Λ Large(c))
  Tet(c) → ¬Cube(c)
  Tet(c)
  ¬Cube(c) (From 2,3: modus ponens).
  ¬(Cube(c) Λ Large(c)) (From 4)
  ¬∃x(Cube(x) Λ Large(x)) (From 1,5: p → q and ¬q entail ¬p)
  ∀x¬(Cube(x) Λ Large(x)) (From 6: ∀x¬Fx = ¬∃xFx)
  

I'm not sure if it is right to put the categories somewhere in a discrete cubby, as though Aristotle's thought were not whole. Some say that is justified by the fact that Aristotle's writings are received in a disconnected and scattered fashion and can not be considered as a "system". 

Aristotle studied all the thinkers who came before him in the Greek world. And considered Plato as a "epigone" of all of them, and one must think as a kind of great synthesizer. 

His chief influence was, however, common opinion, which he though a very reliable measure. But he went beyond this in a peculiar manner, unlike those who tried to ascend from ordinary opinion through diological discussion (e.g., Socrates), he attempted to do so by vision, the eyes of the soul, which he distinguished from observation of Nature. Later, nearer to our own time, a sort of so-called phenomenology was read back into Aristotle, or was it already there? 

Predication in Aristotle simply means "I say something about something", so one must not think so-called grammar, as it were, played any great role. He looked at words, such as "phusis", nature, and asked, what is the essential meaning of this word, the most important meaning. He tried to locate place things, through definition, in the world, in such a way as to know the world order, how they stood in the cosmos. For instance by understanding the genus of man as animal, and within the animal, the particular difference or essence of man as reason. Likewise the categories are orienting devices, as it were.

The connection with Aristotle to linguistic analysis is more due to the 17th century discovery of Evolution (or drift) in languages. Which escalated up until the time of Heidegger who made an uncommonly influential and profound (which is not to say necessarily true) study of Aristotle, with respect to a claim concerning the "primordial" or "orgniary" meaning of Greek, if you like, "grammar". Heidegger would never have called it that. Instead speaking of Greek Language. There's an overlay, of Aristotle's etymological analysis, and modern claims to "understand Aristotle better than he understood himself", in the current scholarship on Aristotle. In the last analysis, so to say, the two blend imperceptibly and inseparably.  

It's hard to tell "our" Aristotle from the Aristotle. 
It's called shifting the burden of proof.


  burden of proof
  You said that the burden of proof lies not with the person making the claim, but with someone else to disprove.
  https://yourlogicalfallacyis.com/burden-of-proof - yourlogicalfallacyis.com


It's related to an https://en.wikipedia.org/wiki/Argument_from_ignorance argument from ignorance, but is slightly different in emphasis. Instead of claiming that a proposition is true because it hasn't been proven false, it sidesteps the claim altogether and challenges the other person to disprove the proposition.
Kant associated synthetic apriori statements (e.g. "every change has a cause") with conditions of possible experience, because this is the only explanation that he found, of what makes these synthetic statements true and apriori. The situation is different with analytic statements, e.g. "this body is extended in space". For analytic statements, we have a ready explanation of what makes them true and apriori: an analytic statement expresses a connection of meanings. As Kant put it, in an analytic statement the predicate is contained in the subject. This makes the analytic statement apriori true, and nothing more is required. So since we already know what makes analytic statements true and apriori, there is no motive to associate them with the conditions of possible experience, or with any other extrinsic truth makers.

(P.s. Kant did not define synthetic a priori statements to be about conditions of possible experience. That conditions of possible experience are synthetic a priori, according to Kant, is proved by an argument. It is not true by definition.)
The empty domain or empty set was introduced for formal reasons only.

Georg Cantor mentioned the empty set with some reservations and only once in all his work: "Further it is useful to have a symbol expressing the absence of points. We choose for that sake the letter O. P = O means that the set P does not contain any single point. So it is, strictly speaking, not existing as such."

Bertrand Russell considered an empty class as not existing: "An existent class is a class having at least one member." 

And even Ernst Zermelo who made the "ZFC-Axiom: There is an (improper) set, the 'null-set' O which does not contain any element" said in private correspondence: "It is not a genuine set and was introduced by me only for formal reasons."

W. Mückenheim: https://www.hs-augsburg.de/~mueckenh/Transfinity/Transfinity/pdf Transfinity - A Source Book.
Hegel seemed to believe that Christianity was the particular religion relatively closest to his rational religion ideal. Hegel was attracted to the structure of the Christian Trinity, the mystical union between God and Man, which appeared to match nicely the movement of Hegel's own trinitarian logic (the so called thesis - anti thesis - synthesis movement). The self-sacrifice of Jesus also fitted well with the idea of self-negation, which is the central operator of Hegel's logic. Hegel therefore thought that e.g. Judaism and Islam were less developed than Christianity. The following quotes are from Hegel's Smaller Logic.


  It is no doubt to be remembered, that the result of independent thought harmonizes with the import of the Christian religion:—for the Christian religion is a revelation of reason...
  
  But God, far from being a Being, even the highest, is the Being. This definition, however, though such a representation of God is an important and necessary stage in the growth of the religious consciousness, does not by any means exhaust the depth of the ordinary Christian idea of God. If we consider God as the Essence only, and nothing more, we know Him only as the universal and irresistible Power; in other words, as the Lord. Now the fear of the Lord is, doubtless, the beginning,--but only the beginning, of wisdom. To look at God in this light, as the Lord, and the Lord alone, is especially characteristic of Judaism and also of Mohammedanism...
  
  The movement of the notion is as it were to be looked upon merely as play: the other which it sets up is in reality not another. Or, as it is expressed in the teaching of Christianity: not merely has God created a world which confronts Him as another; He has also from all eternity begotten a Son in whom He, a Spirit, is at home with Himself.

None of the above. It's E: it's rational to act only in one's own interest, and it's irrational to act only in the interest of others. Since rational is not exactly equivalent to what you ought to do, this is distinct from B (but it could imply B).
Your way of interpreting synthetic a priori knowledge is inadequate since it cannot be distinguished from the rationalist view. It is the rationalists (e.g. Descartes, Spinoza and Leibniz) who believed that certain concepts and knowledge (specifically, metaphysical knowledge) can be gained independently of sense experience by virtue of reason alone. (cf https://plato.stanford.edu/entries/rationalism-empiricism/ https://plato.stanford.edu/entries/rationalism-empiricism/) so your (2) clearly is the rationalist position. The rationalists also believed that once we settle with the first principle (e.g., “Cogito ergo sum”), all other metaphysical principles can be deduced from it, so they held your (1). 

To discriminate Kant’s synthetic a priori knowledge from rationalists’ innate knowledge, you have to appeal to Kant’s theory of transcendental deduction (https://plato.stanford.edu/entries/kant-transcendental/ https://plato.stanford.edu/entries/kant-transcendental/). A transcendental deduction is involved with finding out the preconditions that allow we humans to perceive the world the way we perceive. For example, when I see furriness, snout nose, floppy-ear and an adorable smile, why do I immediately attach objecthood (dog) which sustains all these properties? Or, when I see smoke after fire, why do I think that the fire is the cause of the smoke? Kant wanted to find out the reasons for our thinking these ways, and argued that the reason that we humans perceive that way (e.g., causality and objecthood) must be due to the fact that we humans are wired as such. That is, it is impossible for us not to perceive the world the way we perceive. Stated equivalently, we humans can perceive the world only through the way we are wired. This is why we cannot perceive ultraviolet wavelengths, unlike bees. Had we been wired differently, the world would have looked differently to us humans. Kant argued that we humans are wired by 12 a priori categories. This reasoning of Kant is called  the transcendental deduction. 

Synthetic a priori knowledge (such as math, laws of physics, and metaphysics, according to Kant) must be understood under this transcendental view of human knowledge. Knowledge of this type is necessarily true neither because the truth of the predicate can be deduced from the subject (Hume’s analytic truth), nor because it is known by the rationalist intuition (Leibniz’s dogmatic assertions). To Kant, synthetic a priori knowledge is necessarily true because it can be purely deduced from the nature of the presuppositions, that is, the categories. Knowledge of this type is synthetic since the truth of the predicate is not embedded in the subject; it is a priori since the truth is knowable without resorting to sense perception. 

Kant’s transcendental epistemology is valuable for its synthesizing effect. Hume rejected a priori metaphysical knowledge; Leibniz rejected synthetic metaphysical knowledge. Kant argued that metaphysical knowledge must be synthetic a priori. 
Logical fallacies get too much attention, in my mind.  They only apply to purely logical arguments, which are pretty much the unicorns of the debate world.  But people do like to cling to them.

A straw-man argument is a straw man argument.  It is logically flawed.  That does not mean the argument is wrong, it merely means that it is not being defended with a logical argument.  That doesn't even mean it's a bad argument.  It just means it's not 100% logical.  It means the persuasive element must include something else, such as an emotional plea.  There's nothing wrong with that, so long as they don't claim the argument to be 100% logical.
I think you've largely got the idea down. 

Analytic means something that can be shown true without reference to anything else. This means either pure tautologies or expansions of definitions (if there's some other ingenious category, it does not spring to mind).

The key value of analytic is that it is part of a pair. The other feature in the pair is synthetic where we learn something by adding to two different truth things together, such as combining two axioms. As generally used in philosophy, that's how the pair of terms works.

For most of us, we know the terms through Kant where it occurs in tandem with a two further pairs: necessary vs. contingent and a priori vs. a posteriori.  (These, like synthetic / analytic, does not truly originate in Kant but these terms are studied the most in trying to understand Kant). Here, the idea is how do I know something. Something that is a prior analytic can be known just by thinking of the term and working out what it means. That which is a prior synthetic involves the conjunction of multiple a priori truths to reach its conclusion.

A posteriori items work based on evidence. These two can either be things that are identities or things that require working together multiple pieces. If my memory is correct, something that has an a posteriori element would then make the entire argument a posteriori for Kant.

A second big name on this topic is Kripke who is famous for proposing the idea of a posteriori necessity in https://en.wikipedia.org/wiki/Naming_and_Necessity Naming and Necessity

There's some more examples in a different question: 
https://philosophy.stackexchange.com/questions/474/what-are-examples-of-analytic-a-posteriori-knowledge What are examples of analytic a posteriori knowledge?
Conclusion (I) does not follow. The syllogism is EIE in the fourth figure. No P are M; Some S are M; thus No S are P. The problem is that the minor term (S, CDs) is distributed in the conclusion but not in the minor premise.

Conclusion (II) does not follow. Here, the premises are inverted, as the predicate has been placed in the second premise. Reordering the premises shows that the example is IEO in the first figure. Some M are P; No S are M; thus Some S are not P. The problem is that the major term (P, CDs) is distributed in the conclusion but not in the major premise.
Every domain of discourse that uses vague or fuzzy adjectives (or classified nouns, which are reified adjectives).  Which is every domain of discourse that uses adjectives.  Which is every domain of discourse.

Discreteness is not an aspect of macroscopic reality, things made of particles do not have clear boundaries.  And individuation is not an aspect of quantum reality, particles are not completely separable -- they overlap, and they entangle.  But these concepts are central to our notion of 'thing'.  As Quine points out in his discussions around vagueness, similarity and natural kinds, set theory models something we only wish could be true.  But we cannot remove the basic idea of identifying and classifying individuals from our logic.

From a narrative psychology point of view, we are so biased by our personal notion of individual beings that we project individuation onto nature.  But it is not an aspect of nature.  It is not even an aspect of human beings.  It is only really an aspect of people in stories.  But we cannot shake it as an organizing principle because our picture of the world is made up of stories, not observations.
I haven't used/taught LPL myself, so I don't know the format or standards for finishing a RAA proof.  The indented lines 1-4 are correct; for the un-indented line 2, either it's malformed (Eaa if and only if E?) or I don't understand how to parse it.  

In my logic courses, I would probably accept 4 as sufficient to show that 1 leads to a contradiction, QED.  
Mark Andrews is in the right direction. I add some rudimentary idea from FOL(first order logic) model theory to make the answer more compelling. In a model theory, validity is defined as follows: all models that make the premises true make the conclusion true. So to show that the above argument is invalid, all you need to do is to come up with a model under which the premises are true, but the conclusion is false.


  Biden is heavier than Obama.
  
  Fatima cannot lift Obama.
  
  So there is no way she can lift Biden.


A model provides an interpretation for every (non-logical) term in the language. The following are the non-logical terms that need interpretation: 


  Biden, Obama, Fatima, is heavier than, can lift.


If the interpretation corresponds to the ordinary usage of the words, then a model that satisfies the premises indeed does satisfy the conclusion.

https://i.stack.imgur.com/tZBWb.png 

Now, let Biden and Obama be interpreted in the following way (leaving the rest of the non-logical terms intact):

https://i.stack.imgur.com/gqHjq.png 

Under this interpretation, both of the premises are true: Biden (120 lb weight) is heavier than Obama (70 lb bed frame), and Fatima cannot lift Obama due to the shape of the bed frame. However, Fatima can lift Biden (the 120 lbs weight) .
Yes this is correct. A modal statement "it is possible that P" has a truth value that need not be that of P, because the latter depends on whether P obtains in the actual world, whereas the former depends on whether P obtains in at least one possible world.

Note though that possible world semantics is just a way to analyse modal statements. One must distinguish the semantics (a mathematical construction that represents what a statement is talking about) and the syntax (the form and construction rules of statements). Modal statements usually don't refer to specific worlds, they only contain "possible" and "necessary" operators, so I would have framed the question differently without mentioning specific worlds.

For example, your contradiction example could be "it is possible that P and it is necessary that not-p", which is contradictory.

There are indeed proofs in modal logic. You can either use deduction proofs using the axioms of modal logic (there are different axioms associated with different systems, but a common base) or natural deduction rules. Truth tables are the semantics of non-modal logic. Since the semantics of modal logic is different, you cannot use them: you need a possible world semantics.

Your two examples are as you say: the first is a logical truth and the second a logical contradiction. You don't even need to interpret the modal operator to know that, and you could have constructed contradictions that are specific to modal logic (such as <>p <=> []~p or p^[]~p)

Finally, modal statements have a truth value as a matter of logic, but it is more contentious whether or not they have a truth value as a matter of metaphysics (whether they correspond to something in the world). This depends on the kind of modality involved (nomological necessity, metaphysical necessity...), and some authors deny that they have truth values: they would be a mere way of talking.
According to your theory of truth, the truth value of some propositions are determined by subjective conditions. In your opinion, the proposition "God exists" falls into that category, but in my opinion it doesn't. Therefore, which propositions fall into that category is itself a matter of opinion.

Now, if we apply your theory of truth to the things you claim, someone might be of the opinion that the truth value of everything you say depends on opinion, and, accordingly, it would follow that there is nothing logical in what you say. For that reason, I would suggest adopting another theory of https://plato.stanford.edu/entries/truth/ truth.
I think it is empirical, but I don't see him going on about this very much. In reality the use of predicated sentences in human language happens because utility requires so. Maybe a case of too intuitive to even be discussed.

By the way, "S is S" is a predicate sentence. A better proposition would be: Instead of "S is P", why not "S"?
This is a form of begging the question. We beg the question when we either assume the answer itself as one of our premises, or assume a controversial premise that entails our conclusion. Ideally, to be persuasive, an argument should work from mutually agreeable premises. If we insist on working from a premise that is not mutually agreeable and is too close to our conclusion, then we beg the question.

Suppose you and I disagree about whether lager is delicious. I insist on arguing from the premise that beer in general is delicious, which is disagreeable to you. I am begging the question.
At Descartes' time, only Geometry (see Euclid's Elements) had axioms. 

See https://plato.stanford.edu/entries/descartes-mathematics/ Descartes' Mathematics: Descartes' new (analytical) https://en.wikipedia.org/wiki/La_G%C3%A9om%C3%A9trie Géométrie was not axiomatized.

But Descartes clearly conceived mathematics as the paradigm of certainty; see the https://plato.stanford.edu/entries/descartes/#FirResNewMisMet Rules: 


  "In the Rules, he sought to generalize the methods of mathematics so as to provide a route to clear knowledge of everything that human beings can know. His methodological advice included a suggestion that is familiar to every student of elementary geometry: break your work up into small steps that you can understand completely and about which you have utter certainty, and check your work often."


The https://plato.stanford.edu/entries/descartes-epistemology/#5 Cogito's argument "applies" also to mathematical statements: 


  "[Regarding] those matters which I think I see utterly clearly with my mind's eye... when I turn to the things themselves which I think I perceive very clearly, I am so convinced by them that I spontaneously declare: let whoever can do so deceive me, he will never bring it about that I am nothing, so long as I continue to think I am something; or make it true at some future time that I have never existed, since it is now true that I exist; or bring it about that two and three added together are more or less than five, or anything of this kind in which I see a manifest contradiction. (Med. 3, AT 7:36)"


In conclusion: mathematical truth are "evident" (i.e. their denial implies a contradiction) and they are guaranteed by God. 
yes and no. as a formal logical proposition it's fallacious.  but abductive reasoning (aka inference to the best explanation) is fundamental to science. except when abused (see Chomsky)
There's a lot going on in your question which makes sense considering you're expressing that you don't get where some people are coming from. I will try to address two of the things you raise and hope that this answers what you meant to ask.

Problem 1: Absolute Standards

When you doubt the logic of absolute standards in ethics, I think the trick to all such views lies in a rejection of:


  Surely there is less evil in one individual dying that many.


Reworded, this means that you subscribe to a quantitative approach to good and evil. Generally, this means a consequentialist approach to ethics where the value of an action is judged by whether or not it produces the "best" consequences (where best could be any of a variety of things, such as maximizing happiness, maximizing freedom, or minimizing pain).

Approaches to ethics that assert absolute values reject the idea that all ethical goods can be quantified. Instead, they take it as prima facie that some ethical goods cannot be sacrificed regardless of other consequences.

A practical example might help here. On a merely quantitative approach, the answer to the following type of thought experiment is crystal clear:


  IF you could go back in time and kill https://en.wikipedia.org/wiki/Pol_Pot Pol Pot when he was 5 years old before he murdered all those people and made Cambodia suffer, should you do it?


Assuming our one principle is minimizing suffering, then this absolutely seems like the right thing to do. But many of us would question the rightness of killing a five-year old regardless of the probable future of that five year old. (I don't here want to debate the merits of these views -- just mention their takes). On such accounts, there are values that are absolute and should be extended to say all sentient beings such as the right to live until they are violating the rights of others.

A further consideration in this problem is that consequentialist and utilitarian views are not immune to it. Instead, they seem committed to asserting some quantity is qualitatively valuable (Why should we care about suffering it lacks objective importance?)

Problem 2: Conflicting Principles

A second issue you raise is the question of conflicting principles. This is a common objection to Kant's ethics. Sometimes the objection is raised is confused terms that misunderstand what Kant is doing, so we'll skip over Kant and the specific issues his moral philosophy raises.

If we instead look at Aristotle, there are multiple qualitative features that are important for being ethical. Some times they do conflict but then we need to balance them based on what adheres to a life of excellence -- something which Aristotle claims someone with moral wisdom can do.

A second approach, which is Kant's, is to reject the claim that these things can come into conflict. Kant's trick is complex but we can boil it down (for the purposes of responding to your objection) to qualitatively valuing reason in free rational creatures above all else and to understanding ethics as the application of reason to questions of action.

a further consideration is that again consequentialist views do not lack conflicts in principles. Mill's Utilitarianism asserts that actions are good to the extent they promote happiness and bad to the extent they promote unhappiness, but he also incorporates at least two other principles that seem in conflict with this. First, in the same text, he tries to claim there's a distinction in types of pleasure based on what should make us happy. Second, in On Liberty (and elsewhere), he asserts that we cannot harm anyone to maximize happiness -- but this means there's two principles in his consequentialist view: the greatest happiness principle and the harm principle.

In other words, conflicting principles seems to be a nearly endemic problem... but generally a problem asserted by the critics of any particular view which the defenders (leaving aside Kant's approach) think indicates multiple but non-contradictory values.

summary

Note well, I'm not saying you need to accept these views. Instead, I would say:


Views that think there are inviolable principles usually do so on qualitative grounds rather than quantitative ones.
The conflict of principles problem is a common objection but not identical to a proof that there aren't multiple things we value.




addendum

To clarify based on the OPs addition to their question, philosophical accounts that have absolute principles use these to create absolute prohibitions on certain actions.

For instance, if you believe it is always wrong to use violence presumably you believe so because you think doing violence is qualitatively an absolute wrong that cannot be cancelled out by some other benefit (i.e. you cannot match quality with any quantity). 

To repeat, the justification is following Kant precisely that you take something to be of absolute worth and set this in contrast with things that have a price. Things of absolute worth are not subject to calculation. 

For instance if you take intelligent life to be of absolute value (qualitative claim), include dolphins in the definition of absolute value (empirical claim), then you conclude there's an absolute prohibition on killing dolphins. 

If you object, "but what if ..." and pose some hefty moral cost for this (every child on earth loses an eye, a kidney, and a leg), then one of two things will occur:


The person who claimed to place an absolute value will fold on that position and convert to a quantitative claim about the goodness of saving dolphins


OR


They will hold onto the qualitative claim and say that the wrong that would happen would happen.


There's actually another step usually involved. Generally, the person who believes everything is quantitative makes an assumption not shared by the people who hold to qualitatively distinct absolute values, and that assumption is about agency.

In the example I provided, it's really hard to imagine why or how not killing dolphins would result in this consequence of every child losing 3 things. Generally, the next step is for the objector to move to a more plausible example, but there's several issues:

First, you'll never get a completely convincing causal story that places the qualitative good directly opposite a quantity of some good. This is because  many of the things people qualitatively value (such as freedom) relate to agency, and make it so that it's rarely if ever convincing that protecting the qualitative good is the cause of the quantitative loss. So then why should it be abandoned or reduced?
I'm not sure what your assignment is asking for. One of Russell's hallmark habits of mind is that he sifted through bodies of work then pointed out hidden premises then went on to reduce the number of premises to bare minimum. If your assignment is asking students to use Russell's methods to analyse Russell's work, I am indeed surprised; this is a great idea. 

Premise 1. We cannot draw inferences from experience to what is not experienced unless we know general principles of some kind by means of which such inferences can be drawn.

A sure way to demonstratively prove all cases is to examine each and every case. To prove all ships in your squadron are mission capable, every and each ship must be thoroughly inspected. When it is impossible to demonstratively prove every and each case, general principles of some sort must be known in order to draw inferences from particular to general.

Premise 2. If P implies Q, then the probability of P is less than the probability of Q because whenever P happens Q happens, whereas when Q happens P may or may not happen.

Premise 3. Failed cases cannot disprove a probability statement. An improbable event can nevertheless happen; that an improbable event happened does not invalidate the statement that this event is an improbable event. 

Suppose a man, as of January 1, 2000, had seen a great number of swans, all of which are white, and he speculated that all swans are white. Even if all the swans he saw after he made that statement turned out to be black, his later experience cannot invalidate the statement he made on January 1, 2000 because counter examples cannot invalidate a probability statement.

Premise 4. A principle cannot be used to prove itself. Using examined cases to prove unexamined cases relies on the inductive principle itself, thus inductive principle cannot be proved by enumerating examined cases.

No. 1 says the inductive principle is need in order to make inferences from particulars to general.

No. 2 says the probability of the general law is less likely than the particular case.

No. 3 says the inductive principle cannot be disproved by experience.

No. 4 says the inductive principle cannot be proved by experience.
Your question at least on a quick reading seems to be using terms a bit will nilly. Let's start by defining terms.

Background

Statement - a claim that can be either true or false.

Some examples:


"it is raining" 
"it is not raining"
"if it is raining, John uses an Umbrella"
"Jill is either in Denver or Tokyo"


We also need to be a bit careful about the "can be" in the definition. Here, it means something like "capable of being evaluated to either true or false" -- my point being that it's possible a statement is always true (e.g. A or not A) or always false (e.g. A and not A), but the point is that statements don't include things that don't have a "truth-value."

Self-contradictory - I assume this something that posits both A and not A.

Valid  - this means that an argument would have a true conclusion were all the premises to be true. 

Another term related to validity is "truth-preserving." This is the idea that an argument's premises truth can be carried onto the conclusion.

To your question

In your question you speak as if a statement can be valid or invalid. This is not true in logic. Statements can be well-formed or not. A well-formed statement is one that can be evaluated unambiguously. Any statement that follows the rules of the syntax is a well-formed formula -- not statement is valid because validity is not about statements.

You also speak of a self-contradictory premise. This might make sense, but more commonly this happens with an argument as a whole rather than a single premise.

E.g. "The moon is blue" and "The moon is not blue" would create a self-contradictory premise if it's a single premise or a self-contradictory set of premises if the two claims are separate premises.

To understand why an argument is valid if it has self-contradictory premises, we just need to think through the definition of validity a bit. I will repeat it in pieces:


  An argument is valid  if and only if it is the case that when the premises are true, the conclusion must be true.


This is often thought of it in a backwards way in the sort of introductory logic and critical thinking classes that exist. In other words, 


  an argument is valid just so long as there is no case where the premises could be true and the conclusion false.


(In some types of logic, the definition of validity is different than this, but those types are never taught in introductory logic classes in philosophy)

Given a set of contradictory (or self-contradictory) premises, it is never the case that the premises are all true -- because for one to be true ("the moon is blue"), another must be false ("the moon is not blue").

Thus, any such argument is valid on this definition of validity.
Relatively https://www.theguardian.com/education/2016/mar/15/why-philosophers-make-unsuitable-life-partners few great philosophers have been married, or had children, whether because their minds were on higher things, or because they were impossible to live with is hard to say. It is also hard to know which ones were actually celibate, as opposed to just unmarried. 

Plato famously promoted nonsexual love as superior to sexual love, which is where we get the term "https://en.wikipedia.org/wiki/Platonic_love Platonic relationship" for a nonsexual relationship.  Wittgenstein and Turing are known to have been homosexual.  Kierkegaard broke off his engagement, and then luxuriated in the guilt and regret.  Simone De Beauvoir had a long relationship with Jean-Paul Sartre that never led to marriage, although https://en.wikipedia.org/wiki/Simone_de_Beauvoir he did propose.  Kant lived a life of famously monastic regularity, although he did have plenty of friends.

On the other hand, Socrates was married to https://en.wikipedia.org/wiki/Xanthippe a much younger woman, with whom he had three children (and according to one account actually had two wives simultaneously).  He was perversely proud of her reputation as the most unpleasant woman in Athens, and called her an ideal spouse for a philosopher, because she made him long for the quieter embrace of eternity.
You've run into the much larger issue that it's almost impossible to accurately assign probability in real life. Even determining the probability that the sun will rise tomorrow is philosophically complicated: https://en.wikipedia.org/wiki/Sunrise_problem Sunrise Problem.

If you break an event down into subevents, know the probability of those subevents and the interdependence between them, you can indeed compute the probability of the main event.

In this case, however, all you've done is move the problem "down" one step. You now have subevents whose probability you don't know. You can continue breaking down these subevents further, but you'll just end up with a large number of simple events whose probability you don't know.

Mathematicians (and pure statisticians) work around this problem by using the word "assume" a lot: assume a fair die/coin, assume a random sample/draw, assume people respond accurately to polls, etc.

Probability can tell you: if we make certain assumptions, we can compute the probability of something happening. However, these assumptions may or may not apply in the real world.

Ultimately, probability answers the question: if we were to repeat this experiment repeatedly, how often would various outcomes occur as we increased the number of trials.

In the real world, you never really get to repeat an experiment for the same reason you can't cross the same river twice: the real world is in a constant state of flux, and conditions are never identical twice.
I think you're basically right about how the discussion goes on this point, but it's not necessarily a "fallacy."

First off, a minor correction in structure:


  since the action is obviously good, it makes you happy, and that is the consequence that matters.


I think the consequentialist position is that the reason an action is "obviously good" is that it makes you and/or many happy. And that this is why we can even call it obviously good. (i.e. obviously good follows from rather than causes making happy on this picture).

Second, I'm going to use "consequentialist" here to refer to the sort of utilitarian version of consequentialism you use in your question. There are many others that would gum things up and not work for your question.

Third, let's look at what you're saying vs. what they're saying.

C's Argument (at its most basic and simplified):


Any action is good because it makes you happy
Action q makes you happy.
Ergo Action q is good.


This picture is accepted by consequentialists (broadly speaking and varying by consequence of interest). But it's rejected by Kant and Aristotle.

For Kant, it's something like this:

Kant version:


Any action is good to the extent that its maxim is universalizable and respects humanity (rationality).
Action q is undertaken under condition 1.
Action q is good.
Action q may also make you happy.


and/or some good actions don't make you happy (for Kant, this is one reason why we have to be immortal and there must be a heaven and hell to even this out) and some people experience happiness at evil.

Aristotle version:


Any action is good if the phronemos (man of practical wisdom) would choose to do it under all the same circumstances.
Any good action brings pleasure to the phronemos in expressing excellence.
Action q is undertaken by phronemos.
Action q is good and action q brings the phronemos happiness...


but


Action q is undertaken by Joe Schmoe
Joe Schmoe is not guaranteed to experience happiness.
Action z (some obviously wrong action) is undertaken by Joe Schmoe.
Joe Schmoe might experience happiness.




In a sense, given this picture, we could say is that consequentialists are question-begging when they assert that actions are good when they promote happiness. But I'm not entirely convinced they are doing something fallacious. I take it what they believe they are doing is observing empirical phenomenon and identifying the word "good" with the empirical phenomenon of pleasure-giving. Thus any time they see pleasure giving, they say "good."

People with other views are not convinced because they don't share an empiricist assumption about morality held by the consequentialists. I think for the consequentialist to engage in a fallacy, what has to happen is that they don't acknowledge they are making a metaphysical claim that undergirds how they are interpreting empirical data. Some consequentialists do this; others don't.

Bernard Williams has an interesting article on this point highlighting that consequentialist theories often seem to be post-hoc explanations of right and wrong which are always checked against something else. In other words, most utilitarians want murder to be wrong, so they try a quantitative theory and test it against things they already consider wrong.
Let's try with some examples.


If Alex is walking in the forest, Alex wears shoes.
If Alex is walking on the street, Alex wears shoes.
Alex is not walking in the forest.


if (2) were valid, then it would follow that Alex is not wearing shoes. But clearly we haven't shown that Alex is not wearing shoes, because there are other conditions under which he wears shoes.

(1), however, can remain valid, if 


If Alex is walking in the forest, Alex wears shoes.
If Alex is walking on the street, Alex wears shoes.
Alex is not wearing shoes.


In this case, we can infer both that Alex is not walking in the forest and that Alex is not walking on the street.

Now let's make it a bit more formal. Valid in logic means that if the premises happened to be true, then the conclusion must also be true. 

By the counter example above, we have shown that the pattern you refer to as (2) can have a false conclusion with true premises. This pattern is the fallacy called "denying the antecedent."
This question shifts topic from extrinsic value to extrinsic right and wrong. The need for discussing extrinsic value becomes much more obvious when we consider cases of goodness rather than rightness.

Donuts and cars are good in certain situations. Hungry and in need of a ride, I value them enormously. But seen in a different light or in a different situation, they can look different in value. They pollute or can make me fat, and are worse things relative to bicycling or spinach.

Such variability by context is the mark of the extrinsic. If we called these things intrinsically good, we wouldn’t acknowledge their badness in some contexts.

Surely as part of basic human interaction we want and need to acknowledge that we value many such things, though we may not always value them in every way.

{1}     1.  ~P ∨ Q                         Assum.
{2}     2.  ~P                             Assum. (1 1st Disj.)
{3}     3.  P                              Assum.
{4}     4.  ~Q                             Assum.
{3,4}   5.  P & ~Q                         3,4 &I
{3,4}   6.  P                              5 &E
{3}     7.  ~Q → P                         4,6 CP
{2,3}   8.  ~~Q                            2,7 MT
{2,3}   9.  Q                              8 DNE
{2}     10. P → Q                          3,9 CP (1 1st Conc.)
{11}    11. Q                              Assum (1 2nd Disj)
{3,11}  12. P & Q                          3,11 &I
{3,11}  13. Q                              12 &E
{11}    14. P → Q                          3 13 CP (1 2nd Conc.)
{1}     15. P → Q                          1,2,10,11,14 ∨E
-       16. (~P ∨ Q) → (P → Q)             1,15 CP
{17}    17. P → Q                          Assum.
{4,17}  18. ~~P                            4,17 MT
{4,17}  19. P                              18 DNE
{4,17}  20. Q                              17,19 MP
{4,17}  21. Q & ~Q                         4,20 &I
{17}    22. ~~Q                            4,21 RAA
{17}    23. Q                              22 DNE
{17}    24. ~P ∨ Q                         23 ∨I
-       25. (P → Q) → (~P ∨ Q)             17,25 CP
-       26. (~P ∨ Q) ↔ (P → Q)             16,25 ↔I

The problem with moral relativism is that this system is not absolute by definition and objectivism by definition is absolute.

Relativism seems to make stuff up as you go.  The answer today might not work tomorrow.

Objectivism seems to use patterns to formulate a universal answer for x.  That means come tomorrow the answer will not change.

Which system would you rather?  One is iffy with solutions and the other offers certainty provided certain criteria is met.
Isn't a person who is maintaining themselves as their number one priority with the purpose of having more and being more to share with the people who depend on them in their lives being simultaneously selfish and unselfish?
The short answer is: it really depends on what you mean by ‘statement’. To give the longer answer, let’s recall the distinction between syntax and semantics. Starting on the syntactic side, compare the following three:

i) ∀xPx

ii) ¬Qx

iii) vxR

All three expressions consist only of first-order symbols. Further, (i) and (ii) were formed according to the syntactic rules of first-order logic: they are well-formed formulae. By contrast, (iii) wasn’t thus formed and is, in that sense, gibberish. (But one could imagine a language that accepts ‘vxP’ as well-formed.) Finally, while x occurs freely in (ii), all variables in (i) are bound. Therefore, (i) is a sentence of first-order logic, while (ii) is not. Note that these differences were drawn without considering what the individual expressions ‘mean’ or ‘assert’, i.e. without appealing to semantics.

To give corresponding English expressions, consider:

(i*) Everything is fine.

(ii*) It isn’t green.

(iii*) Or it taller.

Turning now to semantics, (i) above is going to be either true or false under any given interpretation: it ‘says’ or ‘states’ that everything is P – where the individual interpretation needs to specify the value of P and the domain of the universal quantifier. Turning to (ii), let I be a particular interpretation. Because x occurs freely in (ii), I doesn’t assign a truth-value to (ii) directly. However, each variable assignment of I does assign a truth value to (ii): first, a value is assigned to x, and that value then either is or isn’t in the set assigned to P. So, if the free x in (ii) is like 'it' in (ii*), then each variable assignment gives an answer to the question: ‘What do you mean – it’?

Thus, it would be wrong to say that (i) is meaningful while (ii) is not: both express a content only relative to something – viz. relative to an interpretation in the case of (i), and relative to a variable assignment (within an interpretation) in the case of (ii).

So, are (ii) and ‘x is yellow’ statements? Well, it really does depend on what we mean by a statement. If we mean to include expressions like ‘It isn’t green’, then (ii) is a statement. If we want ‘statement’ to be synonymous with ‘sentence’, then (ii) isn’t a statement.
The requirement for intuition is not with morality.  You can have a morality based upon just about anything, including the whims of your master, or the whims of some dead master who imagined talking to God.  Many people do.

But things change when it comes to your intention to justify your thinking.

No system of justification can exist without some sort of axioms.  Whether you explicitly state them or not, you have some basis for your judgments, and you string together judgments according to some set of rules, which are simply the active form of axioms.

What is not covered by accepted or negotiated rules, has to come from interpretation.  That means that assent to the rules themselves requires interpretation as a basis.  The rules cannot be given to you without a grounding in your interpretation of your situation.

As Descartes pointed out a long time ago, if you dwell upon it long enough, it seems obvious that interpretation necessarily involves starting from internal sensation.  The feeling of assent you give to a statement, the idea that you understand anything, is based on an emotional reaction.  Most folks informally call that emotion 'intuition'.
The difference between necessary and sufficient conditions can be explained as follows:

1) Vitamin C is a sufficient condition for preventing scurvy

This means that scurvy can always be prevented by taking vitamin C. But there may be other ways as well.

2) Vitamin C is a necessary and sufficient condition for preventing scurvy

This means that scurvy can always be prevented by taking vitamin C, and it is the only possible way.

As far as I know, scurvy is a disease caused by a lack of vitamin C. Therefore, 2 seems to be the right answer.
From an ordinary operational semantics, that I see you, or do not see you, does not change you, it changes me.

My understanding of phenomenon X is an aspect of myself and does not affect the phenomenon, except to the extent I interfere with it in order to get the necessary information to make the understanding.  Even then, whether the result is to understand, or not, is not relevant to the thing understood.

So the definition is an aspect of the relation between the describer and the thing described, and does not properly belong to the object as an attribute.  There is not a different 'me' each time someone else comes to understand me in a different way.
The sentence you are trying to prove is provable in system K, but your proof is not valid. L(p & ¬p) is not a contradiction. Lp & ¬Lp is a contradiction. System K is sufficiently weak that Lq → Mq is not a theorem. To understand this, it is perhaps better to think in terms of Kripke semantics. Lq says that q is true in every PW that is accessible from the given world, while Mq says that q is true in at least one such PW. While it might seem that Lq entails Mq, there is one way that this can fail, namely if there are no PWs that are accessible to the given world. In that case, Lq is trivially true for any q, while Mq is false. This is similar to the feature of ordinary quantifier logic under which it can be true that all unicorns are white, but false that there is at least one white unicorn. The weak nature of system K manifests itself in that there are no constraints on the accessibility relation between PWs. In particular, the relation is not serial and not reflexive, so there is no guarantee that any PWs are accessible. 

But the sentence you are trying to prove has an antecedent M(p → p). This does assert the existence of at least one PW that is accessible to the given world. So given this, Lq → Mq follows. Your proof does not make use of M(p → p) anywhere - there is no reference to line 2. Lines 4 through 12 attempt to prove Lq → Mq unconditionally, which cannot be done. 
Concrete entities are the specification of mathematical objects. If the mathematical objects are abstract, then the concrete objects are robbed. In this case, there is no difference between abstract and concrete. The logical basis is on concreteness. The teleological mechanism works by associating concretenesses with each other, which is an indicator of the operator. Something is the result of a kind of energy that is turned on by the sensing operator. This energy is causality.
Necessity means that the truth of a proposition follows in ALL possible worlds consequent to that proposition. 

This is contrasted with Possibility which means that the truth of a proposition follows in SOME world consequent to the initial frame of reference. 



As MarkOxford linked above, please see this article for further elucidation: https://plato.stanford.edu/entries/logic-modal/ https://plato.stanford.edu/entries/logic-modal/
This is an issue Frege dealt with.  If I told Lois Lane that Clark Kent is Superman, she would be very surprised.  Thing is, Clark Kent and Superman both refer to the same thing, so it seems as though I'm just asserting identity.  From this perspective the phrase should be as informative as saying Superman is Superman which would not surprise Lois.  Frege describes the concept of "Sense" which means the "Method of reference" used by a word or phrase.  I can refer to the same object with Clark Kent and Superman, but each has a different sense.  Superman refers to that really strong flying guy, Clark Kent refers to that really boring nerdy guy. The extra info offered by "Superman is Kent" is that the really strong flying guy is also the boring nerdy guy, which is what surprises Lois. This exemplifies the difference between propositions of the form A = A and A = B: A and A have the same sense, A and B have different senses.  Make sense?
Compare with the https://books.google.it/books?id=xRCkNvDlRtYC&printsec=frontcover previous statement (D.Q. McInerny, Being Logical: A Guide to Good Thinking, Random House, 2004 edition, page 57):


  Note this about the "completely alike" judgment: No two things can be so alike that they cease to be two things. If two things were to be identical in the literal sense, there would be but one thing.


Thus, "to be alike" must be assessed comparing the properties shared (or not) by the two things to be comapred. If they share absolutely all properties, including their spatio-temporal location, they will be absolutely indistinguishible, and thus identical, and thus "really one".

By contrast, two existing things have at least one common "property": existence. If not, one of them must be non existent, and thus not "a thing" at all.

Question: is existence a property, like color ?

See https://plato.stanford.edu/entries/existence/ Existence as well as:


Francesco Berto, https://books.google.it/books?id=UobUqRMyUawC&printsec=frontcover Existence as a Real Property: The Ontology of Meinongianism, Springer (2013).

Yes, you can use show boxes in Modal Logic Proofs. It also helps when you use Indirect Derivation (aka Reductio Ad absurdum) proofs for innermost nested show box.

In the show box method of proof, as I remember learning it, each nested Show statement is a possible world, a frame, within the scope of the World outside of it. 

1) Show □A->□□A       Line 7, Direct Derivation 
2) □A                 Assume Conditional Derivation
3) A                  By Line 2, (T) 
5) Show A->□A         Line 6, direct derivation
6) □A                 Line 2
7) □A->□□A            By Line 5, (K)

Line 5 expresses a frame relationship between 1R2.



I believe your confusion stems from the fact that The Rule of Necessitation is not an inference rule for use in derivations but a rule concerning the treatment of theorems.

The most interesting and powerful result of the weak system (K) is this: 

The Distribution Axiom:  □(A→B) → (□A→□B).

In (T), If something is Necessary, then it is actual, so:  □P -> P . It is the case that if something is actual, then it follows in a world consequent to it though, which is what this implies.

BUT! If a formula is a THEOREM,then per Zeman: "We use the terms 'thesis' and 'theorem' interchangeably here; either refers to a formula deducible from the null set of hypotheses, that is, to a formula provable within the system in question."

This allows us to import the theorems' truth values from antecedent worlds into those worlds that follow from them--which makes perfect sense when you reflect on it.


It depends what you mean by "all systems of informal logic". If you mean all the ways people can reason, then your question is tantamount to asking if all human thinking can be formalized... and we simply don't know that. Even the lesser question whether all human thinking can be simulated by some machine is basically asking whether a https://en.wikipedia.org/wiki/Strong_AI strong AI can be built. (You can simulate what don't quite understand--for instance there's no deep way to understand an artificial neural network that [say] beats most people at Backgammon.)

If you mean just some system(s) of IL that people have (more or less vaguely) described... usually a FL based on that exists if the IL was interesting enough. (So take this as an experimental postulate.) Some 20th century examples (as the date of formalization): temporal & modal logic(s), paraconsistent logic(s) etc. The IL system may of course be flawed, but then it depends what is a logic, which is itself not a settled question even for the formal ones.
In a certain sense your protagonist is correct. If your starting position is that logic and reason are correct, then applying logic to conclude that logic and reason are correct will always be circular.

The trick is to not get all "truthy" about it. Logic is a useful tool for drawing inferences from a set of statements. That's all, nothing more mystical than that. 

And you need to be a little careful. In a comment you state:


  logic is a valid way of assessing arguments because when used properly it comes to deductions and conclusions that match what one finds in the real world


This is dangerous. Logic says nothing at all about the real world by itself. At best, if your input statements are consistent with the real world then logic can produce other statements that are consistent with the real world. This can be valuable, for sure, and can help to gain insight but is limited by its inputs.

For example, let's reword your initial problem a little:


  The Bible says that God exists, the Bible is true therefore God exists.


This is perfectly reasonable logic, not circular in any way. If you accept the two premises then the inference follows quite naturally. It only gets interesting when you try to map the premises to the real world. The first is fairly uncontroversial, the second somewhat less so.
See page 48 of https://books.google.it/books?id=xRCkNvDlRtYC&printsec=frontcover Being Logical:


  The simplest argument is one composed of two statements, a supporting statement or premise and a supported statement or conclusion. Usually, the context of the argument will allow you to tell which is which, but we attach what are called "logical indicators" to statements in order to mark them clearly as either premises or conclusions. [...] Common logical indicators for conclusions
  are "therefore"...


Thus, the exposition of Conditional Argument (page 63) is a little bit sloppy: "if the weather is nice tomorrow, we will go on a picnic" is not an argument but a compound statement.

The complete argument will be:


  "if the weather is nice tomorrow, we will go on a picnic; the weather is nice tomorrow. Therefore, we will go on a picnic."


It is an instance of the following "argument schema":


  "if A, then B; A. Therefore, B". 


Regarding validity, see page 60: validty regards "form" (the structure of the argument) and not "matter" (its content):


  An argument is valid if its structure is sound, which means that its structure is such that true premises will ensure a true conclusion.


Thus, in order to assess validity of an argument, we have to consider the corresponding "argument schema", i.e. the formula obtained replacing statements with (sentential) variables: 

An argument is valid when every "instance" (obtained substituting real sentences for the variables) of it has the nice property that: 


  if all the premises are true, so is the conclusion.


Please, note the clause "if all the premises are true"; in the above case, we have to assume that both premises are true.

But if we assume that the first premise: "if the weather is nice tomorrow, then we will go on a picnic" is true, then we cannot say "tomorrow there will not be good weather, but we can still decide to go on a picnic", because this is not the original premise.



The "argument schema":


  "if P, then Q, and P; therefore Q",


is valid, as well as: "if P, then Q, and not-Q; therefore not-P".

The "argument schema": "if P, then Q, and Q; therefore P", instead, is not valid (see https://en.wikipedia.org/wiki/Affirming_the_consequent Affirming the consequent).  

The validity of: "if P, then Q, and P; therefore Q" as well as that of: "if P, then Q, and not-Q; therefore not-P" are licensed by the truth table for the https://en.wikipedia.org/wiki/Material_conditional#Truth_table conditional propositional connective: "if..., then...". 

For the first one, we assume that the first premise is true: thus rules out the 2nd line of the truth table (P true and Q false).

But we assume also P true, and this in turn rules out 3rd and 4th lines.

In conclusion, the assumption of both premises gives us only one possible state of affairs: P true and Q true: thus, we are licensed to safely conclude, from the two assumptions above, with the truth of Q.

Same approach to the second argument schema: the assumption of the first premise rules out the 2nd line of the truth table (P true and Q false).

And the assumption of not-Q as true (i.e. of Q as false) in turn rules out 1st and 3rd lines.

Again, we are left with only one line: the 4th one, where P is false, i.e. not-P is true.

Thus, we are licensed to safely conclude, from the two assumptions, with the truth of not-P.

  If existence cannot be proven, apart from as a state of mind, is there any real way in which we can suggest this is true?


If I may rephrase the question: Existence cannot be proven, apart from as a state of mind. Therefore, the question becomes, "Does reality exist for individuals, without minds to perceive it?"

Answer: Collectively, we know that existence is real. But for each individual, it is only real while they live and have a mind with which to experience it. So existence is simultaneously both, absolute and transitory. Unless the mind somehow survives the death of the body, in which case we cannot know the truth of the matter until it is our turn to die.

So there are two possible realities accessible to individual experience:


The individual mind survives the death of the body, and learns that existence is absolute. Then proceeds to deal with reality as it is perceived within that particular circumstance.
The individual mind doesn't survive the death of the body, and from that point on learns no more and remembers nothing. In the latter case, existence would no longer be of any concern to the individual, because the question becomes moot. Without a mind to perceive and form thoughts, existence becomes transitory.

Formal and concrete are adjectives qualifying a concept which is otherwise less permanent over time and space. In pure mathematics (distinct from applied mathematics) consistency will suffice as an adjective. In nature science matter is studied. The word matter, as the prime content of nature, indicates that nature is currently studied only as material nature. What is studied excludes concept of consciousness or awareness, which alone exists by itself and for ever. Only such existence can be called real. 

Names and forms appear in consciousness which is mind of person identifying with a body in mind. Mathematical ideas appear in mind as what are logically connected by intellect in mind. Many systems of mathematical logic are created and they appear separately in mind. Their creation is to design of predispositions and latent tendencies of the person related to other persons and objects in mind.

Axioms are unchanging logic created by intellect as connection of mental ideas. Formal study is logical analysis. The ideas can be (and are) correlated by intellect with concepts of material objects. In the West, matter is mistaken by many persons to be the reality and worse still to be the sole reality. Therefore ideas and concepts in mind are mistaken to be representations of material reality, only failing which they are purely imagined ideas or concepts. Pure mathematical ideas are purely imagined.

Whitehead and Russel defined Peano arithmetic to be mentally logical. Godel introduced ambiguity into it by mixing logical concept of quantitative cardinal numbers with the different logical concept of ordinal numbers which represent spatial order or sequential order. Such order to which free numerals are pegged falls outside the logic of cardinal numbers. Thus Godel claimed that the logic of Paeno arithmetic was both true and false, consistent and inconsistent. Consistency is lacking in multivarious logic of perceived concepts created in physics. The resulting inconsistency is named uncertainty in material reality. 

Correlating concepts, which are called perceived material conceptual reality, with mental concepts of mathematics makes the correlated mixture inconsistent or uncertain; not so the purely logical systems of mathematics. Mathematics by itself is consistent and never concrete in the sense that perceived concepts mistakenly to be material outside the mind are said to be concrete. Whether created concept is concrete or consistent the creative construction is involuntary and creation in the mind is by predispositions and latent tendencies of mathematicians and physicists.   
It is not impossible in principle that there might be a logic of explanation, which is to say, a logic that answers "why?" questions. Different logics, such as intuitionistic, relevance, linear, etc., have different natural semantics. It is not unthinkable that one could have a logic whose natural semantics was that of explanation. Asserting a proposition "A" would then be interpreted as "A is explicable", and a conditional "A → B" would be interpreted as "A explains B", or possibly "an explanation of A can be manipulated into an explanation of B". 

The main problem is that I expect it would be fiendishly hard to come up with satisfactory general rules for such a logic. Carl Hempel attempted to describe a logic of scientific explanation, but it is widely regarded as unsuccessful. Different branches of science seem to use different paradigms for explanations of phenomena, so the rules would be difficult to generalize. 

In some specialized contexts one might be more successful. In computer programming, for example, running a debugging tool might be interpreted as enquiring why one obtained a particular result from a program. Depending on how the program is structured, it might be possible to produce debugging output that takes the form of a formal logic. 
'"I exist" is not a priori since it requires my experience of I to be known.'

If it requires experience in order to be known then, at least on Kant's account, it is not analytic either. Critique of Pure Reason, A7/ B11 : 

'It would be absurd to ground an analytic judgment on experience, since I do not need to go beyond my concept at all in order to formulate the judgment, and therefore no testimony from experience for that' (Kant, 'Critique of Pure Reason', tr. P. Guyer, Cambridge : CUP, 2008, 142). 

For Kant all analytic judgments are a priori and therefore cannot be a posteriori. 

Now, you may well not feel constrained by Kant's analysis but you use his terms and, so far as I can see, his senses of the terms. Maybe you are right : ''I exist" is not a priori since it requires my experience of I to be known.'' But you need in that case a different analysis of terms to get the result that 'I exist' is analytic a posteriori. 

Work on this further. You may be on to something. But you do need to work on it further. 
Mathematical logic isn't logic, but a formalisation thereof; one view is to think of it as mathematics as inspired by logic, in the same way that mathematics has been inspired by physics but is not synonymous with it  - indeed they often have difficult understanding each other despite the laymans view that they are virtually synonymous. 

There is such a thing as naive set theory, and mathematicians that aren't set theorists use this all the time when they reason about sets. Likewise, one can say that there is a naive or intuitive logic that mathematicians use when they reason about mathematics.

Logic remain a field of study in philosophy, and it isn't neccessarily mathematical; linguistics and semantics for example come into it as well as others.
Characterisations such as the Baroque period, Romanticism, Apollonian or Dionysian, Bluegrass or Rave show that the arts have an aspect of uniformity, a family resemblance amongst a genre. 
Hint

Assume S and derive R ∨ P from 1st premise.

Now two sub-proofs, for ∨-elim:

1) Assume R and derive Q ∨ R by ∨-intro, and it is done.

2) Assume P and derive ¬R → Q from 2nd premise.

Now use R ∨ ¬R (Excluded Middle) for a new ∨-elim:

2.1) Assume R and derive Q ∨ R.

2.2) Assume ¬R and derive Q from ¬R → Q and then derive Q ∨ R.

Having derived Q ∨ R in each case, we can conclude with:


  S → (Q ∨ R)


by →-intro.
Let's push a fist through this problem with two examples - then a third. 

Situation 1. You find you have locked yourself out of your flat. How can you get back in ? You look for the key you hid in the garden but can't find it. You think of phoning another flat holder but don't have your mobile and anyway can't remember any numbers. Your flat is on the second floor and the window is open. Is there a ladder nearby ? No. But there is a tree near the window and if you can climb that, you can probably make a short leap on to the widow sill and climb in that way. Up you go and in you get. This is an example of reasoning - of practical reasoning - reasoning about what to do.

Situation 2. You are presented with three sentences : 'All dogs have teeth', 'All dogs are carnivores', 'All carnivores have teeth'. What a jumble of  trite statements. But the logician re-arranges them : 

1 All carnivores have teeth

2 All dogs are carnivores



3.Therefore all dogs have teeth

It doesn't matter if any or all of these sentences are false. The sentences have been set out in an argument in which 3. (the conclusion) cannot be false if 1. and 2. (the premises) are true. The argument is valid regardless of the truth or falsity of 1. and 2. This is an example of logic. 

In the reasoning example it mattered quite a lot whether your beliefs and assumptions were true. If you were wrong about the climbability of the tree, you'd break your neck. In logic the actual truth or falsity of the sentences is irrelevant - at least it involves no such catastrophic risks. 

Situation 3. I've taken an example of practical reasoning but reasoning differs from logic even if we select theoretical reasoning - reasoning about the facts. A laptop has been stolen. You know you didn't steal it. Did your flatmate ? Possibly but s/he had no reason you can think of for doing so - not short of money, has own laptop, &c. There is a new tenant but it is unlikely they could have got access to the laptop. But there was a mystery caller to the house just before the laptop disappeared. Most likely they had a skeleton-key, nipped in and abstracted the laptop. This is sound and sensible reasoning but it is not logic. All your assumptions (premises) could be warranted and rational but the conclusion false. In the event it was the property-owner who stole the laptop. 

▻

Step back now and theorise the differences a little. Reasoning is psychological. It is about working out means towards ends, applying rules to cases, deciding which of two or more incompatible beliefs is better evidenced, puzzling out who stole the laptop, whether X will make a good partner. It centres on belief (formation and revision) and inference. 

▻

Logic by contrast is not psychological. At all. It centres on the correctness or validity of arguments. It is not concerned at all with whether the premises are true or false or whether the conclusion is true or false. Its sole focus is whether the conclusion must be true if (IF) the premises are true. If the conclusion must be true if the premises are true, then the argument is valid. The argument itself cannot be true or false; it can only be valid or invalid. 

▻

Logic and reason are not totally unconnected. One can reason logically. Suppose I reason as follows : I believe that p is the case; I also believe that if p is the case then q is, must be, the case; so I believe that q is the case. I might mentally chug through this. (I believe that x is red; I believe that if x is red then x is coloured; so I conclude that x is coloured.) Rather a simple example but it makes the point - that my reasoning is logical in the sense that it can be represented in logical form as modus ponens : p; p → q : q.

▻

The catch comes - and reason and logic part company - when we adopt a certain type of logic. I don't know how much logic you know but certain forms of 'truth-table' logic can validate implications that make no sense for reasoning of the sorts we've looked at. In certain forms of logic, a conditional is true if the antecedent ('p') is false and the consequent ('q) is true. 'If the moon is made of green cheese (p) then 2 + 3 = 5 (q)'. Truth-table logic of this kind  is capable of perfectly sound defence but that defence does not include its agreement with reasoning of the everyday kind considered here.
This is most succinctly captured in modern analytic terms as 'http://plato.stanford.edu/entries/supervenience supervenience of the ideal upon the physical.' It is one form of 'http://plato.stanford.edu/entries/properties-emergent emergentism' as a theory of mind, which identifies concepts as emergent phenomena of our social interactions (via Wittgenstein's https://plato.stanford.edu/entries/wittgenstein/#LangGameFamiRese language-games or some similar social construction), and therefore of our biological processes, which are ultimately physical processes.

This perspective allows for the 'mind' (and in the process 'life') to exist and have contents, but only as a process entirely derived from physics, even though parts of it are not tractably reducible to the processes from which they derive.
I like the responses addressing Aristotle's logic, so i'll provide a different perspective. 

In any rigorous logical system, generally you have symbols, and rules for manipulating these symbols. The clearest analogy would be if we compared sentences to positions on a chess board. There are some arrangements of pieces which just don't make sense, because they are invalid. This is because there are rules on how you move pieces. Different pieces behave differently, like logical symbols and English words do. So in this sense, a true statement would be a legal arrangement on the board that can be arrived at by legal movements. However, we should ask, how do our starting pieces and positions influence positions we can reach. For instance, pawns cannot move backwards, so White can never have a pawn on it's first rank, as White pawns start on the second rank. 

The initial positions of the pieces, and the type of pieces themselves, are analogous to our axioms. The axioms are initial statements believed/assumed to be true. In this sense, they are the first legal board positions, from which all others derrive. 

Now, one can easily imagine chess on a 10x10 board, a 4x4 board, a board that has 8 ranks but infinitely many collumns, toroidal boards, higher dimensional spaces, chess with different types of pieces, or other games like Shogi and Go which are quite different alltogether. 

We can imagine different logical systems, with different rules of inferences (https://math.stackexchange.com/questions/1201492/is-the-modus-ponens-is-an-axiom-in-formal-logic https://math.stackexchange.com/questions/1201492/is-the-modus-ponens-is-an-axiom-in-formal-logic) and different primitive symbols. 
Are these "true" or just as "valid" as the conventional, classical logic we adhere to? 
This is a tough question. Many of these logical systems share a "truth-like" property we care about, "consistency". I can entertain the notion "If I were a penguin, I wouldn't be able to fly". Despite the initial condition, me being a penguin, isn't exactly "true", this statement still "makes sense". 

Kurt Goedel, a logician and mathematician, composed his Incompleteness Theorems which address some fundemental properties of logical systems. I'd suggest reading up on those on other stackexchanges, but in short it states "any sufficiently complex logical system cannot be complete and consistent". Complete means that it can determine whether any statement (which plays by the rules and uses the right symbols) is true or false, and consistent just means "it makes sense" and nothing can have multiple truth values. 
If we apply this to our chess analogy, we can observe that there are many possible board games, so we may ask "why is ours special", and the answer is really "it isn't, but at least it works". Our logical systems might not be the only possible logical systems, but I believe it's pretty useful to have faith in our initial assumptions on how to make deductions. 

This question often comes up in a mathematical analog in the form "Are our numbers the right way to do numbers". At the moment, there's a few really useful foundations of modern mathematics. Some mathematicians prefer Shogi to Chess, and a few mathematicians argue whether a Bishop should be worth 3 points or 3.25, whether knights behave in the right way, etc, but mostly they care about the midgame and the endgame. 
It "just so happens" with numbers like "3" and properties like "addition" that they have strong correspondences with reality in being able to count things, add velocities, etc. So due to the way reality is, it gives a lot more legitimacy to our current number system, and I would argue logical system too. 

If maths and philosophy are the games we play, I think mostly mathematicians and philosophers care about how the game pans out, what strategies are useful, and less about specific rules. Though give this a read for a starter. https://plato.stanford.edu/entries/axiom-choice/ https://plato.stanford.edu/entries/axiom-choice/
Take inverse square laws. You can see them as mathematical or relying on logic, but they are geometrical and relational and just a part of the beingness of things. The ratios involved come from the conditions for there to be anything rather than nothing, we think. 

Vs infinity. A really useful mental tool, but which never exists in the world. Exactly defining pi, and being able to develop differentiation, rely on imagining series' to be infinite. They help us mentally transfer between contexts, linear & circular, discrete & continuous. 

So we have things that depend on counting, which depend on the properties of solid bodies at roughly our scale & in roughly our environment (not eg. quantum scale or the surface of a neutron star), which do absolutely exist, in specific instances, without us in the world. Then we have generalisations and abstractions of these, which don't; along with idealisations like infinity and perfectly round circles, which also never exist in the world.

Logic helps us organise our experiences, and it exists like a mental constellation around the star of our own concerns. But there are other stars, a whole universe of galaxies of other ways to think and organise experiences. 
Here's how I explain it when I teach logic.  

Formal or mathematical logic uses mathematics to represent "good reasoning."  These models are like maps:  they can be extremely useful for certain purposes; but every useful map introduces simplifications, distortions, and omissions.  A good map has simplifications, distortions, and omissions that make it more useful for its designated purpose.  But there's no such thing as a universal, literally all-purpose map.  

For example, sentence logic or propositional logic — the formal system you learn when you first study logic — assumes truth-functionality and bivalence.  Truth-functionality means that the truth value of a compound sentence (like "either p or q") depends only on the truth value of the component sentences ("p" and "q").  Bivalence means that every sentence is either true or false, and no sentence can be both.  There's no "partly true" or "I'm not sure."  Truth-functionality and bivalence are extremely useful for representing operators like "not," "and," and "or."  But they do weird things to "if-then," and are simply incompatible with "p because q," "I would prefer that p," or "all humans are mortal; and Hypatia is human; therefore Hypatia is mortal."  They also lead to the the "fact" that a contradiction implies anything; this isn't so much a "fact" as a distortion created by the simplifications involved in assuming truth-functionality and bivalence.  

Consider a subway map.  The distances between station markers on the map don't correspond to distances between stations.  When reading the map, we use conventions and prudence to avoid drawing incorrect inferences.  In the same way, when using sentence logic, we should use conventions and prudence to avoid overinterpreting explosion or the weirdness of the material condition.  

Other formal systems use different assumptions, in order to do a better job of representing some of the things that sentence logic can't really represent.  But they have simplifications, distortions, and omissions of their own.  For example, https://en.wikipedia.org/wiki/Paraconsistent_logic#Tradeoffs paraconsistent logic does weird things to "or".  This means that there is no one universal formal system.  A street map isn't a great way to represent the organization of a subway system; and neither is a good way to represent where different species of birds live in the city.  For three different tasks — navigating by bike, navigating by subway, and avian ecology — we need different maps.  

All of this is compatible with some kinds of realism about "good reasoning."  If formal systems of logic are like maps, then actual good reasoning is like the city represented in the maps.  The city is real, even if no one map can perfectly represent it in every aspect and we need to exercise "subjective" prudential judgment in order to correctly use any given map.  In an analogous sense, you might say that reasoning can really be good or bad, even if no one formal system can perfectly represent it in every aspect and we need to exercise non-formal judgment in order to avoid overinterpreting the quirks of any particular formal system.  
You must take into consideration that much of https://plato.stanford.edu/entries/quine/ Quine's "technical" and philosophical work on logic and set theory can be read as a comment and refinement of W&R's https://plato.stanford.edu/entries/principia-mathematica/ Principia.

For an overview, you can see at least the introductory chapters of https://books.google.it/books?id=S_ASNtsaynYC&printsec=frontcover Set Theory and its Logic, revised ed.1969.

See page 9:


  The schematic predicate letters 'F','G',... attach to variables to make dummy clauses 'Fxy', 'Gx', 'Gy', etc., as expository aids when we want to talk about the outward form of a complex sentence without specifying the component clauses. 
  
  So the letters 'F', 'G', ... are never to be thought of as variables, taking say attributes or classes as values. Abstract objects, these or others, may of course still belong to our universe of  discourse, over which our genuine variables 'x', 'y',... of quantification are allowed to range. But the letters 'F', 'G',... are withheld from quantifiers, indeed withheld from sentences altogether, and used only as dummies in depicting the forms of unspecified sentences. 


And see page 35:


  We might therefore do well hereafter to speak of the abstraction notation '{x: Fx}' not as virtual, for us, but as noncommittal: its use merely carries no general presumption of existence of the class (nor, if it exists, of its sethood). 
  
  Whether to say in particular that there is such a class as {x: Fx} 
  will depend on what sentence we interpret 'Fx' to represent and what axioms of existence we may eventually decide to adopt. 


Thus, about e.g. natural numbers, because in set theory every object is a set, and because we can prove, from the axioms of set theory, that there is a set N that behaves like the set of natural numbers, we can conclude that in the framework of set theory, numbers are "real".

See page 81:


  We have been admitting the numbers into the range of values of our variables of quantification, but we have not yet considered what sorts of things numbers are to be. We did see how to define 'N' given '0' and 'S'; the construing in turn of '0' and 'S' is what is needed, now, to fix the idea of number. 

Logic and mathematics are not independent of reality but have been abstracted from reality and are relying on tools of reality when being pursued and expressed in monologue, dialoge and general discourse.

Unfortunately this fact has not been recognized even by otherwise very able men. Einstein asks: "How is it possible that mathematics, which is a product of human thinking independent of all experience, fits reality in such an excellent way?" [A. Einstein: "Geometrie und Erfahrung", Festvortrag, Berlin (1921), reprinted in A. Einstein: "Mein Weltbild", C. Seelig (ed.), Ullstein, Frankfurt (1966) p. 119]

Without mental images from sensory impressions and experience thinking is impossible. Without reality (which includes the apparatus required for thinking as well as the objects of thinking – we never think of an abstractum "number 3" but always of three things or the written 3 or the spoken word or any materialization which could have supplied the abstraction) mathematics could not have evolved like a universe could not have evolved without energy and mass. Therefore real mathematics agrees with reality in the excellent way it does.

Einstein answers his question in a relativizing way: "In so far the theorems of mathematics concern reality they are not certain, and in so far as they are certain they do not concern reality." [loc cit]

He states a contraposition (R ==> ¬C) <==> (C ==> ¬R). Both statements are equivalent. Both statements are false. To contradict them a counterexample is sufficient. A theorem of mathematics is the law of commutation of addition of natural numbers a + b = b + a. It can be proven in every case in the reality of a wallet with two pockets.
Physical theories are mathematical models that approximate reality. If the models of any science were real and not representations, or even if they were accurate representations, that science as a whole would simply be finished with its job. The fields of the field equations aren't real and neither are the mathematical tensor spaces the operate on. Space is real, and they are abstract models of how it behaves.

So being used by physics does not make math real.  On the other hand, from an intuitionistic point of view (a la https://en.wikipedia.org/wiki/Stephen_Cole_Kleene Kleene), mathematical models are as real as your vision or your emotions are.

Mathematics and logic are patterns of communication to which humans naturally respond.  Euclidean space has the same level and kind of reality that fear does.  Is fear real?  Well, that depends...  But however you handle fear ontologically, math goes in the same bucket.  And intuitionism affords that bucket the status of reality.  It is real as a set of patterns, and those patterns can be empirically validated or falsified by exposing them to humans.

Four-dimensional space has the same level of reality as other patterns of reaction to other stimuli, say, the fear of spiders.  If you are one of the people who share and understand it, it produces a given feeling.  In the arachnophobe's case, anxiety of a common form, in the mathematician's, a sense of settledness that is common to most mathematical propositions.
The https://en.wikipedia.org/wiki/Many-worlds_interpretation many-worlds interpretation (MWI) solves the asymmetry in your example.


  Many-worlds implies that all possible alternate histories and futures are real, each representing an actual "world" (or "universe").


Symmetry is established since "A observes B" has a symmetric partner world where "B observes A"

I admit I favor the "collapse of the wave function" interpretation, but please feel free to pick your own favorite interpretation https://en.wikipedia.org/wiki/Interpretations_of_quantum_mechanics here.



Back to the question in the title "Why is there an asymmetry in QM?", I'd like to add the following: 

Observing a quantum system (made up of a superposition of eigenstates), collapses the wave function to a single eigenstate of your observation operator and by that destroys superposition. More important: the observer A gathers information about the system B*. The (asymmetric) information flow from B to A drives change in the world...

*: This can be shown, because the information can be erased (by so called quantum erasers) while traveling to the observer A. By that, the superposition state is not affected.
In https://en.wikipedia.org/wiki/First-order_logic FOL (i.e. predicate logic) no.

We have variables, constants, function and predicate symbols.

With the first three we build terms.

With terms and predicate we build atoms (or atomic formulas).

Example: x=0 and E(x,y) are atoms.

With connectives and quantifiers we build "complex" formulas from atomic ones, like your example: ∃x(Cx ∧ Mx).

Some versions allow also the use of 0-ary predicate symbols Pi, that are simply the propositional letters of propositional calculus.

See Dirk van Dalen, https://books.google.it/books?id=u0wlXPHATDcC&pg=PA56 Logic and Structure, page 56-on.



Your approach is more akin to "formalizing" the meta-language.

We may define (in the meta-tehory) the class Sent of sentences and then we can write e.g.


  "∃x(Cx ∧ Mx)" ∈ Sent.




A more "subtle" approach is via https://plato.stanford.edu/entries/goedel-incompleteness/#AriForLan arithmetization.

For a gentle approach to coding, see Ch.4: M.Fitting, Russell’s Paradox, Gödel’s Theorem, of Melvin Fitting & Brian Rayman (editors), https://books.google.it/books?id=cbRGDwAAQBAJ&pg=PR47 Raymond Smullyan on Self Reference, Springer (2017).
The answer really lies in the simplicity of the division between "logical" and "not logical."  Its too simplified to capture the full nuances of our imagination.


  "If something can be imagined, even though it may be physically impossible, it is logically possible"


If I may borrow from a non-Western culture, consider the concept of the https://en.wikipedia.org/wiki/Tao Dao(道).  It is a fascinating concept which is almost certainly outside of the realm of logical possibility because, as https://en.wikiquote.org/wiki/Laozi Laozi put it, "The Tao that can be told is not the eternal Tao".  Surely one would need to be able to enumerate it (via speech or writing) in order to determine if it is logical or not.  And yet, it is.  It is the is-ing, if I may try to turn "is" into a present transitive verb.  It is perfectly physically possible, for it is.  So it would be an example from another culture of an idea which can be imagined, is physically possible, but I would argue is logically impossible in the sense that it defies logic.
The rules of logic lead to many counterintuitive results, and this is one of the most fundamental such results: VALID expresses a structural condition, such that it can never happen that all the premises are true and the conclusion is false.

If the premises cannot all be true at at the same time, then the argument is trivially VALID because it can never happen that all the premises are true... (regardless of the truth value of the conclusion).  This holds only when the premises are logically contradictory, however, and not in the case where they are incidentally contradictory.

The usefulness of VALID is that it is what is called "truth preserving."  If all your arguments are valid, the truth of your conclusions can never be less secure than that of your premises, considered collectively.
The bi-conditional is boring because you have to split into two parts:


  1) ¬[(A→B) ∧ (B→A)] --- premise


2) A --- assumed [a]

3) B→A --- from 2)

4) B --- assumed [b]

5) A→B --- from 4)

6) (A→B) ∧ (B→A) --- from 3) and 5)

7) contradiction ! with 1)

8) ¬A --- from 2) and 7), discharging [a]


  9) B → ¬A --- from 4) and 8), discharging [b].


In the same way, we have to derive: ¬A  → B.

10) ¬A --- assumed [c]

11) ¬B --- assumed [d]

12) A --- assumed [e]

13) contradiction! with 10)

14) B --- from 13)

15) A→B --- from 12) and 14), discharging [e]

16) B --- assumed [f]

17) contradiction! with 11)

18) A --- from 17)

19) B→A --- from 16) and 18) discharging [f] 

20) (A→B) ∧ (B→A) --- from 15) and 19)

21) contradiction! with 1)

22) B --- from 11) by Double Negation, discharging [d]


  23) ¬A→B --- from 10) and 22), discharging [c].


Now we conclude from 9) and 23) with:


  
    24) ¬A ≡ B.
  

this may help u to understand the concept 


p => q            Premise
~(~p | q)       Assumption
~p            Assumption
~p | q        Or Introduction: 3
~p => ~p | q    Implication Introduction: 3, 4
~p             Assumption
~(~p | q)      Reiteration: 2
~p => ~(~p | q) Implication Introduction: 6, 7
~~p             Negation Introduction: 5, 8
p               Negation Elimination: 9
q               Implication Elimination: 1, 10
~p | q          Or Introduction: 11
~(~p | q) => ~p | q        Implication Introduction: 2, 12
~(~p | q)                 Assumption
~(~p | q) => ~(~p | q)     Implication Introduction: 14, 14
~~(~p | q)                 Negation Introduction: 13, 15
~p | q                     Negation Elimination: 16


Answer by https://www.studytoday.net/ Study Today Team
You can see Frege's https://www.jstor.org/stable/2251513?seq=1#page_scan_tab_contents The Thought: A Logical Inquiry (1918-19):


  Without wishing to give a definition, I call a thought something for which the question of truth arises. [...] Two things must be distinguished in an indicative sentence: the content, which it has in common with the corresponding sentence-question, and the assertion. The former is the thought, or at least contains the thought.


You can see also: 


G.Currie, https://academic.oup.com/mind/article-abstract/LXXXIX/354/234/954455?redirectedFrom=PDF Frege on Thoughts (1980),


as well as:


Tyler Burge, https://books.google.it/books?id=IQDR32Z72dEC&printsec=frontcover Truth, Thought, Reason: Essays on Frege (2005), Part II: Sense and Cognitive Value.

Whether there is a distinction, and what the distinction consists in, is a hotly debated topic. Here are a few things that are typically claimed to be essential to logic:


Universal applicability: the laws of logic apply to every subject matter. This would mean that, e.g., different theories of arithmetic have the same underlying logic (usually something like classical first-order logic).
Ontological neutrality: the thought here is that there are no distinctively logical objects, and nothing need exist for logic to be true. Arithmetic assumes the existence of the natural numbers. Set theory assumes the existence of sets. Logic is supposed to be free of any similar existential assumptions.
Epistemic priority: the fundamental truths of logic are in some sense more immune to doubt and more a priori certain than any other subject matter. There are two thoughts here. The first is that knowledge of everything else, including math, requires knowledge of logic. The second is that there is less or no room for doubt when it comes to logic.


Now, each of those claims are debatable. “Universal applicability” seems hard to come by unless you assume a very weak logic, weaker than people typically assume. Classical logic won’t work for intuitionists, and intuitionistic logic won’t capture distinctions central to paraconsistent logics.

Ontological neutrality is similarly debatable. First-order logic is plausibly neutral, but it is relatively weak expressively. For example, it cannot capture the distinction between “finite” and “infinite” and so will be unable to characterize a lot of theories definitely (i.e., you’ll have non-standard models).

Finally, epistemic priority is also up for debate. Depending on what gets included in “logic”, it’s plausible that simple arithmetic and geometric truths are on surer footing or at least more “obvious” than much of logic.

Now, for the early logicists, logic was pretty strong. Russell and Whitehead’s type theory was strong enough to interpret arithmetic — Gödel’s proof of incompleteness is couched in that system — and at least a weak set theory. The problem for them was that Gödel’s results seemed to show that logic couldn’t be the firm foundation they had hoped for, at least not for any mathematics of interest.

The “neo-logicists” make a more modest claim that logic (a weak second-order logic), combined with some conceptual truths about mathematics (like “Hume’s Principle” for the natural numbers), provides a foundation for mathematics. Here the objections will typically be to either the comprehensiveness of the program — it can’t capture all mathematics — or to the logical status of their “logic” (that it’s just “set theory in sheep’s clothing”).
Moral realists believe (very roughly) that there is an independently existing moral reality; that some moral judgements are true by virtue of that reality; and that some moral judgements can be known to be true. (I use the phrase 'by virtue of' in order not to entangle myself in theories of truth; while a moral realist needs a theory of truth, s/he is not committed as a moral realist to a particular theory, e.g. the correspondence theory.)

Only such moral judgements are true for a moral realist; subjective judgements, e.g. judgements based purely on emotion or on some intellectual error are either no capable of truth (as perhaps in the case of emotive judgements) or are false (as perhaps in the case of judgements based on intellectual error. 

I am talking very roughly but this is the essential picture. But now, while subjective judgements are not and cannot be true, they can still be binding. A moral realist can find a role for conscience. If I sincerely believe that X, even if X is not a moral truth, a moral realist can still accept it as a moral truth that I ought to (try to) do X since it is what I sincerely believe I ought to do. 

There is or need be no inconsistency in this on the part of moral realism. Given the fallibility of human beings, it is reasonable that even error has its rights; and this can itself be a moral fact, true by virtue of an independently existing moral reality. 

So, I don't think a moral realist can accept that subjective judgements have moral truth exactly, but s/he can recognise as a moral truth that even an erroneous conscience is binding : an agent who is sincere in his or her moral judgement, even if that judgement is subjective, is morally bound or obliged to (try to) act on it. 
Defining “miss” as “non-hit”, the premises say:


Only if M non-H, then P H.
Unless if M non-H, then P non-H.


The combined premises read like this: if Mia misses, then Pia both hits and does not hit. As written, these statements look mutually contradictory.
