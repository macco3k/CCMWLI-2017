Some ideas for the project:

* As training data, we can probably look here: https://archive.org/details/stackexchange
* As for the topic, perhaps philosophy? This way we can probably get away with minor nonsense, as we are discussing metaphysics anyway...
* Markov n-grams look nice for generating text. Not really clear how to use them to actually answer questions though. We can 'train' one per tag, so that different networks can generate different texts based on the topic chosen by the user.
* An implementation of seq2seq for python can be found here: https://github.com/farizrahman4u/seq2seq
* We can also consider more pattern-based approaches, e.g. by using AIML: https://en.wikipedia.org/wiki/AIML

In general, I think it would be useful to have some predetermined, hand-crafted responses to account for standard questions/replies such as greetings or to steer the conversation, etc.:

`"Hi, My name is <botname>. I can do this and this and also that. To see a list of commands type /help."`

`"Hello, I'm Daniele. Let's talk about existentialism"`
  
_...the bot look up existensialism in a db of tags..._

`"That seems to be a pretty big topic. Can you be more specific?"`

`"Something about Kierkegaard's vision of life?"`

Of course, we'd need to find a way for the bot to understand the question:
* First, we can probably just stem everything after having removed stopwords, so that we are left with the most important words (in this case _something_, _about_, _Kierkegaard_, _vision_, _life_). 
* From here, we could try to look for names, perhaps to further narrow down the topic (in this case we are really looking for an answer which relates to Kierkegaard).
* The answer could then be generated by either picking a network or training one on-the-fly, depending on the amount of space it takes.


# Details
More details about various aspects to consider.

## Defining topics
In order to make the chatbot behave more naturally, we can train different models on different corpus. Tags are a natural choice for this. However, it is not so easy to exactly define a hierarchy of tags. Even less to define a sensible similarity metric between them. To partly overcome this problem, we can use LDA to 'discover' topics in an unsupervised manner. Unfortunately, we still don't know how to map the recovered topics with the tags we have available. In order to do that, we can take the following approach:

* train an LDA model on the documents (e.g. questions) with a fixed number of topics (say, 10)
* for each topic, pick the documents with the highest likelihood for such topic, so that all documents in the group belongs to the same topic
* take a majority vote to decide which tag to associate with the topic. That is, count how many times each tag appears in the group, and take the one with the highest count as the label for the topic.
* continue in the same fashion with the second, third, etc. topic, so that we end up with labels for every topic.

This procedure can be applied to both questions and answers. This way, we build a separate topic representation for questions (against which we can compare the user's) and answers (perhaps the way people ask questions about a topic is different from the way the answer about the same topic). Of course, answers are assumed to all have the same tags as the question they belong to.

Note the topics don't really need labels attached to them, but we need to have those to retrieve the correct corpus at runtime. In addition, we could also consider adding to the training corpus those documents tagged with the same label as the topic, regardless of whether they belong to the topic according to the LDA model.

## How to choose the model to use
In order to use the correct model for the question, we can infer the topic of the question (maybe with multiple passes in order to narrow it down), and then simply select the model (or an interpolation of models, according to the likelihood associated with each topic) which will generate the answer.
