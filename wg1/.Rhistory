H_o=apply(prob_M,2,ent)
H_w=apply(prob_M,1,ent)
dim(H_0)
dim(H_o)
size(H_o
)
H_o
alpha*M
alpha
parms$alpha
parms$alpha*M
exp(lambda*(H_w+H_o))
exp(parmslambda*(H_w+H_o))
exp(parms$lambda*(H_w+H_o))
chi*exp(parms$lambda*(H_w+H_o))
parms$chi*exp(parms$lambda*(H_w+H_o))
parms$chi*exp(parms$lambda*(H_w+H_o)*M)
sum(M)
parms$chi*exp(parms$lambda*(H_w+H_o)*M)/sum(exp(parms$lambda*(H_w+H_o)*M)
)
parms$alpha*M + (parms$chi*exp(parms$lambda*(H_w+H_o)*M))/sum(exp(parms$lambda*(H_w+H_o)*M))
model <- function(ord, parms) {
# nw = number of words
# no = number of objects
nw = length(ord[0]) # the number of pairs
M = matrix(0, nrow=nw, ncol=nw)
# learning: i.e., not just co-occurrence counting,
# but a process that corresponds to what you think
# people might be doing as they go through the trials
# (guess-and-test hypothesis generation? biased association?)
alpha = parms$alpha
chi = parms$chi
lambda = parms$lambda
# get probabilities via softmax
prob_M = softmax(M, parms$temp)
# compute entropies over words(rows) and objects(cols)
H_w = apply(prob_M,1,ent)
H_o = apply(prob_M,2,ent)
exp_M = exp(lambda*(H_w+H_o)) * M
M = alpha*M + (chi * exp_M)/sum(exp_M)
return(M) # this matrix will then be passed through softmax to extract pr(correct)
}
length(ord)
length(ord[0])
ord[0]
ord[1]
model <- function(ord, parms) {
# nw = number of words
# no = number of objects
nw = length(ord[1]) # the number of pairs
M = matrix(0, nrow=nw, ncol=nw)
# learning: i.e., not just co-occurrence counting,
# but a process that corresponds to what you think
# people might be doing as they go through the trials
# (guess-and-test hypothesis generation? biased association?)
alpha = parms$alpha
chi = parms$chi
lambda = parms$lambda
# get probabilities via softmax
prob_M = softmax(M, parms$temp)
# compute entropies over words(rows) and objects(cols)
H_w = apply(prob_M,1,ent)
H_o = apply(prob_M,2,ent)
exp_M = exp(lambda*(H_w+H_o)) * M
M = alpha*M + (chi * exp_M)/sum(exp_M)
return(M) # this matrix will then be passed through softmax to extract pr(correct)
}
model(ord,parms)
parms$alpha*M + (parms$chi*exp(parms$lambda*(H_w+H_o)*M))/sum(exp(parms$lambda*(H_w+H_o)*M))
exp(1)*M
model <- function(ord, parms) {
# nw = number of words
# no = number of objects
nw = length(ord[1]) # the number of pairs
M = matrix(0, nrow=nw, ncol=nw)
# learning: i.e., not just co-occurrence counting,
# but a process that corresponds to what you think
# people might be doing as they go through the trials
# (guess-and-test hypothesis generation? biased association?)
alpha = parms$alpha
chi = parms$chi
lambda = parms$lambda
# get probabilities via softmax
prob_M = softmax(M, parms$temp)
# compute entropies over words(rows) and objects(cols)
H_w = apply(prob_M,1,ent)
H_o = apply(prob_M,2,ent)
exp_M = exp(lambda*(H_w+H_o)) * M
print(exp_M)
M = alpha*M + (chi * exp_M)/sum(exp_M)
return(M) # this matrix will then be passed through softmax to extract pr(correct)
}
model(ord,parms)
ord
length(ord[1])
length(ord[[1]])
coocMatrix()
coocMatrix(loCDord)
parms$nw=4
model(ord,parms)
model <- function(ord, parms) {
# nw = number of words
# no = number of objects
M = matrix(0, nrow=parms$nw, ncol=parms$nw)
# learning: i.e., not just co-occurrence counting,
# but a process that corresponds to what you think
# people might be doing as they go through the trials
# (guess-and-test hypothesis generation? biased association?)
alpha = parms$alpha
chi = parms$chi
lambda = parms$lambda
# get probabilities via softmax
prob_M = softmax(M, parms$temp)
# compute entropies over words(rows) and objects(cols)
H_w = apply(prob_M,1,ent)
H_o = apply(prob_M,2,ent)
exp_M = exp(lambda*(H_w+H_o)) * M
print(exp_M)
M = alpha*M + (chi * exp_M)/sum(exp_M)
return(M) # this matrix will then be passed through softmax to extract pr(correct)
}
model(ord,parms)
parms
H_o
H_w
exp(lambda*(H_w+H_o))
exp(parmslambda*(H_w+H_o))
exp(parms$lambda*(H_w+H_o))
M*exp(parms$lambda*(H_w+H_o))
M
M = softmax(M, 0.01)
M
M*exp(parms$lambda*(H_w+H_o))
M*exp(parms$lambda*(H_w+H_o))*chi
M*exp(parms$lambda*(H_w+H_o))*parmschi
M*exp(parms$lambda*(H_w+H_o))*parms$chi
ord
ord = genTrialsOrder(1:18, one(18)/18, 36, 3)
ord = genTrialsOrder(1:18, ones(18)/18, 36, 3)
c(1)
matrix(1,nrow=18)
ord = genTrialsOrder(1:18, matrix(1,ncol=18)/18, 36, 3)
ord
coocMatrix(ord)
loCDord
table
?table
table()
table(ord)
ord
as.table(ord)
as.matrix(ord)
as.matrix(ord, ncol=3)
ord
genTrialsOrder <- function(vocab, prob, ntrials, size) {
trials = matrix(0, nrow=ntrials, ncol=size)
for (t in 1:ntrials) {
trials[[t]] = sample(vocab, size, replace=FALSE, prob=prob)
}
return (trials)
}
ord = genTrialsOrder(1:18, one(18)/18, 36, 3)
ord = genTrialsOrder(1:18, matrix(1,ncol=18)/18, 36, 3)
genTrialsOrder <- function(vocab, prob, ntrials, size) {
trials = matrix(0, nrow=ntrials, ncol=size)
for (t in 1:ntrials) {
trials[t] = sample(vocab, size, replace=FALSE, prob=prob)
}
return (trials)
}
ord = genTrialsOrder(1:18, matrix(1,ncol=18)/18, 36, 3)
warnings()
ord
sample(c(1,2))
sample(c(1,2),ntrials=3)
sample(c(1,2))
genTrialsOrder <- function(vocab, prob, ntrials, size) {
trials = matrix(0, nrow=ntrials, ncol=size)
for (t in 1:ntrials) {
trials[t,] = sample(vocab, size, replace=FALSE, prob=prob)
}
return (trials)
}
ord = genTrialsOrder(1:18, matrix(1,ncol=18)/18, 36, 3)
ord
coocMatrix(ord)
model(ord,parms)
ord
parms
parms$nw=18
m = coocMatrix(ord)
m
model(ord,parms)
ord
ord[1,]
ord[-1,]
ord[1:,]
length(ord)
dim(ord)
dim(ord,1)
dim(ord)[[1]]
ord[1:10,]
model <- function(ord, parms) {
# nw = number of words
# no = number of objects
M = matrix(0, nrow=parms$nw, ncol=parms$nw)
# learning: i.e., not just co-occurrence counting,
# but a process that corresponds to what you think
# people might be doing as they go through the trials
# (guess-and-test hypothesis generation? biased association?)
alpha = parms$alpha
chi = parms$chi
lambda = parms$lambda
for(t in 1:dim(ord)[[1]]) {
M = coocMatrix(ord[1:t,])
# get probabilities via softmax
prob_M = softmax(M, parms$temp)
# compute entropies over words(rows) and objects(cols)
H_w = apply(prob_M,1,ent)
H_o = apply(prob_M,2,ent)
exp_M = exp(lambda*(H_w+H_o)) * M
M = alpha*M + (chi * exp_M)/sum(exp_M)
}
return(M) # this matrix will then be passed through softmax to extract pr(correct)
}
model(ord,parms)
M = matrix(0, nrow=parms$nw, ncol=parms$n)
model <- function(ord, parms) {
# nw = number of words
# no = number of objects
# M = matrix(0, nrow=parms$nw, ncol=parms$n)
# learning: i.e., not just co-occurrence counting,
# but a process that corresponds to what you think
# people might be doing as they go through the trials
# (guess-and-test hypothesis generation? biased association?)
alpha = parms$alpha
chi = parms$chi
lambda = parms$lambda
for(t in 1:dim(ord)[[1]]) {
M = coocMatrix(ord[1:t,])
# get probabilities via softmax
prob_M = softmax(M, parms$temp)
# compute entropies over words(rows) and objects(cols)
H_w = apply(prob_M,1,ent)
H_o = apply(prob_M,2,ent)
exp_M = exp(lambda*(H_w+H_o)) * M
M = alpha*M + (chi * exp_M)/sum(exp_M)
}
return(M) # this matrix will then be passed through softmax to extract pr(correct)
}
model(ord,parms)
ord[1,]
ord[1:1,]
coocMatrix(ord[1:1,])
coocMatrix(ord[1:2,])
nrow(ord[1,])
nrow(ord[1:2,])
hiCDord = read.table('freq369-3x3hiCD.txt', sep='\t', header=F)
loCDord = read.table('freq369-3x3loCD.txt', sep='\t', header=F)
coocMatrix <- function(ord) {
N = max(unlist(ord)) # the max pairs we see is the # of pairs
M = matrix(0, nrow=N, ncol=N)
for(t in 1:dim(ord)[[1]]) { # for each trial
tr = unlist(ord[t,])  # count which word-object pair occurred in the trial. Pair each word with each object, as we don't know which word relates to which object
M[tr,tr] = M[tr,tr] + 1
}
return(M)
}
softmax <- function(M, temp) {
expM = exp(M/temp)
return( expM / rowSums(expM) )
}
ent <- function(p) {
H = -sum(p %*% log(p))
}
model <- function(ord, parms) {
# nw = number of words
# no = number of objects
# M = matrix(0, nrow=parms$nw, ncol=parms$n)
# learning: i.e., not just co-occurrence counting,
# but a process that corresponds to what you think
# people might be doing as they go through the trials
# (guess-and-test hypothesis generation? biased association?)
alpha = parms$alpha
chi = parms$chi
lambda = parms$lambda
for(t in 1:dim(ord)[[1]]) {
M = coocMatrix(ord[1:t,])
# get probabilities via softmax
prob_M = softmax(M, parms$temp)
# compute entropies over words(rows) and objects(cols)
H_w = apply(prob_M,1,ent)
H_o = apply(prob_M,2,ent)
exp_M = exp(lambda*(H_w+H_o)) * M
M = alpha*M + (chi * exp_M)/sum(exp_M)
}
return(M) # this matrix will then be passed through softmax to extract pr(correct)
}
genTrialsOrder <- function(vocab, prob, ntrials, size) {
trials = matrix(0, nrow=ntrials, ncol=size)
for (t in 1:ntrials) {
trials[t,] = sample(vocab, size, replace=FALSE, prob=prob)
}
return (trials)
}
require(ggplot2)
plot_performance_by_temperature <- function(ord) {
temp = seq(.1,10,.5)
item_perf = matrix(0, nrow=length(temp), ncol=18)
for(i in 1:length(temp)) {
item_perf[i,] = softmax(coocMatrix(ord), temp[i])
}
dat = data.frame(cbind(temp, perf = rowMeans(item_perf)))
ggplot(dat, aes(temp, perf)) + geom_point(color="red", alpha=.5) + geom_line(alpha=.5)
}
load("human_accuracy_variedCD.RData") # hiCDacc and loCDacc
SSE <- function(p,q) {
sse = (p-q) %*% (p-q)
return(as.numeric(sse))
}
crossEntropy <- function(observed_probs,model_probs) {
xent = -((observed_probs) %*% log(model_probs))
return(as.numeric(xent))
}
negloglik <- function(obs, mod) {
pointwise_ll = obs*log(mod) + (1-obs)*log(1-mod)
nll = -sum(pointwise_ll)
return(as.numeric(nll))
}
bic <- function(ll, n, k) {
bic = log(n)*k - 2*ll
}
ord
parms
model(ord,parms)
dim(ord[1,])
ord[1,]
dim(ord)
dim(ord[1:2,])
dim(ord[1:1,])
as.matrix(ord[1,])
as.matrix(ord[1,],byrow=TRUE)
as.matrix(ord[1,],bycol=TRUE)
?as.matrix
as.matrix(ord[1,],byrow=TRUE)
as.matrix(ord[1,],byrow=TRUE,ncol=3)
as.matrix(ord[1,],byrow=TRUE,ncol=3,nrow=1)
ord[1,]
data.matrix(ord[1,])
as.matrix(ord[1,],byrow=TRUE)
as.matrix(t(ord[1,]),byrow=TRUE)
as.matrix(t(ord[1,]))
model <- function(ord, parms) {
# nw = number of words
# no = number of objects
# M = matrix(0, nrow=parms$nw, ncol=parms$n)
# learning: i.e., not just co-occurrence counting,
# but a process that corresponds to what you think
# people might be doing as they go through the trials
# (guess-and-test hypothesis generation? biased association?)
alpha = parms$alpha
chi = parms$chi
lambda = parms$lambda
for(t in 1:nrow(ord)) {
M = coocMatrix(as.matrix(t(ord[1:t,])))
# get probabilities via softmax
prob_M = softmax(M, parms$temp)
# compute entropies over words(rows) and objects(cols)
H_w = apply(prob_M,1,ent)
H_o = apply(prob_M,2,ent)
exp_M = exp(lambda*(H_w+H_o)) * M
M = alpha*M + (chi * exp_M)/sum(exp_M)
}
return(M) # this matrix will then be passed through softmax to extract pr(correct)
}
model(ord,parms)
model <- function(ord, parms) {
# nw = number of words
# no = number of objects
# M = matrix(0, nrow=parms$nw, ncol=parms$n)
# learning: i.e., not just co-occurrence counting,
# but a process that corresponds to what you think
# people might be doing as they go through the trials
# (guess-and-test hypothesis generation? biased association?)
alpha = parms$alpha
chi = parms$chi
lambda = parms$lambda
for(t in 1:nrow(ord)) {
M = coocMatrix(as.matrix(t(ord[1:t,])))
# get probabilities via softmax
prob_M = softmax(M, parms$temp)
# compute entropies over words(rows) and objects(cols)
H_w = apply(prob_M,1,ent)
H_o = apply(prob_M,2,ent)
exp_M = exp(lambda*(H_w+H_o)) * M
M = alpha*M + (chi * exp_M)/sum(exp_M)
print(M)
}
return(M) # this matrix will then be passed through softmax to extract pr(correct)
}
model(ord,parms)
model <- function(ord, parms) {
# nw = number of words
# no = number of objects
# M = matrix(0, nrow=parms$nw, ncol=parms$n)
# learning: i.e., not just co-occurrence counting,
# but a process that corresponds to what you think
# people might be doing as they go through the trials
# (guess-and-test hypothesis generation? biased association?)
alpha = parms$alpha
chi = parms$chi
lambda = parms$lambda
for(t in 1:nrow(ord)) {
M = coocMatrix(as.matrix(t(ord[1:t,])))
print(M)
# get probabilities via softmax
prob_M = softmax(M, parms$temp)
# compute entropies over words(rows) and objects(cols)
H_w = apply(prob_M,1,ent)
H_o = apply(prob_M,2,ent)
exp_M = exp(lambda*(H_w+H_o)) * M
M = alpha*M + (chi * exp_M)/sum(exp_M)
# print(M)
}
return(M) # this matrix will then be passed through softmax to extract pr(correct)
}
model(ord,parms)
t(ord[1:2,])
model <- function(ord, parms) {
# nw = number of words
# no = number of objects
# M = matrix(0, nrow=parms$nw, ncol=parms$n)
# learning: i.e., not just co-occurrence counting,
# but a process that corresponds to what you think
# people might be doing as they go through the trials
# (guess-and-test hypothesis generation? biased association?)
alpha = parms$alpha
chi = parms$chi
lambda = parms$lambda
for(t in 2:nrow(ord)) {
M = coocMatrix(as.matrix(ord[1:t,]))
print(M)
# get probabilities via softmax
prob_M = softmax(M, parms$temp)
# compute entropies over words(rows) and objects(cols)
H_w = apply(prob_M,1,ent)
H_o = apply(prob_M,2,ent)
exp_M = exp(lambda*(H_w+H_o)) * M
M = alpha*M + (chi * exp_M)/sum(exp_M)
# print(M)
}
return(M) # this matrix will then be passed through softmax to extract pr(correct)
}
model(ord,parms)
model(loCDord,parms)
model <- function(ord, parms) {
# nw = number of words
# no = number of objects
# M = matrix(0, nrow=parms$nw, ncol=parms$n)
# learning: i.e., not just co-occurrence counting,
# but a process that corresponds to what you think
# people might be doing as they go through the trials
# (guess-and-test hypothesis generation? biased association?)
alpha = parms$alpha
chi = parms$chi
lambda = parms$lambda
for(t in 2:nrow(ord)) {
M = coocMatrix(as.matrix(ord[1:t,]))
print(M)
# get probabilities via softmax
prob_M = softmax(M, parms$temp)
print(prob_M)
# compute entropies over words(rows) and objects(cols)
H_w = apply(prob_M,1,ent)
H_o = apply(prob_M,2,ent)
exp_M = exp(lambda*(H_w+H_o)) * M
M = alpha*M + (chi * exp_M)/sum(exp_M)
# print(M)
}
return(M) # this matrix will then be passed through softmax to extract pr(correct)
}
model(loCDord,parms)
ent <- function(p) {
H = -(p %*% log(p))
}
model <- function(ord, parms) {
# nw = number of words
# no = number of objects
# M = matrix(0, nrow=parms$nw, ncol=parms$n)
# learning: i.e., not just co-occurrence counting,
# but a process that corresponds to what you think
# people might be doing as they go through the trials
# (guess-and-test hypothesis generation? biased association?)
alpha = parms$alpha
chi = parms$chi
lambda = parms$lambda
for(t in 2:nrow(ord)) {
M = coocMatrix(as.matrix(ord[1:t,]))
print(M)
# get probabilities via softmax
prob_M = softmax(M, parms$temp)
print(prob_M)
# compute entropies over words(rows) and objects(cols)
H_w = apply(prob_M,1,ent)
H_o = apply(prob_M,2,ent)
exp_M = exp(lambda*(H_w+H_o)) * M
M = alpha*M + (chi * exp_M)/sum(exp_M)
# print(M)
}
return(M) # this matrix will then be passed through softmax to extract pr(correct)
}
model(loCDord,parms)
